{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"InputTestSet-Reviews48_Ann.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Sent</th>\n",
       "      <th>MComp</th>\n",
       "      <th>Cat</th>\n",
       "      <th>SubCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The authors propose to use k-DPP to select a s...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>This paper covers the related work nicely, wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The rest of the paper are also clearly written.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>However, I have some concerns about the propos...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>- It is not clear how to define the kernel, th...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID              PID     Dec  \\\n",
       "0    0  2019_SJf_XhCqKm  Reject   \n",
       "1    1  2019_SJf_XhCqKm  Reject   \n",
       "2    2  2019_SJf_XhCqKm  Reject   \n",
       "3    3  2019_SJf_XhCqKm  Reject   \n",
       "4    4  2019_SJf_XhCqKm  Reject   \n",
       "\n",
       "                                                Sent  MComp  Cat SubCat  \n",
       "0  The authors propose to use k-DPP to select a s...      0  NaN    NaN  \n",
       "1  This paper covers the related work nicely, wit...      0  NaN    NaN  \n",
       "2    The rest of the paper are also clearly written.      0  NaN    NaN  \n",
       "3  However, I have some concerns about the propos...      0  NaN    NaN  \n",
       "4  - It is not clear how to define the kernel, th...      0  NaN    NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dict = {}\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    if not pid in gt_dict:\n",
    "        gt_dict[pid] = {\"dec\": df.loc[i][\"Dec\"], \"mcomp\": set(), \"not_mcomp\": set()}\n",
    "    if df.loc[i][\"MComp\"] == 1:\n",
    "        gt_dict[pid][\"mcomp\"].add(df.loc[i][\"UID\"])\n",
    "    else:\n",
    "        gt_dict[pid][\"not_mcomp\"].add(df.loc[i][\"UID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accept': [48, 644], 'Reject': [69, 744]}\n"
     ]
    }
   ],
   "source": [
    "stats_dict = {\"Accept\": [0, 0], \"Reject\": [0, 0]}\n",
    "\n",
    "for k, v in gt_dict.items():\n",
    "    #print(len(v[\"mcomp\"]), len(v[\"not_mcomp\"]), v[\"dec\"])\n",
    "    stats_dict[v[\"dec\"]][0] += len(v[\"mcomp\"])\n",
    "    stats_dict[v[\"dec\"]][1] += len(v[\"not_mcomp\"])\n",
    "    \n",
    "print(stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestSet length: 32\n",
      " ['2019_SJf_XhCqKm', '2017_Bk0MRI5lg', '2020_SyevYxHtDB', '2018_rJBiunlAW', '2020_rkltE0VKwH', '2018_Hki-ZlbA-', '2019_BJx0sjC5FX', '2020_r1e_FpNFDr', '2020_B1lsXREYvr', '2018_SkZxCk-0Z', '2019_rJzoujRct7', '2018_HkfXMz-Ab', '2017_BJ9fZNqle', '2019_SyxZJn05YX', '2017_B1ckMDqlg', '2017_HJ0NvFzxl', '2017_S1_pAu9xl', '2018_SyYYPdg0-', '2017_BJAA4wKxg', '2019_HyVxPsC9tm', '2019_HylTBhA5tQ', '2019_B1l08oAct7', '2018_H135uzZ0-', '2017_H1oyRlYgg', '2017_r1y1aawlg', '2020_r1eX1yrKwB', '2020_Byg79h4tvB', '2019_H1lFZnR5YX', '2020_BkeWw6VFwr', '2018_HyHmGyZCZ', '2018_HyUNwulC-', '2020_HkgsPhNYPS']\n"
     ]
    }
   ],
   "source": [
    "test_set = list(gt_dict.keys())\n",
    "print(\"TestSet length: %d\\n\"%len(test_set), test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_SJf_XhCqKm     {39, 17, 20, 27, 28, 30}\n",
      "2017_Bk0MRI5lg      {48, 57}\n",
      "2020_SyevYxHtDB     {76, 87}\n",
      "2018_rJBiunlAW      {108, 110, 112, 113, 124, 126}\n",
      "2020_rkltE0VKwH     {160, 155, 184, 159}\n",
      "2018_Hki-ZlbA-      {267, 235, 236, 271}\n",
      "2019_BJx0sjC5FX     {292, 287}\n",
      "2020_r1e_FpNFDr     {312, 322, 315, 308}\n",
      "2020_B1lsXREYvr     {376, 401}\n",
      "2018_SkZxCk-0Z      {449, 443, 445, 486}\n",
      "2019_rJzoujRct7     {518, 519}\n",
      "2018_HkfXMz-Ab      {573, 566}\n",
      "2017_BJ9fZNqle      {627, 623, 615}\n",
      "2019_SyxZJn05YX     {672, 673, 657, 669, 671}\n",
      "2017_B1ckMDqlg      {714, 707}\n",
      "2017_HJ0NvFzxl      {739}\n",
      "2017_S1_pAu9xl      {792, 809, 810, 806}\n",
      "2018_SyYYPdg0-      {834, 867, 868, 869, 870, 872, 873, 844, 830}\n",
      "2017_BJAA4wKxg      {884}\n",
      "2019_HyVxPsC9tm     {931, 933, 905, 909, 912, 913, 919, 926}\n",
      "2019_HylTBhA5tQ     {972, 950}\n",
      "2019_B1l08oAct7     {994, 996, 1064, 1004, 1007, 1044, 1047, 1048, 1055}\n",
      "2018_H135uzZ0-      {1072, 1079}\n",
      "2017_H1oyRlYgg      set()\n",
      "2017_r1y1aawlg      {1125, 1162, 1100, 1102, 1168}\n",
      "2020_r1eX1yrKwB     {1177, 1202, 1212}\n",
      "2020_Byg79h4tvB     {1243, 1268}\n",
      "2019_H1lFZnR5YX     {1281, 1316, 1284, 1318, 1289, 1331, 1333, 1279}\n",
      "2020_BkeWw6VFwr     {1347, 1373}\n",
      "2018_HyHmGyZCZ      {1406, 1421, 1390, 1426}\n",
      "2018_HyUNwulC-      {1451, 1452}\n",
      "2020_HkgsPhNYPS     {1504, 1464, 1497, 1500, 1502}\n"
     ]
    }
   ],
   "source": [
    "for k in test_set:\n",
    "    print('{:20}{}'.format(k, gt_dict[k][\"mcomp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_for_test = defaultdict(list)\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    sents_for_test[pid].append((df.loc[i][\"UID\"], df.loc[i][\"Sent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic baseline patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_patterns = [\"no comparison with paper\", \"a reasonable baseline paper is\", \n",
    "                     \"compare the results with paper\", \"the baseline paper is weak\", \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Can you compare your results? If possible, could the author provide results on different architecture choices for the stolen model as well as the surrogate model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:  It would be interesting to explore the practicability of the method on more large-scale experiments on image related tasks.\n",
      "Verb SubTrees: \n",
      "the practicability of the method \n",
      "\n",
      "more large - scale experiments on image related tasks \n",
      "\n",
      "\n",
      "==============================================================================\n",
      "\n",
      "Original Sentence:  I think a clearer emphasis on the novelty, eg: current algorithm with mixing rate analyses or more thorough empirical comparisons will make the paper stronger for resubmission.\n",
      "Verb SubTrees: \n",
      "more thorough empirical comparisons \n",
      "\n",
      "\n",
      "==============================================================================\n",
      "\n",
      "Original Sentence:  The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).\n",
      "Verb SubTrees: \n",
      "comparison \n",
      "\n",
      "\n",
      "==============================================================================\n",
      "\n",
      "Original Sentence:  Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20) The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.\n",
      "Verb SubTrees: \n",
      " lies , authors do not compare against et\n",
      "\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "\n",
      "Original Sentence:  COMMENTS ON THE CHANGES SINCE THE LAST YEAR\n",
      "I am not convinced by the comparison with Spearmint added by the authors since the previous version.\n",
      "Verb SubTrees: \n",
      "the comparison with Spearmint added by the authors since the previous version \n",
      "\n",
      "\n",
      "==============================================================================\n",
      "\n",
      "Original Sentence:  In addition the authors do not compare against more recent work, e.g., \n",
      "@INPROCEEDINGS{falkner-bayesopt17,\n",
      " author = {S. Falkner and A. Klein and F. Hutter},\n",
      " title = {Combining Hyperband and Bayesian Optimization},\n",
      " booktitle = {NIPS 2017 Bayesian Optimization Workshop},\n",
      " year = {2017},\n",
      " month = dec,\n",
      "}\n",
      "@InProceedings{falkner-icml-18,\n",
      " title = {{BOHB}: Robust and Efficient Hyperparameter Optimization at Scale},\n",
      " author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},\n",
      " booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML 2018)},\n",
      " pages = {1436--1445},\n",
      " year = {2018},\n",
      " month = jul,\n",
      "}\n",
      "Verb SubTrees: \n",
      " In authors do not compare against e.g. , @INPROCEEDINGS{falkner = { Falkner = { Combining = = 2017 =\n",
      "\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp_toks = [\"result\", \"method\", \"task\", \"dataset\", \"metric\"]\n",
    "\n",
    "for pid in gt_dict:\n",
    "    for sent_uid in gt_dict[pid][\"mcomp\"]:\n",
    "        doc = nlp(df.loc[sent_uid][\"Sent\"])\n",
    "        verb_subtree = []\n",
    "        \n",
    "        for s in doc.sents:\n",
    "            find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"task\": [], \"dataset\": [], \"metric\": []}\n",
    "\n",
    "            for tok in s:\n",
    "\n",
    "                if tok.text.lower().startswith(\"compar\"):\n",
    "                    find_special_tokens[\"compar\"].append(tok)\n",
    "                else:\n",
    "                    for k in sp_toks:\n",
    "                        if tok.text.lower().startswith(k):\n",
    "                            find_special_tokens[k].append(tok)\n",
    "                            break\n",
    "            \n",
    "            verb_tokens = []\n",
    "            if find_special_tokens[\"compar\"]:\n",
    "                for t in find_special_tokens[\"compar\"]:\n",
    "#                     verb_subtree.append(t.subtree)\n",
    "                    if t == s.root:\n",
    "                        simplified_sent = \"\"\n",
    "                        for chh in t.lefts:\n",
    "                            simplified_sent = simplified_sent + \" \" + chh.text\n",
    "                        simplified_sent = simplified_sent + \" \" + t.text\n",
    "                        for chh in t.rights:\n",
    "                            simplified_sent = simplified_sent + \" \" + chh.text\n",
    "#                         print(\"SIMP: \", simplified_sent)\n",
    "                        verb_subtree.append(simplified_sent)\n",
    "                    else:\n",
    "                        verb_subtree.append(t.subtree)\n",
    "            else:\n",
    "                for k in sp_toks:\n",
    "                    for i in find_special_tokens[k]:\n",
    "                        local_vt = []\n",
    "                        for j in i.ancestors:\n",
    "                            if j.pos_ == \"NOUN\":\n",
    "                                local_vt.append(j)\n",
    "                        if not local_vt:\n",
    "                            for j in i.ancestors:\n",
    "                                if j.pos_ == \"VERB\":\n",
    "                                    local_vt.append(j)\n",
    "                        verb_tokens = verb_tokens + local_vt\n",
    "                            \n",
    "                \n",
    "                for i in verb_tokens:\n",
    "                    verb_subtree.append(i.subtree)\n",
    "        \n",
    "        print(\"Original Sentence: \", df.loc[sent_uid][\"Sent\"])\n",
    "        print(\"Verb SubTrees: \")\n",
    "        for i in verb_subtree:\n",
    "            if type(i) == str:\n",
    "                print(i)\n",
    "            else:\n",
    "                for k in i:\n",
    "                    print(k, end=\" \")\n",
    "            print(\"\\n\")\n",
    "        print(\"\\n==============================================================================\\n\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "\n",
      "The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf)\n",
      "\n",
      "Alsot he root:  compare\n",
      " lies , authors do not compare against et\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "\n",
      "and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==============================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20) The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.\")\n",
    "for s in doc.sents:\n",
    "    print(s, end=\"\\n\\n\")\n",
    "    find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"task\": [], \"dataset\": [], \"metric\": []}\n",
    "\n",
    "    for tok in s:\n",
    "\n",
    "        if tok.text.lower().startswith(\"compar\"):\n",
    "            find_special_tokens[\"compar\"].append(tok)\n",
    "        else:\n",
    "            for k in sp_toks:\n",
    "                if tok.text.lower().startswith(k):\n",
    "                    find_special_tokens[k].append(tok)\n",
    "                    break\n",
    "    \n",
    "    verb_tokens = []\n",
    "    if find_special_tokens[\"compar\"]:\n",
    "        for t in find_special_tokens[\"compar\"]:\n",
    "            #verb_subtree.append(t.subtree)\n",
    "            if t == s.root:\n",
    "#                 verb_subtree.append(t.children)\n",
    "#                 print(\"Alsot he root: \", t)\n",
    "                simplified_sent = \"\"\n",
    "                for chh in t.lefts:\n",
    "                    simplified_sent = simplified_sent + \" \" + chh.text\n",
    "                simplified_sent = simplified_sent + \" \" + t.text\n",
    "                for chh in t.rights:\n",
    "                    simplified_sent = simplified_sent + \" \" + chh.text\n",
    "                print(simplified_sent)\n",
    "            else:\n",
    "                verb_subtree.append(t.subtree)\n",
    "    else:\n",
    "        \n",
    "        for k in sp_toks:\n",
    "            for i in find_special_tokens[k]:\n",
    "                local_vt = []\n",
    "                for j in i.ancestors:\n",
    "                    if j.pos_ == \"NOUN\":\n",
    "                        local_vt.append(j)\n",
    "                if not local_vt:\n",
    "                    for j in i.ancestors:\n",
    "                        if j.pos_ == \"VERB\":\n",
    "                            local_vt.append(j)\n",
    "                verb_tokens = verb_tokens + local_vt\n",
    "\n",
    "\n",
    "        for i in verb_tokens:\n",
    "            verb_subtree.append(i.subtree)\n",
    "    \n",
    "    for i in verb_subtree:\n",
    "        for k in i:\n",
    "            print(k, end=\" \")\n",
    "        print(\"\\n\")\n",
    "    print(\"\\n==============================================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRON\n",
      "VERB\n",
      "AUX\n",
      "ADJ\n",
      "PART\n",
      "VERB\n",
      "DET\n",
      "NOUN\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "ADP\n",
      "ADV\n",
      "ADJ\n",
      "PUNCT\n",
      "NOUN\n",
      "NOUN\n",
      "ADP\n",
      "NOUN\n",
      "VERB\n",
      "NOUN\n",
      "PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"It would be interesting to explore the practicability of the methods on more large-scale experiments on image related tasks.\")\n",
    "for tok in doc:\n",
    "    print(tok.pos_)\n",
    "#     if tok.text.lower() == \"method\":\n",
    "#         for i in tok.ancestors:\n",
    "#             print(i.text)\n",
    "#         print(\"\\n================\")\n",
    "#     if tok.text.lower() == \"explore\":\n",
    "#         for i in tok.children:\n",
    "#             print(i.text)\n",
    "#         print(\"\\n================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tok.rights:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_extension',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'string',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
