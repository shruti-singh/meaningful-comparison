{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"InputTestSet-Reviews48_Ann.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Sent</th>\n",
       "      <th>MComp</th>\n",
       "      <th>Cat</th>\n",
       "      <th>SubCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The authors propose to use k-DPP to select a s...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>This paper covers the related work nicely, wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The rest of the paper are also clearly written.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>However, I have some concerns about the propos...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>- It is not clear how to define the kernel, th...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID              PID     Dec  \\\n",
       "0    0  2019_SJf_XhCqKm  Reject   \n",
       "1    1  2019_SJf_XhCqKm  Reject   \n",
       "2    2  2019_SJf_XhCqKm  Reject   \n",
       "3    3  2019_SJf_XhCqKm  Reject   \n",
       "4    4  2019_SJf_XhCqKm  Reject   \n",
       "\n",
       "                                                Sent  MComp  Cat SubCat  \n",
       "0  The authors propose to use k-DPP to select a s...      0  NaN    NaN  \n",
       "1  This paper covers the related work nicely, wit...      0  NaN    NaN  \n",
       "2    The rest of the paper are also clearly written.      0  NaN    NaN  \n",
       "3  However, I have some concerns about the propos...      0  NaN    NaN  \n",
       "4  - It is not clear how to define the kernel, th...      0  NaN    NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dict = {}\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    if not pid in gt_dict:\n",
    "        gt_dict[pid] = {\"dec\": df.loc[i][\"Dec\"], \"mcomp\": set(), \"not_mcomp\": set()}\n",
    "    if df.loc[i][\"MComp\"] == 1:\n",
    "        gt_dict[pid][\"mcomp\"].add(df.loc[i][\"UID\"])\n",
    "    else:\n",
    "        gt_dict[pid][\"not_mcomp\"].add(df.loc[i][\"UID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accept': [48, 644], 'Reject': [69, 744]}\n"
     ]
    }
   ],
   "source": [
    "stats_dict = {\"Accept\": [0, 0], \"Reject\": [0, 0]}\n",
    "\n",
    "for k, v in gt_dict.items():\n",
    "    #print(len(v[\"mcomp\"]), len(v[\"not_mcomp\"]), v[\"dec\"])\n",
    "    stats_dict[v[\"dec\"]][0] += len(v[\"mcomp\"])\n",
    "    stats_dict[v[\"dec\"]][1] += len(v[\"not_mcomp\"])\n",
    "    \n",
    "print(stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestSet length: 32\n",
      " ['2019_SJf_XhCqKm', '2017_Bk0MRI5lg', '2020_SyevYxHtDB', '2018_rJBiunlAW', '2020_rkltE0VKwH', '2018_Hki-ZlbA-', '2019_BJx0sjC5FX', '2020_r1e_FpNFDr', '2020_B1lsXREYvr', '2018_SkZxCk-0Z', '2019_rJzoujRct7', '2018_HkfXMz-Ab', '2017_BJ9fZNqle', '2019_SyxZJn05YX', '2017_B1ckMDqlg', '2017_HJ0NvFzxl', '2017_S1_pAu9xl', '2018_SyYYPdg0-', '2017_BJAA4wKxg', '2019_HyVxPsC9tm', '2019_HylTBhA5tQ', '2019_B1l08oAct7', '2018_H135uzZ0-', '2017_H1oyRlYgg', '2017_r1y1aawlg', '2020_r1eX1yrKwB', '2020_Byg79h4tvB', '2019_H1lFZnR5YX', '2020_BkeWw6VFwr', '2018_HyHmGyZCZ', '2018_HyUNwulC-', '2020_HkgsPhNYPS']\n"
     ]
    }
   ],
   "source": [
    "test_set = list(gt_dict.keys())\n",
    "print(\"TestSet length: %d\\n\"%len(test_set), test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_SJf_XhCqKm     {39, 17, 20, 27, 28, 30}\n",
      "2017_Bk0MRI5lg      {48, 57}\n",
      "2020_SyevYxHtDB     {76, 87}\n",
      "2018_rJBiunlAW      {108, 110, 112, 113, 124, 126}\n",
      "2020_rkltE0VKwH     {160, 155, 184, 159}\n",
      "2018_Hki-ZlbA-      {267, 235, 236, 271}\n",
      "2019_BJx0sjC5FX     {292, 287}\n",
      "2020_r1e_FpNFDr     {312, 322, 315, 308}\n",
      "2020_B1lsXREYvr     {376, 401}\n",
      "2018_SkZxCk-0Z      {449, 443, 445, 486}\n",
      "2019_rJzoujRct7     {518, 519}\n",
      "2018_HkfXMz-Ab      {573, 566}\n",
      "2017_BJ9fZNqle      {627, 623, 615}\n",
      "2019_SyxZJn05YX     {672, 673, 657, 669, 671}\n",
      "2017_B1ckMDqlg      {714, 707}\n",
      "2017_HJ0NvFzxl      {739}\n",
      "2017_S1_pAu9xl      {792, 809, 810, 806}\n",
      "2018_SyYYPdg0-      {834, 867, 868, 869, 870, 872, 873, 844, 830}\n",
      "2017_BJAA4wKxg      {884}\n",
      "2019_HyVxPsC9tm     {931, 933, 905, 909, 912, 913, 919, 926}\n",
      "2019_HylTBhA5tQ     {972, 950}\n",
      "2019_B1l08oAct7     {994, 996, 1064, 1004, 1007, 1044, 1047, 1048, 1055}\n",
      "2018_H135uzZ0-      {1072, 1079}\n",
      "2017_H1oyRlYgg      set()\n",
      "2017_r1y1aawlg      {1125, 1162, 1100, 1102, 1168}\n",
      "2020_r1eX1yrKwB     {1177, 1202, 1212}\n",
      "2020_Byg79h4tvB     {1243, 1268}\n",
      "2019_H1lFZnR5YX     {1281, 1316, 1284, 1318, 1289, 1331, 1333, 1279}\n",
      "2020_BkeWw6VFwr     {1347, 1373}\n",
      "2018_HyHmGyZCZ      {1406, 1421, 1390, 1426}\n",
      "2018_HyUNwulC-      {1451, 1452}\n",
      "2020_HkgsPhNYPS     {1504, 1464, 1497, 1500, 1502}\n"
     ]
    }
   ],
   "source": [
    "for k in test_set:\n",
    "    print('{:20}{}'.format(k, gt_dict[k][\"mcomp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_for_test = defaultdict(list)\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    sents_for_test[pid].append((df.loc[i][\"UID\"], df.loc[i][\"Sent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"entities_dict_smaller\", \"r\") as f:\n",
    "    entity_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Material', 'Method', 'Metric', 'Task'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(entity_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('convolutional neural networks', 'Method'),\n",
       " ('convnets', 'Method'),\n",
       " ('recognition', 'Task'),\n",
       " ('visual recognition tasks', 'Task'),\n",
       " ('age estimation', 'Task'),\n",
       " ('head pose estimation', 'Task'),\n",
       " ('multi - label classification', 'Task'),\n",
       " ('semantic segmentation', 'Task'),\n",
       " ('classification', 'Task'),\n",
       " ('deep convnets', 'Method'),\n",
       " ('dldl', 'Method'),\n",
       " ('feature learning', 'Task'),\n",
       " ('deep learning', 'Method'),\n",
       " ('image classification', 'Task'),\n",
       " ('deep learning methods', 'Method'),\n",
       " ('image classification tasks', 'Task'),\n",
       " ('human pose estimation', 'Task'),\n",
       " ('convnet', 'Method'),\n",
       " ('recognition tasks', 'Task'),\n",
       " ('ensemble', 'Method')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_dict.items())[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1784\n"
     ]
    }
   ],
   "source": [
    "entity_key_map = {}\n",
    "for i in entity_dict:\n",
    "    s = re.sub('[^0-9a-zA-Z,:;.?!\\- ]+', '', i)\n",
    "    while s.find(\"  \") > -1:\n",
    "        s = s.replace(\"  \", \" \")\n",
    "    if len(s) > 2:\n",
    "        cl = re.sub('[^0-9a-zA-Z ]+', '', i)\n",
    "        while cl.find(\"  \") > -1:\n",
    "            cl = cl.replace(\"  \", \" \")\n",
    "        entity_key_map[cl.strip()] = i\n",
    "print(len(entity_key_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    }
   ],
   "source": [
    "coun = 0\n",
    "for i in entity_dict:\n",
    "    if len(i) < 5:\n",
    "        coun +=1\n",
    "#         print(i)\n",
    "print(coun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('convolutional neural networks', 'convolutional neural networks'),\n",
       " ('convnets', 'convnets'),\n",
       " ('recognition', 'recognition'),\n",
       " ('visual recognition tasks', 'visual recognition tasks'),\n",
       " ('age estimation', 'age estimation')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_key_map.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Method': 1191, 'Task': 289, 'Metric': 158, 'Material': 165})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(entity_dict.values())\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(c)\n",
    "reverse_map = defaultdict(list)\n",
    "\n",
    "for k, v in entity_dict.items():\n",
    "    reverse_map[v].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recognition\n",
      "visual recognition tasks\n",
      "age estimation\n",
      "head pose estimation\n",
      "multi - label classification\n",
      "semantic segmentation\n",
      "classification\n",
      "feature learning\n",
      "image classification\n",
      "image classification tasks\n",
      "human pose estimation\n",
      "recognition tasks\n",
      "visual recognition\n",
      "feature extraction\n",
      "estimation\n",
      "vision tasks\n",
      "fine - tuning\n",
      "computer vision tasks\n",
      "face alignment\n",
      "data augmentation\n",
      "training process\n",
      "segmentation\n",
      "over - fitting\n",
      "computer vision\n",
      "machine learning\n",
      "language modeling\n",
      "question answering\n",
      "machine translation\n",
      "text summarization\n",
      "nlp tasks\n",
      "speech recognition\n",
      "translation\n",
      "part - of - speech tagging\n",
      "parsing\n",
      "binary classification task\n",
      "named entity recognition\n",
      "computer vision community\n",
      "weakly supervised object detection\n",
      "wod\n",
      "saliency detection\n",
      "object detection\n",
      "optimization problem\n",
      "lvcsr\n",
      "natural language processing\n",
      "decoding\n",
      "compression\n",
      "aggregation\n",
      "vanishing gradient problem\n",
      "object classification\n",
      "localization\n",
      "pose estimation\n",
      "end - to - end training\n",
      "motion compensation\n",
      "face recognition\n",
      "ill - posed problem\n",
      "image processing\n",
      "reconstruction\n",
      "action recognition\n",
      "optical flow\n",
      "node classification\n",
      "link prediction\n",
      "text classification\n",
      "supervised tasks\n",
      "document classification\n",
      "error detection\n",
      "learner writing\n",
      "sequence labeling tasks\n",
      "predictions\n",
      "pedestrian detection\n",
      "hard negative mining\n",
      "dependency parsing\n",
      "supervised training\n",
      "sequence tagging\n",
      "classification task\n",
      "ner\n",
      "part - of - speech\n",
      "semi - supervised learning\n",
      "chunking\n",
      "language\n",
      "knowledge distillation\n",
      "computer vision applications\n",
      "sentence representations\n",
      "downstream tasks\n",
      "image recognition\n",
      "automatic speech recognition\n",
      "ablation\n",
      "classification problems\n",
      "sentiment analysis\n",
      "classification tasks\n",
      "adaptation\n",
      "dimensionality reduction\n",
      "semi - supervised setting\n",
      "transfer tasks\n",
      "person re - identification\n",
      "alignment\n",
      "unsupervised clustering\n",
      "semi - supervised classification\n",
      "natural language inference\n",
      "nli\n",
      "natural language understanding\n",
      "textual entailment\n",
      "image captioning\n",
      "visual question answering\n",
      "vqa\n",
      "recognizing textual entailment\n",
      "rte\n",
      "prediction task\n",
      "ilsvrc 2014\n",
      "overfitting\n",
      "image super - resolution\n",
      "object recognition\n",
      "single image super - resolution\n",
      "sisr\n",
      "regression\n",
      "end - to - end mapping\n",
      "regression problem\n",
      "image retrieval\n",
      "super - resolution\n",
      "neural machine translation\n",
      "abstractive summarization\n",
      "reading comprehension\n",
      "multi - head attention\n",
      "sentence classification\n",
      "ablation study\n",
      "model selection\n",
      "machine comprehension\n",
      "early stopping\n",
      "image restoration\n",
      "denoising\n",
      "restoration\n",
      "inpainting\n",
      "image denoising\n",
      "re - id\n",
      "fine - grained classification\n",
      "real - world applications\n",
      "reid\n",
      "deep reinforcement learning\n",
      "atari games\n",
      "exploration\n",
      "babi task\n",
      "post - processing\n",
      "end training\n",
      "person detection\n",
      "atari - 57\n",
      "robotics\n",
      "unsupervised learning\n",
      "text generation\n",
      "sentiment classification\n",
      "image segmentation\n",
      "sentence modeling\n",
      "face detection\n",
      "multi - scale training\n",
      "multi - scale testing\n",
      "answer sentence selection\n",
      "multi - human parsing\n",
      "instance segmentation\n",
      "human parsing\n",
      "autonomous driving\n",
      "optical flow estimation\n",
      "annotation\n",
      "semantic role labeling\n",
      "sequence prediction\n",
      "pos tagging\n",
      "unsupervised domain adaptation\n",
      "multi - task learning\n",
      "mtl\n",
      "tagging\n",
      "wsod\n",
      "multi - view detection\n",
      "gradient computation\n",
      "imagenet classification\n",
      "scene parsing\n",
      "depth estimation\n",
      "end - to - end learning\n",
      "3d pose estimation\n",
      "tracking\n",
      "inference process\n",
      "artificial intelligence\n",
      "information retrieval\n",
      "binary classification\n",
      "collaborative filtering\n",
      "question attention\n",
      "3d pose\n",
      "nmt\n",
      "few - shot learning\n",
      "3d reconstruction\n",
      "degradation problem\n",
      "semantic segmentation of urban scenes\n",
      "structured prediction\n",
      "segmentations\n",
      "3d detection\n",
      "learning problem\n",
      "auxiliary task\n",
      "classification problem\n",
      "conditional computation\n",
      "face verification\n",
      "image generation\n",
      "image - to - image translation\n",
      "style transfer\n",
      "image synthesis\n",
      "ilsvrc\n",
      "semantic image segmentation\n",
      "segmentation tasks\n",
      "visual qa\n",
      "vqa task\n",
      "3d face reconstruction\n",
      "landmark detection\n",
      "3d face alignment\n",
      "dialogue systems\n",
      "open - ended task\n",
      "multiple - choice task\n",
      "brain tumor segmentation\n",
      "zero - shot learning\n",
      "supervised\n",
      "summarization\n",
      "flow estimation\n",
      "motion segmentation\n",
      "srl\n",
      "machine reading\n",
      "syntactic parsing\n",
      "aspect - based sentiment analysis\n",
      "sentiment analysis task\n",
      "semantic segmentation task\n",
      "natural language tasks\n",
      "language modelling\n",
      "6d pose estimation\n",
      "answer verification\n",
      "dense prediction\n",
      "face hallucination\n",
      "emotion recognition\n",
      "initialization\n",
      "statistical machine translation\n",
      "non - uniform motion blur removal\n",
      "re - i d\n",
      "person re - i d\n",
      "scene understanding\n",
      "boundary detection\n",
      "matrix completion problem\n",
      "relation extraction\n",
      "facial expression recognition\n",
      "keypoint detection\n",
      "label bias problem\n",
      "text categorization\n",
      "one - shot learning\n",
      "text detection\n",
      "fine - grained categorization\n",
      "part localization\n",
      "keypoint prediction\n",
      "sentiment\n",
      "emotion classification\n",
      "person re - id\n",
      "image inpainting\n",
      "scene text detection\n",
      "multi - task loss\n",
      "attribute prediction\n",
      "car verification\n",
      "gec\n",
      "dependency\n",
      "span srl\n",
      "dependency srl\n",
      "ordinal regression\n",
      "ctr estimation\n",
      "face identification\n",
      "sss problem\n",
      "causal inference\n",
      "ite\n",
      "image qa\n",
      "prediction tasks\n",
      "aifr\n",
      "semantic tasks\n",
      "salient object detection\n",
      "noise reduction\n",
      "detail preservation\n",
      "lpnc\n",
      "facial attribute transfer\n",
      "attribute transfer\n",
      "graph classification problem\n",
      "relatedness\n",
      "i2i translation\n",
      "joint detection\n",
      "dense face alignment\n",
      "instance - aware semantic segmentation\n",
      "character - level\n",
      "curve text detection\n",
      "target - dependent sentiment classification\n",
      "deep exploration\n",
      "cross - lingual\n",
      "subcategory classification\n",
      "gender transfer\n"
     ]
    }
   ],
   "source": [
    "for i in reverse_map[\"Task\"]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"MNIST\" in entity_key_map, \"mnist\" in entity_key_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. RoBERTa trained on SciLit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy-transformers            0.6.2\r\n",
      "tokenizers                    0.8.1rc2\r\n",
      "transformers                  3.3.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip3.7 list | grep -E 'transformers|tokenizers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./trained_lm/CLMLModelRoBerta/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./trained_lm/CLMLModelRoBerta/\")\n",
    "model = AutoModel.from_pretrained(\"./trained_lm/CLMLModelRoBerta/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_using_roberta(text):\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_entities(sentence, replace_with_dataset=True):\n",
    "    cleaned_sent = re.sub('[^0-9a-zA-Z,:;.?!\\- ]+', ' ', sentence)\n",
    "    while cleaned_sent.find(\"  \") > -1:\n",
    "        cleaned_sent = cleaned_sent.replace(\"  \", \" \")\n",
    "    \n",
    "    entity_key_map_keys = list(entity_key_map.keys()) # As we will be dunamically adding entries to this dict an dthat will throw an error.\n",
    "    entities_found = []\n",
    "    for i in entity_key_map_keys:\n",
    "        if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "            entities_found.append(i)\n",
    "        elif cleaned_sent.lower().find(\" \" + i + \" \") > -1:\n",
    "            found_idx = cleaned_sent.lower().find(\" \" + i + \" \")\n",
    "            entity_dict[cleaned_sent[found_idx:found_idx+len(\" \" + i + \" \")]] = entity_dict[i]\n",
    "            entity_key_map[cleaned_sent[found_idx:found_idx+len(\" \" + i + \" \")]] = entity_key_map[i]\n",
    "    \n",
    "    entities_found.sort(key=lambda s: len(s))\n",
    "    len_sorted_entities = entities_found.copy()\n",
    "    \n",
    "    subset_entities = []\n",
    "    # Remove subset entities (eg: Among cnn and 3-layer-cnn, prefer the latter)\n",
    "    for fe in len_sorted_entities:\n",
    "        for other_ent in len_sorted_entities:\n",
    "            if fe != other_ent and other_ent.find(fe) > -1:\n",
    "                subset_entities.append(fe)\n",
    "                break\n",
    "    for se in subset_entities:\n",
    "        len_sorted_entities.remove(se)\n",
    "    for maxents in len_sorted_entities:\n",
    "        mask_name = \" \" + entity_dict[entity_key_map[i]].lower() + \" \"\n",
    "        if replace_with_dataset:\n",
    "            if mask_name == \" material \":\n",
    "                mask_name = \" dataset \"\n",
    "        cleaned_sent = cleaned_sent.replace(\" \" + maxents + \" \", mask_name)\n",
    "    words_cleaned = nltk.word_tokenize(cleaned_sent)\n",
    "    dups_removed = [v for i, v in enumerate(words_cleaned) if i == 0 or v != words_cleaned[i-1]]\n",
    "    new_dup_removed_sent = \" \".join(dups_removed)\n",
    "    return new_dup_removed_sent.strip()\n",
    "\n",
    "#     #print(cleaned_sent)\n",
    "#     for i in entity_key_map:\n",
    "#         if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "#             #print(\"Substituting ent: {} with mask: {}\".format(i, entity_dict[entity_key_map[i]].lower()))\n",
    "#             cleaned_sent = cleaned_sent.replace(i, entity_dict[entity_key_map[i]].lower())\n",
    "#     return cleaned_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the BO-PET test , the best method is to take risks . This leads to substantial improvement in results .'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"In the BO-PET test*, the best method is to take\\ risks. This leads to substantial improvement in results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'test']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"this is a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "sp_toks = [\"result\", \"method\", \"task\", \"dataset\", \"metric\", \"baseline\", \"fair\", \"unfair\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunks_using_spacy_dp(conssentence, replace_with_dataset=True):\n",
    "    \n",
    "    conssentence = mask_entities(conssentence, replace_with_dataset)\n",
    "#     print(conssentence)\n",
    "    doc = nlp(conssentence)\n",
    "    verb_subtree = []\n",
    "\n",
    "    for s in doc.sents:\n",
    "#         find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"technique\": [], \"task\": [], \"dataset\": [], \"material\": [], \"metric\": []}\n",
    "        find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"baseline\": [], \"task\": [], \n",
    "                               \"dataset\": [],  \"metric\": [], \"unfair\": [], \"fair\": []}\n",
    "\n",
    "        for tok in s:\n",
    "\n",
    "            if tok.text.lower().startswith(\"compar\"):\n",
    "                find_special_tokens[\"compar\"].append(tok)\n",
    "            else:\n",
    "                for k in sp_toks:\n",
    "                    if tok.text.lower().startswith(k):\n",
    "                        find_special_tokens[k].append(tok)\n",
    "                        break\n",
    "\n",
    "        verb_tokens = []\n",
    "        if find_special_tokens[\"compar\"]:\n",
    "            for t in find_special_tokens[\"compar\"]:\n",
    "#                     verb_subtree.append(t.subtree)\n",
    "                if t == s.root:\n",
    "                    simplified_sent = \"\"\n",
    "                    for chh in t.lefts:\n",
    "                        simplified_sent = simplified_sent + \" \" + chh.text\n",
    "                    simplified_sent = simplified_sent + \" \" + t.text\n",
    "                    for chh in t.rights:\n",
    "                        simplified_sent = simplified_sent + \" \" + chh.text\n",
    "#                         print(\"SIMP: \", simplified_sent)\n",
    "                    verb_subtree.append(simplified_sent)\n",
    "                else:\n",
    "                    verb_subtree.append(t.subtree)\n",
    "        else:\n",
    "            for k in sp_toks:\n",
    "                for i in find_special_tokens[k]:\n",
    "                    local_vt = []\n",
    "                    for j in i.ancestors:\n",
    "                        if j.pos_ == \"NOUN\":\n",
    "                            local_vt.append(j)\n",
    "                    if not local_vt:\n",
    "                        for j in i.ancestors:\n",
    "                            if j.pos_ == \"VERB\":\n",
    "                                local_vt.append(j)\n",
    "                    verb_tokens = verb_tokens + local_vt\n",
    "\n",
    "\n",
    "            for i in verb_tokens:\n",
    "                verb_subtree.append(i.subtree)\n",
    "\n",
    "    eecc = []\n",
    "    for i in verb_subtree:\n",
    "        if type(i) == str:\n",
    "            eecc.append(i)\n",
    "        else:\n",
    "            local_chunk = \"\"\n",
    "            for lcaltok in i:\n",
    "                local_chunk = local_chunk + \" \" + lcaltok.text\n",
    "            eecc.append(local_chunk)\n",
    "#     if not eecc:\n",
    "#         print(conssentence)\n",
    "    return list(set(eecc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' more large - scale experiments on image related tasks',\n",
       " ' the practicability of the method']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"It would be interesting to explore the practicability of the method on more large-scale experiments on image related tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' comparison to SOTA']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"The experimental validation is also not extensive since comparison to SOTA is not included.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_HyVxPsC9tm 938 2\n",
      "2019_HyVxPsC9tm 940 3\n",
      "2020_Byg79h4tvB 1272 [1] Conditional adversarial domain adaptation, Long et.al, in NeurIPS 2018\n",
      "[2] Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation, You et.al, in ICML 2019\n",
      "2018_HyHmGyZCZ 1425 2\n"
     ]
    }
   ],
   "source": [
    "roberta_vectors = defaultdict(dict)\n",
    "skip_uids = []\n",
    "\n",
    "for pid in gt_dict:\n",
    "    roberta_vectors[pid] = {}\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunk = \". \".join(mcomp_chunks_from_sent)\n",
    "            else:\n",
    "                final_chunk = df.loc[mcs][\"Sent\"]\n",
    "            \n",
    "            vec = embed_text_using_roberta(final_chunk.strip()).mean(1).detach().numpy()\n",
    "            roberta_vectors[pid][mcs] = vec / norm(vec)\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunk = \". \".join(mcomp_chunks_from_sent)\n",
    "            else:\n",
    "                final_chunk = df.loc[mcs][\"Sent\"]\n",
    "            \n",
    "            vec = embed_text_using_roberta(final_chunk.strip()).mean(1).detach().numpy()\n",
    "            roberta_vectors[pid][mcs] = vec / norm(vec)\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 1384\n"
     ]
    }
   ],
   "source": [
    "mcomp_sentences = {}\n",
    "not_mcomp_sentences = {}\n",
    "\n",
    "for pid in gt_dict:\n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            mcomp_sentences[mcs] = pid\n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            not_mcomp_sentences[mcs] = pid\n",
    "print(len(mcomp_sentences), len(not_mcomp_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_with_mcomp = defaultdict(dict)\n",
    "sim_with_not_mcomp = defaultdict(dict)\n",
    "sim_with_notmcomp_paper_sents = defaultdict(dict)\n",
    "\n",
    "mean_at_k = [\"1\", \"3\", \"5\", \"7\", \"10\", \"20\", \"30\", \"50\", \"100\", \"500\", \"1000\", \"1380\"]\n",
    "\n",
    "for sid in mcomp_sentences:\n",
    "    \n",
    "    # 1. With other mcomp sentences\n",
    "    temp_list = []    \n",
    "    for osid in mcomp_sentences:\n",
    "        if osid != sid:\n",
    "            temp_list.append(np.inner(roberta_vectors[mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "\n",
    "    \n",
    "    # 2. With other not_mcomp_sentences\n",
    "    temp_list = []\n",
    "    for osid in not_mcomp_sentences:\n",
    "        temp_list.append(np.inner(roberta_vectors[not_mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_not_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_not_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "\n",
    "    \n",
    "    # 3. With not_mcomp_sentences of the same paper\n",
    "    temp_list = []    \n",
    "    for osid in not_mcomp_sentences:\n",
    "        if not_mcomp_sentences[osid] == mcomp_sentences[sid]:\n",
    "            temp_list.append(np.inner(roberta_vectors[not_mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_notmcomp_paper_sents[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_notmcomp_paper_sents[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_sim_plot\n",
    "diff12 = {\"all\": []}\n",
    "for vv in mean_at_k:\n",
    "    diff12[str(vv)] = []\n",
    "\n",
    "diff13 = {\"all\": []}\n",
    "for vv in mean_at_k:\n",
    "    diff13[str(vv)] = []\n",
    "\n",
    "for sid in sim_with_mcomp:\n",
    "    diff12[\"all\"].append(sim_with_mcomp[sid][\"mean\"] - sim_with_not_mcomp[sid][\"mean\"])\n",
    "    diff13[\"all\"].append(sim_with_mcomp[sid][\"mean\"] - sim_with_notmcomp_paper_sents[sid][\"mean\"])\n",
    "    \n",
    "    for vv in mean_at_k:\n",
    "        diff12[str(vv)].append(sim_with_mcomp[sid][\"mean_{}\".format(vv)] - sim_with_not_mcomp[sid][\"mean_{}\".format(vv)])\n",
    "        diff13[str(vv)].append(sim_with_mcomp[sid][\"mean_{}\".format(vv)] - sim_with_notmcomp_paper_sents[sid][\"mean_{}\".format(vv)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinstalling transformers?? and smaller entity set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100</td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000  </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.42</td><td style=\"text-align: right;\">0.39</td><td style=\"text-align: right;\">0.34</td><td style=\"text-align: right;\">0.32</td><td style=\"text-align: right;\"> 0.26</td><td style=\"text-align: right;\"> 0.18</td><td style=\"text-align: right;\"> 0.15</td><td style=\"text-align: right;\"> 0.03</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0.12</td><td style=\"text-align: right;\">   0.5</td><td style=\"text-align: right;\">   0.81</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.74</td><td style=\"text-align: right;\">0.82</td><td style=\"text-align: right;\">0.85</td><td style=\"text-align: right;\">0.85</td><td style=\"text-align: right;\"> 0.87</td><td style=\"text-align: right;\"> 0.92</td><td style=\"text-align: right;\"> 0.95</td><td style=\"text-align: right;\"> 0.97</td><td style=\"text-align: right;\">  0.78</td><td style=\"text-align: right;\">  0.65</td><td style=\"text-align: right;\">   0.65</td><td style=\"text-align: right;\">   0.65</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff13[val])/len(diff13[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteratively adding entites whose lowercase is present in dict AND adding sp token fair/unfair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3  </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100</td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380  </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.46</td><td style=\"text-align: right;\">0.4</td><td style=\"text-align: right;\">0.39</td><td style=\"text-align: right;\">0.32</td><td style=\"text-align: right;\"> 0.27</td><td style=\"text-align: right;\"> 0.17</td><td style=\"text-align: right;\"> 0.13</td><td style=\"text-align: right;\"> 0.09</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0.18</td><td style=\"text-align: right;\">   0.56</td><td style=\"text-align: right;\">   0.9</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.79</td><td style=\"text-align: right;\">0.82</td><td style=\"text-align: right;\">0.84</td><td style=\"text-align: right;\">0.84</td><td style=\"text-align: right;\"> 0.85</td><td style=\"text-align: right;\"> 0.94</td><td style=\"text-align: right;\"> 0.96</td><td style=\"text-align: right;\"> 0.97</td><td style=\"text-align: right;\">  0.82</td><td style=\"text-align: right;\">  0.69</td><td style=\"text-align: right;\">   0.69</td><td style=\"text-align: right;\">   0.69</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff13[val])/len(diff13[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5  </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100</td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.37</td><td style=\"text-align: right;\">0.37</td><td style=\"text-align: right;\">0.3</td><td style=\"text-align: right;\">0.26</td><td style=\"text-align: right;\"> 0.25</td><td style=\"text-align: right;\"> 0.15</td><td style=\"text-align: right;\"> 0.12</td><td style=\"text-align: right;\"> 0.02</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0.15</td><td style=\"text-align: right;\">   0.51</td><td style=\"text-align: right;\">   0.85</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.74</td><td style=\"text-align: right;\">0.82</td><td style=\"text-align: right;\">0.85</td><td style=\"text-align: right;\">0.87</td><td style=\"text-align: right;\"> 0.89</td><td style=\"text-align: right;\"> 0.93</td><td style=\"text-align: right;\"> 0.94</td><td style=\"text-align: right;\"> 0.96</td><td style=\"text-align: right;\">  0.79</td><td style=\"text-align: right;\">  0.66</td><td style=\"text-align: right;\">   0.66</td><td style=\"text-align: right;\">   0.66</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff13[val])/len(diff13[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Chunks ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corenlp = StanfordCoreNLP(\"/home/shruti/Documents/DataNLP/stanford-corenlp-4.1.0/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(sent):\n",
    "    parse_str = corenlp.parse(sent)\n",
    "    nltk_tree = Tree.fromstring(parse_str)\n",
    "    \n",
    "#     print(nltk_tree)\n",
    "    \n",
    "    subtrees_list = list(nltk_tree.subtrees())\n",
    "    subtrees_tpos = nltk_tree.treepositions()\n",
    "    for i in range(0, len(nltk_tree.leaves())):\n",
    "        tp_leaf = nltk_tree.leaf_treeposition(i)\n",
    "        subtrees_tpos.remove(tp_leaf)\n",
    "    \n",
    "    dict_len_st = {}\n",
    "    depth_of_subtree = []\n",
    "    for _, i in enumerate(subtrees_list):\n",
    "        depth_of_subtree.append((i, len(subtrees_tpos[_])))\n",
    "        dict_len_st[str(i)] = len(subtrees_tpos[_])\n",
    "    \n",
    "    cdepths = []\n",
    "    for d in depth_of_subtree:\n",
    "        cdepths.append(d[1])\n",
    "    depth_counter = Counter(cdepths)\n",
    "    sorted_depths = sorted(list(depth_counter.keys()))\n",
    "    print(sorted(depth_counter.items(), key=lambda x: x[0]))\n",
    "    \n",
    "    depth_to_split = None\n",
    "    print(sorted_depths) \n",
    "    for sd in sorted_depths:\n",
    "        if depth_counter[sd] == 3:\n",
    "            depth_to_split = 3\n",
    "        elif depth_counter[sd] > 3:\n",
    "            depth_to_split = sd\n",
    "            break\n",
    "    if depth_to_split == None or depth_to_split == 4:\n",
    "        print(\"Depth to split: {}\".format(depth_to_split))\n",
    "        \n",
    "    print(\"depth: \", depth_to_split)\n",
    "    \n",
    "    subtree_chunks = []\n",
    "    for i in depth_of_subtree:\n",
    "        if i[1] == depth_to_split:\n",
    "            subtree_chunks.append(i)\n",
    "    \n",
    "    final_chunks_sent = []\n",
    "    \n",
    "#     for tt in subtree_chunks:\n",
    "#         print(tt)\n",
    "    \n",
    "    for stchunk in subtree_chunks:\n",
    "        print(len(stchunk[0].leaves()), stchunk[0].leaves())\n",
    "#         print(stchunk)\n",
    "        if len(stchunk[0].leaves()) > 5:\n",
    "            subsubtrees = list(stchunk[0].subtrees())\n",
    "            fnlsubsub = []\n",
    "            for sss in subsubtrees:\n",
    "                if str(sss) in dict_len_st and dict_len_st[str(sss)] == depth_to_split+1:\n",
    "                    fnlsubsub.append(sss)\n",
    "            for subchunk in fnlsubsub:\n",
    "                final_chunks_sent.append(\" \".join(subchunk.leaves()))\n",
    "        else:\n",
    "            final_chunks_sent.append(\" \".join(stchunk[0].leaves()))\n",
    "#         final_chunks_sent.append(\" \".join(stchunk[0].leaves()))\n",
    "    \n",
    "    return final_chunks_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20) The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20) The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.\n",
      "[(0, 1), (1, 1), (2, 10), (3, 10), (4, 21), (5, 16), (6, 12), (7, 24), (8, 15), (9, 8), (10, 6), (11, 9), (12, 6), (13, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Second']\n",
      "1 [',']\n",
      "19 ['their', 'study', 'only', 'applies', 'to', 'a', 'small', 'number', 'like', '3', '-', '6', 'hyperparameters', 'with', 'a', 'small', 'k', '=', '20']\n",
      "1 ['-RRB-']\n",
      "20 ['The', 'real', 'challenge', 'lies', 'in', 'scaling', 'up', 'to', 'many', 'hyperparameters', 'or', 'even', 'k', '-', 'DPP', 'sampling', 'for', 'larger', 'k.', 'Third']\n",
      "1 [',']\n",
      "13 ['the', 'authors', 'do', 'not', 'compare', 'against', 'some', 'relevant', ',', 'recent', 'work', ',', 'e.g.']\n",
      "1 [',']\n",
      "22 ['Springenberg', 'et', 'al', '-LRB-', 'http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf', '-RRB-', 'and', 'Snoek', 'et', 'al', '-LRB-', 'https://arxiv.org/pdf/1502.05700.pdf', '-RRB-', 'that', 'is', 'essential', 'for', 'this', 'kind', 'of', 'empirical', 'study']\n",
      "1 ['.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Second',\n",
       " ',',\n",
       " 'their study',\n",
       " 'only',\n",
       " 'applies to a small number like 3 - 6 hyperparameters with a small k = 20',\n",
       " '-RRB-',\n",
       " 'The real challenge',\n",
       " 'lies in scaling up to many hyperparameters or even k - DPP sampling for larger k. Third',\n",
       " ',',\n",
       " 'the authors',\n",
       " 'do not compare against some relevant , recent work , e.g.',\n",
       " ',',\n",
       " 'Springenberg et al',\n",
       " '-LRB- http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf -RRB- and Snoek et al -LRB- https://arxiv.org/pdf/1502.05700.pdf -RRB- that is essential for this kind of empirical study',\n",
       " '.']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(s)\n",
    "get_chunks(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_from_sent = {\"mcomp\": [], \"nmcomp\": []}\n",
    "\n",
    "for pid in gt_dict:\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = get_chunks(df.loc[mcs][\"Sent\"])\n",
    "            chunks_from_sent[\"mcomp\"].append((df.loc[mcs][\"Sent\"], mcomp_chunks_from_sent))\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs)\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = get_chunks(df.loc[mcs][\"Sent\"])\n",
    "            chunks_from_sent[\"nmcomp\"].append((df.loc[mcs][\"Sent\"], mcomp_chunks_from_sent))\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_from_sent[\"mcomp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(cd.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Guangdong University of Foreign Studies is located in Guangzhou.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP\n",
      "      (NP (NNP Guangdong) (NNP University))\n",
      "      (PP (IN of)\n",
      "        (NP (NNP Foreign) (NNPS Studies))))\n",
      "    (VP (VBZ is)\n",
      "      (VP (VBN located)\n",
      "        (PP (IN in)\n",
      "          (NP (NNP Guangzhou)))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "parse_str = corenlp.parse(sentence)\n",
    "print(parse_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tree = Tree.fromstring(parse_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, (0, 0, 0, 1, 0))"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_tree.leaves()), nltk_tree.leaf_treeposition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtrees_list = list(nltk_tree.subtrees())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtrees_tpos = nltk_tree.treepositions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtrees_tpos[_]\n",
    "for i in range(0, len(nltk_tree.leaves())):\n",
    "    tp_leaf = nltk_tree.leaf_treeposition(i)\n",
    "    subtrees_tpos.remove(tp_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           ROOT                                       \n",
      "                                            |                                          \n",
      "                                            S                                         \n",
      "                           _________________|_______________________________________   \n",
      "                          |                                   VP                    | \n",
      "                          |                        ___________|___                  |  \n",
      "                          NP                      |               VP                | \n",
      "            ______________|_____                  |      _________|___              |  \n",
      "           |                    PP                |     |             PP            | \n",
      "           |               _____|_____            |     |          ___|______       |  \n",
      "           NP             |           NP          |     |         |          NP     | \n",
      "     ______|______        |      _____|_____      |     |         |          |      |  \n",
      "   NNP           NNP      IN   NNP         NNPS  VBZ   VBN        IN        NNP     . \n",
      "    |             |       |     |           |     |     |         |          |      |  \n",
      "Guangdong     University  of Foreign     Studies  is located      in     Guangzhou  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nltk_tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 ROOT ()\n",
      "7 S (0,)\n",
      "5 NP (0, 0)\n",
      "3 NP (0, 0, 0)\n",
      "2 NNP (0, 0, 0, 0)\n",
      "2 NNP (0, 0, 0, 1)\n",
      "4 PP (0, 0, 1)\n",
      "2 IN (0, 0, 1, 0)\n",
      "3 NP (0, 0, 1, 1)\n",
      "2 NNP (0, 0, 1, 1, 0)\n",
      "2 NNPS (0, 0, 1, 1, 1)\n",
      "6 VP (0, 1)\n",
      "2 VBZ (0, 1, 0)\n",
      "5 VP (0, 1, 1)\n",
      "2 VBN (0, 1, 1, 0)\n",
      "4 PP (0, 1, 1, 1)\n",
      "2 IN (0, 1, 1, 1, 0)\n",
      "3 NP (0, 1, 1, 1, 1)\n",
      "2 NNP (0, 1, 1, 1, 1, 0)\n",
      "2 . (0, 2)\n"
     ]
    }
   ],
   "source": [
    "depth_of_subtree = []\n",
    "for _, i in enumerate(subtrees_list):\n",
    "    depth_of_subtree.append((i, len(subtrees_tpos[_])))\n",
    "    print(i.height(), i.label(), subtrees_tpos[_])\n",
    "#     break\n",
    "#     if type(i) != Tree:\n",
    "#         print(i)\n",
    "#           subtrees_tpos[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdepths = []\n",
    "for d in depth_of_subtree:\n",
    "    cdepths.append(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1, 1: 1, 2: 3, 3: 4, 4: 6, 5: 4, 6: 1})"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(cdepths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.treeposition_spanning_leaves(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, 1, 0)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.leaf_treeposition(1)\n",
    "# for x in :\n",
    "#     if not isinstance(x, Tree):\n",
    "#         print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Guangdong',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Foreign',\n",
       " 'Studies',\n",
       " 'is',\n",
       " 'located',\n",
       " 'in',\n",
       " 'Guangzhou',\n",
       " '.']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using chunks from the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This DT swallow\n",
      "an DT swallow\n",
      "unladen JJ swallow\n",
      "swallow NN swallow\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This an unladen swallow.\")\n",
    "for sent in doc.sents:\n",
    "    for tok in sent:\n",
    "        if tok.is_alpha:\n",
    "            print(tok.orth_, tok.tag_, tok.head.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_extension',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'string',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree: (DT This)\n",
      "tree: (DT an)\n",
      "tree: (NNS unladen)\n",
      "tree: (VBP swallow)\n",
      "tree: (, .)\n",
      "tree: (DT This)\n",
      "tree: (VBZ is)\n",
      "tree: (DT a)\n",
      "tree: (NN test)\n",
      "tree: (. .)\n"
     ]
    }
   ],
   "source": [
    "def traverse_tree(tree):\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == nltk.tree.Tree:\n",
    "            traverse_tree(subtree)\n",
    "        else:\n",
    "            print(\"tree:\", tree)\n",
    "traverse_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import breadth_first\n",
    "from nltk.tree import ParentedTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in breadth_first(tree[0]):\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.parse??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = list(parser.parse('This an unladen swallow.'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NP', [Tree('DT', ['This'])]), Tree('NP', [Tree('DT', ['an']), Tree('NNS', ['unladen'])])]), Tree('VP', [Tree('VBP', ['swallow'])]), Tree('.', ['.'])])])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NP', [Tree('DT', ['This'])]), Tree('NP', [Tree('DT', ['an']), Tree('NNS', ['unladen'])])]), Tree('S|<VP-.>', [Tree('VP', [Tree('VBP', ['swallow'])]), Tree('.', ['.'])])])])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(tree[0])\n",
    "tree[0].chomsky_normal_form()\n",
    "tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (S\n",
      "  (NP (NP (DT This)) (NP (DT an) (NNS unladen)))\n",
      "  (VP (VBP swallow))\n",
      "  (. .)) (S This an unladen swallow .)\n"
     ]
    }
   ],
   "source": [
    "for (i,child) in enumerate(tree[0]): \n",
    "    print(i,child, child.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ROOT -> S,\n",
       " S -> NP VP .,\n",
       " NP -> NP NP,\n",
       " NP -> DT,\n",
       " DT -> 'This',\n",
       " NP -> DT NNS,\n",
       " DT -> 'an',\n",
       " NNS -> 'unladen',\n",
       " VP -> VBP,\n",
       " VBP -> 'swallow',\n",
       " . -> '.']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[0].productions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[(governor, dep, dependent) for governor, dep, dependent in parse.triples()] for parse in tree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (NP (DT This)) (NP (DT an) (NNS unladen)))\n",
      "  (VP (VBP swallow))\n",
      "  (. .)) [(), (0,), (0, 0), (0, 0, 0), (0, 0, 0, 0), (0, 1), (0, 1, 0), (0, 1, 0, 0), (0, 1, 1), (0, 1, 1, 0), (1,), (1, 0), (1, 0, 0), (2,), (2, 0)]\n"
     ]
    }
   ],
   "source": [
    "for t in tree[0]:\n",
    "    print(t, t.treepositions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'an', 'unladen', 'swallow', '.']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[0].leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "strees_list = list(tree[0].subtrees())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 11)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(strees_list), len(strees_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(nltk.tree.Tree, ['an', 'unladen'])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(strees_list[0]), strees_list[5].leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__radd__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_frozen_class',\n",
       " '_get_node',\n",
       " '_label',\n",
       " '_parse_error',\n",
       " '_pformat_flat',\n",
       " '_repr_png_',\n",
       " '_set_node',\n",
       " 'append',\n",
       " 'chomsky_normal_form',\n",
       " 'clear',\n",
       " 'collapse_unary',\n",
       " 'convert',\n",
       " 'copy',\n",
       " 'count',\n",
       " 'draw',\n",
       " 'extend',\n",
       " 'flatten',\n",
       " 'freeze',\n",
       " 'fromstring',\n",
       " 'height',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'label',\n",
       " 'leaf_treeposition',\n",
       " 'leaves',\n",
       " 'node',\n",
       " 'pformat',\n",
       " 'pformat_latex_qtree',\n",
       " 'pop',\n",
       " 'pos',\n",
       " 'pprint',\n",
       " 'pretty_print',\n",
       " 'productions',\n",
       " 'remove',\n",
       " 'reverse',\n",
       " 'set_label',\n",
       " 'sort',\n",
       " 'subtrees',\n",
       " 'treeposition_spanning_leaves',\n",
       " 'treepositions',\n",
       " 'un_chomsky_normal_form']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NP', [Tree('DT', ['This'])]), Tree('NP', [Tree('DT', ['an']), Tree('NNS', ['unladen'])])]), Tree('VP', [Tree('VBP', ['swallow'])]), Tree('.', ['.'])])])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ROOT ['This', 'an', 'unladen', 'swallow', '.']\n",
      "6 ROOT ['This', 'an', 'unladen', 'swallow', '.']\n",
      "5 S ['This', 'an', 'unladen', 'swallow', '.']\n",
      "4 NP ['This', 'an', 'unladen']\n",
      "3 NP ['This']\n",
      "2 DT ['This']\n",
      "3 NP ['an', 'unladen']\n",
      "2 DT ['an']\n",
      "2 NNS ['unladen']\n",
      "3 VP ['swallow']\n",
      "2 VBP ['swallow']\n",
      "2 . ['.']\n"
     ]
    }
   ],
   "source": [
    "st_depth_list = []\n",
    "\n",
    "for _, st in enumerate(strees_list):\n",
    "    st_depth_list.append((len(st.treepositions()), st))\n",
    "    print(st.height(), st.label(), st.leaves(), )\n",
    "    ss = list(st.subtrees())\n",
    "    for i in ss:\n",
    "        print(i.height(), i.label(), i.leaves())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.leaf_treeposition??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__radd__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_frozen_class',\n",
       " '_get_node',\n",
       " '_label',\n",
       " '_parse_error',\n",
       " '_pformat_flat',\n",
       " '_repr_png_',\n",
       " '_set_node',\n",
       " 'append',\n",
       " 'chomsky_normal_form',\n",
       " 'clear',\n",
       " 'collapse_unary',\n",
       " 'convert',\n",
       " 'copy',\n",
       " 'count',\n",
       " 'draw',\n",
       " 'extend',\n",
       " 'flatten',\n",
       " 'freeze',\n",
       " 'fromstring',\n",
       " 'height',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'label',\n",
       " 'leaf_treeposition',\n",
       " 'leaves',\n",
       " 'node',\n",
       " 'pformat',\n",
       " 'pformat_latex_qtree',\n",
       " 'pop',\n",
       " 'pos',\n",
       " 'pprint',\n",
       " 'pretty_print',\n",
       " 'productions',\n",
       " 'remove',\n",
       " 'reverse',\n",
       " 'set_label',\n",
       " 'sort',\n",
       " 'subtrees',\n",
       " 'treeposition_spanning_leaves',\n",
       " 'treepositions',\n",
       " 'un_chomsky_normal_form']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(), (0,)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.treepositions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 ROOT ['What', 'is', 'the', 'airspeed', 'of', 'an', 'unladen', 'swallow', '?']\n",
      "26 SBARQ ['What', 'is', 'the', 'airspeed', 'of', 'an', 'unladen', 'swallow', '?']\n",
      "3 WHNP ['What']\n",
      "2 WP ['What']\n",
      "20 SQ ['is', 'the', 'airspeed', 'of', 'an', 'unladen', 'swallow']\n",
      "2 VBZ ['is']\n",
      "14 NP ['the', 'airspeed', 'of', 'an', 'unladen']\n",
      "5 NP ['the', 'airspeed']\n",
      "2 DT ['the']\n",
      "2 NN ['airspeed']\n",
      "8 PP ['of', 'an', 'unladen']\n",
      "2 IN ['of']\n",
      "5 NP ['an', 'unladen']\n",
      "2 DT ['an']\n",
      "2 JJ ['unladen']\n",
      "3 S+VP ['swallow']\n",
      "3 VP ['swallow']\n",
      "2 VB ['swallow']\n",
      "2 . ['?']\n"
     ]
    }
   ],
   "source": [
    "for st in st_depth_list:\n",
    "    print(st[0], st[1].label(), st[1].leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(nltk.tree.Tree,\n",
       " ['__add__',\n",
       "  '__class__',\n",
       "  '__contains__',\n",
       "  '__copy__',\n",
       "  '__deepcopy__',\n",
       "  '__delattr__',\n",
       "  '__delitem__',\n",
       "  '__dict__',\n",
       "  '__dir__',\n",
       "  '__doc__',\n",
       "  '__eq__',\n",
       "  '__format__',\n",
       "  '__ge__',\n",
       "  '__getattribute__',\n",
       "  '__getitem__',\n",
       "  '__gt__',\n",
       "  '__hash__',\n",
       "  '__iadd__',\n",
       "  '__imul__',\n",
       "  '__init__',\n",
       "  '__init_subclass__',\n",
       "  '__iter__',\n",
       "  '__le__',\n",
       "  '__len__',\n",
       "  '__lt__',\n",
       "  '__module__',\n",
       "  '__mul__',\n",
       "  '__ne__',\n",
       "  '__new__',\n",
       "  '__radd__',\n",
       "  '__reduce__',\n",
       "  '__reduce_ex__',\n",
       "  '__repr__',\n",
       "  '__reversed__',\n",
       "  '__rmul__',\n",
       "  '__setattr__',\n",
       "  '__setitem__',\n",
       "  '__sizeof__',\n",
       "  '__str__',\n",
       "  '__subclasshook__',\n",
       "  '__weakref__',\n",
       "  '_frozen_class',\n",
       "  '_get_node',\n",
       "  '_label',\n",
       "  '_parse_error',\n",
       "  '_pformat_flat',\n",
       "  '_repr_png_',\n",
       "  '_set_node',\n",
       "  'append',\n",
       "  'chomsky_normal_form',\n",
       "  'clear',\n",
       "  'collapse_unary',\n",
       "  'convert',\n",
       "  'copy',\n",
       "  'count',\n",
       "  'draw',\n",
       "  'extend',\n",
       "  'flatten',\n",
       "  'freeze',\n",
       "  'fromstring',\n",
       "  'height',\n",
       "  'index',\n",
       "  'insert',\n",
       "  'label',\n",
       "  'leaf_treeposition',\n",
       "  'leaves',\n",
       "  'node',\n",
       "  'pformat',\n",
       "  'pformat_latex_qtree',\n",
       "  'pop',\n",
       "  'pos',\n",
       "  'pprint',\n",
       "  'pretty_print',\n",
       "  'productions',\n",
       "  'remove',\n",
       "  'reverse',\n",
       "  'set_label',\n",
       "  'sort',\n",
       "  'subtrees',\n",
       "  'treeposition_spanning_leaves',\n",
       "  'treepositions',\n",
       "  'un_chomsky_normal_form'])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tree[0]), dir(tree[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (SBARQ\n",
      "    (WHNP (WP What))\n",
      "    (SQ\n",
      "      (VBZ is)\n",
      "      (NP\n",
      "        (NP (DT the) (NN airspeed))\n",
      "        (PP (IN of) (NP (DT an) (JJ unladen))))\n",
      "      (S (VP (VB swallow))))\n",
      "    (. ?)))\n"
     ]
    }
   ],
   "source": [
    "for st in tree:\n",
    "    print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " ['__add__',\n",
       "  '__class__',\n",
       "  '__contains__',\n",
       "  '__delattr__',\n",
       "  '__delitem__',\n",
       "  '__dir__',\n",
       "  '__doc__',\n",
       "  '__eq__',\n",
       "  '__format__',\n",
       "  '__ge__',\n",
       "  '__getattribute__',\n",
       "  '__getitem__',\n",
       "  '__gt__',\n",
       "  '__hash__',\n",
       "  '__iadd__',\n",
       "  '__imul__',\n",
       "  '__init__',\n",
       "  '__init_subclass__',\n",
       "  '__iter__',\n",
       "  '__le__',\n",
       "  '__len__',\n",
       "  '__lt__',\n",
       "  '__mul__',\n",
       "  '__ne__',\n",
       "  '__new__',\n",
       "  '__reduce__',\n",
       "  '__reduce_ex__',\n",
       "  '__repr__',\n",
       "  '__reversed__',\n",
       "  '__rmul__',\n",
       "  '__setattr__',\n",
       "  '__setitem__',\n",
       "  '__sizeof__',\n",
       "  '__str__',\n",
       "  '__subclasshook__',\n",
       "  'append',\n",
       "  'clear',\n",
       "  'copy',\n",
       "  'count',\n",
       "  'extend',\n",
       "  'index',\n",
       "  'insert',\n",
       "  'pop',\n",
       "  'remove',\n",
       "  'reverse',\n",
       "  'sort'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tree), dir(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(sent):\n",
    "    \n",
    "    words = nltk.word_tokenize(sent)\n",
    "    grammar = \"NP:{<DT>?<JJ>*<NN>}\"\n",
    "    Reg_parser = nltk.RegexpParser(grammar)\n",
    "    tree = Reg_parser.parse(nltk.pos_tag(words))\n",
    "    \n",
    "    strees = list(tree.subtrees())\n",
    "    for i in strees:\n",
    "        print(i.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('only', 'RB'), ('a', 'DT'), ('test', 'NN'), ('.', '.')]\n",
      "[('a', 'DT'), ('test', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "get_chunks(\"This is only a test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunks_using_parse_tree(conssentence, replace_with_dataset=True):\n",
    "    \n",
    "    conssentence = mask_entities(conssentence, replace_with_dataset)\n",
    "    doc = nlp(conssentence)\n",
    "    verb_subtree = []\n",
    "\n",
    "    for s in doc.sents:\n",
    "#         find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"technique\": [], \"task\": [], \"dataset\": [], \"material\": [], \"metric\": []}\n",
    "        find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"baseline\": [], \"task\": [], \n",
    "                               \"dataset\": [],  \"metric\": [], \"unfair\": [], \"fair\": []}\n",
    "\n",
    "        for tok in s:\n",
    "\n",
    "            if tok.text.lower().startswith(\"compar\"):\n",
    "                find_special_tokens[\"compar\"].append(tok)\n",
    "            else:\n",
    "                for k in sp_toks:\n",
    "                    if tok.text.lower().startswith(k):\n",
    "                        find_special_tokens[k].append(tok)\n",
    "                        break\n",
    "\n",
    "        verb_tokens = []\n",
    "        if find_special_tokens[\"compar\"]:\n",
    "            for t in find_special_tokens[\"compar\"]:\n",
    "#                     verb_subtree.append(t.subtree)\n",
    "                if t == s.root:\n",
    "                    simplified_sent = \"\"\n",
    "                    for chh in t.lefts:\n",
    "                        simplified_sent = simplified_sent + \" \" + chh.text\n",
    "                    simplified_sent = simplified_sent + \" \" + t.text\n",
    "                    for chh in t.rights:\n",
    "                        simplified_sent = simplified_sent + \" \" + chh.text\n",
    "#                         print(\"SIMP: \", simplified_sent)\n",
    "                    verb_subtree.append(simplified_sent)\n",
    "                else:\n",
    "                    verb_subtree.append(t.subtree)\n",
    "        else:\n",
    "            for k in sp_toks:\n",
    "                for i in find_special_tokens[k]:\n",
    "                    local_vt = []\n",
    "                    for j in i.ancestors:\n",
    "                        if j.pos_ == \"NOUN\":\n",
    "                            local_vt.append(j)\n",
    "                    if not local_vt:\n",
    "                        for j in i.ancestors:\n",
    "                            if j.pos_ == \"VERB\":\n",
    "                                local_vt.append(j)\n",
    "                    verb_tokens = verb_tokens + local_vt\n",
    "\n",
    "\n",
    "            for i in verb_tokens:\n",
    "                verb_subtree.append(i.subtree)\n",
    "\n",
    "    eecc = []\n",
    "    for i in verb_subtree:\n",
    "        if type(i) == str:\n",
    "            eecc.append(i)\n",
    "        else:\n",
    "            local_chunk = \"\"\n",
    "            for lcaltok in i:\n",
    "                local_chunk = local_chunk + \" \" + lcaltok.text\n",
    "            eecc.append(local_chunk)\n",
    "#     if not eecc:\n",
    "#         print(conssentence)\n",
    "    return list(set(eecc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' more large - scale experiments on image related tasks',\n",
       " ' the practicability of the method']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"It would be interesting to explore the practicability of the method on more large-scale experiments on image related tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' comparison to SOTA']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"The experimental validation is also not extensive since comparison to SOTA is not included.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_HyVxPsC9tm 938 2\n",
      "2019_HyVxPsC9tm 940 3\n",
      "2020_Byg79h4tvB 1272 [1] Conditional adversarial domain adaptation, Long et.al, in NeurIPS 2018\n",
      "[2] Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation, You et.al, in ICML 2019\n",
      "2018_HyHmGyZCZ 1425 2\n"
     ]
    }
   ],
   "source": [
    "roberta_vectors = defaultdict(dict)\n",
    "skip_uids = []\n",
    "\n",
    "for pid in gt_dict:\n",
    "    roberta_vectors[pid] = {}\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunk = \". \".join(mcomp_chunks_from_sent)\n",
    "            else:\n",
    "                final_chunk = df.loc[mcs][\"Sent\"]\n",
    "            \n",
    "            vec = embed_text_using_roberta(final_chunk.strip()).mean(1).detach().numpy()\n",
    "            roberta_vectors[pid][mcs] = vec / norm(vec)\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunk = \". \".join(mcomp_chunks_from_sent)\n",
    "            else:\n",
    "                final_chunk = df.loc[mcs][\"Sent\"]\n",
    "            \n",
    "            vec = embed_text_using_roberta(final_chunk.strip()).mean(1).detach().numpy()\n",
    "            roberta_vectors[pid][mcs] = vec / norm(vec)\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 1384\n"
     ]
    }
   ],
   "source": [
    "mcomp_sentences = {}\n",
    "not_mcomp_sentences = {}\n",
    "\n",
    "for pid in gt_dict:\n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            mcomp_sentences[mcs] = pid\n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            not_mcomp_sentences[mcs] = pid\n",
    "print(len(mcomp_sentences), len(not_mcomp_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_with_mcomp = defaultdict(dict)\n",
    "sim_with_not_mcomp = defaultdict(dict)\n",
    "sim_with_notmcomp_paper_sents = defaultdict(dict)\n",
    "\n",
    "mean_at_k = [\"1\", \"3\", \"5\", \"7\", \"10\", \"20\", \"30\", \"50\", \"100\", \"500\", \"1000\", \"1380\"]\n",
    "\n",
    "for sid in mcomp_sentences:\n",
    "    \n",
    "    # 1. With other mcomp sentences\n",
    "    temp_list = []    \n",
    "    for osid in mcomp_sentences:\n",
    "        if osid != sid:\n",
    "            temp_list.append(np.inner(roberta_vectors[mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "\n",
    "    \n",
    "    # 2. With other not_mcomp_sentences\n",
    "    temp_list = []\n",
    "    for osid in not_mcomp_sentences:\n",
    "        temp_list.append(np.inner(roberta_vectors[not_mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_not_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_not_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "\n",
    "    \n",
    "    # 3. With not_mcomp_sentences of the same paper\n",
    "    temp_list = []    \n",
    "    for osid in not_mcomp_sentences:\n",
    "        if not_mcomp_sentences[osid] == mcomp_sentences[sid]:\n",
    "            temp_list.append(np.inner(roberta_vectors[not_mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_notmcomp_paper_sents[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_notmcomp_paper_sents[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_sim_plot\n",
    "diff12 = {\"all\": []}\n",
    "for vv in mean_at_k:\n",
    "    diff12[str(vv)] = []\n",
    "\n",
    "diff13 = {\"all\": []}\n",
    "for vv in mean_at_k:\n",
    "    diff13[str(vv)] = []\n",
    "\n",
    "for sid in sim_with_mcomp:\n",
    "    diff12[\"all\"].append(sim_with_mcomp[sid][\"mean\"] - sim_with_not_mcomp[sid][\"mean\"])\n",
    "    diff13[\"all\"].append(sim_with_mcomp[sid][\"mean\"] - sim_with_notmcomp_paper_sents[sid][\"mean\"])\n",
    "    \n",
    "    for vv in mean_at_k:\n",
    "        diff12[str(vv)].append(sim_with_mcomp[sid][\"mean_{}\".format(vv)] - sim_with_not_mcomp[sid][\"mean_{}\".format(vv)])\n",
    "        diff13[str(vv)].append(sim_with_mcomp[sid][\"mean_{}\".format(vv)] - sim_with_notmcomp_paper_sents[sid][\"mean_{}\".format(vv)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analyse chunks after masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_chunks = {\"mcs\": [], \"nmcs\": []}\n",
    "\n",
    "for pid in gt_dict:\n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunk = \". \".join(mcomp_chunks_from_sent)\n",
    "            else:\n",
    "                final_chunk = df.loc[mcs][\"Sent\"]\n",
    "            \n",
    "            masked_chunks[\"mcs\"].append((df.loc[mcs][\"Sent\"], final_chunk))\n",
    "        except Exception as ex:\n",
    "            continue\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunk = \". \".join(mcomp_chunks_from_sent)\n",
    "            else:\n",
    "                final_chunk = df.loc[mcs][\"Sent\"]\n",
    "            masked_chunks[\"nmcs\"].append((df.loc[mcs][\"Sent\"], final_chunk))\n",
    "        except Exception as ex:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' comparison']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The authors propose k-DPP as an open loop oblivious to the evaluation of configurations method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search , uniform random search , low-discrepancy Sobol sequences , BO-TPE Bayesian optimization using tree-structured Parzen estimator by Bergstra et al 2011 .'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' comparison']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' comparison']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).',\n",
       "  ' comparison'),\n",
       " ('Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20) The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.',\n",
       "  ' a small number like 3 - 6 metric with a small k 20.  lies , authors do not compare against'),\n",
       " ('COMMENTS ON THE CHANGES SINCE THE LAST YEAR\\nI am not convinced by the comparison with Spearmint added by the authors since the previous version.',\n",
       "  ' the comparison with Spearmint added by the authors since the previous version')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_chunks[\"mcs\"][2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The authors propose k-DPP as an open loop oblivious to the evaluation of configurations method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search , uniform random search , low-discrepancy Sobol sequences , BO-TPE Bayesian optimization using tree-structured Parzen estimator by Bergstra et al 2011 .'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Third , the authors do not compare against some relevant , recent work , e.g. , Springenberg et al http : aad.informatik.uni-freiburg.de papers 16-NIPS-BOHamiANN.pdf and Snoek et al https : arxiv.org pdf 1502.05700.pdf that is essential for this kind of empirical study .'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mask_entities(sentence, replace_with_dataset=False):\n",
    "# #     cleaned_sent = re.sub('[^0-9a-zA-Z ]+', ' ', sentence)\n",
    "#     cleaned_sent = sentence\n",
    "#     while cleaned_sent.find(\"  \") > -1:\n",
    "#         cleaned_sent = cleaned_sent.replace(\"  \", \" \")\n",
    "    \n",
    "#     entities_found = []\n",
    "#     for i in entity_key_map:\n",
    "#         if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "#             entities_found.append(i)\n",
    "    \n",
    "#     entities_found.sort(key=lambda s: len(s))\n",
    "#     len_sorted_entities = entities_found.copy()\n",
    "    \n",
    "#     subset_entities = []\n",
    "#     # Remove subset entities (eg: Among cnn and 3-layer-cnn, prefer the latter)\n",
    "#     for fe in len_sorted_entities:\n",
    "#         for other_ent in len_sorted_entities:\n",
    "#             if fe != other_ent and other_ent.find(fe) > -1:\n",
    "#                 subset_entities.append(fe)\n",
    "#                 break\n",
    "#     for se in subset_entities:\n",
    "#         len_sorted_entities.remove(se)\n",
    "#     for maxents in len_sorted_entities:\n",
    "#         mask_name = entity_dict[entity_key_map[i]].lower()\n",
    "#         if replace_with_dataset:\n",
    "#             if mask_name == \"material\":\n",
    "#                 mask_name = \"dataset\"\n",
    "#         cleaned_sent = cleaned_sent.replace(maxents, mask_name)\n",
    "#     words_cleaned = nltk.word_tokenize(cleaned_sent)\n",
    "#     dups_removed = [v for i, v in enumerate(words_cleaned) if i == 0 or v != words_cleaned[i-1]]\n",
    "#     new_dup_removed_sent = \" \".join(dups_removed)\n",
    "#     return new_dup_removed_sent.strip()\n",
    "\n",
    "# #     #print(cleaned_sent)\n",
    "# #     for i in entity_key_map:\n",
    "# #         if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "# #             #print(\"Substituting ent: {} with mask: {}\".format(i, entity_dict[entity_key_map[i]].lower()))\n",
    "# #             cleaned_sent = cleaned_sent.replace(i, entity_dict[entity_key_map[i]].lower())\n",
    "# #     return cleaned_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse meaningful sentences that are more similar to NMCS in comparison to MCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_sim_with_mcomp = defaultdict(list)\n",
    "ana_sim_with_not_mcomp = defaultdict(list)\n",
    "\n",
    "\n",
    "mean_at_k = [\"1\", \"3\", \"5\", \"7\", \"10\", \"20\", \"30\", \"50\", \"100\", \"500\", \"1000\", \"1380\"]\n",
    "\n",
    "for sid in mcomp_sentences:\n",
    "    \n",
    "    # 1. With other mcomp sentences\n",
    "    temp_list = []    \n",
    "    for osid in mcomp_sentences:\n",
    "        if osid != sid:\n",
    "            temp_list.append((osid, np.inner(roberta_vectors[mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0]))\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, key=lambda x: x[1], reverse=True)\n",
    "    ana_sim_with_mcomp[sid] = sorted_temp_list\n",
    "\n",
    "    \n",
    "    # 2. With other not_mcomp_sentences\n",
    "    temp_list = []\n",
    "    for osid in not_mcomp_sentences:\n",
    "        temp_list.append((osid, np.inner(roberta_vectors[not_mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0]))\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, key=lambda x: x[1], reverse=True)\n",
    "    ana_sim_with_not_mcomp[sid] = sorted_temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_sentences_at_k = defaultdict(list)\n",
    "unproblematic_sentences_at_k = defaultdict(list)\n",
    "vv = 1\n",
    "\n",
    "for sid in sim_with_mcomp:\n",
    "    sim_diff = (sim_with_mcomp[sid][\"mean_{}\".format(vv)] - sim_with_not_mcomp[sid][\"mean_{}\".format(vv)])\n",
    "    if sim_diff < 0:\n",
    "        problematic_sentences_at_k[vv].append((sid,-1.0* sim_diff))\n",
    "    else:\n",
    "        unproblematic_sentences_at_k[vv].append((sid, sim_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(113, 0.1638484001159668),\n",
       " (1464, 0.12271469831466675),\n",
       " (931, 0.1226879358291626),\n",
       " (950, 0.12097209692001343),\n",
       " (1318, 0.10917872190475464)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 1\n",
    "sorted_problematic_sentences_at_1 = sorted(problematic_sentences_at_k[k], key=lambda x: x[1], reverse=True)\n",
    "sorted(problematic_sentences_at_k[k], key=lambda x: x[1], reverse=True)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 1\n",
    "# sorted_problematic_sentences_at_3 = sorted(problematic_sentences_at_k[3], key=lambda x: x[1], reverse=True)\n",
    "# sorted(problematic_sentences_at_k[3], key=lambda x: x[1], reverse=True)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_problematic_sentences_at_3[0:3], sorted_problematic_sentences_at_3[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_unproblematic_sentences_at_3 = sorted(unproblematic_sentences_at_k[3], key=lambda x: x[1], reverse=True)\n",
    "# sorted_unproblematic_sentences_at_3[0:4], sorted_unproblematic_sentences_at_3[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sent:  The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM.\n",
      "\n",
      "Meaningful comparison sentences: \n",
      "[(124, 0.74430156)]\n",
      "What is left is the gated incremental pooling operation; but to show that this operation is beneficial when added to autoregressive CNNs, a thorough comparison with an autoregressive CNN baseline is necessary.\n",
      "\n",
      "Non Meaningful comparison sentence: \n",
      "[(842, 0.8440055)]\n",
      "Paper Weaknesses:\n",
      "- The evaluation of the model is not great: (1) It would be interesting to combine bedroom and kitchen images and train jointly to see what it learns.\n",
      "\n",
      "\n",
      "\n",
      "Test sent:  The paper does not consider the more recent and highly relevant Moosavi-Dezfooli et al “Universal Adversarial Perturbations” CVPR 2017.\n",
      "\n",
      "Meaningful comparison sentences: \n",
      "[(1202, 0.5903547)]\n",
      "- I am concerned about whether the proposed method works well with harder datasets such as Office-Home dataset, because each class data are modeled by a simple Gaussian distribution in the proposed method.\n",
      "\n",
      "Non Meaningful comparison sentence: \n",
      "[(989, 0.6897952)]\n",
      "This paper is interesting since most of the existing works focus on Monte Carlo variational inference.\n",
      "\n",
      "\n",
      "\n",
      "Test sent:  Indeed, unsurprisingly, the authors note that \"the probability of correctly labelling a word as a mistake remains low (62%)\" - this admittedly beats a random-chance baseline, but is not compared to something more meaningful, such as simply contrasting the existing system with a more powerful convolutional model and labelling all discrepancies as mistakes.\n",
      "\n",
      "Meaningful comparison sentences: \n",
      "[(869, 0.60373676)]\n",
      "An additional problem is that performance is not compared to any external prior work.\n",
      "\n",
      "Non Meaningful comparison sentence: \n",
      "[(732, 0.69845736)]\n",
      "The preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches.\n",
      "\n",
      "\n",
      "\n",
      "Test sent:  - State of the art is not well-studied in the paper.\n",
      "\n",
      "Meaningful comparison sentences: \n",
      "[(110, 0.63996166)]\n",
      "On the negative side, the authors present the results without fully referencing and acknowledging state-of-the-art.\n",
      "\n",
      "Non Meaningful comparison sentence: \n",
      "[(230, 0.7335436)]\n",
      "The network analysed here does not reach the state-of-the-art on MNIST from almost two decades ago.\n",
      "\n",
      "\n",
      "\n",
      "Test sent:  Minor comments:\n",
      "- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.\n",
      "\n",
      "Meaningful comparison sentences: \n",
      "[(615, 0.64030004)]\n",
      "Overall, the very strong improvements on the text modeling task over NVDM seem hard to understand, and I would like to see an ablation analysis of all the differences between that model and the proposed one.\n",
      "\n",
      "Non Meaningful comparison sentence: \n",
      "[(1388, 0.729555)]\n",
      "2)What is the value of k in Figure 3 and Figure 4?\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in sorted_problematic_sentences_at_1[5:10]:\n",
    "    print(\"Test sent: \", df.loc[s[0]][\"Sent\"])\n",
    "    \n",
    "    print(\"\\nMeaningful comparison sentences: \")\n",
    "    print(ana_sim_with_mcomp[s[0]][0:1])\n",
    "    for i in ana_sim_with_mcomp[s[0]][0:1]:\n",
    "        print(df.loc[i[0]][\"Sent\"])\n",
    "    \n",
    "    print(\"\\nNon Meaningful comparison sentence: \")\n",
    "    print(ana_sim_with_not_mcomp[s[0]][0:1])\n",
    "    for i in ana_sim_with_not_mcomp[s[0]][0:1]:\n",
    "        print(df.loc[i[0]][\"Sent\"])\n",
    "    \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Minor comments : - I believe one should not compare the metric shown between the left and right columns of Figure 3 as they are obtained from two different models .'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"Minor comments:- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' one should not compare the metric shown between the left and right columns of Figure 3 as they are obtained from two different models']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"Minor comments:- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Although I do like the paper on the whole , to really convince me that main objective -- ie that iterative improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular , to show that an metric scheme can really improve over a system closely matched to the attention-based model , both when used in isolation and when used in system combination with a PBMT system , and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model .'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"Although I do like the paper on the whole, to really convince me that main objective -- ie that **iterative** improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' an metric scheme']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"Although I do like the paper on the whole, to really convince me that main objective -- ie that **iterative** improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In summary , while I think the paper is interesting , I suspect that the applicability of this technique is possibly limited at present , and I m unsure how much we can really read into the findings of the paper when the experiments are based on MNIST alone .'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"In summary, while I think the paper is interesting, I suspect that the applicability of this technique is possibly limited at present, and I'm unsure how much we can really read into the findings of the paper when the experiments are based on MNIST alone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' to compare a label - noise semi - supervised method with other label - noise only methods',\n",
       " ' perturbation consistency or other semi - supervised metric']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"However, it's not completely fair to compare a label-noise + semi-supervised method with other label-noise only methods... As a matter of fact, you don't need to apply perturbation consistency (or other semi-supervised) regularization after identifying the training data with incorrect labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
