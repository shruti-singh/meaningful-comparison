{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Feature Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"InputTestSet-Reviews48_Ann_NEW.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Sent</th>\n",
       "      <th>MComp</th>\n",
       "      <th>Cat</th>\n",
       "      <th>SubCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The authors propose to use k-DPP to select a s...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>This paper covers the related work nicely, wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The rest of the paper are also clearly written.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>However, I have some concerns about the propos...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>- It is not clear how to define the kernel, th...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID              PID     Dec  \\\n",
       "0    0  2019_SJf_XhCqKm  Reject   \n",
       "1    1  2019_SJf_XhCqKm  Reject   \n",
       "2    2  2019_SJf_XhCqKm  Reject   \n",
       "3    3  2019_SJf_XhCqKm  Reject   \n",
       "4    4  2019_SJf_XhCqKm  Reject   \n",
       "\n",
       "                                                Sent  MComp  Cat SubCat  \n",
       "0  The authors propose to use k-DPP to select a s...      0  NaN    NaN  \n",
       "1  This paper covers the related work nicely, wit...      0  NaN    NaN  \n",
       "2    The rest of the paper are also clearly written.      0  NaN    NaN  \n",
       "3  However, I have some concerns about the propos...      0  NaN    NaN  \n",
       "4  - It is not clear how to define the kernel, th...      0  NaN    NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dict = {}\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    if not pid in gt_dict:\n",
    "        gt_dict[pid] = {\"dec\": df.loc[i][\"Dec\"], \"mcomp\": set(), \"not_mcomp\": set()}\n",
    "    if df.loc[i][\"MComp\"] == 1:\n",
    "        gt_dict[pid][\"mcomp\"].add(df.loc[i][\"UID\"])\n",
    "    else:\n",
    "        gt_dict[pid][\"not_mcomp\"].add(df.loc[i][\"UID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accept': [49, 643], 'Reject': [68, 745]}\n"
     ]
    }
   ],
   "source": [
    "stats_dict = {\"Accept\": [0, 0], \"Reject\": [0, 0]}\n",
    "\n",
    "for k, v in gt_dict.items():\n",
    "    #print(len(v[\"mcomp\"]), len(v[\"not_mcomp\"]), v[\"dec\"])\n",
    "    stats_dict[v[\"dec\"]][0] += len(v[\"mcomp\"])\n",
    "    stats_dict[v[\"dec\"]][1] += len(v[\"not_mcomp\"])\n",
    "    \n",
    "print(stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestSet length: 32\n",
      " ['2019_SJf_XhCqKm', '2017_Bk0MRI5lg', '2020_SyevYxHtDB', '2018_rJBiunlAW', '2020_rkltE0VKwH', '2018_Hki-ZlbA-', '2019_BJx0sjC5FX', '2020_r1e_FpNFDr', '2020_B1lsXREYvr', '2018_SkZxCk-0Z', '2019_rJzoujRct7', '2018_HkfXMz-Ab', '2017_BJ9fZNqle', '2019_SyxZJn05YX', '2017_B1ckMDqlg', '2017_HJ0NvFzxl', '2017_S1_pAu9xl', '2018_SyYYPdg0-', '2017_BJAA4wKxg', '2019_HyVxPsC9tm', '2019_HylTBhA5tQ', '2019_B1l08oAct7', '2018_H135uzZ0-', '2017_H1oyRlYgg', '2017_r1y1aawlg', '2020_r1eX1yrKwB', '2020_Byg79h4tvB', '2019_H1lFZnR5YX', '2020_BkeWw6VFwr', '2018_HyHmGyZCZ', '2018_HyUNwulC-', '2020_HkgsPhNYPS']\n"
     ]
    }
   ],
   "source": [
    "test_set = list(gt_dict.keys())\n",
    "print(\"TestSet length: %d\\n\"%len(test_set), test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_SJf_XhCqKm     {39, 17, 27, 28, 30}\n",
      "2017_Bk0MRI5lg      {57}\n",
      "2020_SyevYxHtDB     {76, 87}\n",
      "2018_rJBiunlAW      {108, 110, 112, 113, 124, 126}\n",
      "2020_rkltE0VKwH     {160, 184, 155, 157, 159}\n",
      "2018_Hki-ZlbA-      {267, 235, 236, 271}\n",
      "2019_BJx0sjC5FX     {292, 287}\n",
      "2020_r1e_FpNFDr     {312, 322, 315, 308}\n",
      "2020_B1lsXREYvr     {376, 401}\n",
      "2018_SkZxCk-0Z      {449, 443, 445, 486}\n",
      "2019_rJzoujRct7     {518, 519}\n",
      "2018_HkfXMz-Ab      {573, 566}\n",
      "2017_BJ9fZNqle      {627, 623, 615}\n",
      "2019_SyxZJn05YX     {672, 673, 657, 669, 671}\n",
      "2017_B1ckMDqlg      {714, 707}\n",
      "2017_HJ0NvFzxl      {739}\n",
      "2017_S1_pAu9xl      {806, 808, 809, 810, 792}\n",
      "2018_SyYYPdg0-      {834, 867, 868, 869, 870, 872, 873, 844, 830}\n",
      "2017_BJAA4wKxg      {884}\n",
      "2019_HyVxPsC9tm     {931, 933, 905, 909, 912, 913, 919, 926}\n",
      "2019_HylTBhA5tQ     {972, 950}\n",
      "2019_B1l08oAct7     {994, 996, 1064, 1004, 1007, 1044, 1047, 1048, 1055}\n",
      "2018_H135uzZ0-      {1072, 1079}\n",
      "2017_H1oyRlYgg      set()\n",
      "2017_r1y1aawlg      {1125, 1162, 1100, 1102, 1168}\n",
      "2020_r1eX1yrKwB     {1177, 1202, 1212}\n",
      "2020_Byg79h4tvB     {1243, 1268}\n",
      "2019_H1lFZnR5YX     {1281, 1316, 1284, 1318, 1289, 1331, 1333, 1279}\n",
      "2020_BkeWw6VFwr     {1347, 1373}\n",
      "2018_HyHmGyZCZ      {1406, 1421, 1390, 1426}\n",
      "2018_HyUNwulC-      {1451, 1452}\n",
      "2020_HkgsPhNYPS     {1504, 1464, 1497, 1500, 1502}\n"
     ]
    }
   ],
   "source": [
    "for k in test_set:\n",
    "    print('{:20}{}'.format(k, gt_dict[k][\"mcomp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_for_test = defaultdict(list)\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    test_sent_raw = str(df.loc[i][\"Sent\"])\n",
    "    \n",
    "    # Replace URLs with [URL]\n",
    "    test_sent_raw = re.sub(r'http[s]?://[a-zA-z\\.\\-/0-9~]*', \"[URL]\", test_sent_raw)\n",
    "    test_sent_raw = re.sub(r'papers.nips.cc/paper/[a-zA-z\\.\\-/0-9~]*', \"[URL]\", test_sent_raw)\n",
    "    test_sent_raw = re.sub(r'arxiv.org/[a-zA-z\\.\\-/0-9~]*', \"[URL]\", test_sent_raw)\n",
    "    \n",
    "    sents_for_test[pid].append((df.loc[i][\"UID\"], test_sent_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Sent</th>\n",
       "      <th>MComp</th>\n",
       "      <th>Cat</th>\n",
       "      <th>SubCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>243</td>\n",
       "      <td>2020_ryen_CEFwr</td>\n",
       "      <td>Reject</td>\n",
       "      <td>It extends this approach by introducing an add...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179</td>\n",
       "      <td>2018_H1LAqMbRW</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Experimentally, the results are rather weak co...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157</td>\n",
       "      <td>2017_HyTqHL5xg</td>\n",
       "      <td>Accept</td>\n",
       "      <td>The experiments are interesting but I'm still ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>146</td>\n",
       "      <td>2017_HyTqHL5xg</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Section 2.2 says they do the latter in the int...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>2017_ByToKu9ll</td>\n",
       "      <td>Reject</td>\n",
       "      <td>4)This paper proposed an improved version of t...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID              PID     Dec  \\\n",
       "0  243  2020_ryen_CEFwr  Reject   \n",
       "1  179   2018_H1LAqMbRW  Reject   \n",
       "2  157   2017_HyTqHL5xg  Accept   \n",
       "3  146   2017_HyTqHL5xg  Accept   \n",
       "4   90   2017_ByToKu9ll  Reject   \n",
       "\n",
       "                                                Sent  MComp   Cat SubCat  \n",
       "0  It extends this approach by introducing an add...      0  None   None  \n",
       "1  Experimentally, the results are rather weak co...      0  None   None  \n",
       "2  The experiments are interesting but I'm still ...      0  None   None  \n",
       "3  Section 2.2 says they do the latter in the int...      0  None   None  \n",
       "4  4)This paper proposed an improved version of t...      0  None   None  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_excel(\"InputTrainSet-Reviews7_Ann.xlsx\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sets = {\"mcomp\": [], \"non_mcomp\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, df_train.shape[0]):\n",
    "    pid = df_train.loc[i][\"PID\"]\n",
    "    train_sent_raw = str(df_train.loc[i][\"Sent\"])\n",
    "    \n",
    "    type_comp = df_train.loc[i][\"MComp\"]\n",
    "    \n",
    "    if type_comp == 1:\n",
    "        train_sets[\"mcomp\"].append(train_sent_raw)\n",
    "    else:\n",
    "        train_sets[\"non_mcomp\"].append(train_sent_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 270)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sets[\"mcomp\"]), len(train_sets[\"non_mcomp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"entities_dict_smaller\", \"r\") as f:\n",
    "    entity_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Material', 'Method', 'Metric', 'Task'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(entity_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('convolutional neural networks', 'Method'),\n",
       " ('convnets', 'Method'),\n",
       " ('recognition', 'Task'),\n",
       " ('visual recognition tasks', 'Task'),\n",
       " ('age estimation', 'Task'),\n",
       " ('head pose estimation', 'Task'),\n",
       " ('multi - label classification', 'Task'),\n",
       " ('semantic segmentation', 'Task'),\n",
       " ('classification', 'Task'),\n",
       " ('deep convnets', 'Method'),\n",
       " ('dldl', 'Method'),\n",
       " ('feature learning', 'Task'),\n",
       " ('deep learning', 'Method'),\n",
       " ('image classification', 'Task'),\n",
       " ('deep learning methods', 'Method'),\n",
       " ('image classification tasks', 'Task'),\n",
       " ('human pose estimation', 'Task'),\n",
       " ('convnet', 'Method'),\n",
       " ('recognition tasks', 'Task'),\n",
       " ('ensemble', 'Method')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_dict.items())[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1784\n"
     ]
    }
   ],
   "source": [
    "entity_key_map = {}\n",
    "for i in entity_dict:\n",
    "    s = re.sub('[^0-9a-zA-Z,:;.?!\\- ]+', '', i)\n",
    "    while s.find(\"  \") > -1:\n",
    "        s = s.replace(\"  \", \" \")\n",
    "    if len(s) > 2:\n",
    "        cl = re.sub('[^0-9a-zA-Z ]+', '', i)\n",
    "        while cl.find(\"  \") > -1:\n",
    "            cl = cl.replace(\"  \", \" \")\n",
    "        entity_key_map[cl.strip()] = i\n",
    "print(len(entity_key_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    }
   ],
   "source": [
    "coun = 0\n",
    "for i in entity_dict:\n",
    "    if len(i) < 5:\n",
    "        coun +=1\n",
    "#         print(i)\n",
    "print(coun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('convolutional neural networks', 'convolutional neural networks'),\n",
       " ('convnets', 'convnets'),\n",
       " ('recognition', 'recognition'),\n",
       " ('visual recognition tasks', 'visual recognition tasks'),\n",
       " ('age estimation', 'age estimation')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_key_map.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Material': 165, 'Method': 1191, 'Metric': 158, 'Task': 289})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(entity_dict.values())\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(c)\n",
    "reverse_map = defaultdict(list)\n",
    "\n",
    "for k, v in entity_dict.items():\n",
    "    reverse_map[v].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in reverse_map[\"Task\"]:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"MNIST\" in entity_key_map, \"mnist\" in entity_key_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. RoBERTa trained on SciLit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\u001b[0m\n",
      "tokenizers (0.7.0)\n",
      "transformers (2.10.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 list | grep -E 'transformers|tokenizers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75099f4042634d43a6f97ef1ec1e8cd3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_using_roberta(text):\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mask_entities(sentence, replace_with_dataset=True):\n",
    "#     cleaned_sent = re.sub('[^0-9a-zA-Z,:;.?!\\- ]+', ' ', sentence)\n",
    "#     while cleaned_sent.find(\"  \") > -1:\n",
    "#         cleaned_sent = cleaned_sent.replace(\"  \", \" \")\n",
    "    \n",
    "#     entity_key_map_keys = list(entity_key_map.keys()) # As we will be dunamically adding entries to this dict an dthat will throw an error.\n",
    "#     entities_found = []\n",
    "#     for i in entity_key_map_keys:\n",
    "#         if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "#             entities_found.append(i)\n",
    "#         elif cleaned_sent.lower().find(\" \" + i + \" \") > -1:\n",
    "#             found_idx = cleaned_sent.lower().find(\" \" + i + \" \")\n",
    "#             entity_dict[cleaned_sent[found_idx:found_idx+len(\" \" + i + \" \")]] = entity_dict[i]\n",
    "#             entity_key_map[cleaned_sent[found_idx:found_idx+len(\" \" + i + \" \")]] = entity_key_map[i]\n",
    "    \n",
    "#     entities_found.sort(key=lambda s: len(s))\n",
    "#     len_sorted_entities = entities_found.copy()\n",
    "    \n",
    "#     subset_entities = []\n",
    "#     # Remove subset entities (eg: Among cnn and 3-layer-cnn, prefer the latter)\n",
    "#     for fe in len_sorted_entities:\n",
    "#         for other_ent in len_sorted_entities:\n",
    "#             if fe != other_ent and other_ent.find(fe) > -1:\n",
    "#                 subset_entities.append(fe)\n",
    "#                 break\n",
    "#     for se in subset_entities:\n",
    "#         len_sorted_entities.remove(se)\n",
    "#     for maxents in len_sorted_entities:\n",
    "#         mask_name = \" \" + entity_dict[entity_key_map[i]].lower() + \" \"\n",
    "#         if replace_with_dataset:\n",
    "#             if mask_name == \" material \":\n",
    "#                 mask_name = \" dataset \"\n",
    "#         cleaned_sent = cleaned_sent.replace(\" \" + maxents + \" \", mask_name)\n",
    "#     words_cleaned = nltk.word_tokenize(cleaned_sent)\n",
    "#     dups_removed = [v for i, v in enumerate(words_cleaned) if i == 0 or v != words_cleaned[i-1]]\n",
    "#     new_dup_removed_sent = \" \".join(dups_removed)\n",
    "#     return new_dup_removed_sent.strip()\n",
    "\n",
    "# #     #print(cleaned_sent)\n",
    "# #     for i in entity_key_map:\n",
    "# #         if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "# #             #print(\"Substituting ent: {} with mask: {}\".format(i, entity_dict[entity_key_map[i]].lower()))\n",
    "# #             cleaned_sent = cleaned_sent.replace(i, entity_dict[entity_key_map[i]].lower())\n",
    "# #     return cleaned_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "sp_toks = [\"result\", \"method\", \"task\", \"dataset\", \"metric\", \"baseline\", \"fair\", \"unfair\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunks_using_spacy_dp(conssentence, replace_with_dataset=True):\n",
    "    return []\n",
    "# #     conssentence = mask_entities(conssentence, replace_with_dataset)\n",
    "# #     print(conssentence)\n",
    "#     doc = nlp(conssentence)\n",
    "#     verb_subtree = []\n",
    "\n",
    "#     for s in doc.sents:\n",
    "# #         find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"technique\": [], \"task\": [], \"dataset\": [], \"material\": [], \"metric\": []}\n",
    "#         find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"baseline\": [], \"task\": [], \n",
    "#                                \"dataset\": [],  \"metric\": [], \"unfair\": [], \"fair\": []}\n",
    "\n",
    "#         for tok in s:\n",
    "\n",
    "#             if tok.text.lower().startswith(\"compar\"):\n",
    "#                 find_special_tokens[\"compar\"].append(tok)\n",
    "#             else:\n",
    "#                 for k in sp_toks:\n",
    "#                     if tok.text.lower().startswith(k):\n",
    "#                         find_special_tokens[k].append(tok)\n",
    "#                         break\n",
    "\n",
    "#         verb_tokens = []\n",
    "#         if find_special_tokens[\"compar\"]:\n",
    "#             for t in find_special_tokens[\"compar\"]:\n",
    "# #                     verb_subtree.append(t.subtree)\n",
    "#                 if t == s.root:\n",
    "#                     simplified_sent = \"\"\n",
    "#                     for chh in t.lefts:\n",
    "#                         simplified_sent = simplified_sent + \" \" + chh.text\n",
    "#                     simplified_sent = simplified_sent + \" \" + t.text\n",
    "#                     for chh in t.rights:\n",
    "#                         simplified_sent = simplified_sent + \" \" + chh.text\n",
    "# #                         print(\"SIMP: \", simplified_sent)\n",
    "#                     verb_subtree.append(simplified_sent)\n",
    "#                 else:\n",
    "#                     verb_subtree.append(t.subtree)\n",
    "#         else:\n",
    "#             for k in sp_toks:\n",
    "#                 for i in find_special_tokens[k]:\n",
    "#                     local_vt = []\n",
    "#                     for j in i.ancestors:\n",
    "#                         if j.pos_ == \"NOUN\":\n",
    "#                             local_vt.append(j)\n",
    "#                     if not local_vt:\n",
    "#                         for j in i.ancestors:\n",
    "#                             if j.pos_ == \"VERB\":\n",
    "#                                 local_vt.append(j)\n",
    "#                     verb_tokens = verb_tokens + local_vt\n",
    "\n",
    "\n",
    "#             for i in verb_tokens:\n",
    "#                 verb_subtree.append(i.subtree)\n",
    "\n",
    "#     eecc = []\n",
    "#     for i in verb_subtree:\n",
    "#         if type(i) == str:\n",
    "#             eecc.append(i)\n",
    "#         else:\n",
    "#             local_chunk = \"\"\n",
    "#             for lcaltok in i:\n",
    "#                 local_chunk = local_chunk + \" \" + lcaltok.text\n",
    "#             eecc.append(local_chunk)\n",
    "# #     if not eecc:\n",
    "# #         print(conssentence)\n",
    "#     return list(set(eecc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing vectors of the initial training pool of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool_roberta_vecs = {\"mcomp\": [], \"non_mcomp\": []}\n",
    "single_train_pool_roberta_vecs = {\"mcomp\": [], \"non_mcomp\": []}\n",
    "train_pool_uid_vecs = defaultdict(list)\n",
    "mc_nmc_fake = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_idx = 0\n",
    "\n",
    "for i in train_sets[\"mcomp\"]:\n",
    "    mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(i)\n",
    "    if mcomp_chunks_from_sent:\n",
    "        final_chunks = mcomp_chunks_from_sent\n",
    "    else:\n",
    "        final_chunks = [i]\n",
    "    \n",
    "    mc_nmc_fake[fake_idx] = 1\n",
    "    for single_chunk in final_chunks:\n",
    "        vec = embed_text_using_roberta(single_chunk.strip()).mean(1).detach().numpy()\n",
    "        train_pool_uid_vecs[fake_idx].append(vec / norm(vec))\n",
    "        train_pool_roberta_vecs[\"mcomp\"].append(vec/norm(vec))\n",
    "    \n",
    "    collated_chunk = \" \".join(final_chunks)\n",
    "    vec = embed_text_using_roberta(collated_chunk.strip()).mean(1).detach().numpy()\n",
    "    single_train_pool_roberta_vecs[\"mcomp\"].append(vec/norm(vec))\n",
    "    fake_idx += 1\n",
    "\n",
    "\n",
    "for i in train_sets[\"non_mcomp\"]:\n",
    "    mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(i)\n",
    "    if mcomp_chunks_from_sent:\n",
    "        final_chunks = mcomp_chunks_from_sent\n",
    "    else:\n",
    "        final_chunks = [i]\n",
    "    \n",
    "    mc_nmc_fake[fake_idx] = 0\n",
    "    for single_chunk in final_chunks:\n",
    "        vec = embed_text_using_roberta(single_chunk.strip()).mean(1).detach().numpy()\n",
    "        train_pool_uid_vecs[fake_idx].append(vec / norm(vec))\n",
    "        train_pool_roberta_vecs[\"non_mcomp\"].append(vec/norm(vec))\n",
    "    \n",
    "    collated_chunk = \" \".join(final_chunks)\n",
    "    vec = embed_text_using_roberta(collated_chunk.strip()).mean(1).detach().numpy()\n",
    "    single_train_pool_roberta_vecs[\"non_mcomp\"].append(vec/norm(vec))\n",
    "    fake_idx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_HyVxPsC9tm 938 2\n",
      "2019_HyVxPsC9tm 940 3\n",
      "2018_HyHmGyZCZ 1425 2\n"
     ]
    }
   ],
   "source": [
    "roberta_vectors = defaultdict(dict)\n",
    "\n",
    "skip_uids = []\n",
    "\n",
    "for pid in gt_dict:\n",
    "    roberta_vectors[pid] = {}\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunks = mcomp_chunks_from_sent\n",
    "            else:\n",
    "                final_chunks = [df.loc[mcs][\"Sent\"]]\n",
    "            \n",
    "            roberta_vectors[pid][mcs] = []\n",
    "            for single_chunk in final_chunks:\n",
    "                vec = embed_text_using_roberta(single_chunk.strip()).mean(1).detach().numpy()\n",
    "                roberta_vectors[pid][mcs].append(vec / norm(vec))\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunks = mcomp_chunks_from_sent\n",
    "            else:\n",
    "                final_chunks = [df.loc[mcs][\"Sent\"]]\n",
    "            \n",
    "            roberta_vectors[pid][mcs] = []\n",
    "            for single_chunk in final_chunks:\n",
    "                vec = embed_text_using_roberta(single_chunk.strip()).mean(1).detach().numpy()\n",
    "                roberta_vectors[pid][mcs].append(vec / norm(vec))\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 1385\n"
     ]
    }
   ],
   "source": [
    "mcomp_sentences = {}\n",
    "not_mcomp_sentences = {}\n",
    "\n",
    "for pid in gt_dict:\n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            mcomp_sentences[mcs] = pid\n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            not_mcomp_sentences[mcs] = pid\n",
    "print(len(mcomp_sentences), len(not_mcomp_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1505, 769)\n"
     ]
    }
   ],
   "source": [
    "testdf = df.copy()\n",
    "\n",
    "xtest = testdf.drop(columns=[\"PID\", \"Dec\", \"MComp\", \"Sent\", \"Cat\", \"SubCat\"])\n",
    "ytest = testdf.drop(columns=[\"PID\", \"Dec\", \"Sent\", \"Cat\", \"SubCat\"])\n",
    "# print(xtest.head())\n",
    "# print(ytest.head())\n",
    "\n",
    "for i in range(1, 769):\n",
    "    xtest[i] = np.nan\n",
    "\n",
    "for pid in gt_dict:\n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            xtest.iloc[mcs] = [mcs] + list(np.mean(roberta_vectors[pid][mcs], axis=0)[0])\n",
    "        else:\n",
    "            xtest.iloc[mcs] = [mcs] + list(np.zeros(768))\n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            xtest.iloc[mcs] = [mcs] + list(np.mean(roberta_vectors[pid][mcs], axis=0)[0])\n",
    "        else:\n",
    "            xtest.iloc[mcs] = [mcs] + list(np.zeros(768))\n",
    "print(xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.042263</td>\n",
       "      <td>-0.009747</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>-0.023304</td>\n",
       "      <td>0.002282</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.041309</td>\n",
       "      <td>-0.005554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008538</td>\n",
       "      <td>-0.016800</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>-0.038548</td>\n",
       "      <td>-0.027814</td>\n",
       "      <td>-0.013663</td>\n",
       "      <td>0.022429</td>\n",
       "      <td>0.018204</td>\n",
       "      <td>0.005023</td>\n",
       "      <td>0.015423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.032411</td>\n",
       "      <td>0.013325</td>\n",
       "      <td>0.034656</td>\n",
       "      <td>0.016165</td>\n",
       "      <td>0.014038</td>\n",
       "      <td>-0.014843</td>\n",
       "      <td>-0.010618</td>\n",
       "      <td>0.001917</td>\n",
       "      <td>-0.010002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036052</td>\n",
       "      <td>-0.017437</td>\n",
       "      <td>-0.026445</td>\n",
       "      <td>-0.029300</td>\n",
       "      <td>-0.024152</td>\n",
       "      <td>-0.013721</td>\n",
       "      <td>0.035461</td>\n",
       "      <td>0.033084</td>\n",
       "      <td>-0.009003</td>\n",
       "      <td>0.026205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID         1         2         3         4         5         6         7  \\\n",
       "0    0  0.042263 -0.009747  0.003180 -0.005164 -0.023304  0.002282  0.001261   \n",
       "1    1  0.032411  0.013325  0.034656  0.016165  0.014038 -0.014843 -0.010618   \n",
       "\n",
       "          8         9  ...       759       760       761       762       763  \\\n",
       "0  0.041309 -0.005554  ...  0.008538 -0.016800  0.000684 -0.038548 -0.027814   \n",
       "1  0.001917 -0.010002  ...  0.036052 -0.017437 -0.026445 -0.029300 -0.024152   \n",
       "\n",
       "        764       765       766       767       768  \n",
       "0 -0.013663  0.022429  0.018204  0.005023  0.015423  \n",
       "1 -0.013721  0.035461  0.033084 -0.009003  0.026205  \n",
       "\n",
       "[2 rows x 769 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296, 769)\n",
      "(296, 2)\n"
     ]
    }
   ],
   "source": [
    "# xtrain = pd.DataFrame(columns=[\"UID\"]+[str(x) for x in range(1,769)])\n",
    "# ytrain = pd.DataFrame(columns=[\"UID\", \"MComp\"])\n",
    "xtlist = []\n",
    "ytlist = []\n",
    "\n",
    "for i in train_pool_uid_vecs:\n",
    "    xtlist.append([i] + list(np.mean(train_pool_uid_vecs[i], axis=0)[0]))\n",
    "    ytlist.append([i, mc_nmc_fake[i]])\n",
    "\n",
    "xtrain = pd.DataFrame(xtlist)\n",
    "ytrain = pd.DataFrame(ytlist)\n",
    "\n",
    "xtrain = xtrain.rename(columns={0: 'UID'})\n",
    "ytrain = ytrain.rename(columns={0: 'UID', 1: 'MComp'})\n",
    "\n",
    "print(xtrain.shape)\n",
    "print(ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.042346</td>\n",
       "      <td>-0.006763</td>\n",
       "      <td>-0.005576</td>\n",
       "      <td>-0.008125</td>\n",
       "      <td>0.008589</td>\n",
       "      <td>-0.025705</td>\n",
       "      <td>0.015792</td>\n",
       "      <td>0.026114</td>\n",
       "      <td>-0.011291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>-0.022650</td>\n",
       "      <td>-0.049733</td>\n",
       "      <td>-0.013790</td>\n",
       "      <td>0.019725</td>\n",
       "      <td>-0.004193</td>\n",
       "      <td>0.030404</td>\n",
       "      <td>0.016235</td>\n",
       "      <td>0.017051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.041915</td>\n",
       "      <td>0.013722</td>\n",
       "      <td>-0.018442</td>\n",
       "      <td>0.004288</td>\n",
       "      <td>-0.012606</td>\n",
       "      <td>-0.009582</td>\n",
       "      <td>-0.003896</td>\n",
       "      <td>0.033524</td>\n",
       "      <td>0.013773</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001821</td>\n",
       "      <td>0.005968</td>\n",
       "      <td>-0.013871</td>\n",
       "      <td>-0.024714</td>\n",
       "      <td>-0.029293</td>\n",
       "      <td>-0.006079</td>\n",
       "      <td>-0.003119</td>\n",
       "      <td>0.034447</td>\n",
       "      <td>0.012729</td>\n",
       "      <td>-0.010659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID         1         2         3         4         5         6         7  \\\n",
       "0    0  0.042346 -0.006763 -0.005576 -0.008125  0.008589 -0.025705  0.015792   \n",
       "1    1  0.041915  0.013722 -0.018442  0.004288 -0.012606 -0.009582 -0.003896   \n",
       "\n",
       "          8         9  ...       759       760       761       762       763  \\\n",
       "0  0.026114 -0.011291  ...  0.001838  0.000548 -0.022650 -0.049733 -0.013790   \n",
       "1  0.033524  0.013773  ... -0.001821  0.005968 -0.013871 -0.024714 -0.029293   \n",
       "\n",
       "        764       765       766       767       768  \n",
       "0  0.019725 -0.004193  0.030404  0.016235  0.017051  \n",
       "1 -0.006079 -0.003119  0.034447  0.012729 -0.010659  \n",
       "\n",
       "[2 rows x 769 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>MComp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID  MComp\n",
       "0    0      1\n",
       "1    1      1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing & results----------------\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# nlp preprocessing\n",
    "import spacy\n",
    "\n",
    "# Models-------------------------\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "import sklearn.gaussian_process.kernels as kls\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "\n",
    "# for visualizing ---------------\n",
    "from sklearn import tree\n",
    "from six import StringIO \n",
    "from IPython.display import Image, display\n",
    "import seaborn as sns\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# General purpose\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --user graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dict = {\n",
    "    'DecisionTree': {\"model\": DecisionTreeClassifier(random_state=42), \"params\": {'max_depth': list(range(10, 250, 20))}},\n",
    "    'RandomForest': {\"model\": RandomForestClassifier(random_state=42),\n",
    "                     \"params\": {'n_estimators': list(range(5, 100, 5)), 'max_depth': list(range(10, 250, 20))}},\n",
    "    'LogisticR_L1': {\"model\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "                     \"params\": {'penalty': ['l1'], 'solver': ['liblinear', 'saga']}},\n",
    "    'LogisticR_L2': {\"model\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "                     \"params\": {'penalty': ['l2'], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}},\n",
    "    'LogisticR': {\"model\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "                  \"params\": {'penalty': ['none'], 'solver': ['newton-cg', 'lbfgs', 'sag', 'saga']}},\n",
    "    'RidgeClf': {\"model\": RidgeClassifier(max_iter=1000), \"params\": {}},\n",
    "    'SVC_linear': {\"model\": SVC(random_state=42), \"params\": {'kernel': ['linear'], \n",
    "                                                             'C': [0.5, 1.0, 1.5, 2.0, 2.5]}},\n",
    "    'SVC_poly': {\"model\": SVC(random_state=42),\n",
    "                 \"params\": {'kernel': ['poly'], 'degree': [3, 4, 5], 'gamma': ['scale', 'auto'], \n",
    "                            'C': [0.5, 1.0, 1.5, 2.0, 2.5]}},\n",
    "    'SVC_others': {\"model\": SVC(random_state=42), \"params\": {'kernel': ['rbf', 'sigmoid'], \n",
    "                                                             'gamma': ['scale', 'auto'], \n",
    "                                                             'C': [0.5, 1.0, 1.5, 2.0, 2.5]}},\n",
    "    'GussianNB': {\"model\": GaussianNB(), \"params\": {}},\n",
    "    'KNN': {\"model\": KNeighborsClassifier(), \"params\": {'n_neighbors': list(range(1, 20))}},\n",
    "    'GaussianProcessClf': {\"model\": GaussianProcessClassifier(random_state=42, kernel=kls.RBF()), \"params\": {}},\n",
    "    'Bagging_SVC': {\"model\": BaggingClassifier(random_state=42), \"params\": {'n_estimators': list(range(5, 100, 5)),\n",
    "                                                                            'base_estimator': [SVC(kernel='linear'),\n",
    "                                                                                               SVC(kernel='poly',\n",
    "                                                                                                   degree=3,\n",
    "                                                                                                   gamma='scale')]}},\n",
    "    'BaggingDT': {\"model\": BaggingClassifier(random_state=42), \"params\": {'n_estimators': list(range(5, 100, 5)),\n",
    "                                                                          'base_estimator': [\n",
    "                                                                              DecisionTreeClassifier(random_state=42,\n",
    "                                                                                                     max_depth=10),\n",
    "                                                                              DecisionTreeClassifier(random_state=42,\n",
    "                                                                                                     max_depth=50),\n",
    "                                                                              DecisionTreeClassifier(random_state=42,\n",
    "                                                                                                     max_depth=100)]}},\n",
    "    'AdaBoost': {\"model\": AdaBoostClassifier(random_state=42), \"params\": {'n_estimators': list(range(5, 100, 5)),\n",
    "                                                                          'base_estimator': [DecisionTreeClassifier(\n",
    "                                                                                                 random_state=42,\n",
    "                                                                                                 max_depth=10),\n",
    "                                                                                             DecisionTreeClassifier(\n",
    "                                                                                                 random_state=42,\n",
    "                                                                                                 max_depth=50),\n",
    "                                                                                             DecisionTreeClassifier(\n",
    "                                                                                                 random_state=42,\n",
    "                                                                                                 max_depth=100)]}},\n",
    "    'ExtraTrees': {\"model\": ExtraTreesClassifier(random_state=42), \"params\": {'n_estimators': list(range(5, 105, 5)), \n",
    "                                                                              'max_depth': [10, 50, 100, 250, 400]}},\n",
    "    'MLP_l1': {\"model\": MLPClassifier(random_state=42), \"params\": {'hidden_layer_sizes': [(x,) for x in \n",
    "                                                                                          range(50, 600, 100)], \n",
    "                                                                  'activation': ['logistic', 'tanh', 'relu'],\n",
    "                                                                  'solver': ['adam', 'sgd'], 'early_stopping': \n",
    "                                                                   [True]}},\n",
    "    'MLP_l2': {\"model\": MLPClassifier(random_state=42), \"params\": {'hidden_layer_sizes': [(x, y) for x in \n",
    "                                                                                          range(50, 600, 100) \n",
    "                                                                                          for y in range(50, 360, 100)], \n",
    "                                                                  'activation': ['logistic', 'tanh', 'relu'],\n",
    "                                                                  'solver': ['adam', 'sgd'], 'early_stopping': \n",
    "                                                                                               [True]}},\n",
    "#     'MLP_l3': {\"model\": MLPClassifier(random_state=42), \"params\": {'hidden_layer_sizes': [(x, y, z) for x in \n",
    "#                                                                                           range(50, 600, 100) \n",
    "#                                                                                           for y in range(50, 600, 100)\n",
    "#                                                                                           for z in range(50, 360, 100)], \n",
    "#                                                                   'activation': ['logistic', 'tanh', 'relu'],\n",
    "#                                                                   'solver': ['adam', 'sgd'], 'early_stopping': \n",
    "#                                                                                                [True]}},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_results = pd.DataFrame()\n",
    "# model_results['Train_Accuracy'] = None\n",
    "# model_results['Test_Accuracy'] = None\n",
    "# model_results['best_params'] = None\n",
    "\n",
    "# # X_train_final = X_train_normalized.drop(columns=[\"ref_latest\"])\n",
    "# # X_test_normalized_remgsdata = X_test_normalized.drop(columns=[\"ref_latest\"])\n",
    "# # X_train_normalized_remgsdata = X_train_normalized.copy()\n",
    "# # X_test_normalized_remgsdata = X_test_normalized.copy()\n",
    "\n",
    "# xtrain_final = xtrain.drop(columns=[\"UID\"])\n",
    "# ytrain_final = ytrain.drop(columns=[\"UID\"])\n",
    "\n",
    "# xtest_final = xtest.drop(columns=[\"UID\"])\n",
    "# ytest_final = ytest.drop(columns=[\"UID\"])\n",
    "\n",
    "\n",
    "# best_clf_ours = None\n",
    "# best_clf_val = 0\n",
    "\n",
    "# for clf_name, clf in clf_dict.items():\n",
    "#     classifier = GridSearchCV(clf['model'], clf['params'], n_jobs=5)\n",
    "#     classifier.fit(xtrain_final, ytrain_final)\n",
    "#     best_model = classifier.best_estimator_\n",
    "#     print(clf_name, classifier.best_score_, classifier.best_params_)\n",
    "    \n",
    "#     y_predicted = best_model.predict(xtest_final)\n",
    "#     test_acc = accuracy_score(ytest_final, y_predicted)\n",
    "    \n",
    "#     if test_acc > best_clf_val:\n",
    "#         best_clf_val = test_acc\n",
    "#         best_clf_ours = best_model\n",
    "    \n",
    "#     model_results.loc[clf_name, ['Train_Accuracy', 'Test_Accuracy', 'best_params']] = [classifier.best_score_, test_acc, classifier.best_params_]\n",
    "#     clsr = classification_report(ytest_final, y_predicted)\n",
    "\n",
    "# print(\"================================================================================\")\n",
    "# print(best_clf_ours)\n",
    "# best_y_hat = best_clf_ours.predict(xtest_final)\n",
    "# clsr = classification_report(ytest_final, best_y_hat)\n",
    "# print(clsr)\n",
    "# test_acc = accuracy_score(ytest_final, best_y_hat)\n",
    "# print(\"Test acc:\", test_acc )\n",
    "# print(\"Weighted F1 score: \", f1_score(ytest_final, best_y_hat, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision_recall_fscore_support(ytest_final, best_y_hat, average='macro')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree 0.831081081081081 {'max_depth': 10}\n",
      "RandomForest 0.9121621621621622 {'max_depth': 10, 'n_estimators': 10}\n",
      "LogisticR_L1 0.9121621621621622 {'penalty': 'l1', 'solver': 'liblinear'}\n",
      "LogisticR_L2 0.9121621621621622 {'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "LogisticR 0.9222972972972973 {'penalty': 'none', 'solver': 'saga'}\n",
      "RidgeClf 0.9121621621621622 {}\n",
      "SVC_linear 0.9121621621621622 {'C': 0.5, 'kernel': 'linear'}\n",
      "SVC_poly 0.918918918918919 {'C': 2.5, 'degree': 5, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "SVC_others 0.9121621621621622 {'C': 0.5, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "GussianNB 0.8682432432432432 {}\n",
      "KNN 0.9222972972972973 {'n_neighbors': 2}\n",
      "GaussianProcessClf 0.9121621621621622 {}\n",
      "Bagging_SVC 0.9121621621621622 {'base_estimator': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False), 'n_estimators': 5}\n",
      "BaggingDT 0.9087837837837838 {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=42, splitter='best'), 'n_estimators': 60}\n",
      "AdaBoost 0.847972972972973 {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=42, splitter='best'), 'n_estimators': 5}\n",
      "ExtraTrees 0.9155405405405406 {'max_depth': 10, 'n_estimators': 10}\n",
      "MLP_l1 0.9121621621621622 {'activation': 'logistic', 'early_stopping': True, 'hidden_layer_sizes': (150,), 'solver': 'sgd'}\n",
      "MLP_l2 0.9121621621621622 {'activation': 'logistic', 'early_stopping': True, 'hidden_layer_sizes': (50, 150), 'solver': 'adam'}\n",
      "================================================================================\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='warn', n_jobs=None, penalty='none',\n",
      "                   random_state=42, solver='saga', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      1388\n",
      "           1       0.41      0.21      0.27       117\n",
      "\n",
      "    accuracy                           0.92      1505\n",
      "   macro avg       0.67      0.59      0.61      1505\n",
      "weighted avg       0.90      0.92      0.90      1505\n",
      "\n",
      "Test acc: 0.9156146179401994\n",
      "Weighted F1 score:  0.9022677377583248\n"
     ]
    }
   ],
   "source": [
    "model_results = pd.DataFrame()\n",
    "model_results['Train_Accuracy'] = None\n",
    "model_results['Test_Accuracy'] = None\n",
    "model_results['best_params'] = None\n",
    "\n",
    "# X_train_final = X_train_normalized.drop(columns=[\"ref_latest\"])\n",
    "# X_test_normalized_remgsdata = X_test_normalized.drop(columns=[\"ref_latest\"])\n",
    "# X_train_normalized_remgsdata = X_train_normalized.copy()\n",
    "# X_test_normalized_remgsdata = X_test_normalized.copy()\n",
    "\n",
    "xtrain_final = xtrain.drop(columns=[\"UID\"])\n",
    "ytrain_final = ytrain.drop(columns=[\"UID\"])\n",
    "\n",
    "xtest_final = xtest.drop(columns=[\"UID\"])\n",
    "ytest_final = ytest.drop(columns=[\"UID\"])\n",
    "\n",
    "\n",
    "best_clf_ours = None\n",
    "best_clf_val = 0\n",
    "\n",
    "for clf_name, clf in clf_dict.items():\n",
    "    classifier = GridSearchCV(clf['model'], clf['params'], n_jobs=5)\n",
    "    classifier.fit(xtrain_final, ytrain_final)\n",
    "    best_model = classifier.best_estimator_\n",
    "    print(clf_name, classifier.best_score_, classifier.best_params_)\n",
    "    \n",
    "    y_predicted = best_model.predict(xtest_final)\n",
    "    test_acc_macro = precision_recall_fscore_support(ytest_final, y_predicted, average='macro')[2]#accuracy_score(ytest_final, y_predicted)\n",
    "    \n",
    "    if test_acc_macro > best_clf_val:\n",
    "        best_clf_val = test_acc_macro\n",
    "        best_clf_ours = best_model\n",
    "    \n",
    "    model_results.loc[clf_name, ['Train_Accuracy', 'Test_Accuracy', 'best_params']] = [classifier.best_score_, test_acc_macro, classifier.best_params_]\n",
    "    clsr = classification_report(ytest_final, y_predicted)\n",
    "\n",
    "print(\"================================================================================\")\n",
    "print(best_clf_ours)\n",
    "best_y_hat = best_clf_ours.predict(xtest_final)\n",
    "clsr = classification_report(ytest_final, best_y_hat)\n",
    "print(clsr)\n",
    "test_acc = accuracy_score(ytest_final, best_y_hat)\n",
    "print(\"Test acc:\", test_acc )\n",
    "print(\"Weighted F1 score: \", f1_score(ytest_final, best_y_hat, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sim_with_mcomp = defaultdict(dict)\n",
    "mean_sim_with_not_mcomp = defaultdict(dict)\n",
    "max_sim_with_mcomp = defaultdict(dict)\n",
    "max_sim_with_not_mcomp = defaultdict(dict)\n",
    "\n",
    "mean_at_k = [\"1\", \"3\", \"5\", \"7\", \"10\", \"26\"]\n",
    "\n",
    "\n",
    "for sid in mcomp_sentences:\n",
    "    \n",
    "    # 1. With other training set mcomp sentences\n",
    "    temp_list = []    \n",
    "    for init_train_vec in train_pool_roberta_vecs[\"mcomp\"]:\n",
    "        for cvec2 in roberta_vectors[mcomp_sentences[sid]][sid]:\n",
    "            temp_list.append(np.inner(init_train_vec, cvec2)[0][0])\n",
    "        \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    mean_sim_with_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    max_sim_with_mcomp[sid][\"max\"] = max(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        mean_sim_with_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "#         max_sim_with_mcomp[sid][\"max_{}\".format(vv)] = max(sorted_temp_list[0:int(vv)])\n",
    "    \n",
    "    \n",
    "    # 2. With other training set non_mcomp sentences\n",
    "    temp_list = []    \n",
    "    for init_train_vec in train_pool_roberta_vecs[\"non_mcomp\"]:\n",
    "        for cvec2 in roberta_vectors[mcomp_sentences[sid]][sid]:\n",
    "            temp_list.append(np.inner(init_train_vec, cvec2)[0][0])\n",
    "        \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    mean_sim_with_not_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    max_sim_with_not_mcomp[sid][\"max\"] = max(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        mean_sim_with_not_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "#         max_sim_with_not_mcomp[sid][\"max_{}\".format(vv)] = max(sorted_temp_list[0:int(vv)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for sid in not_mcomp_sentences:\n",
    "    \n",
    "    # 1. With other training set mcomp sentences\n",
    "    temp_list = []    \n",
    "    for init_train_vec in train_pool_roberta_vecs[\"mcomp\"]:\n",
    "        for cvec2 in roberta_vectors[not_mcomp_sentences[sid]][sid]:\n",
    "            temp_list.append(np.inner(init_train_vec, cvec2)[0][0])\n",
    "        \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    mean_sim_with_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    max_sim_with_mcomp[sid][\"max\"] = max(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        mean_sim_with_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "#         max_sim_with_mcomp[sid][\"max_{}\".format(vv)] = max(sorted_temp_list[0:int(vv)])\n",
    "    \n",
    "    \n",
    "    # 2. With other training set non_mcomp sentences\n",
    "    temp_list = []    \n",
    "    for init_train_vec in train_pool_roberta_vecs[\"non_mcomp\"]:\n",
    "        for cvec2 in roberta_vectors[not_mcomp_sentences[sid]][sid]:\n",
    "            temp_list.append(np.inner(init_train_vec, cvec2)[0][0])\n",
    "        \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    mean_sim_with_not_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    max_sim_with_not_mcomp[sid][\"max\"] = max(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        mean_sim_with_not_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "#         max_sim_with_not_mcomp[sid][\"max_{}\".format(vv)] = max(sorted_temp_list[0:int(vv)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(sent_sim_dict, k=10, sim_type=\"max\", mean_at=1):\n",
    "    \"\"\"mean_at can take values: 1, 3, 5, 7, 10, 26\"\"\"\n",
    "    \n",
    "    if sim_type == \"max\":\n",
    "        sorted_sims = sorted(sent_sim_dict.items(), key=lambda x: x[1][\"max\"], reverse=True)\n",
    "        top_k_sorted_sims = sorted_sims[0:k]\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        for i in top_k_sorted_sims:\n",
    "            if df.loc[i[0]][\"MComp\"] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        return tp/(tp+fp)\n",
    "            \n",
    "    elif sim_type == \"mean\":\n",
    "        sorted_sims = sorted(sent_sim_dict.items(), key=lambda x: x[1][\"mean_\"+str(mean_at)], reverse=True)\n",
    "        top_k_sorted_sims = sorted_sims[0:k]\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        for i in top_k_sorted_sims:\n",
    "            if df.loc[i[0]][\"MComp\"] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        return tp/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(sent_sim_dict, k=10, sim_type=\"max\", mean_at=1):\n",
    "    if sim_type == \"max\":\n",
    "        sorted_sims = sorted(sent_sim_dict.items(), key=lambda x: x[1][\"max\"], reverse=True)\n",
    "        top_k_sorted_sims = sorted_sims[0:k]\n",
    "        tp = 0\n",
    "        for i in top_k_sorted_sims:\n",
    "            if df.loc[i[0]][\"MComp\"] == 1:\n",
    "                tp += 1\n",
    "        return tp/min(k, 117)\n",
    "            \n",
    "    elif sim_type == \"mean\":\n",
    "        sorted_sims = sorted(sent_sim_dict.items(), key=lambda x: x[1][\"mean_\"+str(mean_at)], reverse=True)\n",
    "        top_k_sorted_sims = sorted_sims[0:k]\n",
    "        tp = 0\n",
    "        for i in top_k_sorted_sims:\n",
    "            if df.loc[i[0]][\"MComp\"] == 1:\n",
    "                tp += 1\n",
    "        return tp/min(k, 117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on max similarity with meaningful group and removing entries more similar to non-meaningful group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "17\n",
      "28\n",
      "76\n",
      "108\n",
      "112\n",
      "124\n",
      "184\n",
      "155\n",
      "157\n",
      "159\n",
      "235\n",
      "236\n",
      "271\n",
      "287\n",
      "312\n",
      "308\n",
      "401\n",
      "449\n",
      "519\n",
      "573\n",
      "627\n",
      "615\n",
      "672\n",
      "673\n",
      "657\n",
      "669\n",
      "671\n",
      "714\n",
      "707\n",
      "739\n",
      "806\n",
      "808\n",
      "809\n",
      "810\n",
      "870\n",
      "830\n",
      "931\n",
      "933\n",
      "909\n",
      "926\n",
      "972\n",
      "950\n",
      "994\n",
      "1004\n",
      "1047\n",
      "1125\n",
      "1162\n",
      "1100\n",
      "1102\n",
      "1212\n",
      "1243\n",
      "1268\n",
      "1281\n",
      "1316\n",
      "1318\n",
      "1331\n",
      "1333\n",
      "1279\n",
      "1347\n",
      "1406\n",
      "1390\n",
      "1452\n",
      "1504\n",
      "1464\n",
      "1497\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "29\n",
      "31\n",
      "32\n",
      "33\n",
      "37\n",
      "38\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "48\n",
      "49\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "85\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "128\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "109\n",
      "111\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "125\n",
      "127\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "156\n",
      "158\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "183\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "268\n",
      "269\n",
      "222\n",
      "223\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "309\n",
      "310\n",
      "311\n",
      "314\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "333\n",
      "334\n",
      "335\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "351\n",
      "352\n",
      "353\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "414\n",
      "416\n",
      "417\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "377\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "439\n",
      "440\n",
      "442\n",
      "444\n",
      "446\n",
      "447\n",
      "451\n",
      "452\n",
      "453\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "506\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "544\n",
      "545\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "579\n",
      "580\n",
      "640\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "624\n",
      "625\n",
      "626\n",
      "628\n",
      "629\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "654\n",
      "655\n",
      "656\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "670\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "692\n",
      "693\n",
      "694\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "730\n",
      "731\n",
      "732\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "804\n",
      "805\n",
      "807\n",
      "811\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "831\n",
      "832\n",
      "833\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "842\n",
      "843\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "861\n",
      "862\n",
      "863\n",
      "865\n",
      "866\n",
      "871\n",
      "874\n",
      "896\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "903\n",
      "904\n",
      "906\n",
      "907\n",
      "908\n",
      "911\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "920\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "932\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "939\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "973\n",
      "974\n",
      "975\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1045\n",
      "1046\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "995\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1005\n",
      "1006\n",
      "1008\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1077\n",
      "1078\n",
      "1080\n",
      "1081\n",
      "1083\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1098\n",
      "1099\n",
      "1101\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1151\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1213\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1255\n",
      "1256\n",
      "1258\n",
      "1259\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1270\n",
      "1271\n",
      "1280\n",
      "1282\n",
      "1283\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1317\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1332\n",
      "1273\n",
      "1274\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1389\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1407\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1475\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1498\n",
      "1499\n",
      "1501\n",
      "1503\n"
     ]
    }
   ],
   "source": [
    "# DIff of max similarity of meaningful and non-meaningful sentences\n",
    "\n",
    "diff_max = {}\n",
    "preserve_non_negative_max = {}\n",
    "\n",
    "for x in max_sim_with_mcomp:\n",
    "    diff_max_sim_for_x = max_sim_with_mcomp[x][\"max\"] - max_sim_with_not_mcomp[x][\"max\"]\n",
    "    diff_max[x] = {\"max\": diff_max_sim_for_x}\n",
    "    \n",
    "    if diff_max_sim_for_x > 0:\n",
    "        preserve_non_negative_max[x] = {\"max\": max_sim_with_mcomp[x][\"max\"]}\n",
    "    else:\n",
    "        print(x)\n",
    "        preserve_non_negative_max[x] = {\"max\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P and R based on preserving non-neg max sim with initial pool of \n",
      " meaningful & non meaningful sents:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Precision at:</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">20  </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">117   </td><td style=\"text-align: right;\">150   </td><td style=\"text-align: right;\">200   </td><td style=\"text-align: right;\">300   </td><td style=\"text-align: right;\">400   </td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">600  </td></tr>\n",
       "<tr><td>Val          </td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 1</td><td style=\"text-align: right;\"> 0.6</td><td style=\"text-align: right;\"> 0.52</td><td style=\"text-align: right;\">  0.38</td><td style=\"text-align: right;\">  0.36</td><td style=\"text-align: right;\">  0.43</td><td style=\"text-align: right;\">  0.57</td><td style=\"text-align: right;\">  0.39</td><td style=\"text-align: right;\">  0.29</td><td style=\"text-align: right;\">  0.23</td><td style=\"text-align: right;\">  0.2</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Recall at:</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">20  </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">117   </td><td style=\"text-align: right;\">150   </td><td style=\"text-align: right;\">200   </td><td style=\"text-align: right;\">300</td><td style=\"text-align: right;\">400</td><td style=\"text-align: right;\">500</td><td style=\"text-align: right;\">600</td><td style=\"text-align: right;\">700</td><td style=\"text-align: right;\">800</td><td style=\"text-align: right;\">900</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">1200</td><td style=\"text-align: right;\">1388</td></tr>\n",
       "<tr><td>Val       </td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 1</td><td style=\"text-align: right;\"> 0.6</td><td style=\"text-align: right;\"> 0.52</td><td style=\"text-align: right;\">  0.38</td><td style=\"text-align: right;\">  0.36</td><td style=\"text-align: right;\">  0.56</td><td style=\"text-align: right;\">  0.98</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"P and R based on preserving non-neg max sim with initial pool of \\n meaningful & non meaningful sents:\")\n",
    "\n",
    "patk = [1, 5, 10, 20, 50, 100, 117, 150, 200, 300, 400, 500, 600]\n",
    "res_table = [[\"Precision at:\"] + patk, [\"Val\"]]\n",
    "\n",
    "for k in patk:\n",
    "    v1 = precision_at_k(preserve_non_negative_max, k) #round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(round(v1, 2))\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))\n",
    "\n",
    "\n",
    "\n",
    "ratk = [1, 5, 10, 20, 50, 100, 117, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1200, 1388]\n",
    "res_table = [[\"Recall at:\"] + ratk, [\"Val\"]]\n",
    "\n",
    "for k in ratk:\n",
    "    v1 = recall_at_k(preserve_non_negative_max, k) #round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(round(v1,2))\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on diff of max similarity with initial pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P and R based on diff of max sim with initial pool of \n",
      " meaningful & non meaningful sents:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Precision at:</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">10  </td><td style=\"text-align: right;\">20  </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">117   </td><td style=\"text-align: right;\">150   </td><td style=\"text-align: right;\">200   </td><td style=\"text-align: right;\">300   </td><td style=\"text-align: right;\">400   </td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">600   </td></tr>\n",
       "<tr><td>Val          </td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 0.6</td><td style=\"text-align: right;\"> 0.5</td><td style=\"text-align: right;\"> 0.44</td><td style=\"text-align: right;\">  0.41</td><td style=\"text-align: right;\">  0.37</td><td style=\"text-align: right;\">  0.37</td><td style=\"text-align: right;\">  0.32</td><td style=\"text-align: right;\">  0.25</td><td style=\"text-align: right;\">  0.21</td><td style=\"text-align: right;\">  0.18</td><td style=\"text-align: right;\">  0.15</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Recall at:</td><td style=\"text-align: right;\">117   </td><td style=\"text-align: right;\">150   </td><td style=\"text-align: right;\">200   </td><td style=\"text-align: right;\">300   </td><td style=\"text-align: right;\">400   </td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">600   </td><td style=\"text-align: right;\">700   </td><td style=\"text-align: right;\">800   </td><td style=\"text-align: right;\">900   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1200   </td><td style=\"text-align: right;\">1388   </td></tr>\n",
       "<tr><td>Val       </td><td style=\"text-align: right;\">  0.37</td><td style=\"text-align: right;\">  0.47</td><td style=\"text-align: right;\">  0.54</td><td style=\"text-align: right;\">  0.65</td><td style=\"text-align: right;\">  0.72</td><td style=\"text-align: right;\">  0.77</td><td style=\"text-align: right;\">  0.79</td><td style=\"text-align: right;\">  0.85</td><td style=\"text-align: right;\">  0.85</td><td style=\"text-align: right;\">  0.89</td><td style=\"text-align: right;\">   0.92</td><td style=\"text-align: right;\">   0.96</td><td style=\"text-align: right;\">   0.99</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"P and R based on diff of max sim with initial pool of \\n meaningful & non meaningful sents:\")\n",
    "\n",
    "patk = [1, 10, 20, 50, 100, 117, 150, 200, 300, 400, 500, 600]\n",
    "res_table = [[\"Precision at:\"] + patk, [\"Val\"]]\n",
    "\n",
    "for k in patk:\n",
    "    v1 = precision_at_k(diff_max, k) #round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(round(v1, 2))\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))\n",
    "\n",
    "\n",
    "\n",
    "ratk = [117, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1200, 1388]\n",
    "res_table = [[\"Recall at:\"] + ratk, [\"Val\"]]\n",
    "\n",
    "for k in ratk:\n",
    "    v1 = recall_at_k(diff_max, k) #round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(round(v1,2))\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
