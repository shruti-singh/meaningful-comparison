{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"InputTestSet-Reviews48_Ann.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Sent</th>\n",
       "      <th>MComp</th>\n",
       "      <th>Cat</th>\n",
       "      <th>SubCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The authors propose to use k-DPP to select a s...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>This paper covers the related work nicely, wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The rest of the paper are also clearly written.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>However, I have some concerns about the propos...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>- It is not clear how to define the kernel, th...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID              PID     Dec  \\\n",
       "0    0  2019_SJf_XhCqKm  Reject   \n",
       "1    1  2019_SJf_XhCqKm  Reject   \n",
       "2    2  2019_SJf_XhCqKm  Reject   \n",
       "3    3  2019_SJf_XhCqKm  Reject   \n",
       "4    4  2019_SJf_XhCqKm  Reject   \n",
       "\n",
       "                                                Sent  MComp  Cat SubCat  \n",
       "0  The authors propose to use k-DPP to select a s...      0  NaN    NaN  \n",
       "1  This paper covers the related work nicely, wit...      0  NaN    NaN  \n",
       "2    The rest of the paper are also clearly written.      0  NaN    NaN  \n",
       "3  However, I have some concerns about the propos...      0  NaN    NaN  \n",
       "4  - It is not clear how to define the kernel, th...      0  NaN    NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dict = {}\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    if not pid in gt_dict:\n",
    "        gt_dict[pid] = {\"dec\": df.loc[i][\"Dec\"], \"mcomp\": set(), \"not_mcomp\": set()}\n",
    "    if df.loc[i][\"MComp\"] == 1:\n",
    "        gt_dict[pid][\"mcomp\"].add(df.loc[i][\"UID\"])\n",
    "    else:\n",
    "        gt_dict[pid][\"not_mcomp\"].add(df.loc[i][\"UID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accept': [48, 644], 'Reject': [69, 744]}\n"
     ]
    }
   ],
   "source": [
    "stats_dict = {\"Accept\": [0, 0], \"Reject\": [0, 0]}\n",
    "\n",
    "for k, v in gt_dict.items():\n",
    "    #print(len(v[\"mcomp\"]), len(v[\"not_mcomp\"]), v[\"dec\"])\n",
    "    stats_dict[v[\"dec\"]][0] += len(v[\"mcomp\"])\n",
    "    stats_dict[v[\"dec\"]][1] += len(v[\"not_mcomp\"])\n",
    "    \n",
    "print(stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestSet length: 32\n",
      " ['2019_SJf_XhCqKm', '2017_Bk0MRI5lg', '2020_SyevYxHtDB', '2018_rJBiunlAW', '2020_rkltE0VKwH', '2018_Hki-ZlbA-', '2019_BJx0sjC5FX', '2020_r1e_FpNFDr', '2020_B1lsXREYvr', '2018_SkZxCk-0Z', '2019_rJzoujRct7', '2018_HkfXMz-Ab', '2017_BJ9fZNqle', '2019_SyxZJn05YX', '2017_B1ckMDqlg', '2017_HJ0NvFzxl', '2017_S1_pAu9xl', '2018_SyYYPdg0-', '2017_BJAA4wKxg', '2019_HyVxPsC9tm', '2019_HylTBhA5tQ', '2019_B1l08oAct7', '2018_H135uzZ0-', '2017_H1oyRlYgg', '2017_r1y1aawlg', '2020_r1eX1yrKwB', '2020_Byg79h4tvB', '2019_H1lFZnR5YX', '2020_BkeWw6VFwr', '2018_HyHmGyZCZ', '2018_HyUNwulC-', '2020_HkgsPhNYPS']\n"
     ]
    }
   ],
   "source": [
    "test_set = list(gt_dict.keys())\n",
    "print(\"TestSet length: %d\\n\"%len(test_set), test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_SJf_XhCqKm     {39, 17, 20, 27, 28, 30}\n",
      "2017_Bk0MRI5lg      {48, 57}\n",
      "2020_SyevYxHtDB     {76, 87}\n",
      "2018_rJBiunlAW      {108, 110, 112, 113, 124, 126}\n",
      "2020_rkltE0VKwH     {160, 155, 184, 159}\n",
      "2018_Hki-ZlbA-      {267, 235, 236, 271}\n",
      "2019_BJx0sjC5FX     {292, 287}\n",
      "2020_r1e_FpNFDr     {312, 322, 315, 308}\n",
      "2020_B1lsXREYvr     {376, 401}\n",
      "2018_SkZxCk-0Z      {449, 443, 445, 486}\n",
      "2019_rJzoujRct7     {518, 519}\n",
      "2018_HkfXMz-Ab      {573, 566}\n",
      "2017_BJ9fZNqle      {627, 623, 615}\n",
      "2019_SyxZJn05YX     {672, 673, 657, 669, 671}\n",
      "2017_B1ckMDqlg      {714, 707}\n",
      "2017_HJ0NvFzxl      {739}\n",
      "2017_S1_pAu9xl      {792, 809, 810, 806}\n",
      "2018_SyYYPdg0-      {834, 867, 868, 869, 870, 872, 873, 844, 830}\n",
      "2017_BJAA4wKxg      {884}\n",
      "2019_HyVxPsC9tm     {931, 933, 905, 909, 912, 913, 919, 926}\n",
      "2019_HylTBhA5tQ     {972, 950}\n",
      "2019_B1l08oAct7     {994, 996, 1064, 1004, 1007, 1044, 1047, 1048, 1055}\n",
      "2018_H135uzZ0-      {1072, 1079}\n",
      "2017_H1oyRlYgg      set()\n",
      "2017_r1y1aawlg      {1125, 1162, 1100, 1102, 1168}\n",
      "2020_r1eX1yrKwB     {1177, 1202, 1212}\n",
      "2020_Byg79h4tvB     {1243, 1268}\n",
      "2019_H1lFZnR5YX     {1281, 1316, 1284, 1318, 1289, 1331, 1333, 1279}\n",
      "2020_BkeWw6VFwr     {1347, 1373}\n",
      "2018_HyHmGyZCZ      {1406, 1421, 1390, 1426}\n",
      "2018_HyUNwulC-      {1451, 1452}\n",
      "2020_HkgsPhNYPS     {1504, 1464, 1497, 1500, 1502}\n"
     ]
    }
   ],
   "source": [
    "for k in test_set:\n",
    "    print('{:20}{}'.format(k, gt_dict[k][\"mcomp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_for_test = defaultdict(list)\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    sents_for_test[pid].append((df.loc[i][\"UID\"], df.loc[i][\"Sent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"entities_dict_smaller\", \"r\") as f:\n",
    "    entity_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Material', 'Method', 'Metric', 'Task'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(entity_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('convolutional neural networks', 'Method'),\n",
       " ('convnets', 'Method'),\n",
       " ('recognition', 'Task'),\n",
       " ('visual recognition tasks', 'Task'),\n",
       " ('age estimation', 'Task'),\n",
       " ('head pose estimation', 'Task'),\n",
       " ('multi - label classification', 'Task'),\n",
       " ('semantic segmentation', 'Task'),\n",
       " ('classification', 'Task'),\n",
       " ('deep convnets', 'Method'),\n",
       " ('dldl', 'Method'),\n",
       " ('feature learning', 'Task'),\n",
       " ('deep learning', 'Method'),\n",
       " ('image classification', 'Task'),\n",
       " ('deep learning methods', 'Method'),\n",
       " ('image classification tasks', 'Task'),\n",
       " ('human pose estimation', 'Task'),\n",
       " ('convnet', 'Method'),\n",
       " ('recognition tasks', 'Task'),\n",
       " ('ensemble', 'Method')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_dict.items())[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1784\n"
     ]
    }
   ],
   "source": [
    "entity_key_map = {}\n",
    "for i in entity_dict:\n",
    "    s = re.sub('[^0-9a-zA-Z,:;.?!\\- ]+', '', i)\n",
    "    while s.find(\"  \") > -1:\n",
    "        s = s.replace(\"  \", \" \")\n",
    "    if len(s) > 2:\n",
    "        cl = re.sub('[^0-9a-zA-Z ]+', '', i)\n",
    "        while cl.find(\"  \") > -1:\n",
    "            cl = cl.replace(\"  \", \" \")\n",
    "        entity_key_map[cl.strip()] = i\n",
    "print(len(entity_key_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    }
   ],
   "source": [
    "coun = 0\n",
    "for i in entity_dict:\n",
    "    if len(i) < 5:\n",
    "        coun +=1\n",
    "#         print(i)\n",
    "print(coun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('convolutional neural networks', 'convolutional neural networks'),\n",
       " ('convnets', 'convnets'),\n",
       " ('recognition', 'recognition'),\n",
       " ('visual recognition tasks', 'visual recognition tasks'),\n",
       " ('age estimation', 'age estimation')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_key_map.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Method': 1191, 'Task': 289, 'Metric': 158, 'Material': 165})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(entity_dict.values())\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(c)\n",
    "reverse_map = defaultdict(list)\n",
    "\n",
    "for k, v in entity_dict.items():\n",
    "    reverse_map[v].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in reverse_map[\"Task\"]:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"MNIST\" in entity_key_map, \"mnist\" in entity_key_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. RoBERTa trained on SciLit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy-transformers            0.6.2\r\n",
      "tokenizers                    0.7.0\r\n",
      "transformers                  2.9.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip3.7 list | grep -E 'transformers|tokenizers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./trained_lm/CLMLModelRoBerta/\")\n",
    "model = AutoModel.from_pretrained(\"./trained_lm/CLMLModelRoBerta/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_using_roberta(text):\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_entities(sentence, replace_with_dataset=True):\n",
    "    cleaned_sent = re.sub('[^0-9a-zA-Z,:;.?!\\- ]+', ' ', sentence)\n",
    "    while cleaned_sent.find(\"  \") > -1:\n",
    "        cleaned_sent = cleaned_sent.replace(\"  \", \" \")\n",
    "    \n",
    "    entity_key_map_keys = list(entity_key_map.keys()) # As we will be dunamically adding entries to this dict an dthat will throw an error.\n",
    "    entities_found = []\n",
    "    for i in entity_key_map_keys:\n",
    "        if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "            entities_found.append(i)\n",
    "        elif cleaned_sent.lower().find(\" \" + i + \" \") > -1:\n",
    "            found_idx = cleaned_sent.lower().find(\" \" + i + \" \")\n",
    "            entity_dict[cleaned_sent[found_idx:found_idx+len(\" \" + i + \" \")]] = entity_dict[i]\n",
    "            entity_key_map[cleaned_sent[found_idx:found_idx+len(\" \" + i + \" \")]] = entity_key_map[i]\n",
    "    \n",
    "    entities_found.sort(key=lambda s: len(s))\n",
    "    len_sorted_entities = entities_found.copy()\n",
    "    \n",
    "    subset_entities = []\n",
    "    # Remove subset entities (eg: Among cnn and 3-layer-cnn, prefer the latter)\n",
    "    for fe in len_sorted_entities:\n",
    "        for other_ent in len_sorted_entities:\n",
    "            if fe != other_ent and other_ent.find(fe) > -1:\n",
    "                subset_entities.append(fe)\n",
    "                break\n",
    "    for se in subset_entities:\n",
    "        len_sorted_entities.remove(se)\n",
    "    for maxents in len_sorted_entities:\n",
    "        mask_name = \" \" + entity_dict[entity_key_map[i]].lower() + \" \"\n",
    "        if replace_with_dataset:\n",
    "            if mask_name == \" material \":\n",
    "                mask_name = \" dataset \"\n",
    "        cleaned_sent = cleaned_sent.replace(\" \" + maxents + \" \", mask_name)\n",
    "    words_cleaned = nltk.word_tokenize(cleaned_sent)\n",
    "    dups_removed = [v for i, v in enumerate(words_cleaned) if i == 0 or v != words_cleaned[i-1]]\n",
    "    new_dup_removed_sent = \" \".join(dups_removed)\n",
    "    return new_dup_removed_sent.strip()\n",
    "\n",
    "#     #print(cleaned_sent)\n",
    "#     for i in entity_key_map:\n",
    "#         if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "#             #print(\"Substituting ent: {} with mask: {}\".format(i, entity_dict[entity_key_map[i]].lower()))\n",
    "#             cleaned_sent = cleaned_sent.replace(i, entity_dict[entity_key_map[i]].lower())\n",
    "#     return cleaned_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the BO-PET test , the best method is to take risks . This leads to substantial improvement in results .'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"In the BO-PET test*, the best method is to take\\ risks. This leads to substantial improvement in results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'test']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"this is a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "sp_toks = [\"result\", \"method\", \"task\", \"dataset\", \"metric\", \"baseline\", \"fair\", \"unfair\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunks_using_spacy_dp(conssentence, replace_with_dataset=True):\n",
    "    \n",
    "    conssentence = mask_entities(conssentence, replace_with_dataset)\n",
    "#     print(conssentence)\n",
    "    doc = nlp(conssentence)\n",
    "    verb_subtree = []\n",
    "\n",
    "    for s in doc.sents:\n",
    "#         find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"technique\": [], \"task\": [], \"dataset\": [], \"material\": [], \"metric\": []}\n",
    "        find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"baseline\": [], \"task\": [], \n",
    "                               \"dataset\": [],  \"metric\": [], \"unfair\": [], \"fair\": []}\n",
    "\n",
    "        for tok in s:\n",
    "\n",
    "            if tok.text.lower().startswith(\"compar\"):\n",
    "                find_special_tokens[\"compar\"].append(tok)\n",
    "            else:\n",
    "                for k in sp_toks:\n",
    "                    if tok.text.lower().startswith(k):\n",
    "                        find_special_tokens[k].append(tok)\n",
    "                        break\n",
    "\n",
    "        verb_tokens = []\n",
    "        if find_special_tokens[\"compar\"]:\n",
    "            for t in find_special_tokens[\"compar\"]:\n",
    "#                     verb_subtree.append(t.subtree)\n",
    "                if t == s.root:\n",
    "                    simplified_sent = \"\"\n",
    "                    for chh in t.lefts:\n",
    "                        simplified_sent = simplified_sent + \" \" + chh.text\n",
    "                    simplified_sent = simplified_sent + \" \" + t.text\n",
    "                    for chh in t.rights:\n",
    "                        simplified_sent = simplified_sent + \" \" + chh.text\n",
    "#                         print(\"SIMP: \", simplified_sent)\n",
    "                    verb_subtree.append(simplified_sent)\n",
    "                else:\n",
    "                    verb_subtree.append(t.subtree)\n",
    "        else:\n",
    "            for k in sp_toks:\n",
    "                for i in find_special_tokens[k]:\n",
    "                    local_vt = []\n",
    "                    for j in i.ancestors:\n",
    "                        if j.pos_ == \"NOUN\":\n",
    "                            local_vt.append(j)\n",
    "                    if not local_vt:\n",
    "                        for j in i.ancestors:\n",
    "                            if j.pos_ == \"VERB\":\n",
    "                                local_vt.append(j)\n",
    "                    verb_tokens = verb_tokens + local_vt\n",
    "\n",
    "\n",
    "            for i in verb_tokens:\n",
    "                verb_subtree.append(i.subtree)\n",
    "\n",
    "    eecc = []\n",
    "    for i in verb_subtree:\n",
    "        if type(i) == str:\n",
    "            eecc.append(i)\n",
    "        else:\n",
    "            local_chunk = \"\"\n",
    "            for lcaltok in i:\n",
    "                local_chunk = local_chunk + \" \" + lcaltok.text\n",
    "            eecc.append(local_chunk)\n",
    "#     if not eecc:\n",
    "#         print(conssentence)\n",
    "    return list(set(eecc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' the practicability of the method',\n",
       " ' more large - scale experiments on image related tasks']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"It would be interesting to explore the practicability of the method on more large-scale experiments on image related tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' comparison to SOTA']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"The experimental validation is also not extensive since comparison to SOTA is not included.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_words_spacy(conssentence):\n",
    "    \n",
    "    doc = nlp(conssentence)\n",
    "    final_sentence = []\n",
    "    \n",
    "    for s in doc.sents:\n",
    "        for tok in s:\n",
    "            if not tok.is_stop:\n",
    "                final_sentence.append(tok.text)\n",
    "    return final_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experimental',\n",
       " 'validation',\n",
       " 'extensive',\n",
       " 'comparison',\n",
       " 'SOTA',\n",
       " 'included',\n",
       " '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_content_words_spacy(\"The experimental validation is also not extensive since comparison to SOTA is not included.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019_SJf_XhCqKm',\n",
       " '28',\n",
       " 'COMMENTS',\n",
       " 'CHANGES',\n",
       " 'YEAR',\n",
       " 'convinced',\n",
       " 'comparison',\n",
       " 'Spearmint',\n",
       " 'added',\n",
       " 'authors',\n",
       " 'previous',\n",
       " 'version',\n",
       " '.',\n",
       " '2019_SJf_XhCqKm',\n",
       " '30',\n",
       " 'addition',\n",
       " 'authors',\n",
       " 'compare',\n",
       " 'recent',\n",
       " 'work',\n",
       " ',',\n",
       " 'e.g.',\n",
       " ',',\n",
       " ' ',\n",
       " '@INPROCEEDINGS{falkner',\n",
       " '-',\n",
       " 'bayesopt17',\n",
       " ',',\n",
       " 'author',\n",
       " '=',\n",
       " '{',\n",
       " 'S.',\n",
       " 'Falkner',\n",
       " 'A.',\n",
       " 'Klein',\n",
       " 'F.',\n",
       " 'Hutter',\n",
       " '}',\n",
       " ',',\n",
       " 'title',\n",
       " '=',\n",
       " '{',\n",
       " 'Combining',\n",
       " 'Hyperband',\n",
       " 'Bayesian',\n",
       " 'Optimization',\n",
       " '}',\n",
       " ',',\n",
       " 'booktitle',\n",
       " '=',\n",
       " '{',\n",
       " 'NIPS',\n",
       " '2017',\n",
       " 'Bayesian',\n",
       " 'Optimization',\n",
       " 'Workshop',\n",
       " '}',\n",
       " ',',\n",
       " 'year',\n",
       " '=',\n",
       " '{',\n",
       " '2017',\n",
       " '}',\n",
       " ',',\n",
       " 'month',\n",
       " '=',\n",
       " 'dec',\n",
       " ',',\n",
       " '}']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_content_words_spacy(\"2019_SJf_XhCqKm 28 COMMENTS ON THE CHANGES SINCE THE LAST YEAR I am not convinced by the comparison with Spearmint added by the authors since the previous version. 2019_SJf_XhCqKm 30 In addition the authors do not compare against more recent work, e.g.,  @INPROCEEDINGS{falkner-bayesopt17, author = {S. Falkner and A. Klein and F. Hutter}, title = {Combining Hyperband and Bayesian Optimization}, booktitle = {NIPS 2017 Bayesian Optimization Workshop}, year = {2017}, month = dec,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_SJf_XhCqKm 28 COMMENTS ON THE CHANGES SINCE THE LAST YEAR\n",
      "I am not convinced by the comparison with Spearmint added by the authors since the previous version.\n",
      "2019_SJf_XhCqKm 30 In addition the authors do not compare against more recent work, e.g., \n",
      "@INPROCEEDINGS{falkner-bayesopt17,\n",
      " author = {S. Falkner and A. Klein and F. Hutter},\n",
      " title = {Combining Hyperband and Bayesian Optimization},\n",
      " booktitle = {NIPS 2017 Bayesian Optimization Workshop},\n",
      " year = {2017},\n",
      " month = dec,\n",
      "}\n",
      "@InProceedings{falkner-icml-18,\n",
      " title = {{BOHB}: Robust and Efficient Hyperparameter Optimization at Scale},\n",
      " author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},\n",
      " booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML 2018)},\n",
      " pages = {1436--1445},\n",
      " year = {2018},\n",
      " month = jul,\n",
      "}\n",
      "2019_SJf_XhCqKm 10 I'm not sure where k log(N) comes from in page 7) \n",
      "- Algorithm 2 is a straight forward extension of Algorithm 1, just with L not explicitly computed.\n",
      "2019_SJf_XhCqKm 12 Other small things:\n",
      "- citation format problems in, for example, Section 4.1) It should be \\citep instead of \\cite.\n",
      "2019_SJf_XhCqKm 14 [Post rebuttal]\n",
      "I would like to thank the authors for their clarifications.\n",
      "2020_SyevYxHtDB 59 Merits:\n",
      "1) In general, this paper is well written and easy to follow.\n",
      "2020_SyevYxHtDB 67 Additional questions on problem formulation:\n",
      "a) Problem (4) only relies on the transfer set, where $x \\sim P_A(x)$, right?\n",
      "2020_SyevYxHtDB 77 ############## Post-feedback ################\n",
      "I am satisfied with the authors' response.\n",
      "2020_SyevYxHtDB 81 Positives:\n",
      "1) The paper was very readable and clear.\n",
      "2020_SyevYxHtDB 84 Concerns: \n",
      "1) You note that the random perturbation to the outputs performs poorly compared to your method, but this performance gap seems to decrease as the dataset becomes more difficult (ie CIFAR100).\n",
      "2018_rJBiunlAW 126 - Slightly unfortunate naming that does not account for autoregressive CNNs\n",
      "- Lack of comparison with autoregressive CNN baselines, which signals a major conceptual error in the paper.\n",
      "2018_rJBiunlAW 102 Significance, Quality and clarity:\n",
      "The idea is well motivated: Faster training is important for rapid experimentation, and altering the RNN cell so it can be paralleled makes sense.\n",
      "2018_rJBiunlAW 104 A few constructive comments:\n",
      "- The experiment’s tables alternate between “time” and “speed”, It will be good to just have one of them.\n",
      "2018_rJBiunlAW 125 Pros:\n",
      "- Fairly well presented\n",
      "- Wide range of experiments, despite underwhelming absolute results\n",
      "Cons:\n",
      "- Quasi-RNNs are almost identical and already have results on small-scale tasks.\n",
      "2020_rkltE0VKwH 130 Contribution:\n",
      "The paper proposes to use a set of handcrafted intrinsic rewards that depend on the novelty of an observation as perceived by the rest of the other agents.\n",
      "2020_rkltE0VKwH 132 Review:\n",
      "\n",
      "The major limitation of the paper in my opinion is the fact that the \"coordination\" that occurs here is only happening at training time, not at execution time.\n",
      "2020_rkltE0VKwH 152 [1] Relational Deep Reinforcement Learning, Zambaldi et al, https://arxiv.org/abs/1806.01830\n",
      "[2] A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning, Carion et al, https://arxiv.org/abs/1910.08809\n",
      "[3] Curiosity-driven Exploration by Self-supervised Prediction, Pathak et al, ICML 2017\n",
      "2020_rkltE0VKwH 158 More detailed feedback:\n",
      "- It would be good to include more learning curves in the main text for the paper.\n",
      "2020_rkltE0VKwH 201 Summary:\n",
      "The paper proposes a method for coordinating the exploration efforts of agents in a multi-agent reinforcement learning setting.\n",
      "2020_rkltE0VKwH 211 Questions to the Authors:\n",
      "1) The second sentence in section 5 is not clear - \"Furthermore, the type of reward ... sufficiently complex\".\n",
      "2018_Hki-ZlbA- 223 Pros:\n",
      "I like the idea and the proposed applications.\n",
      "2018_Hki-ZlbA- 226 Cons:\n",
      "The is not much technical novelty.\n",
      "2018_Hki-ZlbA- 237 Minor details:\n",
      "* I would consider calling them “minimal adversarial samples” instead of “ground-truth”.\n",
      "2018_Hki-ZlbA- 238 * I don’t know if the notation in the Equation in the paragraph describing Carlini & Wagner comes from the original paper, but the inner max would be easier to read as \\max_{i \\neq t} \\{Z(x’)_i \\}\n",
      "* Page 3 “Neural network verification”: I dont agree with the statement that neural networks commonly are trained on “a small set of inputs”.\n",
      "2019_BJx0sjC5FX 278 pros:\n",
      "1) The paper is mostly clearly written and easy to follow.\n",
      "2019_BJx0sjC5FX 279 The diagrams shown in Figure 2 are illustrative;\n",
      "2) TRDN offers a headway to look into and interpret the representations learned in RNNs, which remained largely incomprehensible;\n",
      "3) The analysis and insight provided in section 4 is interesting and insightful.\n",
      "2019_BJx0sjC5FX 281 cons:\n",
      "1) The method relies heavily on these manually crafted role schemes as shown in section 2.1; It is unclear the gap in the approximation of TPRs to the encodings learned in RNNs are due to inaccurate role definition or in fact RNNs learn more complex structural dependencies which TPRs cannot capture;\n",
      "2) The MSE of approximation error shown in Table 1 are not informative.\n",
      "2019_BJx0sjC5FX 288 Pros:\n",
      "1) The paper is well-written and easy to follow.\n",
      "2019_BJx0sjC5FX 294 Cons:\n",
      "1) More detailed and extensive discussion on the contribution of the paper should be included in the introduction part to help readers understand what's the point of investigating TPDN on RNN models.\n",
      "2020_r1e_FpNFDr 313 Detailed comments:\n",
      "I see Wei and Ma ‘19 cited in the beginning only, but there is no further comparison.\n",
      "2020_r1e_FpNFDr 319 Three things remain unclear to me:\n",
      " - How do the constants differ?\n",
      "2020_r1e_FpNFDr 326 In the introduction, the authors: \n",
      " - say that their bounds are size-free, which refers to the bounds not having an explicit dependence on the input size.\n",
      "2020_r1e_FpNFDr 331 *** UPDATE ***\n",
      "I've reread the rebuttals and feel that most of my concerns have been addressed.\n",
      "2020_r1e_FpNFDr 346 Summary\n",
      "This paper studied the generalization power of CNNs and showed several upper bounds of generalization errors.\n",
      "2020_r1e_FpNFDr 352 Decision\n",
      "To the best of my knowledge, this is the first work that proved the size-free generalization bound for multi-layer CNNs.\n",
      "2020_r1e_FpNFDr 361 Comments\n",
      "- The authors claimed that Figure 3 is consistent with theorems because, according to the upper bound of theorems, the distance from the initialization point decreases when the generalization error is the same and the parameter size increases.\n",
      "2020_r1e_FpNFDr 364 Suggestions\n",
      "- Please add the conclusion section which summarizes the paper and discusses the possible research directions.\n",
      "2020_r1e_FpNFDr 365 Minor Comments\n",
      "- page 1, section 1, paragraph 1\n",
      " - ... with roots in (Bartett, 1998) , is that ... → Use \\citet\n",
      "- page 2, section 2.1, paragraph 2\n",
      " - Write the definition of \"expansive\" activations.\n",
      "2020_r1e_FpNFDr 366 - page 3, section 2., theorem 2.1\n",
      " - I think we should replace $\\log(\\lambda n)$ and $\\log(\\lambda)$ in equations with $\\log(\\beta \\lambda n)$ and $\\log(\\beta \\lambda)$, respectively.\n",
      "2020_r1e_FpNFDr 367 - page 3, section 2.2, definition 2.2\n",
      " - $N$ → $\\mathbb{N}$\n",
      "2020_B1lsXREYvr 385 Minor concerns:\n",
      " \n",
      "Equation 3.1 is not clear.\n",
      "2020_B1lsXREYvr 387 Contributions:\n",
      "This paper tackles the problem of One-shot Neural architecture search by proposing a new method.\n",
      "2020_B1lsXREYvr 391 Clarity\n",
      "Overall, the paper is well motivated and the technical content is good.\n",
      "2020_B1lsXREYvr 394 Novelty\n",
      "The main novelty in my opinion is the application of compressive sensing methods to One-shot NAS.\n",
      "2020_B1lsXREYvr 398 Results\n",
      "The experiment section is not self-content, the readers is refered a couple of times to other papers to get details that are critical to reproducibility and understanding.\n",
      "2020_B1lsXREYvr 402 Points of improvement:\n",
      "1 - Structure of the paper\n",
      "2 - Clarify novelty compared to HARMONICA ( not the application domain please)\n",
      "3 - demonstrate that with the same search space your method is competitive.\n",
      "2020_B1lsXREYvr 406 Preliminary decision:\n",
      "For now, I will say *weak reject*\n",
      "2020_B1lsXREYvr 417 I will adjust my score depending on the authors responses concerning the following questions/issues:\n",
      "1) The correlation between the architectures evaluated using the one-shot weights and retrained from scratch, seems to be of crucial importance in your method, since you directly use the one-shot weights to collect the measurements, similarly to Random Search with weight sharing [3], ENAS [4] or Bender et al [5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_B1lsXREYvr 425 References\n",
      "[1] Hanxiao Liu, Karen Simonyan, and Yiming Yang.\n",
      "2020_B1lsXREYvr 435 In ICML, 2018\n",
      "[5] Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, Quoc Le.\n",
      "2020_B1lsXREYvr 437 In ICML, 2018\n",
      "[6] Elad Hazan, Adam Klivans, Yang Yuan.\n",
      "2020_B1lsXREYvr 369 Their approach consists in using a two stage algorithm:\n",
      "- A first Neural Network $f$ is trained for predicting the performances of sub-architectures.\n",
      "2020_B1lsXREYvr 377 Major concern:\n",
      "1/ I did not find the proof of Theorem 3.2 in the main paper and in the appendix, so I do not buy it.\n",
      "2018_SkZxCk-0Z 454 After revision\n",
      "I think the authors have improved the experiments substantially.\n",
      "2018_SkZxCk-0Z 455 SUMMARY \n",
      "The paper is fairly broad in what it is trying to achieve, but the approach is well thought out.\n",
      "2018_SkZxCk-0Z 461 POSITIVES \n",
      "The structure of this paper was very well done.\n",
      "2018_SkZxCk-0Z 473 NEGATIVES\n",
      "One issue I had with the paper is regarding the creation of the logical entailment dataset.\n",
      "2018_SkZxCk-0Z 480 RELATED WORK \n",
      "The paper has an extensive section dedicated to covering related work.\n",
      "2018_SkZxCk-0Z 482 CONCLUSION \n",
      "Given the thorough investigation into previous networks’ capabilities in logical entailment learning, I would accept this paper as a valid scientific contribution.\n",
      "2019_rJzoujRct7 524 - Figure 3, 4 should just say #of games instead of \"iteration\"\n",
      "This paper shows that one choice for a supervised learning system on a CCP game database can achieve amateur level human play.\n",
      "2019_rJzoujRct7 509 The resulting model is able beat MicroWe, the current state of the art in playing CCP, and even are able to beat a few \"top amateur players\"\n",
      "- Why is the bid module also not learned?\n",
      "2018_HkfXMz-Ab 547 Positives:\n",
      " \n",
      " • Novel algorithm and addition of rich java like language in subfield of 'conditional program generation' proposed\n",
      " • Very good abstract: It explains high level overview of topic and sets it into context plus gives a sketch of the algorithm and presents the positive results.\n",
      "2018_HkfXMz-Ab 548 • Excellently structured and presented paper\n",
      " \n",
      " • Motivation given in form of relevant applications and mention that it is relatively unstudied\n",
      " • The hypothesis/ the papers goal is clearly stated.\n",
      "2018_HkfXMz-Ab 554 Examples are given where appropriate in a clear and coherent manner\n",
      " • Problem statement well defined mathematically and understandable for a broad audience\n",
      " • Mentioning of failures and limitations demonstrates a realistic view on the project\n",
      " • Complexity and time analysis provided\n",
      " • Paper written so that it's easy for a reader to implement the methods\n",
      " • Detailed descriptions of all instantiations even parameters and comparison methods\n",
      " • System specified\n",
      " • Validation method specified\n",
      " • Data and repository, as well as cleaning process provided\n",
      " • Every figure and plot is well explained and interpreted\n",
      " • Large successful evaluation section provided\n",
      " • Many different evaluation measures defined to measure different properties of the project\n",
      " • Different observability modes\n",
      " • Evaluation against most compatible methods from other sources \n",
      " • Results are in line with hypothesis\n",
      " • Thorough appendix clearing any open questions \n",
      " \n",
      "It would have been good to have a summary/conclusion/future work section\n",
      " \n",
      "SUMMARY: ACCEPT.\n",
      "2018_HkfXMz-Ab 580 This paper has many strengths:\n",
      "1) The writing is clear, and the paper is well-motivated\n",
      "2) The proposed algorithm is described in excellent detail, which is essential to reproducibility\n",
      "3) As stated previously, the approach is validated with a large number of real Android projects\n",
      "4) The fact that the language generated is non-trivial (Java-like) is a substantial plus\n",
      "5) Good discussion of limitations\n",
      "Overall, this paper is a valuable addition to the empirical software engineering community, and a nice break from more traditional approaches of learning abstract syntax trees.\n",
      "2017_BJ9fZNqle 646 Typos:\n",
      "as well as the well as the generated prior-> as well as the generated prior\n",
      "2017_BJ9fZNqle 583 The authors propose using a mixture prior rather than a uni-modal\n",
      "prior for variational auto-encoders.\n",
      "2017_BJ9fZNqle 584 They argue that the simple\n",
      "uni-modal prior \"hinders the overall expressivity of the learned model\n",
      "as it cannot possibly capture more complex aspects of the data\n",
      "distribution.\"\n",
      "2017_BJ9fZNqle 585 I find the motivation of the paper suspicious because while the prior\n",
      "may be uni-modal, the posterior distribution is certainly not.\n",
      "2017_BJ9fZNqle 586 Furthermore, a uni-modal distribution on the latent variable space can\n",
      "certainly still lead to the capturing of complex, multi-modal data\n",
      "distributions.\n",
      "2017_BJ9fZNqle 587 (As the most trivial case, take the latent variable\n",
      "space to be a uniform distribution; take the likelihood to be a\n",
      "point mass given by applying the true data distribution's inverse CDF\n",
      "to the uniform.\n",
      "2017_BJ9fZNqle 589 In addition, multi-modality is arguably an overfocused concept in the\n",
      "literature, where the (latent variable) space is hardly anymore worth\n",
      "capturing from a mixture of simple distributions when it is often a\n",
      "complex nonlinear space.\n",
      "2017_BJ9fZNqle 590 It is unclear from the experiments how much\n",
      "the influence of the prior's multimodality influences the posterior to\n",
      "capture more complex phenomena, and whether this is any better than\n",
      "considering a more complex (but still reparameterizable) distribution\n",
      "on the latent space.\n",
      "2017_BJ9fZNqle 591 I recommend that this paper be rejected, and encourage the authors to\n",
      "more extensively study the effect of different priors.\n",
      "2017_BJ9fZNqle 592 I'd also like to make two additional comments:\n",
      "While there is no length restriction at ICLR, the 14 page document can\n",
      "be significantly condensed without loss of describing their innovation\n",
      "or clarity.\n",
      "2017_BJ9fZNqle 595 It was submitted with many significant incomplete details (e.g., no experiments,\n",
      "many missing citations, a figure placed inside that was pencilled in\n",
      "by hand, and several missing paragraphs).\n",
      "2017_BJ9fZNqle 596 These details were not\n",
      "completed until roughly a week(?)\n",
      "2017_BJ9fZNqle 598 I recommend the chairs discuss\n",
      "this in light of what should be allowed next year.\n",
      "2017_BJ9fZNqle 632 The detailed comments are as follows:\n",
      "--The author explains the limitations of the VAEs with standard Gaussian prior in the last paragraph of 3.1 and the last paragraph of 5.1) Hence, a multimodal prior would help the VAEs overcome the issues of optimisation.\n",
      "2019_SyxZJn05YX 657 (-) More than two datasets are necessary to show the effectiveness of the methods\n",
      "comments)\n",
      "- What is the higher level feature map P_m?\n",
      "2019_SyxZJn05YX 649 pros)\n",
      "(+) This paper is well-written and easy to follow.\n",
      "2019_SyxZJn05YX 654 cons)\n",
      "(-) The method of obtaining the representative in buffer B is not clearly presented.\n",
      "2019_SyxZJn05YX 674 OVERVIEW:\n",
      "The authors tackle the problem of detecting small/low resolution objects in an image.\n",
      "2019_SyxZJn05YX 684 COMMENTS:\n",
      "Clarity - The paper is well written and easy to follow.\n",
      "2019_SyxZJn05YX 687 QUESTIONS:\n",
      "1) The Class Buffer seems very restricted in having a single element per object category per scale to represent all features.\n",
      "2019_SyxZJn05YX 691 (and baseball bat + bedroll)\n",
      "3) In Table 4 of Appendix where you compare with more object detection results, I find it interesting that Mask RCNN, updated results has a might higher AP_S (43.5) compared to you (27.2) and everyone else.\n",
      "2017_B1ckMDqlg 714 Paper Strengths: \n",
      "-- Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting very large datasets in a computationally feasible manner\n",
      "-- The effective batch size for training the MoE drastically increased also\n",
      "-- Interesting experimental results on the effects of increasing the number of MoEs, which is expected.\n",
      "2017_B1ckMDqlg 707 There are also some glitches in the writing, eg: the end of Section 3.1) \n",
      "- The paper is missing some important references in conditional computation (eg: https://arxiv.org/pdf/1308.3432.pdf) which deal with very similar issues in deep learning.\n",
      "2017_B1ckMDqlg 702 I have the several comments on the paper:\n",
      "- I believe that the authors can do a better job in their presentation.\n",
      "2017_B1ckMDqlg 715 Paper Weaknesses:\n",
      "--- there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss the use of MoE and other alternatives in terms of computational efficiency and other factors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017_B1ckMDqlg 724 Small comment:\n",
      "I like Figure 3, but it's not entirely clear whether datapoints coincide between left and right plots.\n",
      "2017_HJ0NvFzxl 739 A much simpler version of a similar trick may have been proposed in the context of memory networks, also for ICLR'17 (see match type in \"LEARNING END-TO-END GOAL-ORIENTED DIALOG\" by Bordes et al)\n",
      "\n",
      "The authors also mention the time and size needed to train the model: is the issue arising for learning, inference or both?\n",
      "2017_HJ0NvFzxl 745 Revision: I have improved my rating for the following reasons:\n",
      "- Pointers to an highly readable and well structured Theano source is provided.\n",
      "2017_S1_pAu9xl 786 Strengths:\n",
      "- Overall well written and algorithm is presented clearly.\n",
      "2017_S1_pAu9xl 790 Some points:\n",
      "- The energy analysis in Table 3 assumes dense activations due to the unpredictability of sparse activations.\n",
      "2017_S1_pAu9xl 801 Preliminary Rating:\n",
      "I think this is an interesting paper with convincing results but is somewhat lacking in novelty.\n",
      "2017_S1_pAu9xl 802 Minor notes:\n",
      "- Table 3 lists FLOPS rather than Energy for the full precision model.\n",
      "2017_S1_pAu9xl 804 - Section 5 'speeding up'\n",
      "- 5.1.1 figure reference error last line\n",
      "2017_S1_pAu9xl 764 Strengths:\n",
      "1)The paper shows performance improvements over existing solutions\n",
      "2)The idea of learning the quantization instead of using pre-defined human-made algorithm is nice and very much in the spirit of modern machine learning.\n",
      "2017_S1_pAu9xl 765 Weaknesses:\n",
      "1)The paper is very incremental.\n",
      "2018_SyYYPdg0- 817 I don't see any term that would guarantee:\n",
      "1) Each layer is a natural image.\n",
      "2018_SyYYPdg0- 839 Paper Strengths:\n",
      "+ The idea of the paper is interesting.\n",
      "2018_SyYYPdg0- 842 Paper Weaknesses:\n",
      "- The evaluation of the model is not great: (1) It would be interesting to combine bedroom and kitchen images and train jointly to see what it learns.\n",
      "2019_HyVxPsC9tm 931 I suggest the authors compare their model with these approaches. [1] Spatially Adaptive Computation Time for Residual Networks., Figurnov et al ",
      "\n",
      "[2] Recurrent Models of Visual Attention, Mnih et al\n",
      " ",
      "[3] Action recognition using visual attention, Sharma et al\n",
      " ",
      "[4] End-to-end learning of action detection from frame glimpses in videos, Yeung et al\n",
      " ",
      "[5] Two-Stream Convolutional Networks for Action Recognition in Videos, Simonyan et al ",
      "\n",
      "\n",
      "2019_HyVxPsC9tm 912 Paper weaknesses\n",
      "- A simple baseline that only processes a frame if \\sum_{ij} D_{ij} exceeds a threshold is never mentioned or compared against.\n",
      "2019_HyVxPsC9tm 900 I have several concerns:\n",
      "1)The authors should further clarify their advantages over the popular framework of CNN+LSTM.\n",
      "2019_HyVxPsC9tm 910 Paper strengths\n",
      "- The problem of reducing computational requirements when using CNNs for video analysis is well motivated.\n",
      "2019_HyVxPsC9tm 935 Some examples: ",
      "1.\n",
      "2019_HyVxPsC9tm 938 2\n",
      "2019_HyVxPsC9tm 940 3\n",
      "2019_HyVxPsC9tm 941 Long sentences are difficult to follow: “In real surveillance video application, although the calculation reduction on convolution is the main concern of speeding up the overall processing time, the data transfer is another important factor which contributes to the time”\n",
      " ",
      " ",
      "+ The problem of large-scale video understanding is an important and interesting problem to tackle.\n",
      "2019_HylTBhA5tQ 972 Minor comments:\n",
      "- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.\n",
      "2019_HylTBhA5tQ 948 b) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\n",
      "generative models like in Song et al (2018).\n",
      "2019_HylTBhA5tQ 953 example generation) in robust training framework so that the $\\ell_infty$ attack satisfies  max_{\\alpha, \\beta} f(\\alpha x + \\beta) subject to \\| \\alpha x + \\beta \\|\\leq \\epsilon.\n",
      "2019_HylTBhA5tQ 956 d) \"Because we scale the image by a factor of \\alpha, we also set a stricter criterion of success, ..., perturbation must be less\n",
      "than \\alpha \\epsilon to be counted as a successful attack.\"\n",
      "2019_HylTBhA5tQ 958 Even if you have a scaling factor in x^\\prime = \\alpha x + \\beta, the universal perturbation rule should still be | x - x^\\prime  |_\\infty \\leq \\epsilon.\n",
      "2019_HylTBhA5tQ 965 Pros:\n",
      "- Provides insights on why adversarial training is less effective on some datasets.\n",
      "2019_HylTBhA5tQ 967 Cons:\n",
      "- Lack of theoretical analysis.\n",
      "2019_B1l08oAct7 994 (1)  A deep BNN to show that the cumulative error is negligible as the number of the hidden layers increases \n",
      "(2)  Small latent dimension since CLT may not hold\n",
      "(3)  A heavy-tailed variational distribution since the second moment may not be finite \n",
      "(4)  Other nonlinear activations since the Gaussian approximation may not be accurate due to (generalized) Berry-Esseen theorem\n",
      "(5) A BNN with skip connections  since a Bayesian multiplayer perceptron with skip connections is also a feed-forward BNN\n",
      " \n",
      "Among these cases, I am eager to see some results on a deep thin BNN.\n",
      "2019_B1l08oAct7 1064 Why do you not also compare against this, and show it really\n",
      "   does not work?\n",
      "2019_B1l08oAct7 1044 Why not evaluate at least dDVI with diagonal q(w) on\n",
      "   some much larger models and datasets?\n",
      "2019_B1l08oAct7 1047 - Experiments are OK, but on pretty small datasets, and for single hidden\n",
      "   layer NNs.\n",
      "2019_B1l08oAct7 1048 On such data and models, the Barber&Bishop 98 method could\n",
      "   be run as well\n",
      "- Was MCVI run with re-parameterization?\n",
      "2019_B1l08oAct7 1024 PBP also has the property that a DL system gives you the\n",
      "   gradients.\n",
      "2019_B1l08oAct7 1026 While Barber&BIshop 98 is cited, they miss the expression for <h_j h_l> in\n",
      "   there.\n",
      "2019_B1l08oAct7 1028 - Significance: Judging from the existing experiments, the significance may be\n",
      "   rather small, *if one only looks at test log likelihood*.\n",
      "2019_B1l08oAct7 1029 I'd still give this the\n",
      "   benefit of the doubt, as in particular dDVI could be really interesting at large\n",
      "   scale as well.\n",
      "2019_B1l08oAct7 1031 To increase significance, I recommend to comment beyond just test log\n",
      "   likelihood scores.\n",
      "2019_B1l08oAct7 1032 For example:\n",
      "   - Does the optimization become simpler, less tuning required, more automatic?\n",
      "2019_B1l08oAct7 1035 - Can you do something with your posterior that normal DNN methods cannot\n",
      "      do?\n",
      "2019_B1l08oAct7 1039 Experiments:\n",
      "- What is the q(w) family being used here?\n",
      "2019_B1l08oAct7 1041 I\n",
      "   suppose so for dDVI.\n",
      "2019_B1l08oAct7 1043 Not said anywhere, in main paper or\n",
      "   Appendix\n",
      "- A bit disappointing.\n",
      "2019_B1l08oAct7 1045 Why not quote numbers on speed\n",
      "   and robustness of learning, etc?\n",
      "2019_B1l08oAct7 1046 Show what you really gain by reducing the\n",
      "   variance.\n",
      "2019_B1l08oAct7 1050 If not,\n",
      "   this would be an important missing comparison.\n",
      "2019_B1l08oAct7 1051 Please be clear in the main\n",
      "   text\n",
      "- Advantages over MCVI are not very large.\n",
      "2019_B1l08oAct7 1052 At least, dDVI should be faster to\n",
      "   converge than MCVI.\n",
      "2019_B1l08oAct7 1054 Is it easier to train\n",
      "   dDVI than MCVI?\n",
      "2019_B1l08oAct7 1056 Are they\n",
      "   obtained with the same model?\n",
      "2019_B1l08oAct7 1058 Other points:\n",
      "- Please acknowledge the <h_j h_l> expression in Barber&Bishop 98.\n",
      "2019_B1l08oAct7 1059 Yours is\n",
      "   more elegant and faster (does not need 1D quadrature)\n",
      "- Relation to PBP: Note that dDVI has an advantage in practice.\n",
      "2019_B1l08oAct7 1060 With PBP, I need\n",
      "   to compute gradients for every datapoint.\n",
      "2019_B1l08oAct7 1061 In dDVI, I can do mini-batch\n",
      "   updates.\n",
      "2019_B1l08oAct7 1063 I tend to refer to this kind of work\n",
      "   as \"weak analogies\".\n",
      "2019_B1l08oAct7 1010 Summary:\n",
      "This work is tackling two difficulties in current VB applied to DNNs (\"Bayes by backprop\").\n",
      "2019_B1l08oAct7 1017 Approximations are\n",
      "   tested, great figures to explain things.\n",
      "2019_B1l08oAct7 1018 And the major technical novelty, the\n",
      "   expression for <h_j h_l>, is really interesting and useful.\n",
      "2019_B1l08oAct7 1020 Here, important\n",
      "   details are just missing, for example what q(w) is (fully factorized Gaussian?).\n",
      "2019_B1l08oAct7 1022 - Originality: The idea of matching Gaussian moments along the network graph is\n",
      "   previously done in PBP (Lobato, Adams), as acknowledged here.\n",
      "2019_B1l08oAct7 1023 Porting this from\n",
      "   ADF to VB gives dDVI.\n",
      "2018_H135uzZ0- 1078 Potential improvements:\n",
      "\t\n",
      "  - Please define the terms like FPROP and WTGRAD at the first occurance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017_H1oyRlYgg 1088 Comments:\n",
      "Earlier I had some concern about the correctness of a claim made by the authors, which is resolved now.\n",
      "2017_H1oyRlYgg 1086 Pros and Cons:\n",
      "Although there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks.\n",
      "2017_H1oyRlYgg 1087 Significance:\n",
      "I think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes.\n",
      "2017_r1y1aawlg 1162 Specific comments:\n",
      "- It would be interesting to see what the improvements are if the baseline model is a neural system.\n",
      "2017_r1y1aawlg 1152 Here are some minor typos:\n",
      "- p.2: ... a lookup table that replace*S* each word... ?\n",
      "2017_r1y1aawlg 1153 - p.3: I might be mistanken but it seems to me that j is used for two\n",
      "  different things.\n",
      "2017_r1y1aawlg 1171 Minor comments:\n",
      "- Iteratively improving a generated text was also explored in https://arxiv.org/pdf/1510.09202v1.pdf from a reinforcement learning angle.\n",
      "2017_r1y1aawlg 1103 Minor comments:\n",
      "I find the notation excessively fiddly at times - eg: F^i = (F^{i,1}, F^{i,|F^i|}) - why use |F^i| here when F is a matrix, so surely the length of the slice is not dependent on i?\n",
      "2017_r1y1aawlg 1121 Related work:\n",
      "I think that more could be done to put this work in the context of what has come before and what is currently going on in other parts of ML.\n",
      "2017_r1y1aawlg 1141 Summary: A human translator does not come up with the final translation right\n",
      "away.\n",
      "2017_r1y1aawlg 1142 Instead, (s)he uses an iterative process, starting with a rough draft\n",
      "which is corrected little by little.\n",
      "2017_r1y1aawlg 1143 The idea behind this paper is to\n",
      "implement a similar framework for an automated system.\n",
      "2017_r1y1aawlg 1145 It is my opinion however that drawings illustrating the architectures would help\n",
      "understanding how the different algorithms relate to one another.\n",
      "2017_r1y1aawlg 1146 I like a lot that you report on a preliminary experiment to give an\n",
      "intuition of how difficult the task is.\n",
      "2017_r1y1aawlg 1147 You should highlight the links\n",
      "between the task of finding the errors in a guess translation and the task\n",
      "of iterative refinement.\n",
      "2017_r1y1aawlg 1148 Could you use post-edited text to have a more\n",
      "solid ground-truth?\n",
      "2017_r1y1aawlg 1149 My main concern with this paper is that in the experimental section the \n",
      "iterative approach tries to improve upon only one type of machine translation.\n",
      "2017_r1y1aawlg 1150 Which immediately prompts these questions:\n",
      "- why did they choose that approach to improve on?\n",
      "2017_r1y1aawlg 1151 - what is the part of the improvement that comes from the choice of the\n",
      "  initial draft (maybe it was a very bad draft)?\n",
      "2020_r1eX1yrKwB 1173 <Paper summary>\n",
      "The authors proposed Distribution Matching Prototypical Network (DMPN) for unsupervised domain adaptation.\n",
      "2020_r1eX1yrKwB 1178 <Review summary>\n",
      "The proposed method seems simple but empirically performs well.\n",
      "2020_r1eX1yrKwB 1182 <Details>\n",
      "* Strength\n",
      " + The motivation of using ProtoNet for domain adaptation seems reasonable.\n",
      "2020_r1eX1yrKwB 1185 * Weakness and concerns\n",
      " - Several points on the proposed loss (GCMM and PDM) are not sufficiently discussed.\n",
      "2020_r1eX1yrKwB 1193 (specifically, \\mu^{et}_c)\n",
      "  -- Are Equation (3) and Equation (6) correct?\n",
      "2020_r1eX1yrKwB 1203 * Minor concerns that do not have an impact on the score\n",
      " - Using both f^s_i and F(x^s_i; \\theta) is confusing.\n",
      "2020_r1eX1yrKwB 1208 Strengths:\n",
      " + The paper's experiments show an improvement in the model's performance relative to past work, utilizing a large number of comparison models.\n",
      "2020_r1eX1yrKwB 1210 Weaknesses:\n",
      " - The proposed method for unsupervised domain adaptation is very similar to the prototypical networks approach in [3], with the primary difference being a loss term incentivizing a Gaussian mixture distribution over features.\n",
      "2020_Byg79h4tvB 1238 Summary:\n",
      "- key problem: address \"class mismatch\" in adversarial learning methods for unsupervised domain adaptation (UDA);\n",
      "- contributions: 1) extension of the domain adversarial learning objective to leverage class prototypes (exponential moving average of features weighted by predicted class probabilities) in addition to pseudo-labels and intermediate representations (cf.\n",
      "2020_Byg79h4tvB 1242 - The formulation of the prototypes and additional learning objectives for UDA are clear and seem novel, although I would like to see a discussion of additional related works:\n",
      "-- \"Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results\", Tarvainen and Valpola, NeurIPS'17;\n",
      "-- \"Unsupervised Domain Adaptation with Similarity Learning\", Pinheiro, CVPR'18;\n",
      "-- \"Transferable Prototypical Networks for Unsupervised Domain Adaptation\", Pan et al, CVPR'19.\n",
      "2020_Byg79h4tvB 1253 Additional Feedback:\n",
      "- missing references on sim2real UDA: \"DADA: Depth-aware Domain Adaptation in Semantic Segmentation\" (Vu et al, ICCV'19), \"SPIGAN: Privileged Adversarial Learning from Simulation\" (Lee et al, ICLR'19)\n",
      "## Post rebuttal update\n",
      "I would like to thank the authors for replying to our questions.\n",
      "2020_Byg79h4tvB 1261 pros:\n",
      "+ intra-class compactness to help ambiguous classes\n",
      "concerns:\n",
      "-- Prototypes does not come from nowhere.\n",
      "2020_Byg79h4tvB 1269 details:\n",
      "- terminology: \"intra-class\" is better than \"within class\"\n",
      "- separate citations: eg: entropy minimization, mean-teacher, and virtual adversarial training, have been successfully applied to UDA (Vu et al, 2019; French et al, 2018; Shu et al, 2018) -> entropy minimization (Vu et al, 2019), mean-teacher (French et al, 2018), and virtual adversarial training (Shu et al, 2018), have been successfully applied to UDA\n",
      "- confusion: At the last of Section 3.2, it says \\hat{f}=M^{T}p. But in Equation 9, \\hat{f} and M^{T}p are concatenated, which is confusing: why do you concatenate two identical vectors?\n",
      "2020_Byg79h4tvB 1272 [1] Conditional adversarial domain adaptation, Long et.al, in NeurIPS 2018\n",
      "[2] Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation, You et.al, in ICML 2019\n",
      "2019_H1lFZnR5YX 1289 The presented experiments are also not thorough, there are stronger and simpler baselines for regression like random forests, gradient boosted trees  or kernel ridge regression which are not evaluated and compared.\n",
      "2019_H1lFZnR5YX 1280 Furthermore  the two baseline models used are 11 and 34 years old respectively and i do not believe they represent a thorough review of the potential approaches to this problem.\n",
      "2019_H1lFZnR5YX 1285 Summary:\n",
      "This paper presents a neural network based tree model for the regression via classification problem.\n",
      "2019_H1lFZnR5YX 1291 Comments:\n",
      "1)I was not aware of this age and height estimation tasks.\n",
      "2019_H1lFZnR5YX 1293 Can the authors please elaborate in a  line or two why i-vectors would be suitable for age and height estimation?.\n",
      "2019_H1lFZnR5YX 1299 4)In Conclusion the authors say,  \"In addition, we proposed a scan method and a gradient method to optimize the tree.\"\n",
      "2019_H1lFZnR5YX 1301 Miscellaneous comments:\n",
      "1)This line seems incomplete in Section 1: \"Traditional methods for defining the partition T by prior knowledge, such as equally probable intervals, equal width intervals, k-means clustering, etc.\n",
      "2019_H1lFZnR5YX 1307 Summary:\n",
      "The paper presents a novel supervised-learning method for regression using decision trees and neural nets.\n",
      "2019_H1lFZnR5YX 1314 Comment:\n",
      "This is a technically very interesting contribution, but several points can be considered more carefully as below.\n",
      "2019_H1lFZnR5YX 1329 So which of probabilistic consideration or RvC contributes to the observed improvement is unclear... \n",
      "- The target joint optimization of eq (3) is actually optimized by a number of heuristic ways, and it is quite unclear how it is truly optimized.\n",
      "2020_BkeWw6VFwr 1335 - Contributions:\n",
      "1)The authors aim to provide (probabilistic) certification on top-k predictions, which to my knowledge is the first work to consider this setup.\n",
      "2020_BkeWw6VFwr 1343 - Questions:\n",
      "1)Intuitively, when extending top-1 certification to top-k certification, one would expect using ordered statistics of the prediction outputs from the randomly perturbed inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_BkeWw6VFwr 1377 Summary:\n",
      "This paper studies the certifiable bounds for adversarial perturbations in \\ell_2 radius for top-k predictions instead of top-1 predictions.\n",
      "2020_BkeWw6VFwr 1386 Other comments:\n",
      "1)What is the trend of top-k-clean-accuracy and top-k-adversarial-accuracy as a function of k?\n",
      "2018_HyHmGyZCZ 1417 Minor points:\n",
      "- Typo in Equation 10\n",
      "- Typo on page 6 (/cite instead of \\cite)\n",
      "2018_HyHmGyZCZ 1422 Pros:\n",
      "  1.\n",
      "2018_HyHmGyZCZ 1423 Experimental study on retrofitting existing word vectors for ESL and TOEFL lexical similarity datasets\n",
      "Cons:\u000b",
      "  1.\n",
      "2018_HyHmGyZCZ 1425 2\n",
      "2018_HyHmGyZCZ 1401 I think the work has several important issues:\n",
      "1)The work is very light on references.\n",
      "2018_HyUNwulC- 1432 # Summary and Assessment\n",
      "The paper addresses an important issue–that of making learning of recurrent networks tractable for sequence lengths well beyond 1’000s of time steps.\n",
      "2018_HyUNwulC- 1435 Here, if certain conditions are satisfied, the calculation of the output   can be parallelised massively.\n",
      "2018_HyUNwulC- 1442 ## Minor\n",
      "- I challenge the claim that thousands and millions of time steps are a common issue in “robotics, remote sensing, control systems, speech recognition, medicine and finance”, as claimed in the first paragraph of the introduction.\n",
      "2018_HyUNwulC- 1444 This paper abstracts two recently-proposed RNN variants into a family of RNNs called the Linear Surrogate RNNs which satisfy  Blelloch's criteria for parallelizable sequential computation.\n",
      "2020_HkgsPhNYPS 1504 4)The author missed some important baselines here.\n",
      "\"1) Symmetric cross entropy for robust learning with noisy labels, ICCV2019 \n",
      "     2) Joint Optimization Framework for Learning with Noisy Labels, CVPR2018 \n",
      "     3) Dimensionality-driven learning with noisy labels, ICML2018\"\n",
      "2020_HkgsPhNYPS 1468 --- Overall ---\n",
      "This paper proposes an algorithm for learning from data with noisy labels which alternates between updating the model and removing samples that look like they have noisy labels, thereby allowing the training procedure to focus on clean samples.\n",
      "2020_HkgsPhNYPS 1471 --- Major comments ---\n",
      "1)I found it somewhat unclear how large the methodological contribution was.\n",
      "2020_HkgsPhNYPS 1488 --- Minor comments ---\n",
      "1)Figures 1 and 4 are not readable in black and grey-scale.\n",
      "2020_HkgsPhNYPS 1493 Summary:\n",
      "The paper proposed a self-ensemble label filtering (SELF) method to deal with the noisy label learning problem.\n",
      "2020_HkgsPhNYPS 1494 They progressively filter out the wrong labels during training, i.e.,  filtered samples are removed entirely from the supervised\n",
      "training loss, and are leveraged via semi-supervised learning in the unsupervised loss.\n",
      "2020_HkgsPhNYPS 1496 Strengths:\n",
      "1)The motivation of the paper is very clear.\n",
      "2020_HkgsPhNYPS 1498 Weakness:\n",
      "1)The contribution of SELF is not clear.\n"
     ]
    }
   ],
   "source": [
    "roberta_vectors = defaultdict(dict)\n",
    "skip_uids = []\n",
    "\n",
    "for pid in gt_dict:\n",
    "    roberta_vectors[pid] = {}\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_content_words_spacy(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunks = mcomp_chunks_from_sent\n",
    "            else:\n",
    "                final_chunks = [df.loc[mcs][\"Sent\"]]\n",
    "            \n",
    "            roberta_vectors[pid][mcs] = []\n",
    "            for single_chunk in final_chunks:\n",
    "                vec = embed_text_using_roberta(single_chunk.strip()).mean(1).detach().numpy()\n",
    "                roberta_vectors[pid][mcs].append(vec / norm(vec))\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_content_words_spacy(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunks = mcomp_chunks_from_sent\n",
    "            else:\n",
    "                final_chunks = [df.loc[mcs][\"Sent\"]]\n",
    "            \n",
    "            roberta_vectors[pid][mcs] = []\n",
    "            for single_chunk in final_chunks:\n",
    "                vec = embed_text_using_roberta(single_chunk.strip()).mean(1).detach().numpy()\n",
    "                roberta_vectors[pid][mcs].append(vec / norm(vec))\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1190\n"
     ]
    }
   ],
   "source": [
    "mcomp_sentences = {}\n",
    "not_mcomp_sentences = {}\n",
    "\n",
    "for pid in gt_dict:\n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            mcomp_sentences[mcs] = pid\n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            not_mcomp_sentences[mcs] = pid\n",
    "print(len(mcomp_sentences), len(not_mcomp_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_with_mcomp = defaultdict(dict)\n",
    "sim_with_not_mcomp = defaultdict(dict)\n",
    "sim_with_notmcomp_paper_sents = defaultdict(dict)\n",
    "\n",
    "mean_at_k = [\"1\", \"3\", \"5\", \"7\", \"10\", \"20\", \"30\", \"50\", \"100\", \"500\", \"1000\", \"1380\"]\n",
    "\n",
    "for sid in mcomp_sentences:\n",
    "    \n",
    "    # 1. With other mcomp sentences\n",
    "    temp_list = []    \n",
    "    for osid in mcomp_sentences:\n",
    "        if osid != sid:\n",
    "            for cvec1 in roberta_vectors[mcomp_sentences[osid]][osid]:\n",
    "                for cvec2 in roberta_vectors[mcomp_sentences[sid]][sid]:\n",
    "                    temp_list.append(np.inner(cvec1, cvec2)[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "\n",
    "    \n",
    "    # 2. With other not_mcomp_sentences\n",
    "    temp_list = []\n",
    "    for osid in not_mcomp_sentences:\n",
    "        for cvec1 in roberta_vectors[not_mcomp_sentences[osid]][osid]:\n",
    "            for cvec2 in roberta_vectors[mcomp_sentences[sid]][sid]:\n",
    "                temp_list.append(np.inner(cvec1, cvec2)[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_not_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_not_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "\n",
    "    \n",
    "    # 3. With not_mcomp_sentences of the same paper\n",
    "    temp_list = []    \n",
    "    for osid in not_mcomp_sentences:\n",
    "        if not_mcomp_sentences[osid] == mcomp_sentences[sid]:\n",
    "            for cvec1 in roberta_vectors[not_mcomp_sentences[osid]][osid]:\n",
    "                for cvec2 in roberta_vectors[mcomp_sentences[sid]][sid]:\n",
    "                    temp_list.append(np.inner(cvec1, cvec2)[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_notmcomp_paper_sents[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_notmcomp_paper_sents[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_sim_plot\n",
    "diff12 = {\"all\": []}\n",
    "for vv in mean_at_k:\n",
    "    diff12[str(vv)] = []\n",
    "\n",
    "diff13 = {\"all\": []}\n",
    "for vv in mean_at_k:\n",
    "    diff13[str(vv)] = []\n",
    "\n",
    "for sid in sim_with_mcomp:\n",
    "    diff12[\"all\"].append(sim_with_mcomp[sid][\"mean\"] - sim_with_not_mcomp[sid][\"mean\"])\n",
    "    diff13[\"all\"].append(sim_with_mcomp[sid][\"mean\"] - sim_with_notmcomp_paper_sents[sid][\"mean\"])\n",
    "    \n",
    "    for vv in mean_at_k:\n",
    "        diff12[str(vv)].append(sim_with_mcomp[sid][\"mean_{}\".format(vv)] - sim_with_not_mcomp[sid][\"mean_{}\".format(vv)])\n",
    "        diff13[str(vv)].append(sim_with_mcomp[sid][\"mean_{}\".format(vv)] - sim_with_notmcomp_paper_sents[sid][\"mean_{}\".format(vv)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final results of most similar content word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">20</td><td style=\"text-align: right;\">30</td><td style=\"text-align: right;\">50</td><td style=\"text-align: right;\">100</td><td style=\"text-align: right;\">500</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">1380</td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">0.01</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\"> 0</td><td style=\"text-align: right;\"> 0</td><td style=\"text-align: right;\"> 0</td><td style=\"text-align: right;\"> 0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">   0</td><td style=\"text-align: right;\">   0</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3  </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380</td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.19</td><td style=\"text-align: right;\">0.3</td><td style=\"text-align: right;\">0.28</td><td style=\"text-align: right;\">0.21</td><td style=\"text-align: right;\"> 0.27</td><td style=\"text-align: right;\"> 0.18</td><td style=\"text-align: right;\"> 0.19</td><td style=\"text-align: right;\"> 0.25</td><td style=\"text-align: right;\">  0.66</td><td style=\"text-align: right;\">  0.99</td><td style=\"text-align: right;\">   0.99</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))\n",
    "\n",
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff13[val])/len(diff13[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for i in range(0, df.shape[0]):\n",
    "    dataset.append(str(df.loc[i][\"Sent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The authors propose to use k-DPP to select a set of diverse parameters and use them to search for a good a hyperparameter setting.',\n",
       " 'This paper covers the related work nicely, with details on both closed loop and open loop methods.',\n",
       " 'The rest of the paper are also clearly written.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \" \".join(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str,\n",
       " 'The authors propose to use k-DPP to select a set of diverse parameters and use them to search for a good a hyperparameter setting. This paper covers the related work nicely, with details on both close')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset), dataset[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "tfIdftransformer = tfIdfVectorizer.fit_transform([dataset])\n",
    "tfidf_df = pd.DataFrame(tfIdf[0].T.toarray(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           TF-IDF\n",
      "the      0.756239\n",
      "of       0.269876\n",
      "is       0.248390\n",
      "to       0.241554\n",
      "and      0.194675\n",
      "in       0.185234\n",
      "that     0.125009\n",
      "this     0.116219\n",
      "for      0.110685\n",
      "paper    0.100593\n",
      "it       0.096361\n",
      "on       0.095710\n",
      "be       0.085944\n",
      "are      0.084641\n",
      "not      0.075852\n",
      "with     0.067062\n",
      "as       0.063481\n",
      "model    0.044925\n",
      "more     0.044600\n",
      "authors  0.044274\n",
      "would    0.042321\n",
      "an       0.042321\n",
      "by       0.041670\n",
      "can      0.039065\n",
      "which    0.037438\n"
     ]
    }
   ],
   "source": [
    "tfidf_df = tfidf_df.sort_values('TF-IDF', ascending=False)\n",
    "print (tfidf_df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000s</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01830</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03762</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05700</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allowing</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allows</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>almost</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alone</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>along</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          TF-IDF\n",
       "000s         0.0\n",
       "01830        0.0\n",
       "03762        0.0\n",
       "05           0.0\n",
       "05700        0.0\n",
       "...          ...\n",
       "allowing     0.0\n",
       "allows       0.0\n",
       "almost       0.0\n",
       "alone        0.0\n",
       "along        0.0\n",
       "\n",
       "[200 rows x 1 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final results of most similar chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1  </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">500</td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.4</td><td style=\"text-align: right;\">0.38</td><td style=\"text-align: right;\">0.33</td><td style=\"text-align: right;\">0.29</td><td style=\"text-align: right;\"> 0.25</td><td style=\"text-align: right;\"> 0.18</td><td style=\"text-align: right;\"> 0.15</td><td style=\"text-align: right;\"> 0.09</td><td style=\"text-align: right;\">  0.01</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">   0.21</td><td style=\"text-align: right;\">   0.39</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.75</td><td style=\"text-align: right;\">0.79</td><td style=\"text-align: right;\">0.81</td><td style=\"text-align: right;\">0.85</td><td style=\"text-align: right;\"> 0.85</td><td style=\"text-align: right;\"> 0.87</td><td style=\"text-align: right;\"> 0.89</td><td style=\"text-align: right;\"> 0.92</td><td style=\"text-align: right;\">  0.97</td><td style=\"text-align: right;\">  0.73</td><td style=\"text-align: right;\">   0.68</td><td style=\"text-align: right;\">   0.68</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))\n",
    "\n",
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff13[val])/len(diff13[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final results of redo with smaller entity set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100</td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.49</td><td style=\"text-align: right;\">0.45</td><td style=\"text-align: right;\">0.41</td><td style=\"text-align: right;\">0.35</td><td style=\"text-align: right;\"> 0.31</td><td style=\"text-align: right;\"> 0.26</td><td style=\"text-align: right;\"> 0.21</td><td style=\"text-align: right;\"> 0.09</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0.16</td><td style=\"text-align: right;\">   0.56</td><td style=\"text-align: right;\">   0.89</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.79</td><td style=\"text-align: right;\">0.82</td><td style=\"text-align: right;\">0.84</td><td style=\"text-align: right;\">0.84</td><td style=\"text-align: right;\"> 0.84</td><td style=\"text-align: right;\"> 0.93</td><td style=\"text-align: right;\"> 0.96</td><td style=\"text-align: right;\"> 0.97</td><td style=\"text-align: right;\">  0.82</td><td style=\"text-align: right;\">  0.69</td><td style=\"text-align: right;\">   0.69</td><td style=\"text-align: right;\">   0.69</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))\n",
    "\n",
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff13[val])/len(diff13[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30  </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100</td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380  </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.51</td><td style=\"text-align: right;\">0.45</td><td style=\"text-align: right;\">0.38</td><td style=\"text-align: right;\">0.33</td><td style=\"text-align: right;\"> 0.31</td><td style=\"text-align: right;\"> 0.23</td><td style=\"text-align: right;\"> 0.2</td><td style=\"text-align: right;\"> 0.09</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0.16</td><td style=\"text-align: right;\">   0.56</td><td style=\"text-align: right;\">   0.9</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100  </td><td style=\"text-align: right;\">500  </td><td style=\"text-align: right;\">1000  </td><td style=\"text-align: right;\">1380  </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.81</td><td style=\"text-align: right;\">0.84</td><td style=\"text-align: right;\">0.85</td><td style=\"text-align: right;\">0.85</td><td style=\"text-align: right;\"> 0.85</td><td style=\"text-align: right;\"> 0.91</td><td style=\"text-align: right;\"> 0.96</td><td style=\"text-align: right;\"> 0.97</td><td style=\"text-align: right;\">  0.8</td><td style=\"text-align: right;\">  0.7</td><td style=\"text-align: right;\">   0.7</td><td style=\"text-align: right;\">   0.7</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))\n",
    "\n",
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff13[val])/len(diff13[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Till here only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Chunks ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corenlp = StanfordCoreNLP(\"/home/shruti/Documents/DataNLP/stanford-corenlp-4.1.0/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(sent):\n",
    "    parse_str = corenlp.parse(sent)\n",
    "    nltk_tree = Tree.fromstring(parse_str)\n",
    "    \n",
    "#     print(nltk_tree)\n",
    "    \n",
    "    subtrees_list = list(nltk_tree.subtrees())\n",
    "    subtrees_tpos = nltk_tree.treepositions()\n",
    "    for i in range(0, len(nltk_tree.leaves())):\n",
    "        tp_leaf = nltk_tree.leaf_treeposition(i)\n",
    "        subtrees_tpos.remove(tp_leaf)\n",
    "    \n",
    "    dict_len_st = {}\n",
    "    depth_of_subtree = []\n",
    "    for _, i in enumerate(subtrees_list):\n",
    "        depth_of_subtree.append((i, len(subtrees_tpos[_])))\n",
    "        dict_len_st[str(i)] = len(subtrees_tpos[_])\n",
    "    \n",
    "    cdepths = []\n",
    "    for d in depth_of_subtree:\n",
    "        cdepths.append(d[1])\n",
    "    depth_counter = Counter(cdepths)\n",
    "    sorted_depths = sorted(list(depth_counter.keys()))\n",
    "    print(sorted(depth_counter.items(), key=lambda x: x[0]))\n",
    "    \n",
    "    depth_to_split = None\n",
    "    print(sorted_depths) \n",
    "    for sd in sorted_depths:\n",
    "        if depth_counter[sd] == 3:\n",
    "            depth_to_split = 3\n",
    "        elif depth_counter[sd] > 3:\n",
    "            depth_to_split = sd\n",
    "            break\n",
    "    if depth_to_split == None or depth_to_split == 4:\n",
    "        print(\"Depth to split: {}\".format(depth_to_split))\n",
    "        \n",
    "    print(\"depth: \", depth_to_split)\n",
    "    \n",
    "    subtree_chunks = []\n",
    "    for i in depth_of_subtree:\n",
    "        if i[1] == depth_to_split:\n",
    "            subtree_chunks.append(i)\n",
    "    \n",
    "    final_chunks_sent = []\n",
    "    \n",
    "#     for tt in subtree_chunks:\n",
    "#         print(tt)\n",
    "    \n",
    "    for stchunk in subtree_chunks:\n",
    "        print(len(stchunk[0].leaves()), stchunk[0].leaves())\n",
    "#         print(stchunk)\n",
    "        if len(stchunk[0].leaves()) > 5:\n",
    "            subsubtrees = list(stchunk[0].subtrees())\n",
    "            fnlsubsub = []\n",
    "            for sss in subsubtrees:\n",
    "                if str(sss) in dict_len_st and dict_len_st[str(sss)] == depth_to_split+1:\n",
    "                    fnlsubsub.append(sss)\n",
    "            for subchunk in fnlsubsub:\n",
    "                final_chunks_sent.append(\" \".join(subchunk.leaves()))\n",
    "        else:\n",
    "            final_chunks_sent.append(\" \".join(stchunk[0].leaves()))\n",
    "#         final_chunks_sent.append(\" \".join(stchunk[0].leaves()))\n",
    "    \n",
    "    return final_chunks_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20) The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20) The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.\n",
      "[(0, 1), (1, 1), (2, 10), (3, 10), (4, 21), (5, 16), (6, 12), (7, 24), (8, 15), (9, 8), (10, 6), (11, 9), (12, 6), (13, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Second']\n",
      "1 [',']\n",
      "19 ['their', 'study', 'only', 'applies', 'to', 'a', 'small', 'number', 'like', '3', '-', '6', 'hyperparameters', 'with', 'a', 'small', 'k', '=', '20']\n",
      "1 ['-RRB-']\n",
      "20 ['The', 'real', 'challenge', 'lies', 'in', 'scaling', 'up', 'to', 'many', 'hyperparameters', 'or', 'even', 'k', '-', 'DPP', 'sampling', 'for', 'larger', 'k.', 'Third']\n",
      "1 [',']\n",
      "13 ['the', 'authors', 'do', 'not', 'compare', 'against', 'some', 'relevant', ',', 'recent', 'work', ',', 'e.g.']\n",
      "1 [',']\n",
      "22 ['Springenberg', 'et', 'al', '-LRB-', 'http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf', '-RRB-', 'and', 'Snoek', 'et', 'al', '-LRB-', 'https://arxiv.org/pdf/1502.05700.pdf', '-RRB-', 'that', 'is', 'essential', 'for', 'this', 'kind', 'of', 'empirical', 'study']\n",
      "1 ['.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Second',\n",
       " ',',\n",
       " 'their study',\n",
       " 'only',\n",
       " 'applies to a small number like 3 - 6 hyperparameters with a small k = 20',\n",
       " '-RRB-',\n",
       " 'The real challenge',\n",
       " 'lies in scaling up to many hyperparameters or even k - DPP sampling for larger k. Third',\n",
       " ',',\n",
       " 'the authors',\n",
       " 'do not compare against some relevant , recent work , e.g.',\n",
       " ',',\n",
       " 'Springenberg et al',\n",
       " '-LRB- http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf -RRB- and Snoek et al -LRB- https://arxiv.org/pdf/1502.05700.pdf -RRB- that is essential for this kind of empirical study',\n",
       " '.']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(s)\n",
    "get_chunks(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_from_sent = {\"mcomp\": [], \"nmcomp\": []}\n",
    "\n",
    "for pid in gt_dict:\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = get_chunks(df.loc[mcs][\"Sent\"])\n",
    "            chunks_from_sent[\"mcomp\"].append((df.loc[mcs][\"Sent\"], mcomp_chunks_from_sent))\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs)\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = get_chunks(df.loc[mcs][\"Sent\"])\n",
    "            chunks_from_sent[\"nmcomp\"].append((df.loc[mcs][\"Sent\"], mcomp_chunks_from_sent))\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_from_sent[\"mcomp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(cd.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Guangdong University of Foreign Studies is located in Guangzhou.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP\n",
      "      (NP (NNP Guangdong) (NNP University))\n",
      "      (PP (IN of)\n",
      "        (NP (NNP Foreign) (NNPS Studies))))\n",
      "    (VP (VBZ is)\n",
      "      (VP (VBN located)\n",
      "        (PP (IN in)\n",
      "          (NP (NNP Guangzhou)))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "parse_str = corenlp.parse(sentence)\n",
    "print(parse_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tree = Tree.fromstring(parse_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, (0, 0, 0, 1, 0))"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_tree.leaves()), nltk_tree.leaf_treeposition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtrees_list = list(nltk_tree.subtrees())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtrees_tpos = nltk_tree.treepositions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtrees_tpos[_]\n",
    "for i in range(0, len(nltk_tree.leaves())):\n",
    "    tp_leaf = nltk_tree.leaf_treeposition(i)\n",
    "    subtrees_tpos.remove(tp_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           ROOT                                       \n",
      "                                            |                                          \n",
      "                                            S                                         \n",
      "                           _________________|_______________________________________   \n",
      "                          |                                   VP                    | \n",
      "                          |                        ___________|___                  |  \n",
      "                          NP                      |               VP                | \n",
      "            ______________|_____                  |      _________|___              |  \n",
      "           |                    PP                |     |             PP            | \n",
      "           |               _____|_____            |     |          ___|______       |  \n",
      "           NP             |           NP          |     |         |          NP     | \n",
      "     ______|______        |      _____|_____      |     |         |          |      |  \n",
      "   NNP           NNP      IN   NNP         NNPS  VBZ   VBN        IN        NNP     . \n",
      "    |             |       |     |           |     |     |         |          |      |  \n",
      "Guangdong     University  of Foreign     Studies  is located      in     Guangzhou  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nltk_tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 ROOT ()\n",
      "7 S (0,)\n",
      "5 NP (0, 0)\n",
      "3 NP (0, 0, 0)\n",
      "2 NNP (0, 0, 0, 0)\n",
      "2 NNP (0, 0, 0, 1)\n",
      "4 PP (0, 0, 1)\n",
      "2 IN (0, 0, 1, 0)\n",
      "3 NP (0, 0, 1, 1)\n",
      "2 NNP (0, 0, 1, 1, 0)\n",
      "2 NNPS (0, 0, 1, 1, 1)\n",
      "6 VP (0, 1)\n",
      "2 VBZ (0, 1, 0)\n",
      "5 VP (0, 1, 1)\n",
      "2 VBN (0, 1, 1, 0)\n",
      "4 PP (0, 1, 1, 1)\n",
      "2 IN (0, 1, 1, 1, 0)\n",
      "3 NP (0, 1, 1, 1, 1)\n",
      "2 NNP (0, 1, 1, 1, 1, 0)\n",
      "2 . (0, 2)\n"
     ]
    }
   ],
   "source": [
    "depth_of_subtree = []\n",
    "for _, i in enumerate(subtrees_list):\n",
    "    depth_of_subtree.append((i, len(subtrees_tpos[_])))\n",
    "    print(i.height(), i.label(), subtrees_tpos[_])\n",
    "#     break\n",
    "#     if type(i) != Tree:\n",
    "#         print(i)\n",
    "#           subtrees_tpos[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdepths = []\n",
    "for d in depth_of_subtree:\n",
    "    cdepths.append(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1, 1: 1, 2: 3, 3: 4, 4: 6, 5: 4, 6: 1})"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(cdepths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.treeposition_spanning_leaves(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, 1, 0)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.leaf_treeposition(1)\n",
    "# for x in :\n",
    "#     if not isinstance(x, Tree):\n",
    "#         print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Guangdong',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Foreign',\n",
       " 'Studies',\n",
       " 'is',\n",
       " 'located',\n",
       " 'in',\n",
       " 'Guangzhou',\n",
       " '.']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using chunks from the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This DT swallow\n",
      "an DT swallow\n",
      "unladen JJ swallow\n",
      "swallow NN swallow\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This an unladen swallow.\")\n",
    "for sent in doc.sents:\n",
    "    for tok in sent:\n",
    "        if tok.is_alpha:\n",
    "            print(tok.orth_, tok.tag_, tok.head.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_extension',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'string',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree: (DT This)\n",
      "tree: (DT an)\n",
      "tree: (NNS unladen)\n",
      "tree: (VBP swallow)\n",
      "tree: (, .)\n",
      "tree: (DT This)\n",
      "tree: (VBZ is)\n",
      "tree: (DT a)\n",
      "tree: (NN test)\n",
      "tree: (. .)\n"
     ]
    }
   ],
   "source": [
    "def traverse_tree(tree):\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == nltk.tree.Tree:\n",
    "            traverse_tree(subtree)\n",
    "        else:\n",
    "            print(\"tree:\", tree)\n",
    "traverse_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import breadth_first\n",
    "from nltk.tree import ParentedTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in breadth_first(tree[0]):\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.parse??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = list(parser.parse('This an unladen swallow.'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPAAAADLCAIAAADfiomrAAAJN2lDQ1BkZWZhdWx0X3JnYi5pY2MAAHiclZFnUJSHFobP933bCwvssnRYepMqZQHpvUmvogJL7yxLEbEhYgQiiog0RZCggAGjUiRWRLEQFBSwoFkkCCjXYBRRQbk/cmfi3Dv+uM+vZ95555wzcwAoogAAqChASqqA7+dizwkJDePAN0TyMtPtfHw84bt8GAMEAOCB7vc734USHZPJA4AVAMjnpfMFAEguAGjmCNIFAMhxAGBFJaULAJDzAMDih4SGASC3AIAV97dPAAAr6m9fAAAWP8DPAQDFAZBocd941Df+n70AAMp2fEFCbEwuxz8tVpATyY/hZPq52HPcHBw4Pvy02ITkmG8O/r/K30EQkysAAHBIS9/CT4iLF3D+Z6iRgaEh/POLd76AAADCHvzv/wDAN720RgDuIgB24J8sqhqgew+A1JN/MtXjAIxCgK57vCx+9t8ZDgAADxRgAAukQQFUQBN0wQjMwBJswQncwRsCIBQ2AQ/iIQX4kAP5sAuKoAQOwGGogXpoghZoh7PQDRfhGtyEu3AfRuEpCGEaXsMCfIBlBEGICB1hItKIIqKG6CBGCBexRpwQT8QPCUUikDgkFclC8pHdSAlSjtQgDUgL8gtyAbmG3EaGkcfIJDKH/IV8RjGUhrJQeVQd1Ue5qB3qgQagG9E4NAPNQwvR/WgV2oieRrvQa+hddBQVoq/RRQwwKsbGlDBdjIs5YN5YGBaL8bHtWDFWiTVi7VgvNoA9wITYPPYJR8AxcRycLs4S54oLxPFwGbjtuFJcDe4UrgvXj3uAm8Qt4L7i6Xg5vA7eAu+GD8HH4XPwRfhKfDO+E38DP4qfxn8gEAhsggbBjOBKCCUkErYSSglHCR2Eq4RhwhRhkUgkShN1iFZEb2IkUUAsIlYTTxOvEEeI08SPJCpJkWREciaFkVJJBaRKUivpMmmENENaJouS1cgWZG9yNHkLuYzcRO4l3yNPk5cpYhQNihUlgJJI2UWporRTblAmKO+oVKoy1ZzqS02g7qRWUc9Qb1EnqZ9o4jRtmgMtnJZF2087SbtKe0x7R6fT1em29DC6gL6f3kK/Tn9O/yjCFNETcROJFtkhUivSJTIi8oZBZqgx7BibGHmMSsY5xj3GvChZVF3UQTRSdLtoregF0XHRRTGmmKGYt1iKWKlYq9htsVlxori6uJN4tHih+Anx6+JTTIypwnRg8pi7mU3MG8xpFoGlwXJjJbJKWD+zhlgLEuISxhJBErkStRKXJIRsjK3OdmMns8vYZ9lj7M+S8pJ2kjGS+yTbJUckl6RkpWylYqSKpTqkRqU+S3OknaSTpA9Kd0s/k8HJaMv4yuTIHJO5ITMvy5K1lOXJFsuelX0ih8ppy/nJbZU7ITcotyivIO8iny5fLX9dfl6BrWCrkKhQoXBZYU6RqWitmKBYoXhF8RVHgmPHSeZUcfo5C0pySq5KWUoNSkNKy8oayoHKBcodys9UKCpclViVCpU+lQVVRVUv1XzVNtUnamQ1rlq82hG1AbUldQ31YPW96t3qsxpSGm4aeRptGhOadE0bzQzNRs2HWgQtrlaS1lGt+9qotol2vHat9j0dVMdUJ0HnqM7wGvwa8zWpaxrXjOvSdO10s3XbdCf12HqeegV63Xpv9FX1w/QP6g/ofzUwMUg2aDJ4aihu6G5YYNhr+JeRthHPqNbo4Vr6Wue1O9b2rH1rrGMcY3zM+JEJ08TLZK9Jn8kXUzNTvmm76ZyZqlmEWZ3ZOJfF9eGWcm+Z483tzXeYXzT/ZGFqIbA4a/Gnpa5lkmWr5ew6jXUx65rWTVkpW0VaNVgJrTnWEdbHrYU2SjaRNo02L2xVbKNtm21n7LTsEu1O272xN7Dn23faLzlYOGxzuOqIObo4FjsOOYk7BTrVOD13VnaOc25zXnAxcdnqctUV7+rhetB13E3ejefW4rbgbua+zb3fg+bh71Hj8cJT25Pv2euFerl7HfKaWK+2PnV9tzd4u3kf8n7mo+GT4fOrL8HXx7fW96WfoV++34A/03+zf6v/hwD7gLKAp4GagVmBfUGMoPCglqClYMfg8mBhiH7ItpC7oTKhCaE9YcSwoLDmsMUNThsOb5gONwkvCh/bqLExd+PtTTKbkjdd2szYHLn5XAQ+IjiiNWIl0juyMXIxyi2qLmqB58A7wnsdbRtdET0XYxVTHjMTaxVbHjsbZxV3KG4u3ia+Mn4+wSGhJuFtomtifeJSknfSyaTV5ODkjhRSSkTKhVTx1KTU/jSFtNy04XSd9KJ0YYZFxuGMBb4HvzkTydyY2SNgCdIFg1maWXuyJrOts2uzP+YE5ZzLFctNzR3cor1l35aZPOe8n7bitvK29uUr5e/Kn9xmt61hO7I9anvfDpUdhTumd7rsPLWLsitp128FBgXlBe93B+/uLZQv3Fk4tcdlT1uRSBG/aHyv5d76H3A/JPwwtG/tvup9X4uji++UGJRUlqyU8krv/Gj4Y9WPq/tj9w+VmZYdO0A4kHpg7KDNwVPlYuV55VOHvA51VXAqiiveH958+HalcWX9EcqRrCPCKs+qnmrV6gPVKzXxNaO19rUddXJ1++qWjkYfHTlme6y9Xr6+pP7z8YTjjxpcGroa1RsrTxBOZJ942RTUNPAT96eWZpnmkuYvJ1NPCk/5nepvMWtpaZVrLWtD27La5k6Hn77/s+PPPe267Q0d7I6SM3Am68yrXyJ+GTvrcbbvHPdc+3m183WdzM7iLqRrS9dCd3y3sCe0Z/iC+4W+Xsvezl/1fj15Ueli7SWJS2WXKZcLL69eybuyeDX96vy1uGtTfZv7nl4Puf6w37d/6IbHjVs3nW9eH7AbuHLL6tbF2xa3L9zh3um+a3q3a9BksPM3k986h0yHuu6Z3eu5b36/d3jd8OURm5FrDxwf3Hzo9vDu6PrR4bHAsUfj4ePCR9GPZh8nP377JPvJ8tOdE/iJ4meizyqfyz1v/F3r9w6hqfDSpOPk4Av/F0+neFOv/8j8Y2W68CX9ZeWM4kzLrNHsxTnnufuvNryafp3+enm+6F9i/6p7o/nm/J+2fw4uhCxMv+W/Xf2r9J30u5Pvjd/3LfosPv+Q8mF5qfij9MdTn7ifBj4Hf55ZzlkhrlR90frS+9Xj68RqyurqvwFCLJC+vYsN3gAAAAlwSFlzAAAN1wAADdcBQiibeAAAAB10RVh0U29mdHdhcmUAR1BMIEdob3N0c2NyaXB0IDkuMjZYkG63AAARdElEQVR4nO2dP2zjRr7HJy95uLMVJKEP8gHbyKbwGrl4wFJb7wKmCjtIcYDJMusrTAObJkUsCnjA2W4Oor1NAsSAuMXJAa4hc0hluxAXWFdXmEwn4RrRcpNCwomLQ2hc7vagV0x2jifJskT9GXL0+xQLecgVfyN+NfrNkPx93+l0OggAWOG/aAcAAJMEBA0wBQgaYAoQNMAUIGiAKUDQAFO8RzsAYFhc13VdF7/meZ7nefzaNE3btnO5nCiKZOeuRs/zHMcJvltwZ5aAETpOqKqKX5RKJdM0cYvneYVCwXEcXdfJbl2NjuNgQeN3MAyDTgdmQAeID+vr6+T11taWbdulUom05PP5drvdtxFD3qFer88w6pkCI3TMsCxL13VJkgqFgmEYwcwhm806jtO3keM4juNII0lX2AMEHUt4nhcEYWlpyfM80uh5HsdxfRtpxEgHEHTMEEVRUZRcLqeqqiRJwWzYtm1BEPo20oiUDu904OakmGBZlqIokiRpmoYQUlU1nU4jhOr1OkLI87zd3V2sXV3XextxNmKaJk5XWB22QdCxBy/JdS3D9W2cB0DQAFNADg0wBQgaYAoQNMAUIGjW4PN57fycdhTUAEGzxnWr1f7xR9pRUAMEDTAFCBpgChA0wBQgaIApQNAAU4CgWeOjxUXaIdAEBM0awsqKc3NDOwpqgKABpgBBA0wBggaYAgQNMAUIGmAKEDTAFCBo1hBSKafRoB0FNUDQDPL69pZ2CNQAQQNMAYIGmALKGLCG22xyiQSXSNAOhA4gaIApIOUAmAIEDTAFCBpgCvBYYQpsrSLLMsdxDFc1HwCM0OxArFUsyyqVSrTDoQOscrCDJEnYSQghZFnWHNbSRSBolnAcp1QqcRyXzWYlSaIdDh1A0AyCM2lc6H/egByaHYiLoSRJQd+guQJWOdjBsiysac/zcrkc7XDoACkHU8yttQoBBA0wBeTQAFOAoAGmAEEzhef7f/zzn69bLdqBUANWOeKNVa26rVa92XRubpxG4/Xt7Qe//OXf/v73//n1r3/z8GF6eVnMZPjlZdphzg6YFMYJt9l0bm7s62u31XJbre8DRRkfplJ8Msknk7f/+MeLy8uf3rwhm1aTSTGTya6szIO4QdDRxfN9p9Fwbm7qzabbar2s1cim1WSSTyaFVCq9vCysrAgrK13/USmXv7XtxC9+IWYyP/70E/m/zIsbBB0hevMHsmk9kxFSqaX33xdSKWFlZZhHBq1qVTk9vW61trJZTZLcVqtSrTo3N2yLGwRNjWHyh9zaGp9Mhlab5/vFs7Oji4uPFhcLH3+sbm7idqtaxYe2ajX8tVlNJoVUKru6KmYyXeN9vABBz4jQ+cP4OI2GUi5/f3OznsloktT1/k6jYdVqQXF/tLgoZjIxFTcIelpMNn8YH9Uw9MvL17e3+Y0NTZb77sOAuEHQk2EG+cNEglROT1/WaqvJpP70qbi2NmDnmIobBB0GivnD+Gjn58WzMzxUFz7+eJjfB7fZtGo1u9GwajVy1Qb/zuTW1gZ/MWYMCHooopY/jAlZ11tNJjVJkh49Gv7/RlzcIOg+xCJ/GB/z6ko1Tbyup29vh/gqRlDcIOh45w9jcte6XgiIuJ1GgwwBRNwz++2aR0Ezlj+Mj1WtqqaJ1/X0p0/H/9nxfJ9MKIm4H6ZSZE45vQ+WfUHPSf4wPqphHF1cIIQGrOuFYMbiZk3Q85w/jA9Z13uYSmmSNPEkeAbijr2gIX+YOCHW9UJAxB28vQSLe5y7XmMmaMgfZoPbbKqmidf17r0EMxGsanUi905FWtCQP9DFvLpSyuXXt7eh1/XCMY64oyho/dUr4+oK8ocoEFzXM589m/3S8l3i1iSprwaiKGjx+NjzfcgfogNe17P29uiOI8G7Xr2vv+67TxQFDQChgae+AaYAQQNMAYIGmIJ+XQ7HcTzPw/UFLctCCGFzENd18Q48z8+nXQhFyIngeR6fII7jOI6L/kmJxAidy+WIl4JhGBzHoUC141KpRLYCs8F1XcMwyJ/EsSUGJ6UTAdbX13d2dtrtdqfTyefzpDG4A53I5pitrS38ot1ux+ik0E85MIVCoVgsdrko4B++SqWyu7tLKa75JZfLYechXdeDn3/ET0pUBI0TMsdxgo2VSgUhJMuyIAh0wppjJEkqFouiKLbb7WC6HPGTEhVBI4Q0TVMUBSfQpIViPHMOPhG6rmez2WB7FE6K53mmaSqK0mcb7ZynU6lUVldXS6USfo0zM9yYz+dxO0AF27YfPnxI/ozOSSkWiwgh27Z7N8GlbyCWOI7TN+cBQQNMEYl1aACYFCBogClA0ABTREvQnu+rhvG/v/udUi67zSbtcIB/4zabVrVKO4qfcRoNp9HouylCk0JS7zX1q1/d/PWvCKGpPnUMjASu2tH5wx9oB4IQQuLxMULI2tvr3RSJCyv6q1fF8/PrVouUg3AaDdU0jy4u9MvLMUtUAXMF5ZTDqlbF4+Pd01OEUOnpU+fgAD+GKaysWHt7lS++4BKJgmny+bx5dUU3VCAWUBuhSeWHjxYX70otxLU19+hIf/VKNU355GQ9kylsblIv2ApEGQqCJk/GI4R2Hj++63l0gvLkifToUfHsTL+8zD1/vvP4cWFzE54DB/oya0GTmd9IhS65REKT5d0nT4rn5y8uL19cXsJ8EejL7ATdO/Mb9R345WV9e3v3yROYLwJ3MYtJ4V0zv3DAfBEYwHRH6GFmfuGA+SLQl2kJetSZXzhgvgh0MRVBh5v5hQPmi0CQCQt6/JlfOGC+CGAmNimc7MwvHDBfBCYwQk9v5hcOmC/OM2MJejYzv3DAfHE+CS/oWc78wgHzxTkkjKBpzfzCAfPFuWK0SaHTaFCf+YUD5otzwmgjNH70Jb4/3MH5on19LT16RDui2JCL0sgl333iRn4Ey/P9OEq5C8/3EUIMdAToIkLPFALA+ETrqW8AGBMQNMAUgyaFLLmf9Pblvffee/PmDd4ao45Mj3t9VTiOw/URXdeNrAbuGaFZcj/p6suHH34Y045MiXt9VSzLIp9SdD+6wYV4WXI/6e1LTDsyPe71VSE7RPaju38dmiX3k96+xLQjU+IuXxU8HjuOUygUSGM0P7r7Bc2S+0lvX2LakSlxl68KHgJc11UUBc9DUFQ/uqGuFLLkftLVl/h2ZBrc5auC4XleEATXdbHWKX50AzxW3j04OLjrv1mW9c033ywsLAiCsLCwYJrmp59+iht/+OGH6+vrSH01B9PblwcPHsSxI9NmaWnp4ODgyy+/xH+S041nhAsLC7IsU9fAV1999fnnn3/yyScPHjzo2gRXCoFYAh4rwFwAVwoBpgBBA0wBggaYYk4FrZ2f/9+f/nSXTwfQl+h4rAxgtEnhO7/9bX5jQ5Pl6QU0bTzfl05OXtZq//3uu//817/i3p2ZESmPlQHM1whtXl3x+fzLWi2/sfGX3/9+PZM5urgQDg5gqGaGeRG05/vS11/LJydcIlH54gtNlvnlZWtvryhJbqslHh9r5+e0YwQmQCRcsKaNeXWllMuvb297H+9VNzfFTEYplwumWalWo1lgBBgexkdoz/eVcjk4MPc+GCusrDgHB/mNjZe1mnB4CEN1rGFZ0Fa1Khwevri83Hn82NnfH1xCRJNle3+fTyYLpikeH+PHwgFCdnUVxWGhg01BY4vl3PPnnu8bz57p29vDVCwIDtVQiaYLbnGRdghDwWAObVWryunpdau1lc0OKeUgmizn1taU01P55CTcOwAUYW2EDg7M5mefhdOiuLbm7O/nNza+tW0YquMFO4J2Gg3h4ODo4mI9k3GPjsYs84Url+JaePLJiVIuQ1YdCxgRtGoY2cNDt9UqSpK1tzdBry1nf3/n8eMXl5fC4WH0p0RA7HNop9FQyuXvb26mVKaaSyT07e3c2ppSLueeP49voco5Id6C1s7Pi2dnCKGiJE215LP06BG+/nJ0cWHatv70aVzqCE8K/B12Wy3agdxDXFMOt9kUj48Lpsknk9be3gwKmHOJhPnZZ8azZ57v554/VwM1WeYBYWUFIVRvNmkHcg+xHKHxwIwvZc/4Xjk8VEsnJ0cXF1atpm9v4zMNRISYjdCe75OB2d7fp3LnJ5dIkLuasoeH8zZUR5w4jdDBe4yo38Ssbm5K2axyegpDdaSIxwjde/Mn7YgQQghuQI0gowl6PZNJ07i7snh29q1t5zc27r3HaPaom5vW3h6+q4n5BwWonP2RiE1dDqtajZqUu4h+hPNAbAQNAMMQjxwaAIYEBA0wBQgaYIpB69BBbxhBEHDxYM/zuoqfkwrYk4LWcYeh13zI9/1EItFlrYQrKJumadu2LMscx0XKWWdULMvC5c27zA96G6lzzwhNvGFUVcV6chwHv8CbjOlcJ6N13GHoMh/64IMP7rJW8jyvUChYlkUMeGKKKIqapvUOKL2N9BlswRL0g8EuMu12G/vu4E31en0a1i+0jjtkbL3mQ73WSsRfp9PpVCoVGpH+B7Zt599iGEaxWNzZ2cEfY6VS2drawkFWKhWyW9c79DUHCjYGD4E/jbuOMj1GEHRXf6bqfUTruMOwvr5er9fx+SaC7mrpdDq2be/s7GD1UIyWQL5ytm0bhkF8rkqlUicQNsEwjC7xDRZ0u90Ommjt7Ox0Am5adx1l4owwKQx6rMwSWscdQK/5UG+LIAi6ruP8kmRQFCkUCqqqqqpqGIYoivhTNU2zVCp5nkd2U1VVkiS820jv7ziO/PaWBOzYid6eu96jTI9hBe04DpVpDa3j3oumaV2ZcVcLEbEkSbM5l4MxTRN/wQqFArHbsW0bu++k02mEkOM46XTaNE1N0+QRb5jhed62bfInmdb3HmWqDFrlsCzLdV1yYohHneM4hmHgTYVCYeIjKK3jDh+bruuKosiyXCwWe1vInrgLnuflcrnZh9pFpVJpt9s4HizWdDpt2zbP867r4lUanueLxWK9Xse7oberTLgj+JNPp9P4+9DbiP/FbsrkrPUeZarApe8pgpcaqSwv9mXIeCzLIqulUzrE9ABBA0wBVwoBpgBBA0wBggaYYjRBi8fH+qtX04lkEMLBQfSfRY1FkMwzmqBf1mpUKjN4t7fe7e3sjzsSsQiSeeKRcvDJZPRr9sQiSOaJh6ABYEhA0ABTgKABpgBBA0wBggaYAgQNMEU8BC2kUi9rNdpR3EMsgmSeeAgaAIYEBA0wBQgaYAoQNMAUIGiAKUDQAFPEQ9BL77+PEHKj7SkWiyCZJx6CFlIpFHnXx1gEyTzxEDQADEk8yhh4vu/5/sR9vCdLLIJknngIGgCGBFIOgClA0ABTgKABpvi5+mhfBxPspUDMFoJYloULb84ixigRwlhE1/V6vR4pIxKG+XmE7utgIoriXYWNBUGYzzMUwliEFGMGZsDPI7QgCLiCKsdxoigGa4yTIseapuESq2SUEgSB7IaLN+PX2WxWkqTQMZH3R28HQlyEOJvNttvtYCSTgvwWua5bKpXS6bQoisMfsTdg3IgNhJaWlrpaEEK4vvW0+zWP3GWZQf60bbvT6di2XSwWB+zZZeExEb+MoM3HgEgmQq9N0fB97w24Xq9jk5FOp1OpVIImLHgreT3tfs0bgyr4Y/AwLAjCYNMNbOFBhp9xvmOqqrqui2u+7+7ujhrJBBnyiL0Bu65LLB1EUcTGA57nEVuCYC43+34xzP2CHhJs4YEQ8jxPUZS+U8lhwDYf+Ic79JuMw6h+KH0D5jgOe/Ogtz6iPM9zHDefE49Z8m9B9zqYBN1DsAEmPjF9HTd6LTzC0dfmAx8aR6LrOrZznWy6ubu7S4ZPx3G+++67vkfs7XvfgPFwS6YfHMdh3WMLEoTQ0tKSqqrBT3hK/Zo3Jnnpe4L+GuPYfIQGGzeFM93qGzBOM4JTZ+oWJMwD93IATAFXCgGmAEEDTAGCBpgCBA0wBQgaYAoQNMAU/w+VxFGsyGtqVQAAAABJRU5ErkJggg==",
      "text/plain": [
       "Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NP', [Tree('DT', ['This'])]), Tree('NP', [Tree('DT', ['an']), Tree('NNS', ['unladen'])])]), Tree('VP', [Tree('VBP', ['swallow'])]), Tree('.', ['.'])])])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NP', [Tree('DT', ['This'])]), Tree('NP', [Tree('DT', ['an']), Tree('NNS', ['unladen'])])]), Tree('S|<VP-.>', [Tree('VP', [Tree('VBP', ['swallow'])]), Tree('.', ['.'])])])])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(tree[0])\n",
    "tree[0].chomsky_normal_form()\n",
    "tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (S\n",
      "  (NP (NP (DT This)) (NP (DT an) (NNS unladen)))\n",
      "  (VP (VBP swallow))\n",
      "  (. .)) (S This an unladen swallow .)\n"
     ]
    }
   ],
   "source": [
    "for (i,child) in enumerate(tree[0]): \n",
    "    print(i,child, child.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ROOT -> S,\n",
       " S -> NP VP .,\n",
       " NP -> NP NP,\n",
       " NP -> DT,\n",
       " DT -> 'This',\n",
       " NP -> DT NNS,\n",
       " DT -> 'an',\n",
       " NNS -> 'unladen',\n",
       " VP -> VBP,\n",
       " VBP -> 'swallow',\n",
       " . -> '.']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[0].productions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[(governor, dep, dependent) for governor, dep, dependent in parse.triples()] for parse in tree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (NP (DT This)) (NP (DT an) (NNS unladen)))\n",
      "  (VP (VBP swallow))\n",
      "  (. .)) [(), (0,), (0, 0), (0, 0, 0), (0, 0, 0, 0), (0, 1), (0, 1, 0), (0, 1, 0, 0), (0, 1, 1), (0, 1, 1, 0), (1,), (1, 0), (1, 0, 0), (2,), (2, 0)]\n"
     ]
    }
   ],
   "source": [
    "for t in tree[0]:\n",
    "    print(t, t.treepositions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'an', 'unladen', 'swallow', '.']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[0].leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "strees_list = list(tree[0].subtrees())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 11)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(strees_list), len(strees_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(nltk.tree.Tree, ['an', 'unladen'])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(strees_list[0]), strees_list[5].leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__radd__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_frozen_class',\n",
       " '_get_node',\n",
       " '_label',\n",
       " '_parse_error',\n",
       " '_pformat_flat',\n",
       " '_repr_png_',\n",
       " '_set_node',\n",
       " 'append',\n",
       " 'chomsky_normal_form',\n",
       " 'clear',\n",
       " 'collapse_unary',\n",
       " 'convert',\n",
       " 'copy',\n",
       " 'count',\n",
       " 'draw',\n",
       " 'extend',\n",
       " 'flatten',\n",
       " 'freeze',\n",
       " 'fromstring',\n",
       " 'height',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'label',\n",
       " 'leaf_treeposition',\n",
       " 'leaves',\n",
       " 'node',\n",
       " 'pformat',\n",
       " 'pformat_latex_qtree',\n",
       " 'pop',\n",
       " 'pos',\n",
       " 'pprint',\n",
       " 'pretty_print',\n",
       " 'productions',\n",
       " 'remove',\n",
       " 'reverse',\n",
       " 'set_label',\n",
       " 'sort',\n",
       " 'subtrees',\n",
       " 'treeposition_spanning_leaves',\n",
       " 'treepositions',\n",
       " 'un_chomsky_normal_form']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NP', [Tree('DT', ['This'])]), Tree('NP', [Tree('DT', ['an']), Tree('NNS', ['unladen'])])]), Tree('VP', [Tree('VBP', ['swallow'])]), Tree('.', ['.'])])])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ROOT ['This', 'an', 'unladen', 'swallow', '.']\n",
      "6 ROOT ['This', 'an', 'unladen', 'swallow', '.']\n",
      "5 S ['This', 'an', 'unladen', 'swallow', '.']\n",
      "4 NP ['This', 'an', 'unladen']\n",
      "3 NP ['This']\n",
      "2 DT ['This']\n",
      "3 NP ['an', 'unladen']\n",
      "2 DT ['an']\n",
      "2 NNS ['unladen']\n",
      "3 VP ['swallow']\n",
      "2 VBP ['swallow']\n",
      "2 . ['.']\n"
     ]
    }
   ],
   "source": [
    "st_depth_list = []\n",
    "\n",
    "for _, st in enumerate(strees_list):\n",
    "    st_depth_list.append((len(st.treepositions()), st))\n",
    "    print(st.height(), st.label(), st.leaves(), )\n",
    "    ss = list(st.subtrees())\n",
    "    for i in ss:\n",
    "        print(i.height(), i.label(), i.leaves())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.leaf_treeposition??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__radd__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_frozen_class',\n",
       " '_get_node',\n",
       " '_label',\n",
       " '_parse_error',\n",
       " '_pformat_flat',\n",
       " '_repr_png_',\n",
       " '_set_node',\n",
       " 'append',\n",
       " 'chomsky_normal_form',\n",
       " 'clear',\n",
       " 'collapse_unary',\n",
       " 'convert',\n",
       " 'copy',\n",
       " 'count',\n",
       " 'draw',\n",
       " 'extend',\n",
       " 'flatten',\n",
       " 'freeze',\n",
       " 'fromstring',\n",
       " 'height',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'label',\n",
       " 'leaf_treeposition',\n",
       " 'leaves',\n",
       " 'node',\n",
       " 'pformat',\n",
       " 'pformat_latex_qtree',\n",
       " 'pop',\n",
       " 'pos',\n",
       " 'pprint',\n",
       " 'pretty_print',\n",
       " 'productions',\n",
       " 'remove',\n",
       " 'reverse',\n",
       " 'set_label',\n",
       " 'sort',\n",
       " 'subtrees',\n",
       " 'treeposition_spanning_leaves',\n",
       " 'treepositions',\n",
       " 'un_chomsky_normal_form']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(), (0,)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.treepositions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 ROOT ['What', 'is', 'the', 'airspeed', 'of', 'an', 'unladen', 'swallow', '?']\n",
      "26 SBARQ ['What', 'is', 'the', 'airspeed', 'of', 'an', 'unladen', 'swallow', '?']\n",
      "3 WHNP ['What']\n",
      "2 WP ['What']\n",
      "20 SQ ['is', 'the', 'airspeed', 'of', 'an', 'unladen', 'swallow']\n",
      "2 VBZ ['is']\n",
      "14 NP ['the', 'airspeed', 'of', 'an', 'unladen']\n",
      "5 NP ['the', 'airspeed']\n",
      "2 DT ['the']\n",
      "2 NN ['airspeed']\n",
      "8 PP ['of', 'an', 'unladen']\n",
      "2 IN ['of']\n",
      "5 NP ['an', 'unladen']\n",
      "2 DT ['an']\n",
      "2 JJ ['unladen']\n",
      "3 S+VP ['swallow']\n",
      "3 VP ['swallow']\n",
      "2 VB ['swallow']\n",
      "2 . ['?']\n"
     ]
    }
   ],
   "source": [
    "for st in st_depth_list:\n",
    "    print(st[0], st[1].label(), st[1].leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(nltk.tree.Tree,\n",
       " ['__add__',\n",
       "  '__class__',\n",
       "  '__contains__',\n",
       "  '__copy__',\n",
       "  '__deepcopy__',\n",
       "  '__delattr__',\n",
       "  '__delitem__',\n",
       "  '__dict__',\n",
       "  '__dir__',\n",
       "  '__doc__',\n",
       "  '__eq__',\n",
       "  '__format__',\n",
       "  '__ge__',\n",
       "  '__getattribute__',\n",
       "  '__getitem__',\n",
       "  '__gt__',\n",
       "  '__hash__',\n",
       "  '__iadd__',\n",
       "  '__imul__',\n",
       "  '__init__',\n",
       "  '__init_subclass__',\n",
       "  '__iter__',\n",
       "  '__le__',\n",
       "  '__len__',\n",
       "  '__lt__',\n",
       "  '__module__',\n",
       "  '__mul__',\n",
       "  '__ne__',\n",
       "  '__new__',\n",
       "  '__radd__',\n",
       "  '__reduce__',\n",
       "  '__reduce_ex__',\n",
       "  '__repr__',\n",
       "  '__reversed__',\n",
       "  '__rmul__',\n",
       "  '__setattr__',\n",
       "  '__setitem__',\n",
       "  '__sizeof__',\n",
       "  '__str__',\n",
       "  '__subclasshook__',\n",
       "  '__weakref__',\n",
       "  '_frozen_class',\n",
       "  '_get_node',\n",
       "  '_label',\n",
       "  '_parse_error',\n",
       "  '_pformat_flat',\n",
       "  '_repr_png_',\n",
       "  '_set_node',\n",
       "  'append',\n",
       "  'chomsky_normal_form',\n",
       "  'clear',\n",
       "  'collapse_unary',\n",
       "  'convert',\n",
       "  'copy',\n",
       "  'count',\n",
       "  'draw',\n",
       "  'extend',\n",
       "  'flatten',\n",
       "  'freeze',\n",
       "  'fromstring',\n",
       "  'height',\n",
       "  'index',\n",
       "  'insert',\n",
       "  'label',\n",
       "  'leaf_treeposition',\n",
       "  'leaves',\n",
       "  'node',\n",
       "  'pformat',\n",
       "  'pformat_latex_qtree',\n",
       "  'pop',\n",
       "  'pos',\n",
       "  'pprint',\n",
       "  'pretty_print',\n",
       "  'productions',\n",
       "  'remove',\n",
       "  'reverse',\n",
       "  'set_label',\n",
       "  'sort',\n",
       "  'subtrees',\n",
       "  'treeposition_spanning_leaves',\n",
       "  'treepositions',\n",
       "  'un_chomsky_normal_form'])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tree[0]), dir(tree[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (SBARQ\n",
      "    (WHNP (WP What))\n",
      "    (SQ\n",
      "      (VBZ is)\n",
      "      (NP\n",
      "        (NP (DT the) (NN airspeed))\n",
      "        (PP (IN of) (NP (DT an) (JJ unladen))))\n",
      "      (S (VP (VB swallow))))\n",
      "    (. ?)))\n"
     ]
    }
   ],
   "source": [
    "for st in tree:\n",
    "    print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " ['__add__',\n",
       "  '__class__',\n",
       "  '__contains__',\n",
       "  '__delattr__',\n",
       "  '__delitem__',\n",
       "  '__dir__',\n",
       "  '__doc__',\n",
       "  '__eq__',\n",
       "  '__format__',\n",
       "  '__ge__',\n",
       "  '__getattribute__',\n",
       "  '__getitem__',\n",
       "  '__gt__',\n",
       "  '__hash__',\n",
       "  '__iadd__',\n",
       "  '__imul__',\n",
       "  '__init__',\n",
       "  '__init_subclass__',\n",
       "  '__iter__',\n",
       "  '__le__',\n",
       "  '__len__',\n",
       "  '__lt__',\n",
       "  '__mul__',\n",
       "  '__ne__',\n",
       "  '__new__',\n",
       "  '__reduce__',\n",
       "  '__reduce_ex__',\n",
       "  '__repr__',\n",
       "  '__reversed__',\n",
       "  '__rmul__',\n",
       "  '__setattr__',\n",
       "  '__setitem__',\n",
       "  '__sizeof__',\n",
       "  '__str__',\n",
       "  '__subclasshook__',\n",
       "  'append',\n",
       "  'clear',\n",
       "  'copy',\n",
       "  'count',\n",
       "  'extend',\n",
       "  'index',\n",
       "  'insert',\n",
       "  'pop',\n",
       "  'remove',\n",
       "  'reverse',\n",
       "  'sort'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tree), dir(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(sent):\n",
    "    \n",
    "    words = nltk.word_tokenize(sent)\n",
    "    grammar = \"NP:{<DT>?<JJ>*<NN>}\"\n",
    "    Reg_parser = nltk.RegexpParser(grammar)\n",
    "    tree = Reg_parser.parse(nltk.pos_tag(words))\n",
    "    \n",
    "    strees = list(tree.subtrees())\n",
    "    for i in strees:\n",
    "        print(i.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('only', 'RB'), ('a', 'DT'), ('test', 'NN'), ('.', '.')]\n",
      "[('a', 'DT'), ('test', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "get_chunks(\"This is only a test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunks_using_parse_tree(conssentence, replace_with_dataset=True):\n",
    "    \n",
    "    conssentence = mask_entities(conssentence, replace_with_dataset)\n",
    "    doc = nlp(conssentence)\n",
    "    verb_subtree = []\n",
    "\n",
    "    for s in doc.sents:\n",
    "#         find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"technique\": [], \"task\": [], \"dataset\": [], \"material\": [], \"metric\": []}\n",
    "        find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"baseline\": [], \"task\": [], \n",
    "                               \"dataset\": [],  \"metric\": [], \"unfair\": [], \"fair\": []}\n",
    "\n",
    "        for tok in s:\n",
    "\n",
    "            if tok.text.lower().startswith(\"compar\"):\n",
    "                find_special_tokens[\"compar\"].append(tok)\n",
    "            else:\n",
    "                for k in sp_toks:\n",
    "                    if tok.text.lower().startswith(k):\n",
    "                        find_special_tokens[k].append(tok)\n",
    "                        break\n",
    "\n",
    "        verb_tokens = []\n",
    "        if find_special_tokens[\"compar\"]:\n",
    "            for t in find_special_tokens[\"compar\"]:\n",
    "#                     verb_subtree.append(t.subtree)\n",
    "                if t == s.root:\n",
    "                    simplified_sent = \"\"\n",
    "                    for chh in t.lefts:\n",
    "                        simplified_sent = simplified_sent + \" \" + chh.text\n",
    "                    simplified_sent = simplified_sent + \" \" + t.text\n",
    "                    for chh in t.rights:\n",
    "                        simplified_sent = simplified_sent + \" \" + chh.text\n",
    "#                         print(\"SIMP: \", simplified_sent)\n",
    "                    verb_subtree.append(simplified_sent)\n",
    "                else:\n",
    "                    verb_subtree.append(t.subtree)\n",
    "        else:\n",
    "            for k in sp_toks:\n",
    "                for i in find_special_tokens[k]:\n",
    "                    local_vt = []\n",
    "                    for j in i.ancestors:\n",
    "                        if j.pos_ == \"NOUN\":\n",
    "                            local_vt.append(j)\n",
    "                    if not local_vt:\n",
    "                        for j in i.ancestors:\n",
    "                            if j.pos_ == \"VERB\":\n",
    "                                local_vt.append(j)\n",
    "                    verb_tokens = verb_tokens + local_vt\n",
    "\n",
    "\n",
    "            for i in verb_tokens:\n",
    "                verb_subtree.append(i.subtree)\n",
    "\n",
    "    eecc = []\n",
    "    for i in verb_subtree:\n",
    "        if type(i) == str:\n",
    "            eecc.append(i)\n",
    "        else:\n",
    "            local_chunk = \"\"\n",
    "            for lcaltok in i:\n",
    "                local_chunk = local_chunk + \" \" + lcaltok.text\n",
    "            eecc.append(local_chunk)\n",
    "#     if not eecc:\n",
    "#         print(conssentence)\n",
    "    return list(set(eecc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_chunks_using_spacy_dp(\"It would be interesting to explore the practicability of the method on more large-scale experiments on image related tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' more large - scale experiments on image related tasks',\n",
       " ' the practicability of the method']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"It would be interesting to explore the practicability of the method on more large-scale experiments on image related tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' comparison to SOTA']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"The experimental validation is also not extensive since comparison to SOTA is not included.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_HyVxPsC9tm 938 2\n",
      "2019_HyVxPsC9tm 940 3\n",
      "2020_Byg79h4tvB 1272 [1] Conditional adversarial domain adaptation, Long et.al, in NeurIPS 2018\n",
      "[2] Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation, You et.al, in ICML 2019\n",
      "2018_HyHmGyZCZ 1425 2\n"
     ]
    }
   ],
   "source": [
    "roberta_vectors = defaultdict(dict)\n",
    "skip_uids = []\n",
    "\n",
    "for pid in gt_dict:\n",
    "    roberta_vectors[pid] = {}\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunk = \". \".join(mcomp_chunks_from_sent)\n",
    "            else:\n",
    "                final_chunk = df.loc[mcs][\"Sent\"]\n",
    "            \n",
    "            vec = embed_text_using_roberta(final_chunk.strip()).mean(1).detach().numpy()\n",
    "            roberta_vectors[pid][mcs] = vec / norm(vec)\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunk = \". \".join(mcomp_chunks_from_sent)\n",
    "            else:\n",
    "                final_chunk = df.loc[mcs][\"Sent\"]\n",
    "            \n",
    "            vec = embed_text_using_roberta(final_chunk.strip()).mean(1).detach().numpy()\n",
    "            roberta_vectors[pid][mcs] = vec / norm(vec)\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 1384\n"
     ]
    }
   ],
   "source": [
    "mcomp_sentences = {}\n",
    "not_mcomp_sentences = {}\n",
    "\n",
    "for pid in gt_dict:\n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            mcomp_sentences[mcs] = pid\n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            not_mcomp_sentences[mcs] = pid\n",
    "print(len(mcomp_sentences), len(not_mcomp_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_with_mcomp = defaultdict(dict)\n",
    "sim_with_not_mcomp = defaultdict(dict)\n",
    "sim_with_notmcomp_paper_sents = defaultdict(dict)\n",
    "\n",
    "mean_at_k = [\"1\", \"3\", \"5\", \"7\", \"10\", \"20\", \"30\", \"50\", \"100\", \"500\", \"1000\", \"1380\"]\n",
    "\n",
    "for sid in mcomp_sentences:\n",
    "    \n",
    "    # 1. With other mcomp sentences\n",
    "    temp_list = []    \n",
    "    for osid in mcomp_sentences:\n",
    "        if osid != sid:\n",
    "            temp_list.append(np.inner(roberta_vectors[mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "\n",
    "    \n",
    "    # 2. With other not_mcomp_sentences\n",
    "    temp_list = []\n",
    "    for osid in not_mcomp_sentences:\n",
    "        temp_list.append(np.inner(roberta_vectors[not_mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_not_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_not_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "\n",
    "    \n",
    "    # 3. With not_mcomp_sentences of the same paper\n",
    "    temp_list = []    \n",
    "    for osid in not_mcomp_sentences:\n",
    "        if not_mcomp_sentences[osid] == mcomp_sentences[sid]:\n",
    "            temp_list.append(np.inner(roberta_vectors[not_mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_notmcomp_paper_sents[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_notmcomp_paper_sents[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_sim_plot\n",
    "diff12 = {\"all\": []}\n",
    "for vv in mean_at_k:\n",
    "    diff12[str(vv)] = []\n",
    "\n",
    "diff13 = {\"all\": []}\n",
    "for vv in mean_at_k:\n",
    "    diff13[str(vv)] = []\n",
    "\n",
    "for sid in sim_with_mcomp:\n",
    "    diff12[\"all\"].append(sim_with_mcomp[sid][\"mean\"] - sim_with_not_mcomp[sid][\"mean\"])\n",
    "    diff13[\"all\"].append(sim_with_mcomp[sid][\"mean\"] - sim_with_notmcomp_paper_sents[sid][\"mean\"])\n",
    "    \n",
    "    for vv in mean_at_k:\n",
    "        diff12[str(vv)].append(sim_with_mcomp[sid][\"mean_{}\".format(vv)] - sim_with_not_mcomp[sid][\"mean_{}\".format(vv)])\n",
    "        diff13[str(vv)].append(sim_with_mcomp[sid][\"mean_{}\".format(vv)] - sim_with_notmcomp_paper_sents[sid][\"mean_{}\".format(vv)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analyse chunks after masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_chunks = {\"mcs\": [], \"nmcs\": []}\n",
    "\n",
    "for pid in gt_dict:\n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunk = \". \".join(mcomp_chunks_from_sent)\n",
    "            else:\n",
    "                final_chunk = df.loc[mcs][\"Sent\"]\n",
    "            \n",
    "            masked_chunks[\"mcs\"].append((df.loc[mcs][\"Sent\"], final_chunk))\n",
    "        except Exception as ex:\n",
    "            continue\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunk = \". \".join(mcomp_chunks_from_sent)\n",
    "            else:\n",
    "                final_chunk = df.loc[mcs][\"Sent\"]\n",
    "            masked_chunks[\"nmcs\"].append((df.loc[mcs][\"Sent\"], final_chunk))\n",
    "        except Exception as ex:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' comparison']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The authors propose k-DPP as an open loop oblivious to the evaluation of configurations method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search , uniform random search , low-discrepancy Sobol sequences , BO-TPE Bayesian optimization using tree-structured Parzen estimator by Bergstra et al 2011 .'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' comparison']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' comparison']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).',\n",
       "  ' comparison'),\n",
       " ('Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20) The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.',\n",
       "  ' a small number like 3 - 6 metric with a small k 20.  lies , authors do not compare against'),\n",
       " ('COMMENTS ON THE CHANGES SINCE THE LAST YEAR\\nI am not convinced by the comparison with Spearmint added by the authors since the previous version.',\n",
       "  ' the comparison with Spearmint added by the authors since the previous version')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_chunks[\"mcs\"][2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The authors propose k-DPP as an open loop oblivious to the evaluation of configurations method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search , uniform random search , low-discrepancy Sobol sequences , BO-TPE Bayesian optimization using tree-structured Parzen estimator by Bergstra et al 2011 .'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Third , the authors do not compare against some relevant , recent work , e.g. , Springenberg et al http : aad.informatik.uni-freiburg.de papers 16-NIPS-BOHamiANN.pdf and Snoek et al https : arxiv.org pdf 1502.05700.pdf that is essential for this kind of empirical study .'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mask_entities(sentence, replace_with_dataset=False):\n",
    "# #     cleaned_sent = re.sub('[^0-9a-zA-Z ]+', ' ', sentence)\n",
    "#     cleaned_sent = sentence\n",
    "#     while cleaned_sent.find(\"  \") > -1:\n",
    "#         cleaned_sent = cleaned_sent.replace(\"  \", \" \")\n",
    "    \n",
    "#     entities_found = []\n",
    "#     for i in entity_key_map:\n",
    "#         if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "#             entities_found.append(i)\n",
    "    \n",
    "#     entities_found.sort(key=lambda s: len(s))\n",
    "#     len_sorted_entities = entities_found.copy()\n",
    "    \n",
    "#     subset_entities = []\n",
    "#     # Remove subset entities (eg: Among cnn and 3-layer-cnn, prefer the latter)\n",
    "#     for fe in len_sorted_entities:\n",
    "#         for other_ent in len_sorted_entities:\n",
    "#             if fe != other_ent and other_ent.find(fe) > -1:\n",
    "#                 subset_entities.append(fe)\n",
    "#                 break\n",
    "#     for se in subset_entities:\n",
    "#         len_sorted_entities.remove(se)\n",
    "#     for maxents in len_sorted_entities:\n",
    "#         mask_name = entity_dict[entity_key_map[i]].lower()\n",
    "#         if replace_with_dataset:\n",
    "#             if mask_name == \"material\":\n",
    "#                 mask_name = \"dataset\"\n",
    "#         cleaned_sent = cleaned_sent.replace(maxents, mask_name)\n",
    "#     words_cleaned = nltk.word_tokenize(cleaned_sent)\n",
    "#     dups_removed = [v for i, v in enumerate(words_cleaned) if i == 0 or v != words_cleaned[i-1]]\n",
    "#     new_dup_removed_sent = \" \".join(dups_removed)\n",
    "#     return new_dup_removed_sent.strip()\n",
    "\n",
    "# #     #print(cleaned_sent)\n",
    "# #     for i in entity_key_map:\n",
    "# #         if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "# #             #print(\"Substituting ent: {} with mask: {}\".format(i, entity_dict[entity_key_map[i]].lower()))\n",
    "# #             cleaned_sent = cleaned_sent.replace(i, entity_dict[entity_key_map[i]].lower())\n",
    "# #     return cleaned_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse meaningful sentences that are more similar to NMCS in comparison to MCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_sim_with_mcomp = defaultdict(list)\n",
    "ana_sim_with_not_mcomp = defaultdict(list)\n",
    "\n",
    "\n",
    "mean_at_k = [\"1\", \"3\", \"5\", \"7\", \"10\", \"20\", \"30\", \"50\", \"100\", \"500\", \"1000\", \"1380\"]\n",
    "\n",
    "for sid in mcomp_sentences:\n",
    "    \n",
    "    # 1. With other mcomp sentences\n",
    "    temp_list = []    \n",
    "    for osid in mcomp_sentences:\n",
    "        if osid != sid:\n",
    "            temp_list.append((osid, np.inner(roberta_vectors[mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0]))\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, key=lambda x: x[1], reverse=True)\n",
    "    ana_sim_with_mcomp[sid] = sorted_temp_list\n",
    "\n",
    "    \n",
    "    # 2. With other not_mcomp_sentences\n",
    "    temp_list = []\n",
    "    for osid in not_mcomp_sentences:\n",
    "        temp_list.append((osid, np.inner(roberta_vectors[not_mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0]))\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, key=lambda x: x[1], reverse=True)\n",
    "    ana_sim_with_not_mcomp[sid] = sorted_temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_sentences_at_k = defaultdict(list)\n",
    "unproblematic_sentences_at_k = defaultdict(list)\n",
    "vv = 1\n",
    "\n",
    "for sid in sim_with_mcomp:\n",
    "    sim_diff = (sim_with_mcomp[sid][\"mean_{}\".format(vv)] - sim_with_not_mcomp[sid][\"mean_{}\".format(vv)])\n",
    "    if sim_diff < 0:\n",
    "        problematic_sentences_at_k[vv].append((sid,-1.0* sim_diff))\n",
    "    else:\n",
    "        unproblematic_sentences_at_k[vv].append((sid, sim_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(113, 0.1638484001159668),\n",
       " (1464, 0.12271469831466675),\n",
       " (931, 0.1226879358291626),\n",
       " (950, 0.12097209692001343),\n",
       " (1318, 0.10917872190475464)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 1\n",
    "sorted_problematic_sentences_at_1 = sorted(problematic_sentences_at_k[k], key=lambda x: x[1], reverse=True)\n",
    "sorted(problematic_sentences_at_k[k], key=lambda x: x[1], reverse=True)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 1\n",
    "# sorted_problematic_sentences_at_3 = sorted(problematic_sentences_at_k[3], key=lambda x: x[1], reverse=True)\n",
    "# sorted(problematic_sentences_at_k[3], key=lambda x: x[1], reverse=True)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_problematic_sentences_at_3[0:3], sorted_problematic_sentences_at_3[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_unproblematic_sentences_at_3 = sorted(unproblematic_sentences_at_k[3], key=lambda x: x[1], reverse=True)\n",
    "# sorted_unproblematic_sentences_at_3[0:4], sorted_unproblematic_sentences_at_3[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sent:  The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM.\n",
      "\n",
      "Meaningful comparison sentences: \n",
      "[(124, 0.74430156)]\n",
      "What is left is the gated incremental pooling operation; but to show that this operation is beneficial when added to autoregressive CNNs, a thorough comparison with an autoregressive CNN baseline is necessary.\n",
      "\n",
      "Non Meaningful comparison sentence: \n",
      "[(842, 0.8440055)]\n",
      "Paper Weaknesses:\n",
      "- The evaluation of the model is not great: (1) It would be interesting to combine bedroom and kitchen images and train jointly to see what it learns.\n",
      "\n",
      "\n",
      "\n",
      "Test sent:  The paper does not consider the more recent and highly relevant Moosavi-Dezfooli et al “Universal Adversarial Perturbations” CVPR 2017.\n",
      "\n",
      "Meaningful comparison sentences: \n",
      "[(1202, 0.5903547)]\n",
      "- I am concerned about whether the proposed method works well with harder datasets such as Office-Home dataset, because each class data are modeled by a simple Gaussian distribution in the proposed method.\n",
      "\n",
      "Non Meaningful comparison sentence: \n",
      "[(989, 0.6897952)]\n",
      "This paper is interesting since most of the existing works focus on Monte Carlo variational inference.\n",
      "\n",
      "\n",
      "\n",
      "Test sent:  Indeed, unsurprisingly, the authors note that \"the probability of correctly labelling a word as a mistake remains low (62%)\" - this admittedly beats a random-chance baseline, but is not compared to something more meaningful, such as simply contrasting the existing system with a more powerful convolutional model and labelling all discrepancies as mistakes.\n",
      "\n",
      "Meaningful comparison sentences: \n",
      "[(869, 0.60373676)]\n",
      "An additional problem is that performance is not compared to any external prior work.\n",
      "\n",
      "Non Meaningful comparison sentence: \n",
      "[(732, 0.69845736)]\n",
      "The preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches.\n",
      "\n",
      "\n",
      "\n",
      "Test sent:  - State of the art is not well-studied in the paper.\n",
      "\n",
      "Meaningful comparison sentences: \n",
      "[(110, 0.63996166)]\n",
      "On the negative side, the authors present the results without fully referencing and acknowledging state-of-the-art.\n",
      "\n",
      "Non Meaningful comparison sentence: \n",
      "[(230, 0.7335436)]\n",
      "The network analysed here does not reach the state-of-the-art on MNIST from almost two decades ago.\n",
      "\n",
      "\n",
      "\n",
      "Test sent:  Minor comments:\n",
      "- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.\n",
      "\n",
      "Meaningful comparison sentences: \n",
      "[(615, 0.64030004)]\n",
      "Overall, the very strong improvements on the text modeling task over NVDM seem hard to understand, and I would like to see an ablation analysis of all the differences between that model and the proposed one.\n",
      "\n",
      "Non Meaningful comparison sentence: \n",
      "[(1388, 0.729555)]\n",
      "2)What is the value of k in Figure 3 and Figure 4?\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in sorted_problematic_sentences_at_1[5:10]:\n",
    "    print(\"Test sent: \", df.loc[s[0]][\"Sent\"])\n",
    "    \n",
    "    print(\"\\nMeaningful comparison sentences: \")\n",
    "    print(ana_sim_with_mcomp[s[0]][0:1])\n",
    "    for i in ana_sim_with_mcomp[s[0]][0:1]:\n",
    "        print(df.loc[i[0]][\"Sent\"])\n",
    "    \n",
    "    print(\"\\nNon Meaningful comparison sentence: \")\n",
    "    print(ana_sim_with_not_mcomp[s[0]][0:1])\n",
    "    for i in ana_sim_with_not_mcomp[s[0]][0:1]:\n",
    "        print(df.loc[i[0]][\"Sent\"])\n",
    "    \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Minor comments : - I believe one should not compare the metric shown between the left and right columns of Figure 3 as they are obtained from two different models .'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"Minor comments:- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' one should not compare the metric shown between the left and right columns of Figure 3 as they are obtained from two different models']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"Minor comments:- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Although I do like the paper on the whole , to really convince me that main objective -- ie that iterative improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular , to show that an metric scheme can really improve over a system closely matched to the attention-based model , both when used in isolation and when used in system combination with a PBMT system , and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model .'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"Although I do like the paper on the whole, to really convince me that main objective -- ie that **iterative** improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' an metric scheme']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"Although I do like the paper on the whole, to really convince me that main objective -- ie that **iterative** improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In summary , while I think the paper is interesting , I suspect that the applicability of this technique is possibly limited at present , and I m unsure how much we can really read into the findings of the paper when the experiments are based on MNIST alone .'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"In summary, while I think the paper is interesting, I suspect that the applicability of this technique is possibly limited at present, and I'm unsure how much we can really read into the findings of the paper when the experiments are based on MNIST alone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' to compare a label - noise semi - supervised method with other label - noise only methods',\n",
       " ' perturbation consistency or other semi - supervised metric']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"However, it's not completely fair to compare a label-noise + semi-supervised method with other label-noise only methods... As a matter of fact, you don't need to apply perturbation consistency (or other semi-supervised) regularization after identifying the training data with incorrect labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
