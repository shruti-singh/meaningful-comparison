{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/shruti/Desktop/iitgn/courses/SEM2/ML/Project/code/PaperAcceptancePrediction/ICLR data/masterdata_unbalanced/\"\n",
    "\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "rev_dict = {}\n",
    "paper_dict = {}\n",
    "dec_dict = {}\n",
    "iclr_arxiv_map = {}\n",
    "\n",
    "for y in years:\n",
    "    rev_dict[y] = pd.read_pickle(data_path + \"off_rev_dict_{}.pkl\".format(y))\n",
    "    paper_dict[y] = pd.read_pickle(data_path + \"papers_{}.pkl\".format(y))\n",
    "    dec_dict[y] = pd.read_pickle(data_path + \"paper_decision_dict_{}.pkl\".format(y))\n",
    "\n",
    "iclr_arxiv_map = pd.read_pickle(\"./data/iclr_arxiv_map.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"InputTestSet-Reviews48_Ann.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Sent</th>\n",
       "      <th>MComp</th>\n",
       "      <th>Cat</th>\n",
       "      <th>SubCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The authors propose to use k-DPP to select a s...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>This paper covers the related work nicely, wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The rest of the paper are also clearly written.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>However, I have some concerns about the propos...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>- It is not clear how to define the kernel, th...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID              PID     Dec  \\\n",
       "0    0  2019_SJf_XhCqKm  Reject   \n",
       "1    1  2019_SJf_XhCqKm  Reject   \n",
       "2    2  2019_SJf_XhCqKm  Reject   \n",
       "3    3  2019_SJf_XhCqKm  Reject   \n",
       "4    4  2019_SJf_XhCqKm  Reject   \n",
       "\n",
       "                                                Sent  MComp  Cat SubCat  \n",
       "0  The authors propose to use k-DPP to select a s...      0  NaN    NaN  \n",
       "1  This paper covers the related work nicely, wit...      0  NaN    NaN  \n",
       "2    The rest of the paper are also clearly written.      0  NaN    NaN  \n",
       "3  However, I have some concerns about the propos...      0  NaN    NaN  \n",
       "4  - It is not clear how to define the kernel, th...      0  NaN    NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dict = {}\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    if not pid in gt_dict:\n",
    "        gt_dict[pid] = {\"dec\": df.loc[i][\"Dec\"], \"mcomp\": set(), \"not_mcomp\": set()}\n",
    "    if df.loc[i][\"MComp\"] == 1:\n",
    "        gt_dict[pid][\"mcomp\"].add(df.loc[i][\"UID\"])\n",
    "    else:\n",
    "        gt_dict[pid][\"not_mcomp\"].add(df.loc[i][\"UID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accept': [48, 644], 'Reject': [69, 744]}\n"
     ]
    }
   ],
   "source": [
    "stats_dict = {\"Accept\": [0, 0], \"Reject\": [0, 0]}\n",
    "\n",
    "for k, v in gt_dict.items():\n",
    "    #print(len(v[\"mcomp\"]), len(v[\"not_mcomp\"]), v[\"dec\"])\n",
    "    stats_dict[v[\"dec\"]][0] += len(v[\"mcomp\"])\n",
    "    stats_dict[v[\"dec\"]][1] += len(v[\"not_mcomp\"])\n",
    "    \n",
    "print(stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestSet length: 32\n",
      " ['2019_SJf_XhCqKm', '2017_Bk0MRI5lg', '2020_SyevYxHtDB', '2018_rJBiunlAW', '2020_rkltE0VKwH', '2018_Hki-ZlbA-', '2019_BJx0sjC5FX', '2020_r1e_FpNFDr', '2020_B1lsXREYvr', '2018_SkZxCk-0Z', '2019_rJzoujRct7', '2018_HkfXMz-Ab', '2017_BJ9fZNqle', '2019_SyxZJn05YX', '2017_B1ckMDqlg', '2017_HJ0NvFzxl', '2017_S1_pAu9xl', '2018_SyYYPdg0-', '2017_BJAA4wKxg', '2019_HyVxPsC9tm', '2019_HylTBhA5tQ', '2019_B1l08oAct7', '2018_H135uzZ0-', '2017_H1oyRlYgg', '2017_r1y1aawlg', '2020_r1eX1yrKwB', '2020_Byg79h4tvB', '2019_H1lFZnR5YX', '2020_BkeWw6VFwr', '2018_HyHmGyZCZ', '2018_HyUNwulC-', '2020_HkgsPhNYPS']\n"
     ]
    }
   ],
   "source": [
    "test_set = list(gt_dict.keys())\n",
    "print(\"TestSet length: %d\\n\"%len(test_set), test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_SJf_XhCqKm     {39, 17, 20, 27, 28, 30}\n",
      "2017_Bk0MRI5lg      {48, 57}\n",
      "2020_SyevYxHtDB     {76, 87}\n",
      "2018_rJBiunlAW      {108, 110, 112, 113, 124, 126}\n",
      "2020_rkltE0VKwH     {160, 155, 184, 159}\n",
      "2018_Hki-ZlbA-      {267, 235, 236, 271}\n",
      "2019_BJx0sjC5FX     {292, 287}\n",
      "2020_r1e_FpNFDr     {312, 322, 315, 308}\n",
      "2020_B1lsXREYvr     {376, 401}\n",
      "2018_SkZxCk-0Z      {449, 443, 445, 486}\n",
      "2019_rJzoujRct7     {518, 519}\n",
      "2018_HkfXMz-Ab      {573, 566}\n",
      "2017_BJ9fZNqle      {627, 623, 615}\n",
      "2019_SyxZJn05YX     {672, 673, 657, 669, 671}\n",
      "2017_B1ckMDqlg      {714, 707}\n",
      "2017_HJ0NvFzxl      {739}\n",
      "2017_S1_pAu9xl      {792, 809, 810, 806}\n",
      "2018_SyYYPdg0-      {834, 867, 868, 869, 870, 872, 873, 844, 830}\n",
      "2017_BJAA4wKxg      {884}\n",
      "2019_HyVxPsC9tm     {931, 933, 905, 909, 912, 913, 919, 926}\n",
      "2019_HylTBhA5tQ     {972, 950}\n",
      "2019_B1l08oAct7     {994, 996, 1064, 1004, 1007, 1044, 1047, 1048, 1055}\n",
      "2018_H135uzZ0-      {1072, 1079}\n",
      "2017_H1oyRlYgg      set()\n",
      "2017_r1y1aawlg      {1125, 1162, 1100, 1102, 1168}\n",
      "2020_r1eX1yrKwB     {1177, 1202, 1212}\n",
      "2020_Byg79h4tvB     {1243, 1268}\n",
      "2019_H1lFZnR5YX     {1281, 1316, 1284, 1318, 1289, 1331, 1333, 1279}\n",
      "2020_BkeWw6VFwr     {1347, 1373}\n",
      "2018_HyHmGyZCZ      {1406, 1421, 1390, 1426}\n",
      "2018_HyUNwulC-      {1451, 1452}\n",
      "2020_HkgsPhNYPS     {1504, 1464, 1497, 1500, 1502}\n"
     ]
    }
   ],
   "source": [
    "for k in test_set:\n",
    "    print('{:20}{}'.format(k, gt_dict[k][\"mcomp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect SciBERT coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_cased\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_using_scibert(text, verbose=0):\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Input text: {}\".format(text))\n",
    "        print(\"Input ids len: \", len(input_ids[0]))\n",
    "        print(\"Input ids: \", input_ids)\n",
    "        \n",
    "        np_inp = input_ids[0].detach().numpy()\n",
    "        for i in range(np_inp.shape[0]):\n",
    "            print(tokenizer.convert_ids_to_tokens([np_inp[i]]), end=\", \")\n",
    "        \n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: The method should be compared with other state-of-the-art k-shot learning methods (e.g., Matching Networks by Vinyals et al, 2016).\n",
      "Input ids len:  39\n",
      "Input ids:  tensor([[  101,   111,   626,  1053,   203,  1073,   188,   521,  1199,   578,\n",
      "           125,   578,   111,   578, 10126,   324,   578, 19859,  2111,  1372,\n",
      "           143,   142,   211,   175,   211,   430,  5189,  2732,   224, 23208,\n",
      "         19272, 30111,   386,   197,   430,  5582,   551,   211,   102]])\n",
      "['[CLS]'], ['the'], ['method'], ['should'], ['be'], ['compared'], ['with'], ['other'], ['state'], ['-'], ['of'], ['-'], ['the'], ['-'], ['art'], ['k'], ['-'], ['shot'], ['learning'], ['methods'], ['('], ['e'], ['.'], ['g'], ['.'], [','], ['matching'], ['networks'], ['by'], ['vin'], ['##yal'], ['##s'], ['et'], ['al'], [','], ['2016'], [')'], ['.'], ['[SEP]'], "
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6396,  0.1981, -0.3156,  ..., -0.2083,  0.4822,  0.7386],\n",
       "         [ 0.9699,  0.9695, -0.0087,  ...,  0.2130,  0.5376,  0.5488],\n",
       "         [ 0.0733,  0.4639,  0.2113,  ...,  0.3894,  0.7572,  1.6601],\n",
       "         ...,\n",
       "         [-1.1417,  0.9311,  0.1918,  ...,  0.5772, -0.0610,  1.0074],\n",
       "         [ 0.3090,  0.1241, -0.4208,  ...,  0.4226,  0.3077,  0.2547],\n",
       "         [ 0.6555,  0.8356, -0.2718,  ..., -0.2735,  0.4125,  0.8153]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_text_using_scibert(\"The method should be compared with other state-of-the-art k-shot learning methods (e.g., Matching Networks by Vinyals et al, 2016).\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 101, 100, 102, 103], ['[PAD]', '[CLS]', '[UNK]', '[SEP]', '[MASK]'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids, tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the, "
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(111), end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 ['The', 'method', 'should', 'be', 'compared', 'with', 'other', 'state-of-the-art', 'k-shot', 'learning', 'methods', '(e.g.,', 'Matching', 'Networks', 'by', 'Vinyals', 'et', 'al,', '2016).']\n"
     ]
    }
   ],
   "source": [
    "toks = \"The method should be compared with other state-of-the-art k-shot learning methods \\\n",
    "(e.g., Matching Networks by Vinyals et al, 2016).\".split(\" \")\n",
    "\n",
    "print(len(toks), toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Inspect USE coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_text_using_use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE generates sentence embeddings directly. Uses PTB tokenizer in the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
