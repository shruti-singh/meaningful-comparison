{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/shruti/Desktop/iitgn/courses/SEM2/ML/Project/code/PaperAcceptancePrediction/ICLR data/masterdata_unbalanced/\"\n",
    "\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "rev_dict = {}\n",
    "paper_dict = {}\n",
    "dec_dict = {}\n",
    "iclr_arxiv_map = {}\n",
    "\n",
    "for y in years:\n",
    "    rev_dict[y] = pd.read_pickle(data_path + \"off_rev_dict_{}.pkl\".format(y))\n",
    "    paper_dict[y] = pd.read_pickle(data_path + \"papers_{}.pkl\".format(y))\n",
    "    dec_dict[y] = pd.read_pickle(data_path + \"paper_decision_dict_{}.pkl\".format(y))\n",
    "\n",
    "iclr_arxiv_map = pd.read_pickle(\"./data/iclr_arxiv_map.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"InputTestSet-Reviews48_Ann.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Sent</th>\n",
       "      <th>MComp</th>\n",
       "      <th>Cat</th>\n",
       "      <th>SubCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The authors propose to use k-DPP to select a s...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>This paper covers the related work nicely, wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The rest of the paper are also clearly written.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>However, I have some concerns about the propos...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>- It is not clear how to define the kernel, th...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID              PID     Dec  \\\n",
       "0    0  2019_SJf_XhCqKm  Reject   \n",
       "1    1  2019_SJf_XhCqKm  Reject   \n",
       "2    2  2019_SJf_XhCqKm  Reject   \n",
       "3    3  2019_SJf_XhCqKm  Reject   \n",
       "4    4  2019_SJf_XhCqKm  Reject   \n",
       "\n",
       "                                                Sent  MComp  Cat SubCat  \n",
       "0  The authors propose to use k-DPP to select a s...      0  NaN    NaN  \n",
       "1  This paper covers the related work nicely, wit...      0  NaN    NaN  \n",
       "2    The rest of the paper are also clearly written.      0  NaN    NaN  \n",
       "3  However, I have some concerns about the propos...      0  NaN    NaN  \n",
       "4  - It is not clear how to define the kernel, th...      0  NaN    NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dict = {}\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    if not pid in gt_dict:\n",
    "        gt_dict[pid] = {\"dec\": df.loc[i][\"Dec\"], \"mcomp\": set(), \"not_mcomp\": set()}\n",
    "    if df.loc[i][\"MComp\"] == 1:\n",
    "        gt_dict[pid][\"mcomp\"].add(df.loc[i][\"UID\"])\n",
    "    else:\n",
    "        gt_dict[pid][\"not_mcomp\"].add(df.loc[i][\"UID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accept': [48, 644], 'Reject': [69, 744]}\n"
     ]
    }
   ],
   "source": [
    "stats_dict = {\"Accept\": [0, 0], \"Reject\": [0, 0]}\n",
    "\n",
    "for k, v in gt_dict.items():\n",
    "    #print(len(v[\"mcomp\"]), len(v[\"not_mcomp\"]), v[\"dec\"])\n",
    "    stats_dict[v[\"dec\"]][0] += len(v[\"mcomp\"])\n",
    "    stats_dict[v[\"dec\"]][1] += len(v[\"not_mcomp\"])\n",
    "    \n",
    "print(stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestSet length: 32\n",
      " ['2019_SJf_XhCqKm', '2017_Bk0MRI5lg', '2020_SyevYxHtDB', '2018_rJBiunlAW', '2020_rkltE0VKwH', '2018_Hki-ZlbA-', '2019_BJx0sjC5FX', '2020_r1e_FpNFDr', '2020_B1lsXREYvr', '2018_SkZxCk-0Z', '2019_rJzoujRct7', '2018_HkfXMz-Ab', '2017_BJ9fZNqle', '2019_SyxZJn05YX', '2017_B1ckMDqlg', '2017_HJ0NvFzxl', '2017_S1_pAu9xl', '2018_SyYYPdg0-', '2017_BJAA4wKxg', '2019_HyVxPsC9tm', '2019_HylTBhA5tQ', '2019_B1l08oAct7', '2018_H135uzZ0-', '2017_H1oyRlYgg', '2017_r1y1aawlg', '2020_r1eX1yrKwB', '2020_Byg79h4tvB', '2019_H1lFZnR5YX', '2020_BkeWw6VFwr', '2018_HyHmGyZCZ', '2018_HyUNwulC-', '2020_HkgsPhNYPS']\n"
     ]
    }
   ],
   "source": [
    "test_set = list(gt_dict.keys())\n",
    "print(\"TestSet length: %d\\n\"%len(test_set), test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_SJf_XhCqKm     {39, 17, 20, 27, 28, 30}\n",
      "2017_Bk0MRI5lg      {48, 57}\n",
      "2020_SyevYxHtDB     {76, 87}\n",
      "2018_rJBiunlAW      {108, 110, 112, 113, 124, 126}\n",
      "2020_rkltE0VKwH     {160, 155, 184, 159}\n",
      "2018_Hki-ZlbA-      {267, 235, 236, 271}\n",
      "2019_BJx0sjC5FX     {292, 287}\n",
      "2020_r1e_FpNFDr     {312, 322, 315, 308}\n",
      "2020_B1lsXREYvr     {376, 401}\n",
      "2018_SkZxCk-0Z      {449, 443, 445, 486}\n",
      "2019_rJzoujRct7     {518, 519}\n",
      "2018_HkfXMz-Ab      {573, 566}\n",
      "2017_BJ9fZNqle      {627, 623, 615}\n",
      "2019_SyxZJn05YX     {672, 673, 657, 669, 671}\n",
      "2017_B1ckMDqlg      {714, 707}\n",
      "2017_HJ0NvFzxl      {739}\n",
      "2017_S1_pAu9xl      {792, 809, 810, 806}\n",
      "2018_SyYYPdg0-      {834, 867, 868, 869, 870, 872, 873, 844, 830}\n",
      "2017_BJAA4wKxg      {884}\n",
      "2019_HyVxPsC9tm     {931, 933, 905, 909, 912, 913, 919, 926}\n",
      "2019_HylTBhA5tQ     {972, 950}\n",
      "2019_B1l08oAct7     {994, 996, 1064, 1004, 1007, 1044, 1047, 1048, 1055}\n",
      "2018_H135uzZ0-      {1072, 1079}\n",
      "2017_H1oyRlYgg      set()\n",
      "2017_r1y1aawlg      {1125, 1162, 1100, 1102, 1168}\n",
      "2020_r1eX1yrKwB     {1177, 1202, 1212}\n",
      "2020_Byg79h4tvB     {1243, 1268}\n",
      "2019_H1lFZnR5YX     {1281, 1316, 1284, 1318, 1289, 1331, 1333, 1279}\n",
      "2020_BkeWw6VFwr     {1347, 1373}\n",
      "2018_HyHmGyZCZ      {1406, 1421, 1390, 1426}\n",
      "2018_HyUNwulC-      {1451, 1452}\n",
      "2020_HkgsPhNYPS     {1504, 1464, 1497, 1500, 1502}\n"
     ]
    }
   ],
   "source": [
    "for k in test_set:\n",
    "    print('{:20}{}'.format(k, gt_dict[k][\"mcomp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect SciBERT coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_cased\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Err for:  2\n",
      "Err for:  3\n",
      "Err for:  2\n"
     ]
    }
   ],
   "source": [
    "cased_word_dict = defaultdict(int)\n",
    "\n",
    "for s in df[\"Sent\"]:\n",
    "    try:\n",
    "        sent_text = nltk.sent_tokenize(s)\n",
    "        for sent in sent_text:\n",
    "            words = nltk.word_tokenize(sent)\n",
    "            for w in words:\n",
    "                cased_word_dict[w] += 1\n",
    "    except Exception as ex:\n",
    "        print(\"Err for: \", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Sent\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4373,\n",
       " [('the', 1939),\n",
       "  ('.', 1291),\n",
       "  (',', 1178),\n",
       "  ('of', 813),\n",
       "  ('is', 750),\n",
       "  ('to', 726),\n",
       "  ('a', 613),\n",
       "  ('and', 594),\n",
       "  ('in', 488),\n",
       "  (')', 442),\n",
       "  ('that', 375),\n",
       "  ('The', 368),\n",
       "  ('(', 326),\n",
       "  ('for', 317),\n",
       "  ('paper', 300),\n",
       "  ('on', 288),\n",
       "  ('be', 263),\n",
       "  ('are', 254),\n",
       "  ('this', 246),\n",
       "  ('I', 245)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cased_word_dict), sorted(cased_word_dict.items(), key=lambda x: x[1], reverse=True)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 56)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cased_word_dict[\"et\"], cased_word_dict[\"al\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[386, 197]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids([\"et\", \"al\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK]'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in the setences(cased): 4374.\n",
      "Vocab size of SciBERT: 31116\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique words in the setences(cased): {}.\\nVocab size of SciBERT: {}\".format(len(cased_word_dict), tokenizer.vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = list(cased_word_dict.keys()) \n",
    "token_ids = tokenizer.convert_tokens_to_ids(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1340"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids.count(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100, 1340),\n",
       " (186, 1),\n",
       " (2227, 1),\n",
       " (4633, 1),\n",
       " (146, 1),\n",
       " (649, 1),\n",
       " (6155, 1),\n",
       " (105, 1),\n",
       " (634, 1),\n",
       " (125, 1)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr = Counter(token_ids)\n",
    "ctr.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_tokens = []\n",
    "for idx, it in enumerate(token_ids):\n",
    "    if it == 100:\n",
    "        unk_tokens.append((unique_tokens[idx], cased_word_dict[unique_tokens[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1340,\n",
       " [('k-DPP', 4),\n",
       "  ('hyperparameter', 7),\n",
       "  ('nicely', 2),\n",
       "  ('hyperparameters', 6),\n",
       "  ('``', 106)])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unk_tokens), unk_tokens[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('``', 106),\n",
       " (\"''\", 99),\n",
       " (\"'s\", 34),\n",
       " ('adversarial', 31),\n",
       " (\"n't\", 26),\n",
       " ('RNNs', 24),\n",
       " ('entailment', 16),\n",
       " ('baselines', 15),\n",
       " ('CNNs', 15),\n",
       " ('e.g.', 14),\n",
       " ('RNN', 14),\n",
       " ('embeddings', 14),\n",
       " ('state-of-the-art', 12),\n",
       " ('Minor', 11),\n",
       " ('dDVI', 11),\n",
       " ('UDA', 11),\n",
       " ('top-k', 11),\n",
       " ('MNIST', 10),\n",
       " ('SOTA', 9),\n",
       " ('LSTM', 9),\n",
       " ('ICLR', 9),\n",
       " ('convinced', 8),\n",
       " ('well-written', 8),\n",
       " ('autoregressive', 8),\n",
       " ('activations', 8),\n",
       " ('TPDN', 8),\n",
       " ('NAS', 8),\n",
       " ('confusing', 8),\n",
       " ('semi-supervised', 8),\n",
       " ('hyperparameter', 7),\n",
       " ('rebuttal', 7),\n",
       " ('size-free', 7),\n",
       " ('\\\\beta', 7),\n",
       " ('one-shot', 7),\n",
       " ('2019', 7),\n",
       " ('multimodality', 7),\n",
       " ('blind-spot', 7),\n",
       " ('\\\\alpha', 7),\n",
       " ('BNN', 7),\n",
       " ('MCVI', 7),\n",
       " ('top-1', 7),\n",
       " ('hyperparameters', 6),\n",
       " (\"'m\", 6),\n",
       " ('ImageNet', 6),\n",
       " ('multi-agent', 6),\n",
       " ('misclassified', 6),\n",
       " ('amateur', 6),\n",
       " ('MoE', 6),\n",
       " ('Strengths', 6),\n",
       " ('DMPN', 6),\n",
       " ('GloVe', 6),\n",
       " ('ICML', 5),\n",
       " ('stealing', 5),\n",
       " ('SRU', 5),\n",
       " ('gated', 5),\n",
       " ('high-level', 5),\n",
       " ('L1', 5),\n",
       " ('tighter', 5),\n",
       " ('i.e.', 5),\n",
       " ('Comments', 5),\n",
       " ('One-shot', 5),\n",
       " ('CCP', 5),\n",
       " ('multi-modality', 5),\n",
       " ('z=', 5),\n",
       " ('advertised', 5),\n",
       " ('ERROR', 5),\n",
       " ('DVI', 5),\n",
       " ('PBP', 5),\n",
       " ('Adaptation', 5),\n",
       " ('RvC', 5),\n",
       " ('splitters', 5),\n",
       " ('k-DPP', 4),\n",
       " ('clearer', 4),\n",
       " ('k-DPP-RBF', 4),\n",
       " ('ReLU', 4),\n",
       " ('crafted', 4),\n",
       " ('convince', 4),\n",
       " ('LSTMs', 4),\n",
       " ('Could', 4),\n",
       " ('TRDN', 4),\n",
       " ('Bartlett', 4),\n",
       " ('\\\\log', 4),\n",
       " ('\\\\lambda', 4),\n",
       " ('tackles', 4),\n",
       " ('Clarity', 4),\n",
       " ('CoNAS', 4),\n",
       " ('DARTS', 4),\n",
       " ('PossibleWorldNet', 4),\n",
       " ('convolutions', 4),\n",
       " ('encoder-decoder', 4),\n",
       " ('uni-modal', 4),\n",
       " ('z_gaussian', 4),\n",
       " ('Weaknesses', 4),\n",
       " ('Typo', 4),\n",
       " ('GAN', 4),\n",
       " ('NLP', 4),\n",
       " ('Barber', 4),\n",
       " ('Bishop', 4),\n",
       " ('distributional', 4),\n",
       " ('GCMM', 4),\n",
       " ('PDM', 4),\n",
       " ('Unsupervised', 4),\n",
       " ('compactness', 4),\n",
       " ('intra-class', 4),\n",
       " ('\\\\lambda^', 4),\n",
       " ('\\\\ell_2', 4),\n",
       " ('self-ensembling', 4),\n",
       " ('clarifications', 3),\n",
       " ('k.', 3),\n",
       " ('large-scale', 3),\n",
       " ('readable', 3),\n",
       " ('DNN', 3),\n",
       " ('Recurrent', 3),\n",
       " ('elegant', 3),\n",
       " ('unfortunate', 3),\n",
       " ('Would', 3),\n",
       " ('caption', 3),\n",
       " ('aggregating', 3),\n",
       " ('paragraphs', 3),\n",
       " ('Tensor', 3),\n",
       " ('TPRs', 3),\n",
       " ('normalizing', 3),\n",
       " ('interpretable', 3),\n",
       " ('understandable', 3),\n",
       " ('Bender', 3),\n",
       " ('PossibleWorld', 3),\n",
       " ('1D', 3),\n",
       " ('sketches', 3),\n",
       " ('trick', 3),\n",
       " ('VAEs', 3),\n",
       " ('z_piecewise', 3),\n",
       " ('bAbI', 3),\n",
       " ('refinements', 3),\n",
       " ('kitchen', 3),\n",
       " ('bedroom', 3),\n",
       " ('strange', 3),\n",
       " ('NMT', 3),\n",
       " ('convnet', 3),\n",
       " ('so-called', 3),\n",
       " ('x^\\\\prime', 3),\n",
       " ('\\\\epsilon', 3),\n",
       " ('adversarially', 3),\n",
       " ('h_j', 3),\n",
       " ('h_l', 3),\n",
       " ('INT16', 3),\n",
       " ('FP32', 3),\n",
       " ('attention-based', 3),\n",
       " ('y_g', 3),\n",
       " ('Prototypical', 3),\n",
       " ('pseudo-labels', 3),\n",
       " ('Vu', 3),\n",
       " ('k=1', 3),\n",
       " ('certifiable', 3),\n",
       " ('label-noise', 3),\n",
       " ('SELF', 3),\n",
       " ('nicely', 2),\n",
       " ('\\\\cite', 2),\n",
       " ('provable', 2),\n",
       " ('low-discrepancy', 2),\n",
       " ('Sobol', 2),\n",
       " ('BO-TPE', 2),\n",
       " ('reservations', 2),\n",
       " ('COMMENTS', 2),\n",
       " ('Spearmint', 2),\n",
       " ('parallelization', 2),\n",
       " ('Falkner', 2),\n",
       " ('Hutter', 2),\n",
       " ('booktitle', 2),\n",
       " ('Hyperparameter', 2),\n",
       " ('motivates', 2),\n",
       " ('nonlinearities', 2),\n",
       " ('CDF', 2),\n",
       " ('SOI', 2),\n",
       " ('dropout', 2),\n",
       " ('defender', 2),\n",
       " ('F_A', 2),\n",
       " ('pseudocode', 2),\n",
       " ('stolen', 2),\n",
       " ('defending', 2),\n",
       " ('Positives', 2),\n",
       " ('attackers', 2),\n",
       " ('happening', 2),\n",
       " ('GRU', 2),\n",
       " ('BLEU', 2),\n",
       " ('non-RNN', 2),\n",
       " ('reviewer', 2),\n",
       " ('deserves', 2),\n",
       " ('comparably', 2),\n",
       " ('small-scale', 2),\n",
       " ('toy', 2),\n",
       " ('vizdoom', 2),\n",
       " ('grid-world', 2),\n",
       " ('arguably', 2),\n",
       " ('non-stationary', 2),\n",
       " ('bandit', 2),\n",
       " ('actor-critic', 2),\n",
       " ('Reinforcement', 2),\n",
       " ('VizDoom', 2),\n",
       " ('Maybe', 2),\n",
       " ('higher-level', 2),\n",
       " ('off-policy', 2),\n",
       " ('surpassing', 2),\n",
       " ('Show', 2),\n",
       " ('Adversarial', 2),\n",
       " ('L_inf', 2),\n",
       " ('ground-truth', 2),\n",
       " ('Carlini', 2),\n",
       " ('Madry', 2),\n",
       " ('overfit', 2),\n",
       " ('provably', 2),\n",
       " ('within-class', 2),\n",
       " ('reluplex', 2),\n",
       " ('L_p', 2),\n",
       " ('standalone', 2),\n",
       " ('Decomposition', 2),\n",
       " ('filler', 2),\n",
       " ('insightful', 2),\n",
       " ('self-contained', 2),\n",
       " ('\\\\eta', 2),\n",
       " ('trade-off', 2),\n",
       " ('UPDATE', 2),\n",
       " ('size-freeness', 2),\n",
       " ('\\\\|K-K_0\\\\|_\\\\sigma', 2),\n",
       " ('\\\\leq', 2),\n",
       " ('3\\\\times', 2),\n",
       " ('10^6', 2),\n",
       " ('W\\\\approx', 2),\n",
       " ('sub-architectures', 2),\n",
       " ('n^d', 2),\n",
       " ('i.e', 2),\n",
       " ('Novelty', 2),\n",
       " ('Hazan', 2),\n",
       " ('ProxylessNAS', 2),\n",
       " ('speed-up', 2),\n",
       " ('Harmonica', 2),\n",
       " ('stand-alone', 2),\n",
       " ('Simonyan', 2),\n",
       " ('Barret', 2),\n",
       " ('Zoph', 2),\n",
       " ('Quoc', 2),\n",
       " ('Explicitly', 2),\n",
       " ('exceptionally', 2),\n",
       " ('questionable', 2),\n",
       " ('kicker', 2),\n",
       " ('MicroWe', 2),\n",
       " ('MLP', 2),\n",
       " ('Competitive', 2),\n",
       " ('Poker', 2),\n",
       " ('subfield', 2),\n",
       " ('java', 2),\n",
       " ('generalizable', 2),\n",
       " ('instantiations', 2),\n",
       " ('Java-like', 2),\n",
       " ('stochastically', 2),\n",
       " ('curious', 2),\n",
       " ('Variational', 2),\n",
       " ('well-motivated', 2),\n",
       " ('non-trivial', 2),\n",
       " ('multi-modal', 2),\n",
       " (\"'d\", 2),\n",
       " ('unimodal', 2),\n",
       " ('x|z', 2),\n",
       " ('autoencoder', 2),\n",
       " ('tiling', 2),\n",
       " ('G-NVDM', 2),\n",
       " ('NVDM', 2),\n",
       " ('log-likelihood', 2),\n",
       " ('z_gaussian1', 2),\n",
       " ('z_gaussian2', 2),\n",
       " ('ablative', 2),\n",
       " ('make-up', 2),\n",
       " ('f_critic^j', 2),\n",
       " ('self-paced', 2),\n",
       " ('intertwiner', 2),\n",
       " ('t-SNE', 2),\n",
       " ('Originality', 2),\n",
       " ('richer', 2),\n",
       " ('aka', 2),\n",
       " ('GPUs', 2),\n",
       " ('Interesting', 2),\n",
       " ('per-expert', 2),\n",
       " ('hugely', 2),\n",
       " ('T_h', 2),\n",
       " ('Theano', 2),\n",
       " ('DNNs', 2),\n",
       " ('BaBi', 2),\n",
       " ('timestep', 2),\n",
       " ('liked', 2),\n",
       " ('full-precision', 2),\n",
       " ('CIFAR10', 2),\n",
       " ('5.1.1', 2),\n",
       " ('t.', 2),\n",
       " ('captions', 2),\n",
       " ('cares', 2),\n",
       " ('discriminator', 2),\n",
       " ('pretty', 2),\n",
       " ('GAN-based', 2),\n",
       " ('benchmarking', 2),\n",
       " ('parallelizable', 2),\n",
       " ('blind-spots', 2),\n",
       " ('K-L', 2),\n",
       " ('KL-based', 2),\n",
       " ('feed-forward', 2),\n",
       " ('fixes', 2),\n",
       " ('skip', 2),\n",
       " ('factorized', 2),\n",
       " ('mini-batch', 2),\n",
       " ('FP16', 2),\n",
       " ('ConvNets', 2),\n",
       " ('flatter', 2),\n",
       " ('translators', 2),\n",
       " ('PBMT', 2),\n",
       " ('F^', 2),\n",
       " ('|F^i|', 2),\n",
       " ('refining', 2),\n",
       " ('vs.', 2),\n",
       " ('hill', 2),\n",
       " ('climbing', 2),\n",
       " ('T^i', 2),\n",
       " ('y_ref', 2),\n",
       " ('p.3', 2),\n",
       " ('fridge', 2),\n",
       " ('hungry', 2),\n",
       " ('Weakness', 2),\n",
       " ('GMs', 2),\n",
       " ('Office-Home', 2),\n",
       " ('prototypical', 2),\n",
       " ('IEEE/CVF', 2),\n",
       " ('TPN', 2),\n",
       " ('wins', 2),\n",
       " ('sim2real', 2),\n",
       " ('Similarity', 2),\n",
       " ('tackled', 2),\n",
       " ('hyper-parameters', 2),\n",
       " ('mean-teacher', 2),\n",
       " ('Shu', 2),\n",
       " ('\\\\hat', 2),\n",
       " ('et.al', 2),\n",
       " ('NRT', 2),\n",
       " ('i-vectors', 2),\n",
       " ('SVR', 2),\n",
       " ('CART', 2),\n",
       " ('HME', 2),\n",
       " ('Jacobs', 2),\n",
       " ('\\x1b', 2),\n",
       " ('\\\\sigma', 2),\n",
       " ('certify', 2),\n",
       " ('L_2', 2),\n",
       " ('retrofitting', 2),\n",
       " ('centering', 2),\n",
       " ('PPMI', 2),\n",
       " ('TOEFL', 2),\n",
       " ('ESL', 2),\n",
       " ('Blelloch', 2),\n",
       " ('Surrogate', 2),\n",
       " ('long-term', 2),\n",
       " ('self-ensemble', 2),\n",
       " ('class-conditional', 2),\n",
       " ('\\\\citep', 1),\n",
       " ('resubmission', 1),\n",
       " ('appending', 1),\n",
       " ('oblivious', 1),\n",
       " ('tree-structured', 1),\n",
       " ('Parzen', 1),\n",
       " ('Bergstra', 1),\n",
       " ('3-6', 1),\n",
       " ('k=20', 1),\n",
       " ('Springenberg', 1),\n",
       " ('//aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf', 1),\n",
       " ('Snoek', 1),\n",
       " ('//arxiv.org/pdf/1502.05700.pdf', 1),\n",
       " ('CHANGES', 1),\n",
       " ('SINCE', 1),\n",
       " ('LAST', 1),\n",
       " ('YEAR', 1),\n",
       " ('INPROCEEDINGS', 1),\n",
       " ('falkner-bayesopt17', 1),\n",
       " ('S.', 1),\n",
       " ('A.', 1),\n",
       " ('F.', 1),\n",
       " ('Hyperband', 1),\n",
       " ('NIPS', 1),\n",
       " ('InProceedings', 1),\n",
       " ('falkner-icml-18', 1),\n",
       " ('BOHB', 1),\n",
       " ('Aaron', 1),\n",
       " ('35th', 1),\n",
       " ('1436', 1),\n",
       " ('1445', 1),\n",
       " ('jul', 1),\n",
       " ('Determinantal', 1),\n",
       " ('open-loop', 1),\n",
       " ('closed-loop', 1),\n",
       " ('close-loop', 1),\n",
       " ('parallelise', 1),\n",
       " ('practicability', 1),\n",
       " ('regularizer', 1),\n",
       " ('regularizers', 1),\n",
       " ('preactivation', 1),\n",
       " ('probit-Bernoulli', 1),\n",
       " ('parameterizations', 1),\n",
       " ('sigmoidal', 1),\n",
       " ('softsign', 1),\n",
       " ('interrogation', 1),\n",
       " ('CIFAR', 1),\n",
       " ('okay', 1),\n",
       " ('TIMIT', 1),\n",
       " ('Varying', 1),\n",
       " ('grayscale', 1),\n",
       " ('Merits', 1),\n",
       " ('black-box', 1),\n",
       " ('\\\\sim', 1),\n",
       " ('P_A', 1),\n",
       " ('non-replicability', 1),\n",
       " ('D^', 1),\n",
       " ('MAD-argmax', 1),\n",
       " ('Estimating', 1),\n",
       " ('Post-feedback', 1),\n",
       " ('Concerns', 1),\n",
       " ('CIFAR100', 1),\n",
       " ('skeptical', 1),\n",
       " ('Including', 1),\n",
       " ('unreasonable', 1),\n",
       " ('lengthens', 1),\n",
       " ('class-label', 1),\n",
       " ('data/other', 1),\n",
       " ('extractor', 1),\n",
       " ('Attack', 1),\n",
       " ('blackbox', 1),\n",
       " ('perturbing', 1),\n",
       " ('practitioners.I', 1),\n",
       " ('Faster', 1),\n",
       " ('paralleled', 1),\n",
       " ('time/epoch', 1),\n",
       " ('CUDA', 1),\n",
       " ('superbly', 1),\n",
       " ('originality', 1),\n",
       " ('referencing', 1),\n",
       " ('acknowledging', 1),\n",
       " ('English-German', 1),\n",
       " ('WMT', 1),\n",
       " ('OpenNMT', 1),\n",
       " ('setups', 1),\n",
       " ('//arxiv.org/abs/1706.03762', 1),\n",
       " ('revise', 1),\n",
       " ('state-to-gates', 1),\n",
       " ('defaults', 1),\n",
       " ('Bytenet', 1),\n",
       " ('Kalchbrenner', 1),\n",
       " ('Conv', 1),\n",
       " ('Seq2Seq', 1),\n",
       " ('Dauphin', 1),\n",
       " ('Fairly', 1),\n",
       " ('underwhelming', 1),\n",
       " ('Quasi-RNNs', 1),\n",
       " ('Slightly', 1),\n",
       " ('focussing', 1),\n",
       " ('exhaustively', 1),\n",
       " ('ASR', 1),\n",
       " ('Contribution', 1),\n",
       " ('handcrafted', 1),\n",
       " ('meta-policy', 1),\n",
       " ('choses', 1),\n",
       " ('collaboratively', 1),\n",
       " ('relearning', 1),\n",
       " ('factored', 1),\n",
       " ('observation/action', 1),\n",
       " ('MDP', 1),\n",
       " ('single-agent', 1),\n",
       " ('cartesian', 1),\n",
       " ('Vizdoom', 1),\n",
       " ('weakens', 1),\n",
       " ('regret', 1),\n",
       " ('principled', 1),\n",
       " ('Relational', 1),\n",
       " ('Zambaldi', 1),\n",
       " ('//arxiv.org/abs/1806.01830', 1),\n",
       " ('Structured', 1),\n",
       " ('Generalization', 1),\n",
       " ('Multi-Agent', 1),\n",
       " ('Carion', 1),\n",
       " ('//arxiv.org/abs/1910.08809', 1),\n",
       " ('Curiosity-driven', 1),\n",
       " ('Exploration', 1),\n",
       " ('Self-supervised', 1),\n",
       " ('Pathak', 1),\n",
       " ('pronged', 1),\n",
       " ('interesting/complex', 1),\n",
       " ('replay', 1),\n",
       " ('non-centralized', 1),\n",
       " ('authours', 1),\n",
       " ('selector', 1),\n",
       " ('HRL', 1),\n",
       " ('toggle', 1),\n",
       " ('environments/tasks', 1),\n",
       " ('contrived', 1),\n",
       " ('crushed', 1),\n",
       " ('wormhole', 1),\n",
       " ('ICM', 1),\n",
       " ('RND', 1),\n",
       " ('quantifies', 1),\n",
       " ('sticks', 1),\n",
       " ('Changing', 1),\n",
       " ('Analyzing', 1),\n",
       " ('\\\\Pi', 1),\n",
       " ('optimal/minimal', 1),\n",
       " ('flips', 1),\n",
       " ('boils', 1),\n",
       " ('Reluplex', 1),\n",
       " ('2017b', 1),\n",
       " ('State-of-the-art', 1),\n",
       " ('fool', 1),\n",
       " ('Moosavi-Dezfooli', 1),\n",
       " ('Perturbations', 1),\n",
       " ('CVPR', 1),\n",
       " ('GANs', 1),\n",
       " ('\\\\max_', 1),\n",
       " ('\\\\neq', 1),\n",
       " ('_i', 1),\n",
       " ('dont', 1),\n",
       " ('timeout', 1),\n",
       " ('wouldn', 1),\n",
       " ('initialize', 1),\n",
       " ('minimal-distance', 1),\n",
       " ('brittle', 1),\n",
       " ('well-known', 1),\n",
       " ('disturbing', 1),\n",
       " ('previously-unseen', 1),\n",
       " ('over-regularized', 1),\n",
       " ('furthest', 1),\n",
       " ('subsume', 1),\n",
       " ('adversaries', 1),\n",
       " ('certian', 1),\n",
       " ('harden', 1),\n",
       " ('Incidentally', 1),\n",
       " ('truths', 1),\n",
       " ('welcomed', 1),\n",
       " ('judging', 1),\n",
       " ('perceptually', 1),\n",
       " ('unsure', 1),\n",
       " ('basics', 1),\n",
       " ('additively', 1),\n",
       " ('inspecting', 1),\n",
       " ('illustrative', 1),\n",
       " ('headway', 1),\n",
       " ('incomprehensible', 1),\n",
       " ('encodings', 1),\n",
       " ('tensor-product', 1),\n",
       " ('popularly-use', 1),\n",
       " ('sentence-encoding', 1),\n",
       " ('popularly-used', 1),\n",
       " ('regularities', 1),\n",
       " ('unidirectional', 1),\n",
       " ('bag-of-words', 1),\n",
       " ('skip-thought', 1),\n",
       " ('norm-based', 1),\n",
       " ('eta', 1),\n",
       " ('epsilon', 1),\n",
       " ('Dziugate', 1),\n",
       " (\"'ve\", 1),\n",
       " ('reread', 1),\n",
       " ('rebuttals', 1),\n",
       " ('Lipchitz', 1),\n",
       " ('naively', 1),\n",
       " ('Neushubar', 1),\n",
       " ('l_', 1),\n",
       " ('l_2', 1),\n",
       " ('multi-layer', 1),\n",
       " ('eases', 1),\n",
       " ('A.4', 1),\n",
       " ('B\\\\sqrt', 1),\n",
       " ('D/N', 1),\n",
       " ('infimum', 1),\n",
       " ('F_\\\\beta', 1),\n",
       " ('two-layered', 1),\n",
       " ('2\\\\times', 1),\n",
       " ('10^6\\\\leq', 1),\n",
       " ('5\\\\times', 1),\n",
       " ('10^5', 1),\n",
       " ('Suggestions', 1),\n",
       " ('Bartett', 1),\n",
       " ('\\\\citet', 1),\n",
       " ('Write', 1),\n",
       " ('expansive', 1),\n",
       " ('\\\\mathbb', 1),\n",
       " ('automate', 1),\n",
       " ('sub-graphs', 1),\n",
       " ('Bernouilli', 1),\n",
       " ('x^', 1),\n",
       " ('\\\\arg\\\\min_x', 1),\n",
       " ('||y', 1),\n",
       " ('Ax||', 1),\n",
       " ('Lasso', 1),\n",
       " ('f.', 1),\n",
       " ('ill-posed', 1),\n",
       " ('isometry', 1),\n",
       " ('1,2,3', 1),\n",
       " ('pre-trained', 1),\n",
       " ('well-trained', 1),\n",
       " ('Fourier-sparse', 1),\n",
       " ('X_S', 1),\n",
       " ('\\\\alpha_l', 1),\n",
       " ('sub-network', 1),\n",
       " ('enormously', 1),\n",
       " ('pseudo-code', 1),\n",
       " ('framing', 1),\n",
       " ('self-content', 1),\n",
       " ('refered', 1),\n",
       " ('well-appreciated', 1),\n",
       " ('Points', 1),\n",
       " ('Clarify', 1),\n",
       " ('HARMONICA', 1),\n",
       " ('m=1000', 1),\n",
       " ('validation/test', 1),\n",
       " ('outperform/be', 1),\n",
       " ('-structured', 1),\n",
       " ('caveat', 1),\n",
       " ('questions/issues', 1),\n",
       " ('retrained', 1),\n",
       " ('ENAS', 1),\n",
       " ('Bernoulli', 1),\n",
       " ('ScheduledDropPath', 1),\n",
       " ('Hanxiao', 1),\n",
       " ('Karen', 1),\n",
       " ('Yiming', 1),\n",
       " ('Differentiable', 1),\n",
       " ('Ligeng', 1),\n",
       " ('LIAM', 1),\n",
       " ('AMEET', 1),\n",
       " ('TALWALKAR', 1),\n",
       " ('Reproducibility', 1),\n",
       " ('Hieu', 1),\n",
       " ('Pham', 1),\n",
       " ('Melody', 1),\n",
       " ('Y.', 1),\n",
       " ('Guan', 1),\n",
       " ('V.', 1),\n",
       " ('Dean', 1),\n",
       " ('Sharing', 1),\n",
       " ('Gabriel', 1),\n",
       " ('Pieter-Jan', 1),\n",
       " ('Kindermans', 1),\n",
       " ('Vijay', 1),\n",
       " ('Vasudevan', 1),\n",
       " ('Simplifying', 1),\n",
       " ('One-Shot', 1),\n",
       " ('Elad', 1),\n",
       " ('Klivans', 1),\n",
       " ('cross-validation', 1),\n",
       " ('TreeNN', 1),\n",
       " ('inferring', 1),\n",
       " ('sequence-based', 1),\n",
       " ('tasked', 1),\n",
       " ('TreeNet', 1),\n",
       " ('POSITIVES', 1),\n",
       " ('succeeds', 1),\n",
       " ('fronts', 1),\n",
       " ('PossibleWorldNets', 1),\n",
       " ('struggle', 1),\n",
       " ('NEGATIVES', 1),\n",
       " ('non-generated', 1),\n",
       " ('hesitant', 1),\n",
       " ('incredible', 1),\n",
       " ('expressly', 1),\n",
       " ('ex-', 1),\n",
       " ('perimentation', 1),\n",
       " ('wonderful', 1),\n",
       " ('unexplained', 1),\n",
       " ('criptic', 1),\n",
       " ('Symbolic', 1),\n",
       " ('Distributional', 1),\n",
       " ('Representations', 1),\n",
       " ('Era', 1),\n",
       " ('Thousands', 1),\n",
       " ('outputting', 1),\n",
       " ('Were', 1),\n",
       " ('2D', 1),\n",
       " ('3D', 1),\n",
       " ('XZ', 1),\n",
       " ('unseen', 1),\n",
       " ('Missing', 1),\n",
       " ('accomplishments', 1),\n",
       " ('double-blinded', 1),\n",
       " ('literally', 1),\n",
       " ('-in', 1),\n",
       " ('trust-worthiness', 1),\n",
       " ('formality', 1),\n",
       " ('super-short', 1),\n",
       " ('unprofessional', 1),\n",
       " ('-it', 1),\n",
       " ('anyway', 1),\n",
       " ('understands', 1),\n",
       " ('ill-illustrated', 1),\n",
       " ('unanswered', 1),\n",
       " ('mysteries', 1),\n",
       " ('super-human', 1),\n",
       " ('imitation', 1),\n",
       " ('human-level', 1),\n",
       " ('-but', 1),\n",
       " ('Did', 1),\n",
       " ('sketches-', 1),\n",
       " ('abstractions', 1),\n",
       " ('discard', 1),\n",
       " ('Conditioned', 1),\n",
       " (\"'conditional\", 1),\n",
       " ('Excellently', 1),\n",
       " ('unstudied', 1),\n",
       " ('hypothesis/', 1),\n",
       " (\"'We\", 1),\n",
       " ('Explanations', 1),\n",
       " ('Mentioning', 1),\n",
       " ('observability', 1),\n",
       " ('Thorough', 1),\n",
       " ('summary/conclusion/future', 1),\n",
       " ('ACCEPT', 1),\n",
       " ('conclusive', 1),\n",
       " ('formulates', 1),\n",
       " ('sampling-like', 1),\n",
       " ('Experimentally', 1),\n",
       " ('~1500', 1),\n",
       " ('~150k', 1),\n",
       " ('respectable', 1),\n",
       " ('realisticness', 1),\n",
       " ('Combinatorial', 1),\n",
       " ('Concretization', 1),\n",
       " ('conditionally', 1),\n",
       " ('Progs', 1),\n",
       " ('determinstic', 1),\n",
       " ('Grammar', 1),\n",
       " ('Autoencoder', 1),\n",
       " ('Kusner', 1),\n",
       " ('DeepCoder', 1),\n",
       " ('generating/inferring', 1),\n",
       " ('leveraging', 1),\n",
       " ('auto-encoders', 1),\n",
       " ('hinders', 1),\n",
       " ('expressivity', 1),\n",
       " ('overfocused', 1),\n",
       " ('anymore', 1),\n",
       " ('reparameterizable', 1),\n",
       " ('pencilled', 1),\n",
       " ('chairs', 1),\n",
       " ('autoencoders', 1),\n",
       " ('reparameterization', 1),\n",
       " ('overly', 1),\n",
       " ('int_z', 1),\n",
       " ('VAE', 1),\n",
       " ('regularized', 1),\n",
       " ('sphere-packing', 1),\n",
       " ('posteriors', 1),\n",
       " ('hypercube-based', 1),\n",
       " ('sloppy', 1),\n",
       " ('x_i', 1),\n",
       " ('z_i|x_i', 1),\n",
       " ('flexibly', 1),\n",
       " ('fine-grained', 1),\n",
       " ('unsatisfactory', 1),\n",
       " ('scale-invariant', 1),\n",
       " ('reimplemented', 1),\n",
       " ('prior-posterior', 1),\n",
       " ('surprised', 1),\n",
       " ('word-senses', 1),\n",
       " ('time-related', 1),\n",
       " ('holidays', 1),\n",
       " ('H-NVDM', 1),\n",
       " ('parameterisation', 1),\n",
       " ('perplexity', 1),\n",
       " ('seemly', 1),\n",
       " ('dialogues', 1),\n",
       " ('z=z_gaussian', 1),\n",
       " ('regularisation', 1),\n",
       " ('Certainly', 1),\n",
       " ('implausible', 1),\n",
       " ('Typos', 1),\n",
       " ('prior-', 1),\n",
       " ('vague', 1),\n",
       " ('P_m', 1),\n",
       " ('m-th', 1),\n",
       " ('backbones', 1),\n",
       " ('VGG', 1),\n",
       " ('ResNets', 1),\n",
       " ('FPN', 1),\n",
       " ('p.4', 1),\n",
       " ('only~', 1),\n",
       " ('j-th', 1),\n",
       " ('F_critic', 1),\n",
       " ('readability', 1),\n",
       " ('leaning', 1),\n",
       " ('OVERVIEW', 1),\n",
       " ('small/low', 1),\n",
       " ('Intertwiner', 1),\n",
       " ('L2', 1),\n",
       " ('Sinkhorn', 1),\n",
       " ('COCO', 1),\n",
       " ('Fig.1', 1),\n",
       " ('QUESTIONS', 1),\n",
       " ('Buffer', 1),\n",
       " ('not-so-good', 1),\n",
       " ('couch', 1),\n",
       " ('baseball', 1),\n",
       " ('bedroll', 1),\n",
       " ('Mask', 1),\n",
       " ('RCNN', 1),\n",
       " ('AP_S', 1),\n",
       " ('43.5', 1),\n",
       " ('27.2', 1),\n",
       " ('expecting', 1),\n",
       " ('MaskRCNN', 1),\n",
       " ('AP_M', 1),\n",
       " ('AP_L', 1),\n",
       " ('ResNeXt', 1),\n",
       " ('adaptively', 1),\n",
       " ('distributing', 1),\n",
       " ('batch-size', 1),\n",
       " ('30x', 1),\n",
       " ('incurring', 1),\n",
       " ('crux', 1),\n",
       " ('regained', 1),\n",
       " ('finishing', 1),\n",
       " ('glitches', 1),\n",
       " ('//arxiv.org/pdf/1308.3432.pdf', 1),\n",
       " ('lesson', 1),\n",
       " ('ops/timestep', 1),\n",
       " ('pervious', 1),\n",
       " ('Elegant', 1),\n",
       " ('MoEs', 1),\n",
       " ('Mixture', 1),\n",
       " ('Experts', 1),\n",
       " ('timesteps', 1),\n",
       " ('sparsely', 1),\n",
       " ('sub-batch', 1),\n",
       " ('redistributes', 1),\n",
       " ('SoA', 1),\n",
       " ('computationally-matched', 1),\n",
       " ('counts/histogram', 1),\n",
       " ('well-described', 1),\n",
       " ('nifty', 1),\n",
       " ('datapoints', 1),\n",
       " ('H-H', 1),\n",
       " ('provoking', 1),\n",
       " ('graph-based', 1),\n",
       " ('//www.public.asu.edu/~cbaral/papers/aaai2016-sub.pdf', 1),\n",
       " (\"ICLR'17\", 1),\n",
       " ('LEARNING', 1),\n",
       " ('END-TO-END', 1),\n",
       " ('GOAL-ORIENTED', 1),\n",
       " ('DIALOG', 1),\n",
       " ('Bordes', 1),\n",
       " ('mentions', 1),\n",
       " ('back-propagated', 1),\n",
       " ('appendices', 1),\n",
       " ('B.2', 1),\n",
       " ('B.2.1', 1),\n",
       " ('_nu', 1),\n",
       " ('h_nu', 1),\n",
       " ('pioneering', 1),\n",
       " ('Giles', 1),\n",
       " ('Revision', 1),\n",
       " ('Pointers', 1),\n",
       " ('impactful', 1),\n",
       " ('plateauing', 1),\n",
       " ('simplifications', 1),\n",
       " ('graph-', 1),\n",
       " ('0/1', 1),\n",
       " ('GGS-NN', 1),\n",
       " ('Gated', 1),\n",
       " ('build/modify', 1),\n",
       " ('graph-structure', 1),\n",
       " ('question-answering', 1),\n",
       " ('occurence', 1),\n",
       " ('learnable', 1),\n",
       " ('gradient-descent', 1),\n",
       " ('succesfull', 1),\n",
       " (\"'symbolic\", 1),\n",
       " ('stuffs', 1),\n",
       " ('NTM', 1),\n",
       " ('Babi', 1),\n",
       " ('pre-defined', 1),\n",
       " ('human-made', 1),\n",
       " ('concise', 1),\n",
       " ('unsuitable', 1),\n",
       " ('gimmick', 1),\n",
       " ('pursuing', 1),\n",
       " ('diversified', 1),\n",
       " ('compressing', 1),\n",
       " ('compresses', 1),\n",
       " ('AlexNet', 1),\n",
       " ('style-network', 1),\n",
       " ('flops', 1),\n",
       " ('quantizes', 1),\n",
       " ('enjoyed', 1),\n",
       " ('unpredictability', 1),\n",
       " ('owed', 1),\n",
       " ('5.1.2', 1),\n",
       " ('Quantized', 1),\n",
       " ('FLOPS', 1),\n",
       " (\"'speeding\", 1),\n",
       " (\"up'\", 1),\n",
       " ('quantizing', 1),\n",
       " ('per-layer', 1),\n",
       " ('MNIST-free', 1),\n",
       " ('sacrifice', 1),\n",
       " ('plausibly', 1),\n",
       " ('resource-constrained', 1),\n",
       " ('loved', 1),\n",
       " ('3-4', 1),\n",
       " ('PPGN', 1),\n",
       " ('CVAE-GAN', 1),\n",
       " ('G_K', 1),\n",
       " ('Nothing', 1),\n",
       " ('regularizing', 1),\n",
       " ('non-degenerate', 1),\n",
       " ('overblown', 1),\n",
       " ('believing', 1),\n",
       " ('weakly-supervised', 1),\n",
       " ('Reviewer', 1),\n",
       " ('decomposes', 1),\n",
       " ('fine-tuning', 1),\n",
       " ('unfair', 1),\n",
       " ('foreground/background', 1),\n",
       " ('weird', 1),\n",
       " ('toned', 1),\n",
       " ('subnetwork', 1),\n",
       " ('reused', 1),\n",
       " ('compose', 1),\n",
       " ('appliance', 1),\n",
       " ('PASCAL', 1),\n",
       " ('ADE20K', 1),\n",
       " ('//placeschallenge.csail.mit.edu/', 1),\n",
       " ('kmeans', 1),\n",
       " ('bi-directional', 1),\n",
       " ('Convnets', 1),\n",
       " ('encoders', 1),\n",
       " ('beside', 1),\n",
       " ('unfortunately', 1),\n",
       " ('well-executed', 1),\n",
       " ('biLSTM', 1),\n",
       " ('convnet-based', 1),\n",
       " ('convnets', 1),\n",
       " ('application-specific', 1),\n",
       " ('EMNLP', 1),\n",
       " ('venue', 1),\n",
       " ('inter-scene', 1),\n",
       " ('reusing', 1),\n",
       " ('CNN+LSTM', 1),\n",
       " ('tesing', 1),\n",
       " ('PETS', 1),\n",
       " ('AVSS', 1),\n",
       " ('VIRAT', 1),\n",
       " ('\\\\sum_', 1),\n",
       " ('D_', 1),\n",
       " ('spelling', 1),\n",
       " ('siliarlity', 1),\n",
       " ('critiria', 1),\n",
       " ('Dynamic-Convolutions', 1),\n",
       " ('cell-based', 1),\n",
       " ('inefficiency', 1),\n",
       " ('Convolution', 1),\n",
       " ('Frame', 1),\n",
       " ('differencing', 1),\n",
       " ('Dyn-Convolution', 1),\n",
       " ('well-studied', 1),\n",
       " ('spatio-temporal', 1),\n",
       " ('1-4', 1),\n",
       " ('Spatially', 1),\n",
       " ('Residual', 1),\n",
       " ('Networks.', 1),\n",
       " ('Figurnov', 1),\n",
       " ('Attention', 1),\n",
       " ('Mnih', 1),\n",
       " ('End-to-end', 1),\n",
       " ('glimpses', 1),\n",
       " ('Yeung', 1),\n",
       " ('Two-Stream', 1),\n",
       " ('Convolutional', 1),\n",
       " ('Videos', 1),\n",
       " ('cheaper', 1),\n",
       " ('Unnecessary', 1),\n",
       " ('tense', 1),\n",
       " ('SoTA', 1),\n",
       " ('common.', 1),\n",
       " ('ShuffleNet', 1),\n",
       " ('speeding', 1),\n",
       " (\"'blind-spot\", 1),\n",
       " ('non-parametric', 1),\n",
       " ('resorting', 1),\n",
       " ('defeat', 1),\n",
       " ('\\\\ell_infty', 1),\n",
       " ('max_', 1),\n",
       " ('\\\\|', 1),\n",
       " ('\\\\|\\\\leq', 1),\n",
       " ('stricter', 1),\n",
       " ('|_\\\\infty', 1),\n",
       " ('KDE', 1),\n",
       " ('Provides', 1),\n",
       " ('Proposes', 1),\n",
       " ('bling-spot', 1),\n",
       " ('elaborates', 1),\n",
       " ('Either', 1),\n",
       " ('layerwise', 1),\n",
       " ...]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(unk_tokens, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in the dataset(cased): 4374.\n",
      "Vocab size of SciBERT: 31116\n",
      "1340 tokens out of 4374 unique tokens are not present.\n",
      "\n",
      "Total words in the dataset: 34868\n",
      "Absent word count in the dataset: 2484\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique words in the dataset(cased): {}.\\nVocab size of SciBERT: {}\".format(len(cased_word_dict), tokenizer.vocab_size))\n",
    "print(\"{} tokens out of {} unique tokens are not present.\\n\".format(token_ids.count(100), len(cased_word_dict)))\n",
    "\n",
    "print(\"Total words in the dataset: {}\".format(sum(cased_word_dict.values())))\n",
    "print(\"Absent word count in the dataset: {}\".format(sum(x[1] for x in unk_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_uncased = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "model_uncased = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Err for:  2\n",
      "Err for:  3\n",
      "Err for:  2\n"
     ]
    }
   ],
   "source": [
    "uncased_word_dict = defaultdict(int)\n",
    "\n",
    "for s in df[\"Sent\"]:\n",
    "    try:\n",
    "        sent_text = nltk.sent_tokenize(s)\n",
    "        for sent in sent_text:\n",
    "            words = nltk.word_tokenize(sent)\n",
    "            for w in words:\n",
    "                w = w.lower()\n",
    "                uncased_word_dict[w] += 1\n",
    "    except Exception as ex:\n",
    "        print(\"Err for: \", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505,)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Sent\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3937,\n",
       " [('the', 2310),\n",
       "  ('.', 1291),\n",
       "  (',', 1178),\n",
       "  ('of', 815),\n",
       "  ('is', 765),\n",
       "  ('to', 737),\n",
       "  ('a', 637),\n",
       "  ('and', 597),\n",
       "  ('in', 568),\n",
       "  (')', 442),\n",
       "  ('that', 383),\n",
       "  ('this', 357),\n",
       "  ('for', 340),\n",
       "  ('(', 326),\n",
       "  ('paper', 309),\n",
       "  ('it', 295),\n",
       "  ('on', 294),\n",
       "  ('be', 263),\n",
       "  ('are', 262),\n",
       "  ('i', 255)])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uncased_word_dict), sorted(uncased_word_dict.items(), key=lambda x: x[1], reverse=True)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 56)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncased_word_dict[\"et\"], uncased_word_dict[\"al\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[365, 186]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_uncased.convert_tokens_to_ids([\"et\", \"al\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'unk_token': '[UNK]',\n",
       "  'sep_token': '[SEP]',\n",
       "  'pad_token': '[PAD]',\n",
       "  'cls_token': '[CLS]',\n",
       "  'mask_token': '[MASK]'},\n",
       " 101)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_uncased.special_tokens_map, tokenizer_uncased.convert_tokens_to_ids(\"UNK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK]'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_uncased.convert_ids_to_tokens(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in the setences(cased): 3937.\n",
      "Vocab size of SciBERT: 31090\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique words in the setences(cased): {}.\\nVocab size of SciBERT: {}\".format(len(uncased_word_dict), tokenizer_uncased.vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens = list(uncased_word_dict.keys()) \n",
    "token_ids = tokenizer_uncased.convert_tokens_to_ids(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"RNN\" in uncased_word_dict\n",
    "# del uncased_word_dict[\"RNN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1086"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids.count(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(101, 1086),\n",
       " (111, 1),\n",
       " (1991, 1),\n",
       " (4459, 1),\n",
       " (147, 1),\n",
       " (626, 1),\n",
       " (5470, 1),\n",
       " (106, 1),\n",
       " (610, 1),\n",
       " (131, 1)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctr = Counter(token_ids)\n",
    "ctr.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_tokens = []\n",
    "for idx, it in enumerate(token_ids):\n",
    "    if it == 101:\n",
    "        unk_tokens.append((unique_tokens[idx], uncased_word_dict[unique_tokens[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1086,\n",
       " [('k-dpp', 4),\n",
       "  ('hyperparameter', 9),\n",
       "  ('nicely', 2),\n",
       "  ('hyperparameters', 6),\n",
       "  ('``', 106)])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unk_tokens), unk_tokens[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('``', 106),\n",
       " (\"'s\", 34),\n",
       " ('adversarial', 33),\n",
       " (\"n't\", 26),\n",
       " ('rnns', 24),\n",
       " ('entailment', 16),\n",
       " ('baselines', 15),\n",
       " ('cnns', 15),\n",
       " ('e.g.', 14),\n",
       " ('rnn', 14),\n",
       " ('state-of-the-art', 13),\n",
       " ('one-shot', 13),\n",
       " ('ddvi', 11),\n",
       " ('uda', 11),\n",
       " ('top-k', 11),\n",
       " ('mnist', 10),\n",
       " ('sota', 10),\n",
       " ('hyperparameter', 9),\n",
       " ('lstm', 9),\n",
       " ('iclr', 9)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(unk_tokens, key=lambda x: x[1], reverse=True)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncased\n",
      "Unique words in the dataset: 3937.\n",
      "Vocab size of SciBERT: 31090\n",
      "1086 tokens out of 3937 unique tokens are not present.\n",
      "\n",
      "Total words in the dataset: 34868\n",
      "Absent word count in the dataset: 2031\n"
     ]
    }
   ],
   "source": [
    "print(\"Uncased\")\n",
    "print(\"Unique words in the dataset: {}.\\nVocab size of SciBERT: {}\".format(len(uncased_word_dict), tokenizer_uncased.vocab_size))\n",
    "print(\"{} tokens out of {} unique tokens are not present.\\n\".format(token_ids.count(101), len(uncased_word_dict)))\n",
    "\n",
    "print(\"Total words in the dataset: {}\".format(sum(uncased_word_dict.values())))\n",
    "print(\"Absent word count in the dataset: {}\".format(sum(x[1] for x in unk_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rest analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_using_scibert(text, verbose=0):\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Input text: {}\".format(text))\n",
    "        print(\"Input ids len: \", len(input_ids[0]))\n",
    "        print(\"Input ids: \", input_ids)\n",
    "        \n",
    "        np_inp = input_ids[0].detach().numpy()\n",
    "        for i in range(np_inp.shape[0]):\n",
    "            print(tokenizer.convert_ids_to_tokens([np_inp[i]]), end=\", \")\n",
    "        \n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: The method should be compared with other state-of-the-art k-shot learning methods (e.g., Matching Networks by Vinyals et al, 2016).\n",
      "Input ids len:  39\n",
      "Input ids:  tensor([[  101,   111,   626,  1053,   203,  1073,   188,   521,  1199,   578,\n",
      "           125,   578,   111,   578, 10126,   324,   578, 19859,  2111,  1372,\n",
      "           143,   142,   211,   175,   211,   430,  5189,  2732,   224, 23208,\n",
      "         19272, 30111,   386,   197,   430,  5582,   551,   211,   102]])\n",
      "['[CLS]'], ['the'], ['method'], ['should'], ['be'], ['compared'], ['with'], ['other'], ['state'], ['-'], ['of'], ['-'], ['the'], ['-'], ['art'], ['k'], ['-'], ['shot'], ['learning'], ['methods'], ['('], ['e'], ['.'], ['g'], ['.'], [','], ['matching'], ['networks'], ['by'], ['vin'], ['##yal'], ['##s'], ['et'], ['al'], [','], ['2016'], [')'], ['.'], ['[SEP]'], "
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6396,  0.1981, -0.3156,  ..., -0.2083,  0.4822,  0.7386],\n",
       "         [ 0.9699,  0.9695, -0.0087,  ...,  0.2130,  0.5376,  0.5488],\n",
       "         [ 0.0733,  0.4639,  0.2113,  ...,  0.3894,  0.7572,  1.6601],\n",
       "         ...,\n",
       "         [-1.1417,  0.9311,  0.1918,  ...,  0.5772, -0.0610,  1.0074],\n",
       "         [ 0.3090,  0.1241, -0.4208,  ...,  0.4226,  0.3077,  0.2547],\n",
       "         [ 0.6555,  0.8356, -0.2718,  ..., -0.2735,  0.4125,  0.8153]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_text_using_scibert(\"The method should be compared with other state-of-the-art k-shot learning methods (e.g., Matching Networks by Vinyals et al, 2016).\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 101, 100, 102, 103], ['[PAD]', '[CLS]', '[UNK]', '[SEP]', '[MASK]'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids, tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the, "
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(111), end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 ['The', 'method', 'should', 'be', 'compared', 'with', 'other', 'state-of-the-art', 'k-shot', 'learning', 'methods', '(e.g.,', 'Matching', 'Networks', 'by', 'Vinyals', 'et', 'al,', '2016).']\n"
     ]
    }
   ],
   "source": [
    "toks = \"The method should be compared with other state-of-the-art k-shot learning methods \\\n",
    "(e.g., Matching Networks by Vinyals et al, 2016).\".split(\" \")\n",
    "\n",
    "print(len(toks), toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Inspect USE coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_text_using_use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE generates sentence embeddings directly. Uses PTB tokenizer in the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
