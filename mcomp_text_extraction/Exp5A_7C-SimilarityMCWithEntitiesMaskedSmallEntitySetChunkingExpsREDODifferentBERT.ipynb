{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"InputTestSet-Reviews48_Ann.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Sent</th>\n",
       "      <th>MComp</th>\n",
       "      <th>Cat</th>\n",
       "      <th>SubCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The authors propose to use k-DPP to select a s...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>This paper covers the related work nicely, wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The rest of the paper are also clearly written.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>However, I have some concerns about the propos...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>- It is not clear how to define the kernel, th...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID              PID     Dec  \\\n",
       "0    0  2019_SJf_XhCqKm  Reject   \n",
       "1    1  2019_SJf_XhCqKm  Reject   \n",
       "2    2  2019_SJf_XhCqKm  Reject   \n",
       "3    3  2019_SJf_XhCqKm  Reject   \n",
       "4    4  2019_SJf_XhCqKm  Reject   \n",
       "\n",
       "                                                Sent  MComp  Cat SubCat  \n",
       "0  The authors propose to use k-DPP to select a s...      0  NaN    NaN  \n",
       "1  This paper covers the related work nicely, wit...      0  NaN    NaN  \n",
       "2    The rest of the paper are also clearly written.      0  NaN    NaN  \n",
       "3  However, I have some concerns about the propos...      0  NaN    NaN  \n",
       "4  - It is not clear how to define the kernel, th...      0  NaN    NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dict = {}\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    if not pid in gt_dict:\n",
    "        gt_dict[pid] = {\"dec\": df.loc[i][\"Dec\"], \"mcomp\": set(), \"not_mcomp\": set()}\n",
    "    if df.loc[i][\"MComp\"] == 1:\n",
    "        gt_dict[pid][\"mcomp\"].add(df.loc[i][\"UID\"])\n",
    "    else:\n",
    "        gt_dict[pid][\"not_mcomp\"].add(df.loc[i][\"UID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accept': [48, 644], 'Reject': [69, 744]}\n"
     ]
    }
   ],
   "source": [
    "stats_dict = {\"Accept\": [0, 0], \"Reject\": [0, 0]}\n",
    "\n",
    "for k, v in gt_dict.items():\n",
    "    #print(len(v[\"mcomp\"]), len(v[\"not_mcomp\"]), v[\"dec\"])\n",
    "    stats_dict[v[\"dec\"]][0] += len(v[\"mcomp\"])\n",
    "    stats_dict[v[\"dec\"]][1] += len(v[\"not_mcomp\"])\n",
    "    \n",
    "print(stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestSet length: 32\n",
      " ['2019_SJf_XhCqKm', '2017_Bk0MRI5lg', '2020_SyevYxHtDB', '2018_rJBiunlAW', '2020_rkltE0VKwH', '2018_Hki-ZlbA-', '2019_BJx0sjC5FX', '2020_r1e_FpNFDr', '2020_B1lsXREYvr', '2018_SkZxCk-0Z', '2019_rJzoujRct7', '2018_HkfXMz-Ab', '2017_BJ9fZNqle', '2019_SyxZJn05YX', '2017_B1ckMDqlg', '2017_HJ0NvFzxl', '2017_S1_pAu9xl', '2018_SyYYPdg0-', '2017_BJAA4wKxg', '2019_HyVxPsC9tm', '2019_HylTBhA5tQ', '2019_B1l08oAct7', '2018_H135uzZ0-', '2017_H1oyRlYgg', '2017_r1y1aawlg', '2020_r1eX1yrKwB', '2020_Byg79h4tvB', '2019_H1lFZnR5YX', '2020_BkeWw6VFwr', '2018_HyHmGyZCZ', '2018_HyUNwulC-', '2020_HkgsPhNYPS']\n"
     ]
    }
   ],
   "source": [
    "test_set = list(gt_dict.keys())\n",
    "print(\"TestSet length: %d\\n\"%len(test_set), test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_SJf_XhCqKm     {39, 17, 20, 27, 28, 30}\n",
      "2017_Bk0MRI5lg      {48, 57}\n",
      "2020_SyevYxHtDB     {76, 87}\n",
      "2018_rJBiunlAW      {108, 110, 112, 113, 124, 126}\n",
      "2020_rkltE0VKwH     {160, 155, 184, 159}\n",
      "2018_Hki-ZlbA-      {267, 235, 236, 271}\n",
      "2019_BJx0sjC5FX     {292, 287}\n",
      "2020_r1e_FpNFDr     {312, 322, 315, 308}\n",
      "2020_B1lsXREYvr     {376, 401}\n",
      "2018_SkZxCk-0Z      {449, 443, 445, 486}\n",
      "2019_rJzoujRct7     {518, 519}\n",
      "2018_HkfXMz-Ab      {573, 566}\n",
      "2017_BJ9fZNqle      {627, 623, 615}\n",
      "2019_SyxZJn05YX     {672, 673, 657, 669, 671}\n",
      "2017_B1ckMDqlg      {714, 707}\n",
      "2017_HJ0NvFzxl      {739}\n",
      "2017_S1_pAu9xl      {792, 809, 810, 806}\n",
      "2018_SyYYPdg0-      {834, 867, 868, 869, 870, 872, 873, 844, 830}\n",
      "2017_BJAA4wKxg      {884}\n",
      "2019_HyVxPsC9tm     {931, 933, 905, 909, 912, 913, 919, 926}\n",
      "2019_HylTBhA5tQ     {972, 950}\n",
      "2019_B1l08oAct7     {994, 996, 1064, 1004, 1007, 1044, 1047, 1048, 1055}\n",
      "2018_H135uzZ0-      {1072, 1079}\n",
      "2017_H1oyRlYgg      set()\n",
      "2017_r1y1aawlg      {1125, 1162, 1100, 1102, 1168}\n",
      "2020_r1eX1yrKwB     {1177, 1202, 1212}\n",
      "2020_Byg79h4tvB     {1243, 1268}\n",
      "2019_H1lFZnR5YX     {1281, 1316, 1284, 1318, 1289, 1331, 1333, 1279}\n",
      "2020_BkeWw6VFwr     {1347, 1373}\n",
      "2018_HyHmGyZCZ      {1406, 1421, 1390, 1426}\n",
      "2018_HyUNwulC-      {1451, 1452}\n",
      "2020_HkgsPhNYPS     {1504, 1464, 1497, 1500, 1502}\n"
     ]
    }
   ],
   "source": [
    "for k in test_set:\n",
    "    print('{:20}{}'.format(k, gt_dict[k][\"mcomp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_for_test = defaultdict(list)\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    sents_for_test[pid].append((df.loc[i][\"UID\"], df.loc[i][\"Sent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"entities_dict_smaller\", \"r\") as f:\n",
    "    entity_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Material', 'Method', 'Metric', 'Task'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(entity_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('convolutional neural networks', 'Method'),\n",
       " ('convnets', 'Method'),\n",
       " ('recognition', 'Task'),\n",
       " ('visual recognition tasks', 'Task'),\n",
       " ('age estimation', 'Task'),\n",
       " ('head pose estimation', 'Task'),\n",
       " ('multi - label classification', 'Task'),\n",
       " ('semantic segmentation', 'Task'),\n",
       " ('classification', 'Task'),\n",
       " ('deep convnets', 'Method'),\n",
       " ('dldl', 'Method'),\n",
       " ('feature learning', 'Task'),\n",
       " ('deep learning', 'Method'),\n",
       " ('image classification', 'Task'),\n",
       " ('deep learning methods', 'Method'),\n",
       " ('image classification tasks', 'Task'),\n",
       " ('human pose estimation', 'Task'),\n",
       " ('convnet', 'Method'),\n",
       " ('recognition tasks', 'Task'),\n",
       " ('ensemble', 'Method')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_dict.items())[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1784\n"
     ]
    }
   ],
   "source": [
    "entity_key_map = {}\n",
    "for i in entity_dict:\n",
    "    s = re.sub('[^0-9a-zA-Z,:;.?!\\- ]+', '', i)\n",
    "    while s.find(\"  \") > -1:\n",
    "        s = s.replace(\"  \", \" \")\n",
    "    if len(s) > 2:\n",
    "        cl = re.sub('[^0-9a-zA-Z ]+', '', i)\n",
    "        while cl.find(\"  \") > -1:\n",
    "            cl = cl.replace(\"  \", \" \")\n",
    "        entity_key_map[cl.strip()] = i\n",
    "print(len(entity_key_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    }
   ],
   "source": [
    "coun = 0\n",
    "for i in entity_dict:\n",
    "    if len(i) < 5:\n",
    "        coun +=1\n",
    "#         print(i)\n",
    "print(coun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('convolutional neural networks', 'convolutional neural networks'),\n",
       " ('convnets', 'convnets'),\n",
       " ('recognition', 'recognition'),\n",
       " ('visual recognition tasks', 'visual recognition tasks'),\n",
       " ('age estimation', 'age estimation')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_key_map.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Method': 1191, 'Task': 289, 'Metric': 158, 'Material': 165})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(entity_dict.values())\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(c)\n",
    "reverse_map = defaultdict(list)\n",
    "\n",
    "for k, v in entity_dict.items():\n",
    "    reverse_map[v].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in reverse_map[\"Task\"]:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"MNIST\" in entity_key_map, \"mnist\" in entity_key_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. RoBERTa trained on SciLit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy-transformers            0.6.2\r\n",
      "tokenizers                    0.7.0\r\n",
      "transformers                  2.9.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip3.7 list | grep -E 'transformers|tokenizers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./trained_lm/MaskedRoBERTa/\")\n",
    "model = AutoModel.from_pretrained(\"./trained_lm/MaskedRoBERTa/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_using_roberta(text):\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_entities(sentence, replace_with_dataset=True):\n",
    "    cleaned_sent = re.sub('[^0-9a-zA-Z,:;.?!\\- ]+', ' ', sentence)\n",
    "    while cleaned_sent.find(\"  \") > -1:\n",
    "        cleaned_sent = cleaned_sent.replace(\"  \", \" \")\n",
    "    \n",
    "    entity_key_map_keys = list(entity_key_map.keys()) # As we will be dunamically adding entries to this dict an dthat will throw an error.\n",
    "    entities_found = []\n",
    "    for i in entity_key_map_keys:\n",
    "        if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "            entities_found.append(i)\n",
    "        elif cleaned_sent.lower().find(\" \" + i + \" \") > -1:\n",
    "            found_idx = cleaned_sent.lower().find(\" \" + i + \" \")\n",
    "            entity_dict[cleaned_sent[found_idx:found_idx+len(\" \" + i + \" \")]] = entity_dict[i]\n",
    "            entity_key_map[cleaned_sent[found_idx:found_idx+len(\" \" + i + \" \")]] = entity_key_map[i]\n",
    "    \n",
    "    entities_found.sort(key=lambda s: len(s))\n",
    "    len_sorted_entities = entities_found.copy()\n",
    "    \n",
    "    subset_entities = []\n",
    "    # Remove subset entities (eg: Among cnn and 3-layer-cnn, prefer the latter)\n",
    "    for fe in len_sorted_entities:\n",
    "        for other_ent in len_sorted_entities:\n",
    "            if fe != other_ent and other_ent.find(fe) > -1:\n",
    "                subset_entities.append(fe)\n",
    "                break\n",
    "    for se in subset_entities:\n",
    "        len_sorted_entities.remove(se)\n",
    "    for maxents in len_sorted_entities:\n",
    "        mask_name = \" \" + entity_dict[entity_key_map[i]].lower() + \" \"\n",
    "        if replace_with_dataset:\n",
    "            if mask_name == \" material \":\n",
    "                mask_name = \" dataset \"\n",
    "        cleaned_sent = cleaned_sent.replace(\" \" + maxents + \" \", mask_name)\n",
    "    words_cleaned = nltk.word_tokenize(cleaned_sent)\n",
    "    dups_removed = [v for i, v in enumerate(words_cleaned) if i == 0 or v != words_cleaned[i-1]]\n",
    "    new_dup_removed_sent = \" \".join(dups_removed)\n",
    "    return new_dup_removed_sent.strip()\n",
    "\n",
    "#     #print(cleaned_sent)\n",
    "#     for i in entity_key_map:\n",
    "#         if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "#             #print(\"Substituting ent: {} with mask: {}\".format(i, entity_dict[entity_key_map[i]].lower()))\n",
    "#             cleaned_sent = cleaned_sent.replace(i, entity_dict[entity_key_map[i]].lower())\n",
    "#     return cleaned_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the BO-PET test , the best method is to take risks . This leads to substantial improvement in results .'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"In the BO-PET test*, the best method is to take\\ risks. This leads to substantial improvement in results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'test']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"this is a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "sp_toks = [\"result\", \"method\", \"task\", \"dataset\", \"metric\", \"baseline\", \"fair\", \"unfair\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunks_using_spacy_dp(conssentence, replace_with_dataset=True):\n",
    "    \n",
    "    conssentence = mask_entities(conssentence, replace_with_dataset)\n",
    "#     print(conssentence)\n",
    "    doc = nlp(conssentence)\n",
    "    verb_subtree = []\n",
    "\n",
    "    for s in doc.sents:\n",
    "#         find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"technique\": [], \"task\": [], \"dataset\": [], \"material\": [], \"metric\": []}\n",
    "        find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"baseline\": [], \"task\": [], \n",
    "                               \"dataset\": [],  \"metric\": [], \"unfair\": [], \"fair\": []}\n",
    "\n",
    "        for tok in s:\n",
    "\n",
    "            if tok.text.lower().startswith(\"compar\"):\n",
    "                find_special_tokens[\"compar\"].append(tok)\n",
    "            else:\n",
    "                for k in sp_toks:\n",
    "                    if tok.text.lower().startswith(k):\n",
    "                        find_special_tokens[k].append(tok)\n",
    "                        break\n",
    "\n",
    "        verb_tokens = []\n",
    "        if find_special_tokens[\"compar\"]:\n",
    "            for t in find_special_tokens[\"compar\"]:\n",
    "#                     verb_subtree.append(t.subtree)\n",
    "                if t == s.root:\n",
    "                    simplified_sent = \"\"\n",
    "                    for chh in t.lefts:\n",
    "                        simplified_sent = simplified_sent + \" \" + chh.text\n",
    "                    simplified_sent = simplified_sent + \" \" + t.text\n",
    "                    for chh in t.rights:\n",
    "                        simplified_sent = simplified_sent + \" \" + chh.text\n",
    "#                         print(\"SIMP: \", simplified_sent)\n",
    "                    verb_subtree.append(simplified_sent)\n",
    "                else:\n",
    "                    verb_subtree.append(t.subtree)\n",
    "        else:\n",
    "            for k in sp_toks:\n",
    "                for i in find_special_tokens[k]:\n",
    "                    local_vt = []\n",
    "                    for j in i.ancestors:\n",
    "                        if j.pos_ == \"NOUN\":\n",
    "                            local_vt.append(j)\n",
    "                    if not local_vt:\n",
    "                        for j in i.ancestors:\n",
    "                            if j.pos_ == \"VERB\":\n",
    "                                local_vt.append(j)\n",
    "                    verb_tokens = verb_tokens + local_vt\n",
    "\n",
    "\n",
    "            for i in verb_tokens:\n",
    "                verb_subtree.append(i.subtree)\n",
    "\n",
    "    eecc = []\n",
    "    for i in verb_subtree:\n",
    "        if type(i) == str:\n",
    "            eecc.append(i)\n",
    "        else:\n",
    "            local_chunk = \"\"\n",
    "            for lcaltok in i:\n",
    "                local_chunk = local_chunk + \" \" + lcaltok.text\n",
    "            eecc.append(local_chunk)\n",
    "#     if not eecc:\n",
    "#         print(conssentence)\n",
    "    return list(set(eecc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' the practicability of the method',\n",
       " ' more large - scale experiments on image related tasks']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"It would be interesting to explore the practicability of the method on more large-scale experiments on image related tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' comparison to SOTA']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"The experimental validation is also not extensive since comparison to SOTA is not included.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_HyVxPsC9tm 938 2\n",
      "2019_HyVxPsC9tm 940 3\n",
      "2020_Byg79h4tvB 1272 [1] Conditional adversarial domain adaptation, Long et.al, in NeurIPS 2018\n",
      "[2] Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation, You et.al, in ICML 2019\n",
      "2018_HyHmGyZCZ 1425 2\n"
     ]
    }
   ],
   "source": [
    "roberta_vectors = defaultdict(dict)\n",
    "skip_uids = []\n",
    "\n",
    "for pid in gt_dict:\n",
    "    roberta_vectors[pid] = {}\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunk = \". \".join(mcomp_chunks_from_sent)\n",
    "            else:\n",
    "                final_chunk = df.loc[mcs][\"Sent\"]\n",
    "            \n",
    "            vec = embed_text_using_roberta(final_chunk.strip()).mean(1).detach().numpy()\n",
    "            roberta_vectors[pid][mcs] = vec / norm(vec)\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunk = \". \".join(mcomp_chunks_from_sent)\n",
    "            else:\n",
    "                final_chunk = df.loc[mcs][\"Sent\"]\n",
    "            \n",
    "            vec = embed_text_using_roberta(final_chunk.strip()).mean(1).detach().numpy()\n",
    "            roberta_vectors[pid][mcs] = vec / norm(vec)\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 1384\n"
     ]
    }
   ],
   "source": [
    "mcomp_sentences = {}\n",
    "not_mcomp_sentences = {}\n",
    "\n",
    "for pid in gt_dict:\n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            mcomp_sentences[mcs] = pid\n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            not_mcomp_sentences[mcs] = pid\n",
    "print(len(mcomp_sentences), len(not_mcomp_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_with_mcomp = defaultdict(dict)\n",
    "sim_with_not_mcomp = defaultdict(dict)\n",
    "sim_with_notmcomp_paper_sents = defaultdict(dict)\n",
    "\n",
    "mean_at_k = [\"1\", \"3\", \"5\", \"7\", \"10\", \"20\", \"30\", \"50\", \"100\", \"500\", \"1000\", \"1380\"]\n",
    "\n",
    "for sid in mcomp_sentences:\n",
    "    \n",
    "    # 1. With other mcomp sentences\n",
    "    temp_list = []    \n",
    "    for osid in mcomp_sentences:\n",
    "        if osid != sid:\n",
    "            temp_list.append(np.inner(roberta_vectors[mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "\n",
    "    \n",
    "    # 2. With other not_mcomp_sentences\n",
    "    temp_list = []\n",
    "    for osid in not_mcomp_sentences:\n",
    "        temp_list.append(np.inner(roberta_vectors[not_mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_not_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_not_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "\n",
    "    \n",
    "    # 3. With not_mcomp_sentences of the same paper\n",
    "    temp_list = []    \n",
    "    for osid in not_mcomp_sentences:\n",
    "        if not_mcomp_sentences[osid] == mcomp_sentences[sid]:\n",
    "            temp_list.append(np.inner(roberta_vectors[not_mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_notmcomp_paper_sents[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_notmcomp_paper_sents[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_sim_plot\n",
    "diff12 = {\"all\": []}\n",
    "for vv in mean_at_k:\n",
    "    diff12[str(vv)] = []\n",
    "\n",
    "diff13 = {\"all\": []}\n",
    "for vv in mean_at_k:\n",
    "    diff13[str(vv)] = []\n",
    "\n",
    "for sid in sim_with_mcomp:\n",
    "    diff12[\"all\"].append(sim_with_mcomp[sid][\"mean\"] - sim_with_not_mcomp[sid][\"mean\"])\n",
    "    diff13[\"all\"].append(sim_with_mcomp[sid][\"mean\"] - sim_with_notmcomp_paper_sents[sid][\"mean\"])\n",
    "    \n",
    "    for vv in mean_at_k:\n",
    "        diff12[str(vv)].append(sim_with_mcomp[sid][\"mean_{}\".format(vv)] - sim_with_not_mcomp[sid][\"mean_{}\".format(vv)])\n",
    "        diff13[str(vv)].append(sim_with_mcomp[sid][\"mean_{}\".format(vv)] - sim_with_notmcomp_paper_sents[sid][\"mean_{}\".format(vv)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30  </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">500  </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.48</td><td style=\"text-align: right;\">0.44</td><td style=\"text-align: right;\">0.39</td><td style=\"text-align: right;\">0.38</td><td style=\"text-align: right;\"> 0.34</td><td style=\"text-align: right;\"> 0.24</td><td style=\"text-align: right;\"> 0.2</td><td style=\"text-align: right;\"> 0.14</td><td style=\"text-align: right;\">  0.01</td><td style=\"text-align: right;\">  0.3</td><td style=\"text-align: right;\">   0.62</td><td style=\"text-align: right;\">   0.88</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1  </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20  </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.8</td><td style=\"text-align: right;\">0.79</td><td style=\"text-align: right;\">0.84</td><td style=\"text-align: right;\">0.85</td><td style=\"text-align: right;\"> 0.86</td><td style=\"text-align: right;\"> 0.9</td><td style=\"text-align: right;\"> 0.91</td><td style=\"text-align: right;\"> 0.96</td><td style=\"text-align: right;\">  0.85</td><td style=\"text-align: right;\">  0.74</td><td style=\"text-align: right;\">   0.74</td><td style=\"text-align: right;\">   0.74</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff13[val])/len(diff13[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30  </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">500  </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.48</td><td style=\"text-align: right;\">0.44</td><td style=\"text-align: right;\">0.39</td><td style=\"text-align: right;\">0.38</td><td style=\"text-align: right;\"> 0.34</td><td style=\"text-align: right;\"> 0.24</td><td style=\"text-align: right;\"> 0.2</td><td style=\"text-align: right;\"> 0.14</td><td style=\"text-align: right;\">  0.01</td><td style=\"text-align: right;\">  0.3</td><td style=\"text-align: right;\">   0.62</td><td style=\"text-align: right;\">   0.88</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1  </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20  </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.8</td><td style=\"text-align: right;\">0.79</td><td style=\"text-align: right;\">0.84</td><td style=\"text-align: right;\">0.85</td><td style=\"text-align: right;\"> 0.86</td><td style=\"text-align: right;\"> 0.9</td><td style=\"text-align: right;\"> 0.91</td><td style=\"text-align: right;\"> 0.96</td><td style=\"text-align: right;\">  0.85</td><td style=\"text-align: right;\">  0.74</td><td style=\"text-align: right;\">   0.74</td><td style=\"text-align: right;\">   0.74</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))\n",
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff13[val])/len(diff13[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3  </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10  </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50  </td><td style=\"text-align: right;\">100</td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.48</td><td style=\"text-align: right;\">0.4</td><td style=\"text-align: right;\">0.39</td><td style=\"text-align: right;\">0.33</td><td style=\"text-align: right;\"> 0.3</td><td style=\"text-align: right;\"> 0.21</td><td style=\"text-align: right;\"> 0.19</td><td style=\"text-align: right;\"> 0.1</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0.18</td><td style=\"text-align: right;\">   0.51</td><td style=\"text-align: right;\">   0.84</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100  </td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.76</td><td style=\"text-align: right;\">0.84</td><td style=\"text-align: right;\">0.85</td><td style=\"text-align: right;\">0.85</td><td style=\"text-align: right;\"> 0.87</td><td style=\"text-align: right;\"> 0.92</td><td style=\"text-align: right;\"> 0.95</td><td style=\"text-align: right;\"> 0.96</td><td style=\"text-align: right;\">  0.8</td><td style=\"text-align: right;\">  0.68</td><td style=\"text-align: right;\">   0.68</td><td style=\"text-align: right;\">   0.68</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff13[val])/len(diff13[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinstalling transformers?? and smaller entity set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10  </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50  </td><td style=\"text-align: right;\">100</td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.49</td><td style=\"text-align: right;\">0.42</td><td style=\"text-align: right;\">0.38</td><td style=\"text-align: right;\">0.33</td><td style=\"text-align: right;\"> 0.3</td><td style=\"text-align: right;\"> 0.21</td><td style=\"text-align: right;\"> 0.19</td><td style=\"text-align: right;\"> 0.1</td><td style=\"text-align: right;\">  0</td><td style=\"text-align: right;\">  0.19</td><td style=\"text-align: right;\">   0.52</td><td style=\"text-align: right;\">   0.83</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td></td><td style=\"text-align: right;\">1   </td><td style=\"text-align: right;\">3   </td><td style=\"text-align: right;\">5   </td><td style=\"text-align: right;\">7   </td><td style=\"text-align: right;\">10   </td><td style=\"text-align: right;\">20   </td><td style=\"text-align: right;\">30   </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1380   </td></tr>\n",
       "<tr><td></td><td style=\"text-align: right;\">0.76</td><td style=\"text-align: right;\">0.84</td><td style=\"text-align: right;\">0.85</td><td style=\"text-align: right;\">0.85</td><td style=\"text-align: right;\"> 0.86</td><td style=\"text-align: right;\"> 0.92</td><td style=\"text-align: right;\"> 0.95</td><td style=\"text-align: right;\"> 0.97</td><td style=\"text-align: right;\">  0.79</td><td style=\"text-align: right;\">  0.68</td><td style=\"text-align: right;\">   0.68</td><td style=\"text-align: right;\">   0.68</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# With dataset as mask\n",
    "res_table = [[\"\"] + mean_at_k, [\"\"]]\n",
    "\n",
    "for val in mean_at_k:\n",
    "    v1 = round(sum(i > 0 for i in diff13[val])/len(diff13[val]), 2)\n",
    "    res_table[1].append(v1)\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./trained_lm//\")\n",
    "model = AutoModel.from_pretrained(\"./trained_lm//\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Till here only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Chunks ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "corenlp = StanfordCoreNLP(\"/home/shruti/Documents/DataNLP/stanford-corenlp-4.1.0/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(sent):\n",
    "    parse_str = corenlp.parse(sent)\n",
    "    nltk_tree = Tree.fromstring(parse_str)\n",
    "    \n",
    "#     print(nltk_tree)\n",
    "    \n",
    "    subtrees_list = list(nltk_tree.subtrees())\n",
    "    subtrees_tpos = nltk_tree.treepositions()\n",
    "    for i in range(0, len(nltk_tree.leaves())):\n",
    "        tp_leaf = nltk_tree.leaf_treeposition(i)\n",
    "        subtrees_tpos.remove(tp_leaf)\n",
    "    \n",
    "    dict_len_st = {}\n",
    "    depth_of_subtree = []\n",
    "    for _, i in enumerate(subtrees_list):\n",
    "        depth_of_subtree.append((i, len(subtrees_tpos[_])))\n",
    "        dict_len_st[str(i)] = len(subtrees_tpos[_])\n",
    "    \n",
    "    cdepths = []\n",
    "    for d in depth_of_subtree:\n",
    "        cdepths.append(d[1])\n",
    "    depth_counter = Counter(cdepths)\n",
    "    sorted_depths = sorted(list(depth_counter.keys()))\n",
    "    print(sorted(depth_counter.items(), key=lambda x: x[0]))\n",
    "    \n",
    "    depth_to_split = None\n",
    "    print(sorted_depths) \n",
    "    for sd in sorted_depths:\n",
    "        if depth_counter[sd] == 3:\n",
    "            depth_to_split = 3\n",
    "        elif depth_counter[sd] > 3:\n",
    "            depth_to_split = sd\n",
    "            break\n",
    "    if depth_to_split == None or depth_to_split == 4:\n",
    "        print(\"Depth to split: {}\".format(depth_to_split))\n",
    "        \n",
    "    print(\"depth: \", depth_to_split)\n",
    "    \n",
    "    subtree_chunks = []\n",
    "    for i in depth_of_subtree:\n",
    "        if i[1] == depth_to_split:\n",
    "            subtree_chunks.append(i)\n",
    "    \n",
    "    final_chunks_sent = []\n",
    "    \n",
    "#     for tt in subtree_chunks:\n",
    "#         print(tt)\n",
    "    \n",
    "    for stchunk in subtree_chunks:\n",
    "        print(len(stchunk[0].leaves()), stchunk[0].leaves())\n",
    "#         print(stchunk)\n",
    "        if len(stchunk[0].leaves()) > 5:\n",
    "            subsubtrees = list(stchunk[0].subtrees())\n",
    "            fnlsubsub = []\n",
    "            for sss in subsubtrees:\n",
    "                if str(sss) in dict_len_st and dict_len_st[str(sss)] == depth_to_split+1:\n",
    "                    fnlsubsub.append(sss)\n",
    "            for subchunk in fnlsubsub:\n",
    "                final_chunks_sent.append(\" \".join(subchunk.leaves()))\n",
    "        else:\n",
    "            final_chunks_sent.append(\" \".join(stchunk[0].leaves()))\n",
    "#         final_chunks_sent.append(\" \".join(stchunk[0].leaves()))\n",
    "    \n",
    "    return final_chunks_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20) The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20) The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.\n",
      "[(0, 1), (1, 1), (2, 10), (3, 10), (4, 21), (5, 16), (6, 12), (7, 24), (8, 15), (9, 8), (10, 6), (11, 9), (12, 6), (13, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Second']\n",
      "1 [',']\n",
      "19 ['their', 'study', 'only', 'applies', 'to', 'a', 'small', 'number', 'like', '3', '-', '6', 'hyperparameters', 'with', 'a', 'small', 'k', '=', '20']\n",
      "1 ['-RRB-']\n",
      "20 ['The', 'real', 'challenge', 'lies', 'in', 'scaling', 'up', 'to', 'many', 'hyperparameters', 'or', 'even', 'k', '-', 'DPP', 'sampling', 'for', 'larger', 'k.', 'Third']\n",
      "1 [',']\n",
      "13 ['the', 'authors', 'do', 'not', 'compare', 'against', 'some', 'relevant', ',', 'recent', 'work', ',', 'e.g.']\n",
      "1 [',']\n",
      "22 ['Springenberg', 'et', 'al', '-LRB-', 'http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf', '-RRB-', 'and', 'Snoek', 'et', 'al', '-LRB-', 'https://arxiv.org/pdf/1502.05700.pdf', '-RRB-', 'that', 'is', 'essential', 'for', 'this', 'kind', 'of', 'empirical', 'study']\n",
      "1 ['.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Second',\n",
       " ',',\n",
       " 'their study',\n",
       " 'only',\n",
       " 'applies to a small number like 3 - 6 hyperparameters with a small k = 20',\n",
       " '-RRB-',\n",
       " 'The real challenge',\n",
       " 'lies in scaling up to many hyperparameters or even k - DPP sampling for larger k. Third',\n",
       " ',',\n",
       " 'the authors',\n",
       " 'do not compare against some relevant , recent work , e.g.',\n",
       " ',',\n",
       " 'Springenberg et al',\n",
       " '-LRB- http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf -RRB- and Snoek et al -LRB- https://arxiv.org/pdf/1502.05700.pdf -RRB- that is essential for this kind of empirical study',\n",
       " '.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(s)\n",
    "get_chunks(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 1), (7, 2), (8, 3), (9, 4), (10, 6), (11, 7), (12, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  9\n",
      "2 ['the', 'practicability']\n",
      "3 ['of', 'the', 'method']\n",
      "1 ['on']\n",
      "9 ['more', 'large', '-', 'scale', 'experiments', 'on', 'image', 'related', 'tasks']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 8), (5, 11), (6, 7), (7, 9), (8, 6), (9, 7), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "10 ['I', 'think', 'a', 'clearer', 'emphasis', 'on', 'the', 'novelty', ',', 'eg']\n",
      "1 [':']\n",
      "18 ['current', 'algorithm', 'with', 'mixing', 'rate', 'analyses', 'or', 'more', 'thorough', 'empirical', 'comparisons', 'will', 'make', 'the', 'paper', 'stronger', 'for', 'resubmission']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 7), (5, 13), (6, 16), (7, 23), (8, 18), (9, 11), (10, 7), (11, 5), (12, 4), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "20 ['propose', 'k', '-', 'DPP', 'as', 'an', 'open', 'loop', '-LRB-', 'oblivious', 'to', 'the', 'evaluation', 'of', 'configurations', '-RRB-', 'method', 'for', 'hyperparameter', 'optimization']\n",
      "1 ['and']\n",
      "44 ['provide', 'its', 'empirical', 'study', 'and', 'comparison', 'with', 'other', 'methods', 'such', 'as', 'grid', 'search', ',', 'uniform', 'random', 'search', ',', 'low', '-', 'discrepancy', 'Sobol', 'sequences', ',', 'BO', '-', 'TPE', '-LRB-', 'Bayesian', 'optimization', 'using', 'tree', '-', 'structured', 'Parzen', 'estimator', '-RRB-', 'by', 'Bergstra', 'et', 'al', '-LRB-', '2011', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 10), (3, 10), (4, 21), (5, 16), (6, 12), (7, 24), (8, 15), (9, 8), (10, 6), (11, 9), (12, 6), (13, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Second']\n",
      "1 [',']\n",
      "19 ['their', 'study', 'only', 'applies', 'to', 'a', 'small', 'number', 'like', '3', '-', '6', 'hyperparameters', 'with', 'a', 'small', 'k', '=', '20']\n",
      "1 ['-RRB-']\n",
      "20 ['The', 'real', 'challenge', 'lies', 'in', 'scaling', 'up', 'to', 'many', 'hyperparameters', 'or', 'even', 'k', '-', 'DPP', 'sampling', 'for', 'larger', 'k.', 'Third']\n",
      "1 [',']\n",
      "13 ['the', 'authors', 'do', 'not', 'compare', 'against', 'some', 'relevant', ',', 'recent', 'work', ',', 'e.g.']\n",
      "1 [',']\n",
      "22 ['Springenberg', 'et', 'al', '-LRB-', 'http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf', '-RRB-', 'and', 'Snoek', 'et', 'al', '-LRB-', 'https://arxiv.org/pdf/1502.05700.pdf', '-RRB-', 'that', 'is', 'essential', 'for', 'this', 'kind', 'of', 'empirical', 'study']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 4), (6, 7), (7, 6), (8, 6), (9, 5), (10, 2), (11, 2), (12, 4), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['COMMENTS']\n",
      "16 ['ON', 'THE', 'CHANGES', 'SINCE', 'THE', 'LAST', 'YEAR', 'I', 'am', 'not', 'convinced', 'by', 'the', 'comparison', 'with', 'Spearmint']\n",
      "1 ['added']\n",
      "7 ['by', 'the', 'authors', 'since', 'the', 'previous', 'version']\n",
      "2019_SJf_XhCqKm 30\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 1), (5, 2), (6, 3), (7, 3), (8, 5), (9, 5), (10, 7), (11, 4), (12, 7), (13, 4), (14, 2), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['propose']\n",
      "23 ['to', 'use', 'k', '-', 'DPP', 'to', 'select', 'a', 'set', 'of', 'diverse', 'parameters', 'and', 'use', 'them', 'to', 'search', 'for', 'a', 'good', 'a', 'hyperparameter', 'setting']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 7), (4, 6), (5, 2), (6, 3), (7, 4), (8, 3), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['covers']\n",
      "3 ['the', 'related', 'work']\n",
      "1 ['nicely']\n",
      "1 [',']\n",
      "10 ['with', 'details', 'on', 'both', 'closed', 'loop', 'and', 'open', 'loop', 'methods']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 7), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "2 ['The', 'rest']\n",
      "3 ['of', 'the', 'paper']\n",
      "1 ['are']\n",
      "1 ['also']\n",
      "1 ['clearly']\n",
      "1 ['written']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 2), (5, 4), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "7 ['have', 'some', 'concerns', 'about', 'the', 'proposed', 'method']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 4), (6, 3), (7, 3), (8, 7), (9, 11)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "2 ['-', 'It']\n",
      "1 ['is']\n",
      "1 ['not']\n",
      "1 ['clear']\n",
      "17 ['how', 'to', 'define', 'the', 'kernel', ',', 'the', 'feature', 'function', 'and', 'the', 'quality', 'function', 'for', 'the', 'proposed', 'method']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 3), (6, 2), (7, 2), (8, 5), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "2 ['The', 'choices']\n",
      "2 ['of', 'those']\n",
      "1 ['seem']\n",
      "8 ['to', 'have', 'a', 'huge', 'impact', 'on', 'the', 'performance']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 6), (6, 7), (7, 3), (8, 4), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "5 ['How', 'was', 'those', 'functions', 'decided']\n",
      "1 ['and']\n",
      "10 ['how', 'sensitive', 'is', 'the', 'result', 'to', 'hyperparameters', 'of', 'those', 'functions']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 5), (5, 10), (6, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "7 ['-', 'If', 'the', 'search', 'space', 'is', 'continuous']\n",
      "1 [',']\n",
      "1 ['what']\n",
      "7 ['is', 'the', 'mixing', 'rate', 'of', 'Algorithm', '2']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 4), (6, 1), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "2 ['In', 'practice']\n",
      "1 [',']\n",
      "1 ['how']\n",
      "5 ['is', '\"', 'mixed', '\"', 'decided']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['What']\n",
      "1 ['exactly']\n",
      "1 ['is']\n",
      "5 ['the', 'space', 'and', 'time', 'complexity']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 3), (6, 5), (7, 6), (8, 3), (9, 5), (10, 4), (11, 3), (12, 3), (13, 6), (14, 9), (15, 4), (16, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 [\"'m\"]\n",
      "1 ['not']\n",
      "1 ['sure']\n",
      "30 ['where', 'k', 'log', '-LRB-', 'N', '-RRB-', 'comes', 'from', 'in', 'page', '7', '-RRB-', '-', 'Algorithm', '2', 'is', 'a', 'straight', 'forward', 'extension', 'of', 'Algorithm', '1', ',', 'just', 'with', 'L', 'not', 'explicitly', 'computed']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 3), (7, 3), (8, 5), (9, 4), (10, 6), (11, 9), (12, 2), (13, 2), (14, 2), (15, 3), (16, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  8\n",
      "1 ['more']\n",
      "1 ['novelty']\n",
      "11 ['if', 'some', 'theoretical', 'analyses', 'can', 'be', 'shown', 'on', 'the', 'mixing', 'rate']\n",
      "1 ['and']\n",
      "6 ['how', 'good', 'this', 'optimization', 'algorithm', 'is']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 5), (5, 4), (6, 4), (7, 3), (8, 5), (9, 5), (10, 2), (11, 3), (12, 5), (13, 2), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "3 ['Other', 'small', 'things']\n",
      "1 [':']\n",
      "21 ['-', 'citation', 'format', 'problems', 'in', ',', 'for', 'example', ',', 'Section', '4.1', '-RRB-', 'It', 'should', 'be', '\\\\', 'citep', 'instead', 'of', '\\\\', 'cite']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 2), (5, 2), (6, 1), (7, 2), (8, 3), (9, 5), (10, 6), (11, 4), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "1 ['it']\n",
      "14 ['would', 'be', 'good', 'to', 'mention', 'Figure', '2', 'in', 'the', 'text', 'first', 'before', 'showing', 'it']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 4), (5, 1), (6, 2), (7, 3), (8, 4), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "4 ['[', 'Post', 'rebuttal', ']']\n",
      "1 ['I']\n",
      "9 ['would', 'like', 'to', 'thank', 'the', 'authors', 'for', 'their', 'clarifications']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 2), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "6 ['am', 'still', 'concerned', 'with', 'the', 'novelty']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 8), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "2 ['The', 'absence']\n",
      "4 ['of', 'provable', 'mixing', 'rate']\n",
      "1 ['is']\n",
      "1 ['also']\n",
      "3 ['a', 'potential', 'weakness']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['reviewed']\n",
      "3 ['the', 'same', 'paper']\n",
      "2 ['last', 'year']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 5), (6, 2), (7, 2), (8, 4), (9, 2), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  5\n",
      "1 ['a']\n",
      "1 ['few']\n",
      "1 ['lines']\n",
      "1 ['based']\n",
      "6 ['on', 'the', 'changes', 'made', 'by', 'authors']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 8), (4, 10), (5, 12), (6, 9), (7, 6), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "6 ['The', 'k', '-', 'DPP', 'sampling', 'algorithm']\n",
      "1 ['and']\n",
      "10 ['the', 'concept', 'of', 'k', '-', 'DPP', '-', 'RBF', 'over', 'hyperparameters']\n",
      "1 ['are']\n",
      "1 ['not']\n",
      "1 ['new']\n",
      "1 [',']\n",
      "9 ['so', 'the', 'main', 'contribution', 'here', 'is', 'the', 'empirical', 'study']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 7), (5, 4), (6, 6), (7, 10), (8, 11), (9, 6), (10, 4), (11, 4), (12, 8), (13, 5), (14, 3), (15, 3), (16, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  3\n",
      "3 ['The', 'first', 'experiment']\n",
      "3 ['by', 'the', 'authors']\n",
      "1 ['shows']\n",
      "40 ['that', 'k', '-', 'DPP', '-', 'RBF', 'gives', 'better', 'star', 'discrepancy', 'than', 'uniform', 'random', 'search', 'while', 'being', 'comparable', 'to', 'low', '-', 'discrepancy', 'Sobol', 'sequences', 'in', 'other', 'metrics', 'such', 'as', 'distance', 'from', 'the', 'center', 'or', 'an', 'arbitrary', 'corner', '-LRB-', 'Figure', '1', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 11), (5, 14), (6, 10), (7, 6), (8, 9), (9, 8), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "24 ['The', 'second', 'experiment', 'shows', 'surprisingly', 'that', 'for', 'the', 'hard', 'learning', 'rate', 'range', ',', 'k', '-', 'DPP', '-', 'RBF', 'performs', 'better', 'than', 'uniform', 'random', 'search']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "16 ['moreover', ',', 'both', 'of', 'these', 'outperform', 'BO', '-', 'TPE', '-LRB-', 'Figure', '2', ',', 'column', '1', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 5), (6, 10), (7, 11), (8, 12), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['third']\n",
      "1 ['experiment']\n",
      "1 ['shows']\n",
      "27 ['that', 'on', 'good', 'or', 'stable', 'ranges', ',', 'k', '-', 'DPP', '-', 'RBF', 'and', 'its', 'discrete', 'analog', 'slightly', 'outperform', 'uniform', 'random', 'search', 'and', 'its', 'discrete', 'analog', ',', 'respectively']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['have']\n",
      "3 ['a', 'few', 'reservations']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 5), (5, 9), (6, 12), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['First']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "22 ['do', 'not', 'find', 'these', 'outcomes', 'very', 'surprising', 'or', 'informative', ',', 'except', 'for', 'the', 'second', 'experiment', '-LRB-', 'Figure', '2', ',', 'column', '1', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 4), (6, 6), (7, 9), (8, 9), (9, 7), (10, 4), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['is']\n",
      "3 ['unclear', 'to', 'me']\n",
      "21 ['if', 'the', 'comparison', 'of', 'wall', 'clock', 'time', 'and', 'accuracy', 'holds', 'for', 'larger', 'number', 'of', 'hyperparameters', 'or', 'against', 'Spearmint', 'with', 'more', 'parallelization']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 3), (5, 1), (6, 2), (7, 3), (8, 3), (9, 2), (10, 4), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "2 ['This', 'paper']\n",
      "12 ['proposes', 'an', 'approach', 'to', 'get', 'samples', 'with', 'high', 'dispersion', 'for', 'hyperparameter', 'optimisation']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 6), (6, 3), (7, 5), (8, 1), (9, 2), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "2 ['-', 'It']\n",
      "1 ['theoretically']\n",
      "1 ['motivates']\n",
      "10 ['the', 'use', 'of', 'Determinantal', 'Point', 'Processes', 'in', 'yielding', 'such', 'samples']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 7), (5, 6), (6, 3), (7, 2), (8, 3), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "5 ['-', 'Further', ',', 'an', 'iterative']\n",
      "2 ['mixing', 'algorithm']\n",
      "1 ['is']\n",
      "8 ['proposed', 'to', 'handle', 'continuous', 'and', 'discrete', 'sample', 'space']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 2), (6, 5), (7, 4), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "2 ['-', 'Experiments']\n",
      "6 ['on', 'finding', 'hyperparameter', 'for', 'sentence', 'classification']\n",
      "1 ['are']\n",
      "1 ['presented']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 4), (5, 6), (6, 4), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "4 ['In', 'terms', 'of', 'accuracy']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "8 ['performs', 'better', 'than', 'other', 'open', '-', 'loop', 'methods']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 7), (5, 7), (6, 6), (7, 6), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "7 ['In', 'comparison', 'to', 'closed', '-', 'loop', 'methods']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "13 ['yields', 'parameter', 'settings', 'with', 'comparable', 'performance', 'but', 'with', 'gains', 'in', 'wall', 'clock', 'time']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 5), (6, 4), (7, 2), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "7 ['The', 'distinction', 'from', 'close', '-', 'loop', 'approaches']\n",
      "5 ['makes', 'it', 'easy', 'to', 'parallelise']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 10), (5, 11), (6, 5), (7, 2), (8, 4), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "12 ['This', 'paper', 'is', 'novel', 'in', 'its', 'modelling', 'of', 'hyperparameter', 'optimisation', 'with', 'DPP']\n",
      "1 ['and']\n",
      "9 ['the', 'theoretical', 'justification', 'and', 'experiments', 'have', 'been', 'clearly', 'presented']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 3), (7, 3), (8, 4), (9, 8), (10, 6), (11, 3), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  8\n",
      "3 ['more', 'empirical', 'interrogation']\n",
      "4 ['of', 'why', 'this', 'works']\n",
      "1 ['exploration']\n",
      "5 ['of', 'the', 'nearby', 'conceptual', 'space']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 8), (4, 4), (5, 2), (6, 5), (7, 4), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['experimental']\n",
      "1 ['validation']\n",
      "1 ['is']\n",
      "1 ['also']\n",
      "1 ['not']\n",
      "1 ['extensive']\n",
      "7 ['since', 'comparison', 'to', 'SOTA', 'is', 'not', 'included']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 1), (5, 2), (6, 2), (7, 2), (8, 5), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['proposed']\n",
      "1 ['regularizer']\n",
      "1 ['seems']\n",
      "8 ['to', 'be', 'a', 'particular', 'combination', 'of', 'existing', 'methods']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 8), (4, 4), (5, 8), (6, 14), (7, 10), (8, 11), (9, 2), (10, 2), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "15 ['Though', 'the', 'implied', 'connection', 'between', 'nonlinearities', 'and', 'stochastic', 'regularizers', 'is', 'intriguing', ',', 'in', 'my', 'opinion']\n",
      "3 ['the', 'empirical', 'performance']\n",
      "20 ['does', 'not', 'exceed', 'the', 'performance', 'achieved', 'by', 'similar', 'methods', 'by', 'a', 'large', 'enough', 'margin', 'to', 'arrive', 'at', 'a', 'meaningful', 'conclusion']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 10), (5, 7), (6, 12), (7, 6), (8, 6), (9, 8), (10, 5), (11, 4), (12, 2), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "25 ['The', 'method', 'proposed', 'essential', 'trains', 'neural', 'networks', 'without', 'a', 'traditional', 'nonlinearity', ',', 'using', 'multiplicative', 'gating', 'by', 'the', 'CDF', 'of', 'a', 'Gaussian', 'evaluated', 'at', 'the', 'preactivation']\n",
      "1 [';']\n",
      "13 ['this', 'is', 'motivated', 'as', 'a', 'relaxation', 'of', 'a', 'probit', '-', 'Bernoulli', 'stochastic', 'gate']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['Experiments']\n",
      "1 ['are']\n",
      "3 ['performed', 'with', 'both']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['work']\n",
      "1 ['is']\n",
      "4 ['somewhat', 'novel', 'and', 'interesting']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 2), (7, 3), (8, 3), (9, 2), (10, 2), (11, 2), (12, 5), (13, 2), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  12\n",
      "1 ['other']\n",
      "1 ['similar']\n",
      "1 ['parameterizations']\n",
      "1 ['of']\n",
      "4 ['the', 'same', '-LRB-', 'sigmoidal']\n",
      "[(0, 1), (1, 1), (2, 2)]\n",
      "[0, 1, 2]\n",
      "Depth to split: None\n",
      "depth:  None\n",
      "[(0, 1), (1, 1), (2, 2), (3, 3)]\n",
      "[0, 1, 2, 3]\n",
      "depth:  3\n",
      "1 ['etc.']\n",
      "1 ['.']\n",
      "1 ['-RRB-']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 17), (5, 16), (6, 10), (7, 12), (8, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "16 ['The', 'CIFAR', 'results', 'look', 'okay', 'by', 'today', \"'s\", 'standards', 'but', 'the', 'MNIST', 'results', 'are', 'quite', 'bad']\n",
      "1 [',']\n",
      "11 ['neural', 'nets', 'were', 'doing', 'better', 'than', '1.5', '%', 'a', 'decade', 'ago']\n",
      "1 ['and']\n",
      "14 ['the', 'SOI', 'map', 'results', '-LRB-', 'and', 'the', 'ReLU', 'baseline', '-RRB-', 'are', 'above', '2', '%']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 5), (4, 9), (5, 10), (6, 4), (7, 1), (8, 2), (9, 2), (10, 5), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['-LRB-']\n",
      "19 ['TIMIT', 'results', 'on', 'frame', 'classification', 'also', 'are', \"n't\", 'that', 'interesting', 'without', 'evaluating', 'word', 'error', 'rate', 'within', 'a', 'speech', 'pipeline']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "5 ['this', 'is', 'a', 'minor', 'point']\n",
      "1 ['.']\n",
      "1 ['-RRB-']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 10), (5, 8), (6, 7), (7, 10), (8, 9), (9, 4), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "2 ['The', 'idea']\n",
      "14 ['put', 'forth', 'that', 'SOI', 'map', 'networks', 'without', 'additional', 'nonlinearities', 'are', 'comparable', 'to', 'linear', 'functions']\n",
      "1 ['is']\n",
      "5 ['rather', 'misleading', 'as', 'they', 'are']\n",
      "1 [',']\n",
      "8 ['in', 'expectation', ',', 'nonlinear', 'functions', 'of', 'their', 'input']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 8), (6, 3), (7, 6), (8, 7), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "10 ['Varying', 'an', 'input', 'example', 'by', 'multiplying', 'or', 'adding', 'a', 'constant']\n",
      "1 ['will']\n",
      "1 ['not']\n",
      "10 ['be', 'linearly', 'reflected', 'in', 'the', 'expected', 'output', 'of', 'the', 'network']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 5), (5, 2), (6, 2), (7, 4), (8, 2), (9, 3), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "3 ['In', 'this', 'sense']\n",
      "1 ['they']\n",
      "12 ['are', 'more', 'nonlinear', 'than', 'ReLU', 'networks', 'which', 'are', 'at', 'least', 'locally', 'linear']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 1), (6, 2), (7, 2), (8, 2), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['plots']\n",
      "1 ['are']\n",
      "6 ['very', 'difficult', 'to', 'read', 'in', 'grayscale']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 9), (5, 7), (6, 6), (7, 5), (8, 3), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "4 ['Approaches', 'like', 'adaptive', 'dropout']\n",
      "1 ['also']\n",
      "18 ['have', 'the', 'binary', 'mask', 'as', 'a', 'function', 'of', 'input', 'to', 'a', 'neuron', 'very', 'similar', 'to', 'the', 'proposed', 'approach']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 8), (4, 6), (5, 7), (6, 5), (7, 2), (8, 2), (9, 4), (10, 2), (11, 3), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['is']\n",
      "1 ['not']\n",
      "1 ['clear']\n",
      "1 [',']\n",
      "5 ['even', 'from', 'the', 'new', 'draft']\n",
      "1 [',']\n",
      "12 ['how', 'the', 'proposed', 'approach', 'differs', 'to', 'Adaptive', 'dropout', 'in', 'terms', 'of', 'functionality']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 6), (5, 4), (6, 3), (7, 8), (8, 5), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['If', 'possible']\n",
      "1 [',']\n",
      "19 ['could', 'the', 'author', 'provide', 'results', 'on', 'different', 'architecture', 'choices', 'for', 'the', 'stolen', 'model', 'as', 'well', 'as', 'the', 'surrogate', 'model']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 8), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "3 ['Including', 'results', 'on']\n",
      "4 ['a', 'dataset', 'like', 'ImageNet']\n",
      "3 ['would', 'be', 'nice']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 6), (5, 3), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['proposed']\n",
      "3 ['an', 'effective', 'defense']\n",
      "2 ['against', 'model']\n",
      "2 ['stealing', 'attacks']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 8), (5, 4), (6, 4), (7, 1), (8, 2), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Merits']\n",
      "1 [':']\n",
      "14 ['1', '-RRB-', 'In', 'general', ',', 'this', 'paper', 'is', 'well', 'written', 'and', 'easy', 'to', 'follow']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 2), (5, 5), (6, 2), (7, 4), (8, 2), (9, 3), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "2 ['The', 'approach']\n",
      "11 ['is', 'a', 'significant', 'supplement', 'to', 'existing', 'defense', 'against', 'model', 'stealing', 'attacks']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4)]\n",
      "[0, 1, 2, 3]\n",
      "depth:  3\n",
      "1 ['3']\n",
      "1 ['-RRB-']\n",
      "1 ['Extensive']\n",
      "1 ['experiments']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 5), (4, 2), (5, 3), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "1 ['still']\n",
      "6 ['have', 'concerns', 'about', 'the', 'current', 'version']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 4), (6, 2), (7, 2), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['will']\n",
      "1 ['possibly']\n",
      "9 ['adjust', 'my', 'score', 'based', 'on', 'the', 'authors', \"'\", 'response']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 9), (4, 5), (5, 4), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "2 ['1', '-RRB-']\n",
      "5 ['In', 'the', 'model', 'stealing', 'setting']\n",
      "1 [',']\n",
      "3 ['attacker', 'and', 'defender']\n",
      "4 ['are', 'seemingly', 'knowledge', 'limited']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 13), (5, 6), (6, 10), (7, 9), (8, 5), (9, 2), (10, 2), (11, 4), (12, 2), (13, 4), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "8 ['This', 'should', 'be', 'clarified', 'better', 'in', 'Section', '3']\n",
      "1 ['-RRB-']\n",
      "13 ['It', 'is', 'important', 'to', 'highlight', 'that', 'the', 'defender', 'has', 'no', 'access', 'to', 'F_A']\n",
      "1 [',']\n",
      "14 ['thus', 'problem', '-LRB-', '4', '-RRB-', 'is', 'a', 'black', '-', 'box', 'optimization', 'problem', 'for', 'defense']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 2), (5, 2), (6, 3), (7, 4), (8, 1), (9, 2), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['Also']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "10 ['is', 'better', 'to', 'have', 'a', 'table', 'to', 'summarize', 'the', 'notations']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 8), (5, 9), (6, 8), (7, 11), (8, 5), (9, 3), (10, 2), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "12 ['Additional', 'questions', 'on', 'problem', 'formulation', ':', 'a', '-RRB-', 'Problem', '-LRB-', '4', '-RRB-']\n",
      "1 ['only']\n",
      "15 ['relies', 'on', 'the', 'transfer', 'set', ',', 'where', '$', 'x', '\\\\', 'sim', 'P_A', '-LRB-', 'x', '-RRB-']\n",
      "1 ['$']\n",
      "1 [',']\n",
      "1 ['right']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 6), (5, 6), (6, 3), (7, 3), (8, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "2 ['b', '-RRB-']\n",
      "7 ['For', 'evaluation', 'metrics', ',', 'utility', 'and', 'non-replicability']\n",
      "1 [',']\n",
      "10 ['do', 'they', 'have', 'the', 'same', 'D', '^', '{', 'test', '}']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 2), (5, 4), (6, 3), (7, 3), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  5\n",
      "1 ['determine']\n",
      "1 ['them']\n",
      "1 [',']\n",
      "4 ['in', 'particularly', 'for', 'F_A']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 2), (5, 2), (6, 2), (7, 3), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "2 ['c', '-RRB-']\n",
      "3 ['One', 'utility', 'constraint']\n",
      "7 ['is', 'missing', 'in', 'problem', '-LRB-', '4', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 7), (5, 5), (6, 5), (7, 6), (8, 6), (9, 3), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "12 ['I', 'noticed', 'that', 'it', 'was', 'mentioned', 'in', 'MAD', '-', 'argmax', ',', 'however']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "10 ['suggest', 'to', 'add', 'it', 'to', 'the', 'formulation', '-LRB-', '4', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 5), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "5 ['The', 'details', 'of', 'heuristic', 'solver']\n",
      "2 ['are', 'unclear']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 4), (5, 5), (6, 3), (7, 4), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "10 ['Although', 'the', 'authors', 'pointed', 'out', 'the', 'pseudocode', 'in', 'the', 'appendix']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "3 ['lacks', 'detailed', 'analysis']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 4), (4, 5), (5, 3), (6, 5), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['3']\n",
      "1 ['-RRB-']\n",
      "3 ['In', 'Estimating', 'G']\n",
      "1 [',']\n",
      "6 ['how', 'to', 'select', 'the', 'surrogate', 'model']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 7), (4, 4), (5, 4), (6, 4), (7, 8), (8, 7), (9, 5), (10, 2), (11, 3), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['Moreover']\n",
      "1 [',']\n",
      "3 ['in', 'the', 'experiment']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "21 ['mentioned', 'that', 'defense', 'performances', 'are', 'unaffected', 'by', 'choice', 'of', 'architectures', ',', 'and', 'hence', 'use', 'the', 'victim', 'architecture', 'for', 'the', 'stolen', 'model']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 4), (4, 2), (5, 2), (6, 2), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['##############']\n",
      "1 ['Post-feedback']\n",
      "1 ['################']\n",
      "1 ['I']\n",
      "7 ['am', 'satisfied', 'with', 'the', 'authors', \"'\", 'response']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 2), (5, 1), (6, 2), (7, 3), (8, 5), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Thus']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "10 ['would', 'like', 'to', 'keep', 'my', 'positive', 'comments', 'on', 'this', 'paper']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 3), (5, 6), (6, 5), (7, 10), (8, 3), (9, 2), (10, 4), (11, 3), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "8 ['Although', 'the', 'paper', 'is', 'between', '6', 'and', '8']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "1 ['finally']\n",
      "16 ['decide', 'to', 'increase', 'my', 'score', 'to', '8', 'due', 'to', 'its', 'novelty', 'in', 'formulation', 'and', 'extensive', 'experiments']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 1), (6, 2), (7, 2), (8, 1), (9, 2), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['proposes']\n",
      "3 ['a', 'new', 'method']\n",
      "5 ['for', 'defending', 'against', 'stealing', 'attacks']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 4)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['Positives']\n",
      "1 [':']\n",
      "9 ['1', '-RRB-', 'The', 'paper', 'was', 'very', 'readable', 'and', 'clear']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 3), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "3 ['The', 'proposed', 'method']\n",
      "5 ['is', 'straightforward', 'and', 'well', 'motivated']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 2), (5, 5), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "2 ['3', '-RRB-']\n",
      "2 ['The', 'authors']\n",
      "7 ['included', 'a', 'good', 'amount', 'of', 'experimental', 'results']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 5), (5, 11), (6, 7), (7, 6), (8, 7), (9, 10), (10, 6), (11, 6), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['Concerns']\n",
      "1 [':']\n",
      "35 ['1', '-RRB-', 'You', 'note', 'that', 'the', 'random', 'perturbation', 'to', 'the', 'outputs', 'performs', 'poorly', 'compared', 'to', 'your', 'method', ',', 'but', 'this', 'performance', 'gap', 'seems', 'to', 'decrease', 'as', 'the', 'dataset', 'becomes', 'more', 'difficult', '-LRB-', 'ie', 'CIFAR100', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 10), (5, 5), (6, 4), (7, 3), (8, 2), (9, 2), (10, 2), (11, 4), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "13 ['I', '’m', 'concerned', 'that', 'this', 'may', 'indicate', 'that', 'the', 'attackers', 'are', 'generally', 'weak']\n",
      "1 ['and']\n",
      "8 ['this', 'threat', 'model', 'may', 'not', 'be', 'very', 'serious']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 5), (4, 9), (5, 7), (6, 7), (7, 12), (8, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['Overall']\n",
      "1 [',']\n",
      "7 ['I', '’m', 'skeptical', 'of', 'this', 'threat', 'model']\n",
      "1 ['-']\n",
      "19 ['the', 'attackers', 'require', 'a', 'very', 'large', 'number', 'of', 'queries', ',', 'and', 'do', 'n’t', 'achieve', 'great', 'results', 'on', 'difficult', 'datasets']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 4)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "2 ['How', 'long']\n",
      "5 ['does', 'this', 'optimization', 'procedure', 'take']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 4), (6, 5), (7, 4), (8, 3), (9, 4), (10, 3), (11, 1), (12, 2), (13, 2), (14, 2), (15, 3), (16, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['seems']\n",
      "1 ['possibly']\n",
      "4 ['unreasonable', 'for', 'the', 'victim']\n",
      "15 ['to', 'implement', 'this', 'defense', 'if', 'it', 'significantly', 'lengthens', 'the', 'time', 'to', 'return', 'outputs', 'of', 'queries']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 5), (5, 6), (6, 5), (7, 4), (8, 2), (9, 2), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "2 ['3', '-RRB-']\n",
      "6 ['Although', 'this', 'is', 'a', 'defense', 'paper']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "11 ['would', 'be', 'nice', 'if', 'the', 'attacks', 'were', 'explained', 'a', 'bit', 'more']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "1 ['Specifically']\n",
      "1 [',']\n",
      "1 ['how']\n",
      "4 ['are', 'these', 'attacks', 'tested']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 4), (6, 2), (7, 4), (8, 2), (9, 3), (10, 2), (11, 5), (12, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['You']\n",
      "4 ['use', 'the', 'validation', 'set']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "14 ['does', 'the', 'attacker', 'have', 'knowledge', 'about', 'the', 'class', '-', 'label', 'space', 'of', 'the', 'victim']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 7), (4, 7), (5, 11), (6, 4), (7, 6), (8, 8), (9, 3), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "11 ['If', 'the', 'attacker', 'trained', 'with', 'some', 'synthetic', 'data', '/', 'other', 'dataset']\n",
      "1 [',']\n",
      "1 ['do']\n",
      "1 ['you']\n",
      "1 ['then']\n",
      "17 ['freeze', 'the', 'feature', 'extractor', 'and', 'train', 'a', 'linear', 'layer', 'to', 'validate', 'on', 'the', 'victim', '’s', 'test', 'set']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 7), (5, 5), (6, 4), (7, 5), (8, 3), (9, 4), (10, 4), (11, 6), (12, 2), (13, 4), (14, 2), (15, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "19 ['It', 'seems', 'like', 'this', 'is', 'discussed', 'in', 'the', 'context', 'of', 'the', 'victim', 'in', 'the', '“', 'Attack', 'Models', '”', 'subsection']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "9 ['it', '’s', 'unclear', 'what', '’s', 'happening', 'with', 'the', 'attacker']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 7), (5, 7), (6, 10), (7, 4), (8, 2), (9, 4), (10, 8), (11, 5), (12, 6), (13, 2), (14, 2), (15, 2), (16, 3), (17, 2), (18, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  2\n",
      "2 ['4', '-RRB-']\n",
      "27 ['It', 'would', 'be', 'nice', 'to', 'see', 'an', 'angular', 'histogram', 'plot', 'for', 'a', 'model', 'where', 'the', 'perturbed', 'labels', 'were', 'not', 'crafted', 'with', 'knowledge', 'of', 'this', 'model', '’s', 'parameters']\n",
      "1 ['-']\n",
      "14 ['ie', 'transfer', 'the', 'proposed', 'defense', 'to', 'a', 'blackbox', 'attacker', 'and', 'produce', 'this', 'same', 'plot']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['would']\n",
      "4 ['motivate', 'the', 'defense', 'more']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 1), (6, 3), (7, 5), (8, 6), (9, 4), (10, 3), (11, 1), (12, 2), (13, 2), (14, 5), (15, 2), (16, 5), (17, 2), (18, 5), (19, 1), (20, 2), (21, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['aims']\n",
      "27 ['at', 'defending', 'against', 'model', 'stealing', 'attacks', 'by', 'perturbing', 'the', 'posterior', 'prediction', 'of', 'a', 'protected', 'DNN', 'with', 'a', 'balanced', 'goal', 'of', 'maintaining', 'accuracy', 'and', 'maximizing', 'misleading', 'gradient', 'deviation']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 7), (4, 4), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['maximizing']\n",
      "1 ['angular']\n",
      "1 ['deviation']\n",
      "1 ['formulation']\n",
      "1 ['makes']\n",
      "4 ['sense', 'and', 'seemingly', 'correct']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 7), (5, 3), (6, 2), (7, 2), (8, 3), (9, 2), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "3 ['The', 'heuristic', 'solver']\n",
      "3 ['toward', 'this', 'objective']\n",
      "1 ['is']\n",
      "8 ['shown', 'to', 'be', 'relatively', 'effective', 'in', 'the', 'experiments']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 12), (5, 16), (6, 15), (7, 15), (8, 10), (9, 7), (10, 7), (11, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "24 ['While', 'the', 'theoretical', 'novelty', 'of', 'the', 'method', 'is', 'limited', ',', 'the', 'application', 'in', 'adversarial', 'settings', 'may', 'be', 'useful', 'to', 'advance', 'of', 'this', 'research', 'field']\n",
      "1 [',']\n",
      "26 ['especially', 'when', 'it', 'is', 'relatively', 'easy', 'to', 'apply', 'by', 'practitioners.I', 'recommend', 'toward', 'acceptance', 'of', 'this', 'paper', 'even', 'though', 'can', 'be', 'convinced', 'otherwise', 'by', 'better', 'field', 'experts']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 9), (5, 12), (6, 7), (7, 6), (8, 3), (9, 2), (10, 4), (11, 4), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "14 ['Authors', 'perform', 'experiments', 'on', 'numerous', 'tasks', 'showing', 'that', 'SRU', 'performs', 'on', 'par', 'with', 'LSTMs']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "13 ['the', 'baselines', 'for', 'these', 'tasks', 'are', 'a', 'little', 'problematic', '-LRB-', 'see', 'below', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 7), (5, 2), (6, 4), (7, 3), (8, 1), (9, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "4 ['On', 'the', 'negative', 'side']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "15 ['present', 'the', 'results', 'without', 'fully', 'referencing', 'and', 'acknowledging', 'state', '-', 'of', '-', 'the', '-', 'art']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 9), (4, 12), (5, 18), (6, 21), (7, 17), (8, 13), (9, 3), (10, 2), (11, 2), (12, 3), (13, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "40 ['As', 'another', 'example', ':', 'Table', '5', 'that', 'presents', 'results', 'for', 'English', '-', 'German', 'WMT', 'translation', 'only', 'compares', 'to', 'OpenNMT', 'setups', 'with', 'maximum', 'BLEU', 'about', '21', '-RRB-', 'But', 'already', 'a', 'long', 'time', 'ago', 'Wu', 'et', 'al', 'presented', 'LSTMs', 'reaching', '25', 'BLEU']\n",
      "1 ['and']\n",
      "14 ['current', 'SOTA', 'is', 'above', '28', 'with', 'training', 'time', 'much', 'faster', 'than', 'those', 'early', 'models']\n",
      "3 ['-LRB-', 'https://arxiv.org/abs/1706.03762', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 11), (5, 10), (6, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "6 ['While', 'the', 'latest', 'are', 'non-RNN', 'architectures']\n",
      "1 [',']\n",
      "5 ['a', 'table', 'like', 'Table', '5']\n",
      "9 ['should', 'include', 'them', 'too', ',', 'for', 'a', 'fair', 'presentation']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 9), (5, 15), (6, 8), (7, 3), (8, 2), (9, 5), (10, 3), (11, 2), (12, 2), (13, 2), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "9 ['What', 'is', 'left', 'is', 'the', 'gated', 'incremental', 'pooling', 'operation']\n",
      "1 [';']\n",
      "1 ['but']\n",
      "23 ['to', 'show', 'that', 'this', 'operation', 'is', 'beneficial', 'when', 'added', 'to', 'autoregressive', 'CNNs', ',', 'a', 'thorough', 'comparison', 'with', 'an', 'autoregressive', 'CNN', 'baseline', 'is', 'necessary']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 6), (6, 7), (7, 8), (8, 10), (9, 2), (10, 2), (11, 2), (12, 6), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "3 ['-', 'Slightly', 'unfortunate']\n",
      "1 ['naming']\n",
      "1 ['that']\n",
      "24 ['does', 'not', 'account', 'for', 'autoregressive', 'CNNs', '-', 'Lack', 'of', 'comparison', 'with', 'autoregressive', 'CNN', 'baselines', ',', 'which', 'signals', 'a', 'major', 'conceptual', 'error', 'in', 'the', 'paper']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 13), (5, 7), (6, 5), (7, 3), (8, 2), (9, 4), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "4 ['I', 'recommend', 'showing', 'exhaustively']\n",
      "1 ['and']\n",
      "14 ['experimentally', 'that', 'gated', 'incremental', 'pooling', 'can', 'be', 'helpful', 'for', 'autoregressive', 'CNNs', 'on', 'sequence', 'tasks']\n",
      "7 ['-LRB-', 'MT', ',', 'LM', 'and', 'ASR', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 4), (5, 5), (6, 2), (7, 4), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['adjust']\n",
      "2 ['my', 'score']\n",
      "1 ['accordingly']\n",
      "5 ['if', 'the', 'experiments', 'are', 'presented']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 7), (6, 2), (7, 4), (8, 6), (9, 4), (10, 2), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['work']\n",
      "1 ['presents']\n",
      "17 ['the', 'Simple', 'Recurrent', 'Unit', 'architecture', 'which', 'allows', 'more', 'parallelism', 'than', 'the', 'LSTM', 'architecture', 'while', 'maintaining', 'high', 'performance']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 8), (4, 6), (5, 8), (6, 9), (7, 6), (8, 7), (9, 4), (10, 3), (11, 2), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "5 ['Significance', ',', 'Quality', 'and', 'clarity']\n",
      "1 [':']\n",
      "26 ['The', 'idea', 'is', 'well', 'motivated', ':', 'Faster', 'training', 'is', 'important', 'for', 'rapid', 'experimentation', ',', 'and', 'altering', 'the', 'RNN', 'cell', 'so', 'it', 'can', 'be', 'paralleled', 'makes', 'sense']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 9), (5, 4), (6, 2), (7, 7), (8, 7), (9, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "5 ['The', 'idea', 'is', 'well', 'explained']\n",
      "1 ['and']\n",
      "15 ['the', 'experiments', 'convince', 'that', 'the', 'new', 'architecture', 'is', 'indeed', 'much', 'faster', 'yet', 'performs', 'very', 'well']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 4), (5, 4), (6, 6), (7, 7), (8, 9), (9, 3), (10, 2), (11, 3), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "4 ['A', 'few', 'constructive', 'comments']\n",
      "1 [':']\n",
      "1 ['-']\n",
      "24 ['The', 'experiment', '’s', 'tables', 'alternate', 'between', '“', 'time', '”', 'and', '“', 'speed', '”', ',', 'It', 'will', 'be', 'good', 'to', 'just', 'have', 'one', 'of', 'them']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 4), (4, 5), (5, 4), (6, 2), (7, 5), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "2 ['-', 'Table']\n",
      "1 ['4']\n",
      "1 ['has']\n",
      "8 ['time', '/', 'epoch', 'yet', 'only', 'time', 'is', 'stated']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 3), (6, 6), (7, 2), (8, 2), (9, 2), (10, 2), (11, 2), (12, 3), (13, 4), (14, 8), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['introduce']\n",
      "20 ['SRU', ',', 'the', 'Simple', 'Recurrent', 'Unit', 'that', 'can', 'be', 'used', 'as', 'a', 'substitute', 'for', 'LSTM', 'or', 'GRU', 'cells', 'in', 'RNNs']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 7), (6, 8), (7, 3), (8, 4), (9, 5), (10, 2), (11, 5), (12, 2), (13, 2), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['SRU']\n",
      "1 ['is']\n",
      "9 ['much', 'more', 'parallel', 'than', 'the', 'standard', 'LSTM', 'or', 'GRU']\n",
      "1 [',']\n",
      "18 ['so', 'it', 'trains', 'much', 'faster', ':', 'almost', 'as', 'fast', 'as', 'a', 'convolutional', 'layer', 'with', 'properly', 'optimized', 'CUDA', 'code']\n",
      "[(0, 1), (1, 1), (2, 9), (3, 8), (4, 15), (5, 7), (6, 12), (7, 7), (8, 10), (9, 9), (10, 7), (11, 6), (12, 5), (13, 6), (14, 3), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "4 ['On', 'the', 'positive', 'side']\n",
      "1 [',']\n",
      "9 ['the', 'paper', 'is', 'very', 'clear', 'and', 'well', '-', 'written']\n",
      "1 [',']\n",
      "16 ['the', 'SRU', 'is', 'a', 'superbly', 'elegant', 'architecture', 'with', 'a', 'fair', 'bit', 'of', 'originality', 'in', 'its', 'structure']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "26 ['the', 'results', 'show', 'that', 'it', 'could', 'be', 'a', 'significant', 'contribution', 'to', 'the', 'field', 'as', 'it', 'can', 'probably', 'replace', 'LSTMs', 'in', 'most', 'cases', 'but', 'yield', 'fast', 'training']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 5), (6, 5), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['Some']\n",
      "2 ['of', 'this']\n",
      "1 ['has']\n",
      "8 ['been', 'pointed', 'out', 'in', 'the', 'comments', 'below', 'already']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 2), (5, 2), (6, 2), (7, 1), (8, 2), (9, 2), (10, 4), (11, 3), (12, 5), (13, 3), (14, 5), (15, 6), (16, 2), (17, 3), (18, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  2\n",
      "2 ['In', 'conclusion']\n",
      "1 [':']\n",
      "2 ['the', 'authors']\n",
      "24 ['seem', 'to', 'avoid', 'discussing', 'the', 'problem', 'that', 'current', 'non-RNN', 'architectures', 'could', 'be', 'both', 'faster', 'and', 'yield', 'better', 'results', 'on', 'some', 'of', 'the', 'studied', 'problems']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 4), (6, 6), (7, 4), (8, 4), (9, 5), (10, 4), (11, 6), (12, 2), (13, 1), (14, 2), (15, 2), (16, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  3\n",
      "1 ['That']\n",
      "6 [\"'s\", 'bad', 'presentation', 'of', 'related', 'work']\n",
      "1 ['and']\n",
      "20 ['should', 'be', 'improved', 'in', 'the', 'next', 'versions', '-LRB-', 'at', 'which', 'point', 'this', 'reviewer', 'is', 'willing', 'to', 'revise', 'the', 'score', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 6), (5, 3), (6, 5), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['But']\n",
      "3 ['in', 'all', 'cases']\n",
      "1 [',']\n",
      "1 ['this']\n",
      "10 ['is', 'a', 'significant', 'contribution', 'to', 'deep', 'learning', 'and', 'deserves', 'acceptance']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 8), (6, 10), (7, 12), (8, 6), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Update']\n",
      "1 [':']\n",
      "26 ['the', 'revised', 'version', 'of', 'the', 'paper', 'addresses', 'all', 'my', 'concerns', 'and', 'the', 'comments', 'show', 'new', 'evidence', 'of', 'potential', 'applications', ',', 'so', 'I', \"'m\", 'increasing', 'my', 'score']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 1), (5, 2), (6, 3), (7, 3), (8, 7), (9, 6), (10, 6), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['propose']\n",
      "15 ['to', 'drop', 'the', 'recurrent', 'state', '-', 'to-gates', 'connections', 'from', 'RNNs', 'to', 'speed', 'up', 'the', 'model']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 3), (5, 3), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "3 ['The', 'recurrent', 'connections']\n",
      "1 ['however']\n",
      "5 ['are', 'core', 'to', 'an', 'RNN']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 7), (5, 3), (6, 4), (7, 2), (8, 3), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['Without', 'them']\n",
      "1 [',']\n",
      "11 ['the', 'RNN', 'defaults', 'simply', 'to', 'a', 'CNN', 'with', 'gated', 'incremental', 'pooling']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 3), (6, 3), (7, 1), (8, 2), (9, 12), (10, 8), (11, 3), (12, 5), (13, 8), (14, 6), (15, 7), (16, 6), (17, 13), (18, 11), (19, 10), (20, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "depth:  9\n",
      "1 ['naming']\n",
      "1 ['-LRB-']\n",
      "1 ['simple']\n",
      "1 ['*']\n",
      "1 ['recurrent']\n",
      "1 ['*']\n",
      "1 ['unit']\n",
      "1 ['-RRB-']\n",
      "1 [',']\n",
      "29 ['but', 'most', 'importantly', 'makes', 'a', 'comparison', 'with', 'autoregressive', 'sequence', 'CNNs', '[', 'Bytenet', '-LRB-', 'Kalchbrenner', 'et', 'al', '2016', '-RRB-', ',', 'Conv', 'Seq2Seq', '-LRB-', 'Dauphin', 'et', 'al', ',', '2017', '-RRB-', ']']\n",
      "1 ['crucial']\n",
      "15 ['in', 'order', 'to', 'show', 'that', 'gated', 'incremental', 'pooling', 'is', 'beneficial', 'over', 'a', 'simple', 'CNN', 'architecture']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 3), (5, 2), (6, 4), (7, 5), (8, 2), (9, 6), (10, 3), (11, 5), (12, 4), (13, 4), (14, 3), (15, 4), (16, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "2 ['In', 'essence']\n",
      "1 [',']\n",
      "2 ['the', 'paper']\n",
      "22 ['shows', 'that', 'autoregressive', 'CNNs', 'with', 'gated', 'incremental', 'pooling', 'perform', 'comparably', 'to', 'RNNs', 'on', 'a', 'number', 'of', 'tasks', 'while', 'being', 'faster', 'to', 'compute']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 8), (5, 11), (6, 9), (7, 8), (8, 3), (9, 5), (10, 6), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "15 ['Since', 'it', 'is', 'already', 'extensively', 'known', 'that', 'autoregressive', 'CNNs', 'and', 'attentional', 'models', 'can', 'achieve', 'this']\n",
      "1 [',']\n",
      "8 ['the', '*', 'CNN', '*', 'part', 'of', 'the', 'paper']\n",
      "8 ['can', 'not', 'be', 'counted', 'as', 'a', 'novel', 'contribution']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 11), (5, 8), (6, 8), (7, 4), (8, 6), (9, 5), (10, 3), (11, 3), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "5 ['Pros', ':', '-', 'Fairly', 'well']\n",
      "15 ['presented', '-', 'Wide', 'range', 'of', 'experiments', ',', 'despite', 'underwhelming', 'absolute', 'results', 'Cons', ':', '-', 'Quasi-RNNs']\n",
      "3 ['are', 'almost', 'identical']\n",
      "1 ['and']\n",
      "1 ['already']\n",
      "7 ['have', 'results', 'on', 'small', '-', 'scale', 'tasks']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 4), (5, 4), (6, 3), (7, 9), (8, 10), (9, 8), (10, 5), (11, 9), (12, 5), (13, 5), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "1 ['I']\n",
      "38 ['would', 'suggest', 'to', 'focus', 'on', 'a', 'small', 'set', 'of', 'tasks', 'and', 'show', 'that', 'the', 'model', 'achieves', 'very', 'good', 'or', 'SOTA', 'performance', 'on', 'them', ',', 'instead', 'of', 'focussing', 'on', 'many', 'tasks', 'with', 'just', 'relative', 'improvements', 'over', 'the', 'RNN', 'baseline']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 3), (6, 1), (7, 2), (8, 2), (9, 2), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['might']\n",
      "7 ['be', 'another', 'baseline', 'to', 'consider', 'for', 'comparison']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 6), (6, 6), (7, 9), (8, 11), (9, 8), (10, 10), (11, 12), (12, 4), (13, 6), (14, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "2 ['The', 'parts']\n",
      "27 ['that', 'a', 'bit', 'lacking', 'with', 'the', 'current', 'version', 'of', 'the', 'paper', 'in', 'this', 'are', 'the', 'evaluation', 'tasks', 'are', 'few', 'and', 'a', 'bit', 'simple', 'and', 'I', 'think', 'there']\n",
      "1 ['needs']\n",
      "14 ['to', 'be', 'more', 'discussion', 'on', 'the', '\"', 'coverage', '\"', 'of', 'the', 'intrinsic', 'reward', 'types']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 2), (7, 2), (8, 3), (9, 5), (10, 2), (11, 3), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  9\n",
      "1 ['more']\n",
      "1 ['standard']\n",
      "1 ['tasks']\n",
      "1 ['if']\n",
      "3 ['they', 'are', 'available']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 4), (5, 4), (6, 7), (7, 10), (8, 6), (9, 2), (10, 7), (11, 1), (12, 2), (13, 2), (14, 2), (15, 2), (16, 7), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "2 ['The', 'fact']\n",
      "12 ['that', 'applying', 'intrinsic', 'motivation', 'to', 'multi-agent', 'simulations', 'seems', 'like', 'a', 'natural', 'idea']\n",
      "24 ['would', 'be', 'to', 'convert', 'the', 'problem', 'to', 'a', '\"', 'single', '\"', 'agent', 'problem', 'to', 'compare', 'against', 'the', '\"', 'normal', '\"', 'application', 'of', 'intrinsic', 'rewards']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 1), (6, 2), (7, 2), (8, 2), (9, 4), (10, 2), (11, 5), (12, 2), (13, 3), (14, 4), (15, 3), (16, 6), (17, 4), (18, 2), (19, 4), (20, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "depth:  2\n",
      "1 ['Contribution']\n",
      "1 [':']\n",
      "28 ['The', 'paper', 'proposes', 'to', 'use', 'a', 'set', 'of', 'handcrafted', 'intrinsic', 'rewards', 'that', 'depend', 'on', 'the', 'novelty', 'of', 'an', 'observation', 'as', 'perceived', 'by', 'the', 'rest', 'of', 'the', 'other', 'agents']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 8), (5, 12), (6, 10), (7, 7), (8, 13), (9, 13), (10, 8), (11, 9), (12, 6), (13, 3), (14, 2), (15, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "7 ['For', 'each', 'pair', 'of', 'reward', 'and', 'agent']\n",
      "1 [',']\n",
      "1 ['they']\n",
      "47 ['learn', 'a', 'policy', 'and', 'a', 'value', 'through', 'actor', 'critic', 'method', ',', 'and', 'then', 'a', 'meta', '-', 'policy', 'choses', 'at', 'the', 'beginning', 'of', 'each', 'episode', 'which', 'intrinsic', 'rewards', 'to', 'use', ',', 'meaning', 'that', 'the', 'policy', 'used', 'by', 'the', 'agents', 'corresponds', 'to', 'the', 'one', 'that', 'maximizes', 'the', 'reward', 'chosen']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 5), (5, 9), (6, 4), (7, 9), (8, 11), (9, 6), (10, 6), (11, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['Review']\n",
      "1 [':']\n",
      "31 ['The', 'major', 'limitation', 'of', 'the', 'paper', 'in', 'my', 'opinion', 'is', 'the', 'fact', 'that', 'the', '\"', 'coordination', '\"', 'that', 'occurs', 'here', 'is', 'only', 'happening', 'at', 'training', 'time', ',', 'not', 'at', 'execution', 'time']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 3), (5, 2), (6, 3), (7, 1), (8, 2), (9, 5), (10, 4), (11, 1), (12, 2), (13, 3), (14, 3), (15, 2), (16, 4), (17, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "2 ['The', 'agents']\n",
      "1 ['eventually']\n",
      "21 ['learn', 'whatever', 'trajectory', 'they', 'need', 'to', 'perform', ',', 'and', 'then', 'proceed', 'to', 'do', 'so', 'without', 'any', 'interaction', 'with', 'the', 'other', 'agents']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 5), (5, 1), (6, 2), (7, 2), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "3 ['In', 'a', 'sense']\n",
      "1 [',']\n",
      "1 ['they']\n",
      "7 ['do', \"n't\", 'even', 'learn', 'to', 'explore', 'collaboratively']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 8), (5, 6), (6, 6), (7, 2), (8, 3), (9, 4), (10, 6), (11, 7), (12, 3), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "3 ['In', 'other', 'words']\n",
      "1 [',']\n",
      "9 ['agents', 'trained', 'on', 'task', '1', 'in', 'a', 'given', 'maze']\n",
      "18 ['would', 'not', 'be', 'able', 'to', 'solve', 'task', '2', 'on', 'the', 'same', 'maze', 'without', 'essentially', 'relearning', 'everything', 'from', 'scratch']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 7), (5, 4), (6, 8), (7, 12), (8, 11), (9, 11), (10, 5), (11, 4), (12, 10), (13, 6), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "3 ['The', 'other', 'corollary']\n",
      "10 ['of', 'the', 'fact', 'that', 'each', 'agent', 'learns', 'its', 'own', 'policy']\n",
      "1 ['is']\n",
      "34 ['that', 'the', 'number', 'of', 'agents', 'is', 'fixed', 'at', 'training', 'time', ',', 'preventing', 'testing', 'with', 'a', 'different', 'number', 'of', 'agents', ',', 'as', 'sometimes', 'done', 'in', 'the', 'literature', '-LRB-', '[', '1', ']', '[', '2', ']', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 10), (5, 7), (6, 12), (7, 5), (8, 2), (9, 2), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "3 ['Given', 'this', 'limitation']\n",
      "6 ['the', 'scope', 'of', 'the', 'work', 'basically']\n",
      "18 ['reduces', 'to', 'the', 'exploration', 'of', 'a', 'fixed', 'environment', 'when', 'the', 'action', 'space', 'can', 'be', 'factored', 'into', 'different', 'agents']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 8), (4, 3), (5, 1), (6, 2), (7, 3), (8, 3), (9, 5), (10, 1), (11, 2), (12, 4), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['\"']\n",
      "1 ['multi-agent']\n",
      "1 ['\"']\n",
      "1 ['formulation']\n",
      "1 ['is']\n",
      "1 ['presumably']\n",
      "15 ['meant', 'to', 'break', 'down', 'the', 'computational', 'complexity', 'of', 'having', 'a', 'joint', 'observation', '/', 'action', 'space']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 3), (6, 2), (7, 5), (8, 6), (9, 3), (10, 3), (11, 4), (12, 5), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "2 ['the', 'experiments']\n",
      "21 ['are', 'conducted', 'only', 'with', 'a', 'very', 'limited', 'number', 'of', 'agents', '-LRB-', 'only', '2', 'in', 'the', 'non', 'toy', 'environment', 'of', 'vizdoom', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 14), (5, 12), (6, 13), (7, 5), (8, 6), (9, 5), (10, 6), (11, 5), (12, 4), (13, 5), (14, 4), (15, 4), (16, 5), (17, 5), (18, 4), (19, 5), (20, 5), (21, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "depth:  2\n",
      "38 ['This', 'small', 'scale', 'does', \"n't\", ',', 'in', 'my', 'opinion', ',', 'demonstrate', 'the', 'advantage', 'of', 'the', 'decomposition', 'of', 'the', 'MDP', 'over', 'say', 'SOTA', 'single', '-', 'agent', 'exploration', 'methods', 'applied', 'to', 'the', 'cartesian', 'product', 'of', 'all', 'the', 'agents', 'action', 'spaces']\n",
      "1 ['-LRB-']\n",
      "25 ['in', 'vizdoom', 'the', 'paper', 'considers', 'only', '3', 'actions', ',', 'so', 'with', 'two', 'agents', 'it', 'would', 'amount', 'to', '9', 'actions', ',', 'which', 'is', 'still', 'very', 'tractable']\n",
      "1 ['-RRB-']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 4), (5, 7), (6, 9), (7, 8), (8, 6), (9, 5), (10, 2), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "8 ['Once', 'the', 'trajectories', 'of', 'both', 'agents', 'are', 'found']\n",
      "1 [',']\n",
      "1 ['they']\n",
      "17 ['can', 'be', 'distilled', 'to', 'each', 'of', 'them', 'individually', 'so', 'that', 'they', 'only', 'depend', 'on', 'the', 'local', 'observation']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 4), (5, 6), (6, 8), (7, 9), (8, 2), (9, 5), (10, 7), (11, 10), (12, 8), (13, 14), (14, 7), (15, 5), (16, 2), (17, 2), (18, 2), (19, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  2\n",
      "7 ['Regarding', 'the', 'experiments', 'on', 'the', 'Vizdoom', 'environment']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "49 ['appears', 'that', 'the', 'traditional', 'evaluation', 'setup', '[', '3', ']', 'does', \"n't\", 'involve', 'providing', 'the', 'global', 'position', '-LRB-', 'x', ',', 'y', '-RRB-', 'to', 'the', 'agents', 'as', 'part', 'of', 'the', 'observations', '-LRB-', 'they', 'must', 'be', 'inferred', 'from', 'the', 'visual', 'feed', '-RRB-', ',', 'contrary', 'to', 'the', 'experimental', 'setup', 'presented', 'in', 'this', 'paper']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 6), (5, 2), (6, 6), (7, 6), (8, 3), (9, 6), (10, 10), (11, 10), (12, 7), (13, 8), (14, 8)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "3 ['In', 'my', 'opinion']\n",
      "1 [',']\n",
      "1 ['this']\n",
      "40 ['weakens', 'the', 'claim', 'that', 'the', 'method', '\"', 'scales', 'to', 'more', 'complex', 'environments', '\"', 'since', 'providing', 'the', 'position', 'essentially', 'makes', 'the', 'environment', 'similar', 'to', 'a', 'grid', '-', 'world', '-LRB-', 'arguably', 'the', 'visual', 'feed', 'is', \"n't\", 'even', 'needed', 'to', 'solve', 'the', 'task']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 8), (5, 8), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "2 ['The', 'use']\n",
      "5 ['of', 'a', 'dynamic', 'policy', 'selection']\n",
      "3 ['is', 'somewhat', 'interesting']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "4 ['would', 'benefit', 'better', 'investigation']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 6), (5, 6), (6, 5), (7, 8), (8, 13), (9, 13), (10, 3), (11, 2), (12, 2), (13, 2), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['Firstly']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "36 ['is', 'not', 'clear', 'to', 'me', 'if', 'all', 'the', 'selection', 'of', 'the', 'policy', 'to', 'use', 'during', 'training', 'affects', 'all', 'the', 'trajectories', 'of', 'the', 'batch', ',', 'or', 'if', 'different', 'episodes', 'of', 'the', 'batch', 'may', 'have', 'a', 'different', 'policy']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 2), (5, 2), (6, 7), (7, 5), (8, 7), (9, 7), (10, 12), (11, 4), (12, 4), (13, 2), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['Secondly']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "32 ['seems', 'that', 'the', 'setting', 'is', 'typically', 'the', 'one', 'of', 'a', '-LRB-', 'non-stationary', '-RRB-', 'bandit', ',', 'since', 'there', 'is', 'no', 'state', 'and', 'the', '\"', 'reward', '\"', 'is', 'the', 'return', 'obtained', 'by', 'the', 'policy']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 4), (5, 2), (6, 4), (7, 2), (8, 5), (9, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Could']\n",
      "1 ['you']\n",
      "16 ['share', 'the', 'reason', 'behind', 'the', 'choice', 'of', 'an', 'actor', '-', 'critic', 'algorithm', 'over', 'classical', 'bandit', 'algorithms']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 8), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "3 ['One', 'obvious', 'advantage']\n",
      "3 ['of', 'the', 'latter']\n",
      "1 ['are']\n",
      "3 ['provable', 'regret', 'bounds']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 2), (5, 2), (6, 4), (7, 4), (8, 5), (9, 6), (10, 4), (11, 2), (12, 3), (13, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "2 ['In', 'all']\n",
      "1 [',']\n",
      "3 ['the', 'selection', 'policy']\n",
      "20 ['seems', 'to', 'be', 'useful', 'during', 'training', ',', 'since', 'it', 'sometimes', 'yields', 'better', 'solutions', 'than', 'any', 'of', 'the', 'individual', 'reward', 'schemes']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 10), (5, 7), (6, 8), (7, 11), (8, 18), (9, 14), (10, 15), (11, 5), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "14 ['It', 'suggests', 'that', 'some', 'form', 'of', 'curriculum', 'over', 'the', 'rewards', 'is', 'occurring', 'during', 'training']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "37 ['if', 'this', 'is', 'really', 'what', 'is', 'going', 'on', ',', 'then', 'it', \"'s\", 'possible', 'that', 'the', 'relevant', 'literature', 'about', 'curriculum', 'learning', 'may', 'offer', 'more', 'stable', 'and', 'principled', 'solutions', 'than', 'an', 'actor', 'critic', ',', 'for', 'example', 'population', 'based', 'training']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 2), (6, 4), (7, 2), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['could']\n",
      "1 ['potentially']\n",
      "7 ['solve', 'the', 'issues', 'observed', 'in', 'task', '2']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 13), (4, 19), (5, 12), (6, 11), (7, 6), (8, 8), (9, 7), (10, 8), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "3 ['[', '1', ']']\n",
      "13 ['Relational', 'Deep', 'Reinforcement', 'Learning', ',', 'Zambaldi', 'et', 'al', ',', 'https://arxiv.org/abs/1806.01830', '[', '2', ']']\n",
      "23 ['A', 'Structured', 'Prediction', 'Approach', 'for', 'Generalization', 'in', 'Cooperative', 'Multi-Agent', 'Reinforcement', 'Learning', ',', 'Carion', 'et', 'al', ',', 'https://arxiv.org/abs/1910.08809', '[', '3', ']', 'Curiosity', '-', 'driven']\n",
      "13 ['Exploration', 'by', 'Self', '-', 'supervised', 'Prediction', ',', 'Pathak', 'et', 'al', ',', 'ICML', '2017']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 4), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['Overall']\n",
      "1 ['I']\n",
      "6 ['like', 'the', 'approach', 'in', 'the', 'paper']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 3), (6, 2), (7, 2), (8, 3), (9, 2), (10, 4), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['proposes']\n",
      "5 ['a', 'nice', '2', 'pronged', 'method']\n",
      "9 ['for', 'exploiting', 'exploration', 'via', 'intrinsic', 'rewards', 'for', 'multi-agent', 'systems']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 1), (5, 3), (6, 4), (7, 5), (8, 7), (9, 4), (10, 2), (11, 3), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['Are']\n",
      "2 ['the', 'ones']\n",
      "16 ['proposed', 'motivated', 'by', 'the', 'tasks', 'in', 'the', 'paper', 'or', 'are', 'they', 'sufficient', 'for', 'tasks', 'in', 'general']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 7), (6, 4), (7, 2), (8, 2), (9, 3), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['Last']\n",
      "6 ['using', 'a', 'more', 'recent', 'novelty', 'metric']\n",
      "1 ['could']\n",
      "11 ['allow', 'the', 'method', 'to', 'work', 'on', 'more', 'interesting', '/', 'complex', 'tasks']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 8), (5, 4), (6, 1), (7, 2), (8, 3), (9, 5), (10, 2), (11, 5), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "4 ['More', 'detailed', 'feedback', ':']\n",
      "2 ['-', 'It']\n",
      "1 ['would']\n",
      "14 ['be', 'good', 'to', 'include', 'more', 'learning', 'curves', 'in', 'the', 'main', 'text', 'for', 'the', 'paper']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 4), (6, 5), (7, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  5\n",
      "1 ['-']\n",
      "1 ['It']\n",
      "2 ['all', 'agents']\n",
      "5 ['share', 'the', 'same', 'replay', 'buffer']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 4), (5, 4), (6, 2), (7, 2), (8, 4), (9, 3), (10, 4), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "13 ['Does', 'this', 'also', 'imply', 'that', 'every', 'agent', 'is', 'performing', 'the', 'same', 'task', 'there']\n",
      "1 ['are']\n",
      "3 ['just', 'many', 'agents']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 4), (6, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['does']\n",
      "1 ['not']\n",
      "8 ['make', 'the', 'problem', 'very', 'multi-agent', 'with', 'different', 'goals']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 4), (5, 2), (6, 3), (7, 4), (8, 5), (9, 4), (10, 2), (11, 4), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['Would']\n",
      "1 ['it']\n",
      "17 ['affect', 'the', 'algorithm', 'significantly', 'to', 'work', 'on', 'an', 'environment', 'where', 'the', 'agents', 'have', 'various', 'types', 'of', 'goals']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 3), (5, 3), (6, 5), (7, 5), (8, 4), (9, 8), (10, 3), (11, 3), (12, 2), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "8 ['-', 'As', 'is', 'noted', 'in', 'the', 'text', ',']\n",
      "2 ['this', 'method']\n",
      "14 ['appears', 'to', 'work', 'well', 'in', 'the', 'centralized', 'training', 'scheme', 'that', 'many', 'have', 'adopted', 'recently']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 2), (5, 3), (6, 2), (7, 2), (8, 3), (9, 3), (10, 1), (11, 2), (12, 3), (13, 5), (14, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "1 ['It']\n",
      "18 ['makes', 'me', 'wonder', 'if', 'there', 'is', 'a', 'way', 'to', 'employ', 'these', 'exploration', 'schemes', 'in', 'a', 'non-centralized', 'training', 'form']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 4), (6, 7), (7, 10), (8, 8), (9, 6), (10, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['ability']\n",
      "14 ['to', 'ask', 'other', 'agents', 'in', 'the', 'world', 'about', 'there', 'preferences', 'and', 'novelty', 'of', 'states']\n",
      "1 ['appears']\n",
      "12 ['to', 'be', 'a', 'strong', 'assumption', ',', 'especially', 'in', 'a', 'multi-agent', 'robotics', 'problem']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 2), (5, 4), (6, 2), (7, 2), (8, 5), (9, 7), (10, 3), (11, 4), (12, 3), (13, 2), (14, 2), (15, 1), (16, 2), (17, 2), (18, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "23 ['While', 'the', 'authors', 'note', 'that', 'the', 'intrinsic', 'rewards', 'used', 'in', 'this', 'work', 'are', 'not', 'comprehensive', 'it', 'would', 'be', 'good', 'to', 'note', 'how', 'comprehensive']\n",
      "1 ['they']\n",
      "1 ['are']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 2), (6, 2), (7, 3), (8, 3), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Are']\n",
      "1 ['there']\n",
      "8 ['a', 'few', 'that', 'were', 'left', 'out', 'on', 'purpose']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 2), (5, 4), (6, 1), (7, 2), (8, 4), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  5\n",
      "1 ['the']\n",
      "1 ['authours']\n",
      "1 ['believe']\n",
      "4 ['this', 'set', 'is', 'sufficient']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 3), (6, 2), (7, 3), (8, 6), (9, 4), (10, 5), (11, 3), (12, 2), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['statement']\n",
      "1 ['makes']\n",
      "15 ['it', 'seem', 'like', 'the', 'authors', 'just', 'tried', 'a', 'few', 'options', 'and', 'found', 'one', 'that', 'worked']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 1), (7, 2), (8, 2), (9, 2), (10, 2), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['would']\n",
      "8 ['be', 'good', 'to', 'expand', 'on', 'this', 'discussion', 'more']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 5), (6, 3), (7, 3), (8, 2), (9, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "3 ['-', 'More', 'detail']\n",
      "3 ['for', 'Figure', '1']\n",
      "1 ['would']\n",
      "8 ['be', 'helpful', 'to', 'understand', 'the', 'overall', 'network', 'design']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 5), (5, 7), (6, 2), (7, 2), (8, 2), (9, 2), (10, 4), (11, 2), (12, 2), (13, 2), (14, 2), (15, 3), (16, 3), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "6 ['While', 'that', 'figure', 'it', 'helpful', 'maybe']\n",
      "1 ['it']\n",
      "16 ['would', 'be', 'good', 'to', 'include', 'a', 'version', 'that', 'goes', 'into', 'detail', 'for', 'the', '2', 'agent', 'environment']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 9), (4, 5), (5, 1)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['Then']\n",
      "6 ['a', 'more', 'compressed', 'n', 'agent', 'version']\n",
      "4 ['can', 'also', 'be', 'shown']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 5), (6, 2), (7, 2), (8, 2), (9, 4), (10, 2), (11, 4), (12, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "2 ['The', 'paper']\n",
      "15 ['describes', 'a', 'policy', 'selector', 'that', 'is', 'a', 'type', 'of', 'high', '-', 'level', 'policy', 'for', 'HRL']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 2), (6, 2), (7, 2), (8, 4), (9, 5), (10, 6), (11, 6), (12, 6), (13, 2), (14, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['design']\n",
      "1 ['seems']\n",
      "23 ['rather', 'unique', 'in', 'that', 'this', 'part', 'of', 'the', 'policy', 'can', 'optimizing', 'for', 'which', 'intrinsic', 'reward', 'to', 'toggle', 'based', 'on', 'the', 'extrinsic', 'rewards', 'observed']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['like']\n",
      "1 ['it']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 2), (7, 3), (8, 2), (9, 2), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['is']\n",
      "8 ['noted', 'that', 'entropy', 'is', 'important', 'for', 'this', 'design']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 2), (5, 2), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['Can']\n",
      "1 ['this']\n",
      "6 ['be', 'analyzed', 'in', 'an', 'empirical', 'way']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 2), (5, 3), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['Is']\n",
      "1 ['this']\n",
      "6 ['true', 'for', 'most', 'environments', '/', 'tasks']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 4), (5, 2), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "3 ['-', 'Task', '2']\n",
      "1 ['a']\n",
      "1 ['bit']\n",
      "1 ['contrived']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 2), (6, 4), (7, 2), (8, 3), (9, 2), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['Is']\n",
      "1 ['there']\n",
      "11 ['another', 'instance', 'of', 'this', 'type', 'of', 'task', 'elsewhere', 'in', 'another', 'paper']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 4), (5, 2), (6, 2), (7, 2), (8, 3), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "3 ['Before', 'section', '6.1']\n",
      "2 ['the', 'paper']\n",
      "6 ['is', 'discussing', 'rewards', 'the', 'are', 'received']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 2), (7, 2), (8, 4), (9, 2), (10, 3), (11, 4), (12, 2), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  8\n",
      "1 ['more']\n",
      "1 ['explicit']\n",
      "1 ['about']\n",
      "6 ['where', 'these', 'rewards', 'are', 'coming', 'from']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 7), (5, 2), (6, 2), (7, 3), (8, 2), (9, 2), (10, 2), (11, 4), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "12 ['I', 'think', 'it', 'is', 'meant', 'that', 'these', 'rewards', 'are', 'the', 'extrinsic', 'rewards']\n",
      "1 ['but']\n",
      "4 ['it', 'does', 'not', 'say']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 6), (5, 4), (6, 7), (7, 11), (8, 8), (9, 6), (10, 4), (11, 5), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "6 ['As', 'noted', 'just', 'before', 'section', '6.1']\n",
      "22 ['it', 'seems', 'for', 'the', 'collection', 'of', 'tasks', '1', '-', '3', 'it', 'is', 'already', 'obvious', 'what', 'types', 'of', 'intrinsic', 'rewards', 'should', 'be', 'used']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 1), (7, 2), (8, 3), (9, 4), (10, 3), (11, 4), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  9\n",
      "1 ['more']\n",
      "1 ['tasks']\n",
      "1 ['where']\n",
      "5 ['this', 'decision', 'is', 'less', 'obvious']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 3), (7, 3), (8, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['-']\n",
      "8 ['Why', 'are', 'there', '\"', 'black', 'holes', '\"', 'in']\n",
      "1 ['the']\n",
      "1 ['environment']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 6), (5, 6), (6, 4), (7, 5), (8, 2), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "9 ['Also', 'if', 'an', 'agent', 'steps', 'into', 'a', 'black', 'hole']\n",
      "1 ['they']\n",
      "7 ['are', 'crushed', 'never', 'to', 'be', 'seen', 'again']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 5), (5, 7), (6, 3), (7, 6), (8, 4), (9, 3), (10, 2), (11, 7), (12, 4), (13, 3), (14, 4), (15, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "17 ['What', 'you', 'describe', 'sounds', 'more', 'like', 'a', 'wormhole', 'where', 'one', 'end', 'is', 'non-stationary', '...', 'Also', ',', 'can']\n",
      "2 ['the', 'agents']\n",
      "10 ['detect', 'the', 'presence', 'of', 'a', 'black', 'hole', 'in', 'some', 'way']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 4), (6, 6), (7, 2), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  5\n",
      "1 ['-']\n",
      "1 ['It']\n",
      "3 ['the', 'novel', 'metric']\n",
      "3 ['is', 'count', 'based']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 5), (5, 5), (6, 2), (7, 2), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "6 ['While', 'this', 'can', 'work', 'in', 'practice']\n",
      "1 ['it']\n",
      "5 ['seems', 'a', 'rather', 'simple', 'metric']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 3), (4, 2), (5, 3), (6, 4), (7, 2), (8, 5), (9, 2), (10, 2), (11, 2), (12, 2), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Is']\n",
      "1 ['it']\n",
      "1 ['possible']\n",
      "14 ['to', 'use', 'something', 'more', 'like', 'ICM', 'or', 'RND', 'that', 'was', 'referenced', 'in', 'the', 'paper']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['Especially']\n",
      "1 ['for']\n",
      "3 ['the', 'VizDoom', 'environment']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 6), (5, 3), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "4 ['-', 'In', 'table', '2']\n",
      "1 ['where']\n",
      "6 ['are', 'some', 'of', 'the', 'numbers', 'bold']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 1), (7, 2), (8, 3), (9, 4), (10, 2), (11, 4), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  9\n",
      "1 ['this']\n",
      "1 ['information']\n",
      "1 ['in']\n",
      "5 ['the', 'caption', 'for', 'the', 'table']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 2), (6, 2), (7, 4), (8, 6), (9, 2), (10, 3), (11, 2), (12, 6), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "1 ['I']\n",
      "18 ['am', 'not', 'sure', 'if', 'the', 'discussion', 'on', 'the', 'behaviours', 'the', 'intrinsic', 'reward', 'functions', 'result', 'in', 'are', 'very', 'surprising']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 5), (6, 4), (7, 2), (8, 2), (9, 2), (10, 4), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['Maybe']\n",
      "1 ['there']\n",
      "14 ['is', 'a', 'more', 'interesting', 'behaviour', 'that', 'results', 'from', 'the', 'combination', 'of', 'two', 'intrinsic', 'rewards']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 5), (5, 4), (6, 1), (7, 2), (8, 2), (9, 5), (10, 2), (11, 3), (12, 2), (13, 5), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['Summary']\n",
      "1 [':']\n",
      "18 ['The', 'paper', 'proposes', 'a', 'method', 'for', 'coordinating', 'the', 'exploration', 'efforts', 'of', 'agents', 'in', 'a', 'multi-agent', 'reinforcement', 'learning', 'setting']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 7), (4, 13), (5, 10), (6, 7), (7, 11), (8, 3), (9, 3), (10, 5), (11, 5), (12, 2), (13, 2), (14, 2), (15, 4), (16, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "2 ['The', 'approach']\n",
      "5 ['has', 'two', 'main', 'components', ':']\n",
      "14 ['-LRB-', 'i', '-RRB-', 'learning', 'different', 'exploration', 'policies', 'using', 'different', '\"', 'joint', '\"', 'intrinsic', 'rewards']\n",
      "1 [';']\n",
      "1 ['and']\n",
      "25 ['-LRB-', 'ii', '-RRB-', 'learning', 'a', 'higher', '-', 'level', 'policy', 'that', 'selects', 'one', 'of', 'the', 'exploration', 'policies', 'to', 'be', 'executed', 'at', 'the', 'beginning', 'of', 'each', 'episode']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 6), (6, 2), (7, 2), (8, 2), (9, 4), (10, 2), (11, 3), (12, 2), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['Each']\n",
      "1 ['agent']\n",
      "1 ['has']\n",
      "14 ['its', 'own', 'novelty', 'function', 'which', 'quantifies', 'the', 'novelty', 'of', 'observation', 'seen', 'by', 'that', 'agent']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 4), (5, 3), (6, 4), (7, 3), (8, 2), (9, 3), (10, 4), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "3 ['To', 'coordinate', 'exploration']\n",
      "1 [',']\n",
      "3 ['these', 'novelty', 'functions']\n",
      "12 ['are', 'combined', 'using', 'aggregation', 'functions', 'to', 'produce', 'intrinsic', 'reward', 'for', 'the', 'agent']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 4)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['Each']\n",
      "1 ['such']\n",
      "1 ['aggregating']\n",
      "1 ['function']\n",
      "1 ['yields']\n",
      "4 ['a', 'different', 'intrinsic', 'reward']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 1), (5, 2), (6, 5), (7, 2), (8, 5), (9, 8), (10, 5), (11, 6), (12, 5), (13, 7), (14, 2), (15, 3), (16, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['propose']\n",
      "29 ['several', 'such', 'aggregating', 'functions', 'as', 'examples', ',', 'however', 'the', 'method', 'is', 'applicable', 'to', 'other', 'aggregating', 'functions', 'as', 'well', ',', 'as', 'long', 'as', 'they', 'can', 'be', 'computed', 'off', '-', 'policy']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 5), (5, 3), (6, 2), (7, 5), (8, 2), (9, 3), (10, 3), (11, 2), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "2 ['During', 'training']\n",
      "1 [',']\n",
      "4 ['the', 'higher', 'level', 'policy']\n",
      "14 ['selects', 'one', 'of', 'the', 'exploration', 'policies', 'which', 'is', 'then', 'executed', 'for', 'the', 'entire', 'episode']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 2), (6, 3), (7, 5), (8, 6), (9, 7), (10, 8), (11, 5), (12, 11), (13, 16), (14, 7), (15, 9), (16, 8), (17, 8), (18, 3), (19, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['episode']\n",
      "1 ['data']\n",
      "1 ['is']\n",
      "59 ['used', 'in', 'two', 'ways', ':', '-LRB-', 'i', '-RRB-', 'to', 'train', 'the', 'higher', '-', 'level', 'policy', 'using', 'policy', 'gradients', 'for', 'maximizing', 'extrinsic', 'rewards', 'along', 'with', 'an', 'entropy', 'term', ';', 'and', '-LRB-', 'ii', '-RRB-', 'to', 'train', 'each', 'exploration', 'policy', 'using', 'soft', 'actor', '-', 'critic', 'on', 'its', 'own', 'intrinsic', 'reward', 'function', '-LRB-', 'and', 'extrinsic', 'reward', '-RRB-', 'in', 'an', 'off', '-', 'policy', 'manner']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 9), (6, 17), (7, 7), (8, 9)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['Experiments']\n",
      "12 ['done', 'on', 'grid', '-', 'world', 'and', 'VizDoom', 'environment', 'for', 'three', 'different', 'tasks']\n",
      "1 ['demonstrate']\n",
      "20 ['that', ',', 'on', 'most', 'tasks', ',', 'the', 'proposed', 'approach', 'performs', 'at', 'least', 'as', 'well', 'as', 'separately', 'trained', 'individual', 'intrinsic', 'rewards']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 2), (6, 6), (7, 10)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['Further']\n",
      "1 ['ablation']\n",
      "1 ['studies']\n",
      "1 ['confirm']\n",
      "14 ['that', 'both', 'the', 'hierarchical', 'setup', 'and', 'the', '\"', 'joint', '\"', 'intrinsic', 'rewards', 'are', 'useful']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 10), (5, 15), (6, 9), (7, 3), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "4 ['Questions', 'to', 'the', 'Authors']\n",
      "1 [':']\n",
      "8 ['1', '-RRB-', 'The', 'second', 'sentence', 'in', 'section', '5']\n",
      "1 ['is']\n",
      "1 ['not']\n",
      "13 ['clear', '-', '\"', 'Furthermore', ',', 'the', 'type', 'of', 'reward', '...', 'sufficiently', 'complex', '\"']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 7), (4, 10), (5, 9), (6, 5), (7, 9), (8, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "3 ['high', '-', 'level']\n",
      "1 ['policy']\n",
      "10 ['selects', 'an', 'exploration', 'strategy', 'at', 'the', 'beginning', 'of', 'each', 'episode']\n",
      "1 ['and']\n",
      "1 ['then']\n",
      "10 ['sticks', 'to', 'it', 'for', 'the', 'entire', 'duration', 'of', 'the', 'episode']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 6), (5, 9), (6, 7), (7, 8), (8, 2), (9, 2), (10, 3), (11, 4), (12, 5), (13, 2), (14, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['Changing']\n",
      "3 ['the', 'exploration', 'strategy']\n",
      "5 ['over', 'the', 'course', 'of', 'training']\n",
      "1 ['be']\n",
      "3 ['useful', 'in', 'cases']\n",
      "16 ['when', 'agent', 'needs', 'to', 'switch', 'to', 'a', 'different', 'exploration', 'strategy', 'after', 'reaching', 'a', 'particular', 'bottleneck', 'state']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 3), (5, 4), (6, 2), (7, 2), (8, 2), (9, 2), (10, 2), (11, 4), (12, 2), (13, 4), (14, 2), (15, 3), (16, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "1 ['this']\n",
      "18 ['would', 'require', 'the', 'exploration', 'strategy', 'to', 'be', 'changed', 'in', 'the', 'middle', 'of', 'an', 'episode', 'which', 'is', 'not', 'supported']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 4), (5, 3), (6, 5), (7, 2), (8, 3), (9, 5), (10, 4), (11, 5), (12, 4), (13, 2), (14, 4), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "1 ['Could']\n",
      "1 ['you']\n",
      "25 ['give', 'an', 'example', 'where', 'the', 'exploration', 'strategy', 'must', 'be', 'changed', 'over', 'time', 'even', 'if', 'one', 'only', 'selects', 'the', 'strategy', 'at', 'the', 'beginning', 'of', 'each', 'episode']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 3), (5, 5), (6, 2), (7, 5), (8, 2), (9, 4), (10, 4), (11, 4), (12, 1), (13, 3), (14, 4), (15, 7), (16, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "1 ['Also']\n",
      "1 [',']\n",
      "1 ['why']\n",
      "29 ['not', 'select', 'the', 'exploration', 'strategy', 'after', 'every', 'fixed', 'number', 'of', 'time', 'steps', 'within', 'each', 'episode', '-LRB-', 'by', 'making', 'high', '-', 'level', 'policy', 'a', 'function', 'of', 'the', 'current', 'state', '-RRB-']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 6), (5, 9), (6, 11), (7, 9), (8, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "8 ['Analyzing', 'the', 'role', 'of', 'high', '-', 'level', 'policy']\n",
      "1 ['and']\n",
      "16 ['its', 'evolution', 'over', 'time', 'on', 'different', 'tasks', 'would', 'be', 'a', 'very', 'nice', 'addition', 'to', 'the', 'paper']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 3), (6, 2), (7, 3), (8, 2), (9, 4), (10, 2), (11, 3), (12, 4), (13, 1), (14, 2), (15, 2), (16, 4), (17, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  3\n",
      "2 ['Qualitative', 'experiments']\n",
      "18 ['demonstrating', 'that', 'it', 'provides', 'a', 'curriculum', 'which', 'helps', 'the', 'agents', 'in', 'surpassing', 'the', 'performance', 'of', 'individual', 'intrinsic', 'rewards']\n",
      "1 ['would']\n",
      "2 ['be', 'helpful']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 4), (5, 5), (6, 4), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "2 ['3', '-RRB-']\n",
      "8 ['Should', '\\\\', 'Pi', 'in', '-LRB-', '10', '-RRB-', 'also']\n",
      "3 ['depend', 'on', 'i']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 3), (5, 6), (6, 7), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "6 ['Though', 'paper', 'is', 'reasonably', 'well', 'written']\n",
      "1 ['I']\n",
      "6 ['find', 'the', 'contributions', 'are', 'very', 'marginal']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 4), (5, 4), (6, 3), (7, 6), (8, 7), (9, 6), (10, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "19 ['If', 'authors', 'can', 'position', 'the', 'paper', 'well', 'with', 'the', 'existing', 'literature', 'and', 'bring', 'out', 'the', 'impact', 'of', 'the', 'contributions']\n",
      "1 ['it']\n",
      "3 ['will', 'be', 'helpful']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 4), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['Tests']\n",
      "3 ['on', 'another', 'dataset']\n",
      "1 ['would']\n",
      "3 ['have', 'been', 'welcomed']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 3), (6, 5), (7, 10), (8, 5), (9, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['does']\n",
      "1 ['not']\n",
      "19 ['consider', 'the', 'more', 'recent', 'and', 'highly', 'relevant', 'Moosavi', '-', 'Dezfooli', 'et', 'al', '“', 'Universal', 'Adversarial', 'Perturbations', '”', 'CVPR', '2017']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 7), (4, 11), (5, 6), (6, 5), (7, 3), (8, 2), (9, 1), (10, 2), (11, 2), (12, 2), (13, 4), (14, 7), (15, 2), (16, 4), (17, 4), (18, 3), (19, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  3\n",
      "3 ['The', 'distance', 'metrics']\n",
      "3 ['that', 'are', 'considered']\n",
      "1 ['are']\n",
      "1 ['only']\n",
      "3 ['L_inf', 'and', 'L1']\n",
      "1 [',']\n",
      "25 ['whereas', 'it', 'would', 'be', 'interesting', 'to', 'see', 'more', 'relevant', '“', 'perceptual', 'losses', '”', 'such', 'as', 'those', 'used', 'in', 'style', 'transfer', 'and', 'domain', 'adaptation', 'with', 'GANs']\n",
      "[(0, 1), (1, 1), (2, 9), (3, 8), (4, 10), (5, 8), (6, 7), (7, 11), (8, 15), (9, 9), (10, 6), (11, 8), (12, 5), (13, 3), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "2 ['In', 'summary']\n",
      "1 [',']\n",
      "7 ['while', 'I', 'think', 'the', 'paper', 'is', 'interesting']\n",
      "1 [',']\n",
      "13 ['I', 'suspect', 'that', 'the', 'applicability', 'of', 'this', 'technique', 'is', 'possibly', 'limited', 'at', 'present']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "23 ['I', \"'m\", 'unsure', 'how', 'much', 'we', 'can', 'really', 'read', 'into', 'the', 'findings', 'of', 'the', 'paper', 'when', 'the', 'experiments', 'are', 'based', 'on', 'MNIST', 'alone']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 5), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "5 ['Adding', 'these', 'as', 'adversarial', 'examples']\n",
      "1 ['could']\n",
      "1 ['seriously']\n",
      "3 ['degrade', 'the', 'accuracy']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 6), (4, 3), (5, 4), (6, 3), (7, 5), (8, 8), (9, 7), (10, 9), (11, 8), (12, 8), (13, 10), (14, 6), (15, 3), (16, 2), (17, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "1 ['Also']\n",
      "1 [',']\n",
      "3 ['for', 'building', 'robustness']\n",
      "1 [',']\n",
      "1 ['one']\n",
      "42 ['could', 'argue', 'that', 'adding', 'misclassified', 'examples', 'that', 'are', '\"', 'furthest', '\"', '-LRB-', 'ie', 'closest', 'to', 'the', 'true', 'decision', 'boundary', '-RRB-', 'is', 'a', 'much', 'more', 'efficient', 'training', 'approach', ',', 'since', 'a', 'few', 'of', 'these', 'can', 'possibly', 'subsume', 'a', 'large', 'number', 'of', 'close', 'examples']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 4), (6, 7), (7, 3), (8, 4), (9, 7), (10, 7), (11, 5), (12, 8), (13, 1), (14, 2), (15, 2), (16, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['describes']\n",
      "29 ['a', 'method', 'for', 'generating', 'so', 'called', 'ground', 'truth', 'adversarial', 'examples', ':', 'adversaries', 'that', 'have', 'minimal', '-LRB-', 'L1', 'or', 'L_inf', '-RRB-', 'distance', 'to', 'the', 'training', 'example', 'used', 'to', 'generate', 'them']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 5), (6, 4), (7, 2), (8, 2), (9, 2), (10, 1), (11, 2), (12, 2), (13, 2), (14, 4), (15, 2), (16, 5), (17, 2), (18, 2), (19, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['technique']\n",
      "1 ['uses']\n",
      "21 ['the', 'recently', 'developed', 'reluplex', ',', 'which', 'can', 'be', 'used', 'to', 'verify', 'certian', 'properties', 'of', 'deep', 'neural', 'networks', 'that', 'use', 'ReLU', 'activations']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 5), (5, 3), (6, 6), (7, 10), (8, 4), (9, 4), (10, 1), (11, 2), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "11 ['show', 'how', 'the', 'L1', 'distance', 'can', 'be', 'formulated', 'using', 'a', 'ReLU']\n",
      "1 ['and']\n",
      "1 ['therefore']\n",
      "8 ['extend', 'the', 'reluplex', 'also', 'work', 'with', 'L1', 'distances']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 3), (6, 7), (7, 4), (8, 4), (9, 8), (10, 3), (11, 2), (12, 4), (13, 2), (14, 2), (15, 3), (16, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  3\n",
      "2 ['The', 'experiments']\n",
      "2 ['on', 'MNIST']\n",
      "1 ['suggest']\n",
      "26 ['that', 'the', 'C&W', 'attack', 'produces', 'close', 'to', 'optimal', 'adversarial', 'examples', ',', 'although', 'it', 'is', 'not', 'clear', 'if', 'these', 'findings', 'would', 'transfer', 'to', 'larger', 'more', 'complex', 'networks']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 2), (6, 5), (7, 8), (8, 8), (9, 4), (10, 2), (11, 3), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "2 ['The', 'evaluation']\n",
      "1 ['also']\n",
      "21 ['suggests', 'that', 'training', 'with', 'iterative', 'adversarial', 'examples', 'does', 'not', 'overfit', 'and', 'does', 'indeed', 'harden', 'the', 'network', 'to', 'attacks', 'in', 'many', 'cases']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 8), (5, 6), (6, 2), (7, 6), (8, 3), (9, 4), (10, 6), (11, 8), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "8 ['In', 'general', ',', 'this', 'is', 'a', 'nice', 'idea']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "23 ['it', 'seems', 'like', 'the', 'inherent', 'computational', 'cost', 'will', 'limit', 'the', 'applicability', 'of', 'this', 'approach', 'to', 'small', 'networks', 'and', 'datasets', 'for', 'the', 'time', 'being']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 2), (5, 3), (6, 3), (7, 2), (8, 6), (9, 8), (10, 6), (11, 8), (12, 13), (13, 7), (14, 3), (15, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "1 ['Incidentally']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "36 ['would', 'have', 'been', 'useful', 'if', 'the', 'authors', 'provided', 'indicative', 'information', 'on', 'the', 'computational', 'cost', '-LRB-', 'eg', ':', 'in', 'the', 'form', 'of', 'time', 'on', 'a', 'standard', 'GPU', '-RRB-', 'for', 'generating', 'these', 'ground', 'truths', 'and', 'carrying', 'out', 'experiments']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 1), (6, 2), (7, 5), (8, 5), (9, 5), (10, 4), (11, 5), (12, 1), (13, 2), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['experiments']\n",
      "1 ['are']\n",
      "18 ['quite', 'small', 'scale', ',', 'which', 'I', 'expect', 'is', 'due', 'to', 'the', 'computational', 'cost', 'of', 'generating', 'the', 'adversarial', 'examples']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 1), (6, 2), (7, 2), (8, 2), (9, 4), (10, 4), (11, 2), (12, 3), (13, 4), (14, 3), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  9\n",
      "1 ['how']\n",
      "1 ['far']\n",
      "2 ['the', 'findings']\n",
      "9 ['can', 'be', 'generalized', 'from', 'MNIST', 'to', 'more', 'realistic', 'situations']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 5), (4, 3), (5, 5), (6, 2), (7, 2), (8, 2), (9, 4), (10, 10), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['Also']\n",
      "1 [',']\n",
      "2 ['while', 'interesting']\n",
      "1 [',']\n",
      "16 ['are', 'adversarial', 'examples', 'that', 'have', 'minimal', 'L_p', 'distance', 'from', 'training', 'examples', 'really', 'that', 'useful', 'in', 'practice']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 9), (5, 5), (6, 4), (7, 5), (8, 4), (9, 6), (10, 2), (11, 5), (12, 1), (13, 3), (14, 4), (15, 7), (16, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "11 ['Of', 'course', ',', 'it', \"'s\", 'nice', 'that', 'we', 'can', 'find', 'these']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "24 ['it', 'could', 'be', 'argued', 'that', 'L_p', 'norms', 'are', 'not', 'a', 'good', 'way', 'of', 'judging', 'the', 'similarity', 'of', 'an', 'adversarial', 'example', 'to', 'a', 'true', 'example']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 3), (7, 4), (8, 5), (9, 3), (10, 5), (11, 5), (12, 5), (13, 6), (14, 8), (15, 6), (16, 8), (17, 7), (18, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  7\n",
      "1 ['be']\n",
      "18 ['more', 'useful', 'to', 'investigate', 'attacks', 'that', 'are', 'perceptually', 'insignificant', ',', 'or', 'attacks', 'that', 'operate', 'in', 'the', 'physical', 'world']\n",
      "1 [',']\n",
      "13 ['as', 'these', 'are', 'more', 'likely', 'to', 'be', 'a', 'concern', 'for', 'real', 'world', 'systems']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 3), (6, 1), (7, 4), (8, 4), (9, 7), (10, 9), (11, 8), (12, 4), (13, 5), (14, 6), (15, 8), (16, 7), (17, 9), (18, 4), (19, 3), (20, 4), (21, 3), (22, 11), (23, 9), (24, 5), (25, 7), (26, 4), (27, 7), (28, 2), (29, 2), (30, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "depth:  2\n",
      "1 ['Summary']\n",
      "1 [':']\n",
      "76 ['The', 'paper', 'proposes', 'a', 'method', 'to', 'compute', 'adversarial', 'examples', 'with', 'minimum', 'distance', 'to', 'the', 'original', 'inputs', ',', 'and', 'to', 'use', 'the', 'method', 'to', 'do', 'two', 'things', ':', 'Show', 'how', 'well', 'heuristic', 'methods', 'do', 'in', 'finding', '\"', 'optimal', '/', 'minimal', '\"', 'adversarial', 'examples', '-LRB-', 'how', 'close', 'the', 'come', 'to', 'the', 'minimal', 'change', 'that', 'flips', 'the', 'label', '-RRB-', 'and', 'to', 'assess', 'how', 'a', 'method', 'that', 'is', 'designed', 'to', 'make', 'the', 'model', 'more', 'robust', 'to', 'adversarial', 'examples', 'actually', 'works']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 3), (5, 3), (6, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['Pros']\n",
      "1 [':']\n",
      "8 ['I', 'like', 'the', 'idea', 'and', 'the', 'proposed', 'applications']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 7), (5, 6), (6, 5), (7, 4), (8, 4), (9, 6), (10, 5), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['is']\n",
      "1 ['certainly']\n",
      "2 ['highly', 'relevant']\n",
      "1 [',']\n",
      "20 ['both', 'in', 'terms', 'of', 'assessing', 'models', 'for', 'critical', 'use', 'cases', 'as', 'well', 'as', 'a', 'tool', 'to', 'better', 'understand', 'the', 'phenomenon']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 2), (6, 5), (7, 2), (8, 4), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['Some']\n",
      "10 ['of', 'the', 'suggested', 'insights', 'in', 'the', 'analysis', 'of', 'defense', 'techniques']\n",
      "1 ['are']\n",
      "1 ['interesting']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 5), (5, 1), (6, 1), (7, 3), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['Cons']\n",
      "1 ['The']\n",
      "3 ['is', 'not', 'much']\n",
      "1 ['technical']\n",
      "1 ['novelty']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 1), (6, 4), (7, 5), (8, 6), (9, 10), (10, 6), (11, 5), (12, 5), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['method']\n",
      "1 ['boils']\n",
      "1 ['down']\n",
      "25 ['to', 'applying', 'Reluplex', '-LRB-', 'Katz', 'et', 'al', '2017b', '-RRB-', 'in', 'a', 'binary', 'search', '-LRB-', 'although', 'I', 'acknowledge', 'the', 'extension', 'to', 'L1', 'as', 'distance', 'metric', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 9), (5, 4), (6, 5), (7, 5), (8, 5), (9, 2), (10, 2), (11, 3), (12, 2), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "3 ['The', 'practical', 'application']\n",
      "3 ['of', 'the', 'method']\n",
      "1 ['is']\n",
      "2 ['very', 'limited']\n",
      "16 ['since', 'the', 'search', 'is', 'very', 'slow', 'and', 'is', 'only', 'feasible', 'at', 'all', 'for', 'relatively', 'small', 'models']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 9), (5, 13), (6, 3), (7, 2), (8, 4), (9, 2), (10, 2), (11, 2), (12, 3), (13, 2), (14, 2), (15, 3), (16, 3), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  3\n",
      "9 ['State', '-', 'of', '-', 'the', '-', 'art', 'practical', 'models']\n",
      "15 ['that', 'achieve', 'accuracy', 'rates', 'that', 'make', 'them', 'interesting', 'for', 'deployment', 'in', 'potentially', 'safety', 'critical', 'applications']\n",
      "1 ['are']\n",
      "3 ['out', 'of', 'reach']\n",
      "3 ['for', 'this', 'analysis']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 8), (5, 6), (6, 6), (7, 6), (8, 4), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "2 ['The', 'network']\n",
      "2 ['analysed', 'here']\n",
      "1 ['does']\n",
      "1 ['not']\n",
      "16 ['reach', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'on', 'MNIST', 'from', 'almost', 'two', 'decades', 'ago']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 1), (5, 2), (6, 2), (7, 2), (8, 2), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['The', 'analysis']\n",
      "1 ['also']\n",
      "7 ['has', 'to', 'be', 'done', 'for', 'each', 'sample']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 2), (5, 1), (6, 2), (7, 2), (8, 2), (9, 4), (10, 3), (11, 4), (12, 2), (13, 2), (14, 2), (15, 4), (16, 4), (17, 2), (18, 3), (19, 2), (20, 4), (21, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['long']\n",
      "1 ['runtime']\n",
      "1 ['does']\n",
      "1 ['not']\n",
      "22 ['permit', 'to', 'analyse', 'large', 'amounts', 'of', 'input', 'samples', ',', 'which', 'makes', 'the', 'analysis', 'in', 'terms', 'of', 'the', 'increase', 'in', 'robustness', 'rather', 'weak']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 2), (6, 2), (7, 2), (8, 5), (9, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['statement']\n",
      "1 ['can']\n",
      "1 ['only']\n",
      "10 ['be', 'made', 'for', 'the', 'very', 'limited', 'set', 'of', 'tested', 'samples']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 2), (6, 3), (7, 2), (8, 1), (9, 2), (10, 2), (11, 2), (12, 4), (13, 2), (14, 2), (15, 2), (16, 4), (17, 4), (18, 3), (19, 4), (20, 3), (21, 4), (22, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['is']\n",
      "1 ['also']\n",
      "1 ['unclear']\n",
      "23 ['whether', 'it', 'is', 'possible', 'to', 'include', 'distance', 'metrics', 'that', 'capture', 'more', 'sophisticated', 'attacks', 'that', 'fool', 'network', 'even', 'under', 'various', 'transformations', 'of', 'the', 'input']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 10), (4, 8), (5, 3), (6, 2), (7, 1), (8, 3), (9, 3), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "2 ['Minor', 'details']\n",
      "1 [':']\n",
      "1 ['*']\n",
      "9 ['I', 'would', 'consider', 'calling', 'them', '“', 'minimal', 'adversarial', 'samples']\n",
      "1 ['”']\n",
      "1 ['instead']\n",
      "1 ['of']\n",
      "1 ['“']\n",
      "3 ['ground', '-', 'truth']\n",
      "1 ['”']\n",
      "2018_Hki-ZlbA- 238\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 3), (5, 5), (6, 3), (7, 4), (8, 2), (9, 3), (10, 2), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['*']\n",
      "2 ['Algorithm', '1']\n",
      "14 ['is', 'essentially', 'only', 'a', 'description', 'of', 'binary', 'search', ',', 'which', 'should', 'not', 'be', 'necessary']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 3), (5, 4), (6, 6), (7, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['*']\n",
      "1 ['What']\n",
      "11 ['is', 'the', 'timeout', 'for', 'the', 'computation', ',', 'mentioned', 'in', 'Section', '4']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 8), (5, 2), (6, 1), (7, 2), (8, 7), (9, 6), (10, 6), (11, 4), (12, 7), (13, 6), (14, 4), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "1 ['*']\n",
      "5 ['Page', '7', ',', 'second', 'paragraph']\n",
      "1 [':']\n",
      "30 ['I', 'would', 'n’t', 'say', 'the', 'observation', 'is', 'in', 'line', 'with', 'Carlini', '&', 'Wagner', ',', 'because', 'they', 'take', 'a', 'random', 'step', ',', 'not', 'necessarily', 'one', 'in', 'the', 'direction', 'of', 'the', 'optimum']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 5), (5, 5), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "8 ['That', '’s', 'also', 'the', 'conclusion', 'two', 'paragraphs', 'below']\n",
      "1 [',']\n",
      "1 ['no']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 4), (5, 4), (6, 4), (7, 5), (8, 6), (9, 3), (10, 2), (11, 5), (12, 1), (13, 2), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['*']\n",
      "1 ['I']\n",
      "23 ['do', 'n’t', 'fully', 'agree', 'with', 'the', 'conclusion', 'that', 'the', 'defense', 'of', 'Madry', 'does', 'not', 'overfit', 'to', 'the', 'specific', 'method', 'of', 'creating', 'adversarial', 'examples']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 4), (6, 4), (7, 5), (8, 3), (9, 2), (10, 1), (11, 2), (12, 2), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['Those']\n",
      "7 ['were', 'not', 'created', 'with', 'the', 'CW', 'attack']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "10 ['are', 'related', 'because', 'CW', 'was', 'used', 'to', 'initialize', 'the', 'search']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 1), (5, 2), (6, 3), (7, 4), (8, 6), (9, 3), (10, 2), (11, 2), (12, 2), (13, 4), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['propose']\n",
      "18 ['to', 'employ', 'provably', 'minimal', '-', 'distance', 'examples', 'as', 'a', 'tool', 'to', 'evaluate', 'the', 'robustness', 'of', 'a', 'trained', 'network']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 2), (7, 5), (8, 5), (9, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  7\n",
      "1 ['a']\n",
      "3 ['small', '-', 'scale']\n",
      "1 ['network']\n",
      "1 ['using']\n",
      "4 ['the', 'MNIST', 'data', 'set']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 4), (5, 6), (6, 7), (7, 10), (8, 6), (9, 10), (10, 18), (11, 4), (12, 6), (13, 3), (14, 2), (15, 2), (16, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "3 ['First', 'of', 'all']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "45 ['find', 'it', 'striking', 'that', 'a', 'trained', 'network', 'with', '97', '%', 'accuracy', '-LRB-', 'as', 'claimed', 'by', 'the', 'authors', '-RRB-', 'seems', 'extremely', 'brittle', '--', 'considering', 'the', 'fact', 'that', 'all', 'the', 'adversarial', 'examples', 'in', 'Figure', '1', 'are', 'hardly', 'borderline', 'examples', 'at', 'all', ',', 'at', 'least', 'to', 'my', 'eyes']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 2), (5, 1), (6, 3), (7, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['This']\n",
      "7 ['does', 'reinforce', 'the', '-LRB-', 'well', '-', 'known']\n",
      "1 ['?']\n",
      "1 ['-RRB-']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['weakness']\n",
      "3 ['of', 'neural', 'networks']\n",
      "1 ['in']\n",
      "1 ['general']\n",
      "2018_Hki-ZlbA- 250\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 10), (5, 2), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['Secondly']\n",
      "1 [',']\n",
      "7 ['the', 'term', '\"', 'ground', 'truth', '\"', 'example']\n",
      "5 ['seems', 'very', 'misleading', 'to', 'me']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 2), (6, 3), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['Perhaps']\n",
      "5 ['\"', 'closest', 'misclassified', 'examples', '\"']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 7), (4, 5), (5, 8), (6, 10), (7, 5), (8, 4), (9, 4), (10, 1), (11, 2), (12, 2), (13, 2), (14, 3), (15, 3), (16, 2), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "1 ['Finally']\n",
      "1 [',']\n",
      "11 ['while', 'the', 'idea', 'of', '\"', 'closest', 'misclassified', 'examples', '\"', 'seems', 'interesting']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "20 ['am', 'not', 'convinced', 'that', 'they', 'are', 'the', 'right', 'way', 'to', 'go', 'when', 'it', 'comes', 'to', 'both', 'building', 'and', 'evaluating', 'robustness']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 8), (5, 6), (6, 5), (7, 5), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "3 ['All', 'such', 'examples']\n",
      "4 ['shown', 'in', 'the', 'paper']\n",
      "1 ['are']\n",
      "1 ['indeed']\n",
      "7 ['within', '-', 'class', 'examples', 'that', 'are', 'misclassified']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 4), (5, 5), (6, 7), (7, 9), (8, 10), (9, 5), (10, 5), (11, 7), (12, 3), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['But']\n",
      "1 ['we']\n",
      "34 ['could', 'equally', 'consider', 'another', 'extreme', ',', 'where', 'the', 'trained', 'network', 'is', '\"', 'over-regularized', '\"', 'in', 'the', 'sense', 'that', 'the', 'closest', 'misclassified', 'examples', 'are', 'indeed', 'from', 'another', 'class', ',', 'and', 'therefore', '\"', 'correctly', '\"', 'misclassified']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 11), (5, 7), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "4 ['Not', 'only', 'synthetic', 'data']\n",
      "2 ['but', 'also']\n",
      "7 ['several', 'popularly', '-', 'used', 'data', 'and', 'models']\n",
      "1 ['are']\n",
      "4 ['being', 'conducted', 'and', 'compared']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 7), (5, 8), (6, 7), (7, 6), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "4 ['To', 'provide', 'better', 'understanding']\n",
      "1 [',']\n",
      "2 ['the', 'paper']\n",
      "16 ['evaluates', 'the', 'performance', 'on', 'synthesized', 'digit', 'sequence', 'data', 'as', 'well', 'as', 'several', 'sentence', '-', 'encoding', 'tasks']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 1)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['is']\n",
      "1 ['not']\n",
      "1 ['standalone']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 4), (6, 5), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "2 ['A', 'section']\n",
      "6 ['on', 'the', 'basics', 'of', 'document', 'analysis']\n",
      "1 ['would']\n",
      "3 ['have', 'been', 'nice']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 6), (5, 8), (6, 2), (7, 2), (8, 2), (9, 2), (10, 4), (11, 2), (12, 4), (13, 4), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['work']\n",
      "1 ['proposes']\n",
      "7 ['Tensor', 'Product', 'Decomposition', 'Networks', '-LRB-', 'TRDN', '-RRB-']\n",
      "15 ['as', 'a', 'way', 'to', 'uncover', 'the', 'representation', 'learned', 'in', 'recurrent', 'neural', 'networks', '-LRB-', 'RNNs', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 6), (5, 10), (6, 4), (7, 4), (8, 6), (9, 7), (10, 14), (11, 8), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "6 ['TRDN', 'trains', 'a', 'Tensor', 'Product', 'Representation']\n",
      "1 [',']\n",
      "26 ['which', 'additively', 'combine', 'tensor', 'products', 'of', 'role', '-LRB-', 'e.g.', ',', 'sequence', 'position', '-RRB-', 'embeddings', 'and', 'filler', '-LRB-', 'e.g.', ',', 'word', '-RRB-', 'embeddings', 'to', 'approximate', 'the', 'encoding']\n",
      "1 ['produced']\n",
      "2 ['by', 'RNNs']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 3), (6, 5), (7, 3), (8, 1), (9, 4), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['TRDN']\n",
      "10 ['as', 'a', 'result', 'shed', 'light', 'into', 'inspecting', 'and', 'interpreting', 'representation']\n",
      "1 ['learned']\n",
      "2 ['through', 'RNNs']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 5), (7, 8), (8, 7), (9, 5), (10, 4), (11, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['suggest']\n",
      "20 ['that', 'the', 'structures', 'captured', 'in', 'RNNs', 'are', 'largely', 'compositional', 'and', 'can', 'be', 'well', 'captured', 'by', 'TPRs', 'without', 'recurrence', 'and', 'nonlinearity']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 7), (5, 4), (6, 4), (7, 1), (8, 2), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['pros']\n",
      "1 [':']\n",
      "12 ['1', '-RRB-', 'The', 'paper', 'is', 'mostly', 'clearly', 'written', 'and', 'easy', 'to', 'follow']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 8), (4, 15), (5, 17), (6, 5), (7, 6), (8, 3), (9, 4), (10, 3), (11, 4), (12, 2), (13, 3), (14, 3), (15, 2), (16, 3), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "8 ['The', 'diagrams', 'shown', 'in', 'Figure', '2', 'are', 'illustrative']\n",
      "1 [';']\n",
      "21 ['2', '-RRB-', 'TRDN', 'offers', 'a', 'headway', 'to', 'look', 'into', 'and', 'interpret', 'the', 'representations', 'learned', 'in', 'RNNs', ',', 'which', 'remained', 'largely', 'incomprehensible']\n",
      "1 [';']\n",
      "14 ['3', '-RRB-', 'The', 'analysis', 'and', 'insight', 'provided', 'in', 'section', '4', 'is', 'interesting', 'and', 'insightful']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 7), (5, 4), (6, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "2 ['In', 'particular']\n",
      "1 [',']\n",
      "6 ['how', 'does', 'the', 'training', 'task', 'influence']\n",
      "5 ['the', 'kinds', 'of', 'structural', 'representation']\n",
      "1 ['learned']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 9), (5, 16), (6, 12), (7, 7), (8, 12), (9, 16), (10, 15), (11, 11), (12, 12), (13, 6), (14, 4), (15, 2), (16, 4), (17, 2), (18, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  2\n",
      "1 ['cons']\n",
      "1 [':']\n",
      "69 ['1', '-RRB-', 'The', 'method', 'relies', 'heavily', 'on', 'these', 'manually', 'crafted', 'role', 'schemes', 'as', 'shown', 'in', 'section', '2.1', ';', 'It', 'is', 'unclear', 'the', 'gap', 'in', 'the', 'approximation', 'of', 'TPRs', 'to', 'the', 'encodings', 'learned', 'in', 'RNNs', 'are', 'due', 'to', 'inaccurate', 'role', 'definition', 'or', 'in', 'fact', 'RNNs', 'learn', 'more', 'complex', 'structural', 'dependencies', 'which', 'TPRs', 'can', 'not', 'capture', ';', '2', '-RRB-', 'The', 'MSE', 'of', 'approximation', 'error', 'shown', 'in', 'Table', '1', 'are', 'not', 'informative']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 1)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['How']\n",
      "1 ['should']\n",
      "2 ['these', 'numbers']\n",
      "2 ['be', 'interpreted']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 2), (5, 2), (6, 1), (7, 3), (8, 4), (9, 3), (10, 3), (11, 3), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  8\n",
      "1 ['by']\n",
      "2 ['the', 'MSE']\n",
      "1 ['from']\n",
      "5 ['training', 'TPDN', 'on', 'random', 'vectors']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 7), (5, 12), (6, 7), (7, 8), (8, 4), (9, 2), (10, 3), (11, 4), (12, 5), (13, 4), (14, 2), (15, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  3\n",
      "9 ['3', '-RRB-', 'The', 'alignment', 'between', 'prediction', 'using', 'RNN', 'representations']\n",
      "1 ['and']\n",
      "6 ['TPDN', 'approximations', 'shown', 'in', 'Table', '2']\n",
      "1 ['are']\n",
      "18 ['far', 'from', 'perfect', ',', 'which', 'would', 'contradict', 'with', 'the', 'claim', 'that', 'RNNs', 'only', 'learn', 'tensor', '-', 'product', 'representation']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 5), (6, 5), (7, 8), (8, 8), (9, 3), (10, 6), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['presents']\n",
      "12 ['an', 'analysis', 'of', 'popularly', '-', 'use', 'RNN', 'model', 'for', 'structure', 'modeling', 'abilities']\n",
      "10 ['by', 'designing', 'Tensor', 'Product', 'Decomposition', 'Networks', 'to', 'approximate', 'the', 'encoder']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 4), (7, 3), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['results']\n",
      "1 ['show']\n",
      "7 ['that', 'the', 'representations', 'exhibit', 'interpretable', 'compositional', 'structure']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 3), (6, 5), (7, 1), (8, 2), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Pros']\n",
      "1 [':']\n",
      "12 ['1', '-RRB-', 'The', 'paper', 'is', 'well', '-', 'written', 'and', 'easy', 'to', 'follow']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 2), (6, 6), (7, 8), (8, 2), (9, 3), (10, 4), (11, 6), (12, 6), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "2 ['The', 'design']\n",
      "20 ['of', 'the', 'TPDN', 'and', 'corresponding', 'settings', '-LRB-', 'including', 'what', 'an', 'filler', 'is', 'and', 'what', 'roles', 'are', 'included', '-RRB-', 'for', 'experiments']\n",
      "1 ['are']\n",
      "1 ['understandable']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 4), (6, 9), (7, 6), (8, 3), (9, 4), (10, 2), (11, 3), (12, 4), (13, 6), (14, 4), (15, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['makes']\n",
      "2 ['good', 'point']\n",
      "25 ['at', 'the', 'end', 'of', 'the', 'paper', '-LRB-', 'section', '4', '-RRB-', 'on', 'how', 'these', 'analysis', 'contribute', 'to', 'further', 'design', 'of', 'RNN', 'models', ',', 'which', 'seems', 'useful']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 2), (5, 1), (6, 2), (7, 2), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "2 ['The', 'experiments']\n",
      "6 ['are', 'extensive', 'to', 'support', 'their', 'claims']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 8), (5, 9), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "5 ['An', 'addition', 'of', 'analogy', 'dataset']\n",
      "1 ['further']\n",
      "9 ['demonstrate', 'the', 'effect', 'of', 'TPDN', 'on', 'modeling', 'structural', 'regularities']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 7), (6, 11), (7, 7), (8, 7), (9, 2), (10, 2), (11, 3), (12, 2), (13, 2), (14, 2), (15, 2), (16, 4), (17, 1), (18, 3), (19, 3), (20, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "depth:  2\n",
      "1 ['Cons']\n",
      "1 [':']\n",
      "34 ['1', '-RRB-', 'More', 'detailed', 'and', 'extensive', 'discussion', 'on', 'the', 'contribution', 'of', 'the', 'paper', 'should', 'be', 'included', 'in', 'the', 'introduction', 'part', 'to', 'help', 'readers', 'understand', 'what', \"'s\", 'the', 'point', 'of', 'investigating', 'TPDN', 'on', 'RNN', 'models']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 2), (5, 1), (6, 3), (7, 3), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "2 ['Some', 'details']\n",
      "7 ['are', 'missing', 'to', 'better', 'understand', 'the', 'construction']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 7), (4, 6), (5, 5), (6, 2), (7, 3), (8, 4), (9, 5), (10, 3), (11, 3), (12, 3), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "2 ['For', 'example']\n",
      "1 [',']\n",
      "5 ['on', 'page', '4', ',', 'Evaluation']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "15 ['is', 'unclear', 'of', 'how', 'TPDN', 'encoder', 'is', 'trained', ',', 'specifically', ',', 'which', 'parameters', 'are', 'updated']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 4), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  5\n",
      "1 ['the']\n",
      "1 ['objective']\n",
      "1 ['for']\n",
      "1 ['training']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 4), (5, 3), (6, 2), (7, 2), (8, 2), (9, 4), (10, 7), (11, 3), (12, 3), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "18 ['It', 'is', 'also', 'unclear', 'of', 'whether', 'the', 'models', 'in', 'Figure', '3', '-LRB-', 'c', '-RRB-', 'use', 'bidirectional', 'or', 'unidirectional']\n",
      "1 ['or']\n",
      "2 ['tree', 'decoder']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 4), (5, 2), (6, 2), (7, 3), (8, 3), (9, 2), (10, 3), (11, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "3 ['In', 'Section', '3']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "12 ['could', 'be', 'better', 'to', 'roughly', 'introduce', 'each', 'of', 'the', 'existing', '4', 'models']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 5), (6, 5), (7, 8), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['How']\n",
      "1 ['do']\n",
      "8 ['TPDN', 'trained', 'for', 'these', '4', 'sentence', 'encoding', 'models']\n",
      "5 ['need', 'to', 'be', 'further', 'illustrated']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 2), (7, 2), (8, 4), (9, 4), (10, 4), (11, 3), (12, 4), (13, 4), (14, 6), (15, 5), (16, 3), (17, 3), (18, 4), (19, 5), (20, 3), (21, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "depth:  3\n",
      "1 ['More']\n",
      "1 ['reasons']\n",
      "1 ['should']\n",
      "31 ['be', 'discussed', 'for', 'the', 'results', 'in', 'Table', '2', '-LRB-', 'why', 'bag', '-', 'of', '-', 'words', 'role', 'seem', 'to', 'be', 'ok', ',', 'why', 'skip', '-', 'thought', 'can', 'not', 'be', 'approximated', 'well', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 2), (6, 2), (7, 2), (8, 3), (9, 6), (10, 6), (11, 2), (12, 3), (13, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "2 ['3', '-RRB-']\n",
      "1 ['It']\n",
      "19 ['could', 'be', 'better', 'to', 'provide', 'the', 'actual', 'performance', '-LRB-', 'accuracy', '-RRB-', 'given', 'by', 'TPDN', 'on', 'the', '4', 'existing', 'tasks']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 4), (5, 1), (6, 3), (7, 4), (8, 2), (9, 4), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "2 ['4', '-RRB-']\n",
      "2 ['Further', 'thoughts']\n",
      "1 [':']\n",
      "1 ['have']\n",
      "10 ['you', 'considered', 'applying', 'these', 'analysis', 'on', 'other', 'models', 'besides', 'RNN']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 7), (5, 4), (6, 4), (7, 3), (8, 8), (9, 9), (10, 8), (11, 11), (12, 10), (13, 5), (14, 3), (15, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "20 ['I', 'would', 'be', 'willing', 'to', 'increase', 'my', 'score', 'if', 'the', 'authors', 'added', 'a', 'comparison', 'to', 'Wei', 'and', 'Ma', '‘', '19']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "23 ['more', 'evidence', 'was', 'provided', 'that', 'the', 'bound', 'is', 'tighter', 'for', 'typical', 'convolutional', 'networks', 'found', 'in', 'practice', '-LRB-', 'please', 'see', 'detailed', 'comments', 'below', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 5), (6, 12), (7, 5), (8, 5), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "2 ['An', 'analysis']\n",
      "7 ['of', 'a', 'few', 'networks', 'used', 'in', 'practice']\n",
      "1 ['would']\n",
      "14 ['make', 'the', 'comparison', 'more', 'meaningful', '-LRB-', 'included', 'the', 'comparison', 'to', 'Wei', 'and', 'Ma', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 6), (6, 6), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['How']\n",
      "1 ['do']\n",
      "6 ['the', 'bounds', 'presented', 'in', 'the', 'paper']\n",
      "6 ['compare', 'to', 'Wei', 'and', 'Ma', 'bounds']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 5), (7, 5), (8, 3), (9, 4), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  6\n",
      "1 ['the']\n",
      "1 ['authors']\n",
      "1 ['added']\n",
      "3 ['a', 'detailed', 'comparison']\n",
      "7 ['to', 'Bartlett', 'et', 'al', '‘', '17', 'bound']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 5), (6, 5), (7, 5), (8, 3), (9, 2), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['describes']\n",
      "14 ['new', 'norm', '-', 'based', 'generalization', 'bounds', 'that', 'were', 'specifically', 'adapted', 'to', 'convolutional', 'neural', 'networks']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 5), (5, 7), (6, 3), (7, 2), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "12 ['Since', 'convolutional', 'neural', 'networks', 'do', 'not', 'explicitly', 'depend', 'on', 'the', 'input', 'dimension']\n",
      "1 [',']\n",
      "2 ['these', 'bounds']\n",
      "4 ['share', 'the', 'same', 'property']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 7), (5, 5), (6, 8), (7, 9), (8, 2), (9, 4), (10, 4), (11, 6), (12, 5), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "10 ['Further', 'additional', 'improvement', 'over', 'Bartlett', 'et', 'al', '‘', '17', 'bound']\n",
      "1 [',']\n",
      "22 ['is', 'that', 'this', 'new', 'bound', 'depends', 'on', 'the', 'sum', 'of', 'the', 'operator', 'norms', 'of', 'the', 'parameter', 'matrices', ',', 'rather', 'than', 'the', 'product']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 4)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['is']\n",
      "1 ['clearly']\n",
      "5 ['written', 'and', 'self', '-', 'contained']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 1), (5, 2), (6, 2), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "3 ['the', 'main', 'result']\n",
      "5 ['seems', 'to', 'be', 'very', 'incremental']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 5)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['experiments']\n",
      "1 ['are']\n",
      "1 ['also']\n",
      "6 ['very', 'limited', 'and', 'not', 'too', 'convincing']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 1), (6, 2), (7, 2), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['Further']\n",
      "1 ['empirical']\n",
      "1 ['evaluation']\n",
      "1 ['is']\n",
      "4 ['needed', 'to', 'demonstrate', 'progress']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 4), (5, 8), (6, 8), (7, 3), (8, 2), (9, 2), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "2 ['Detailed', 'comments']\n",
      "1 [':']\n",
      "19 ['I', 'see', 'Wei', 'and', 'Ma', '‘', '19', 'cited', 'in', 'the', 'beginning', 'only', ',', 'but', 'there', 'is', 'no', 'further', 'comparison']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 3), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['They']\n",
      "1 ['also']\n",
      "5 ['proved', 'bounds', 'with', 'similar', 'dependencies']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 6), (6, 4), (7, 5), (8, 2), (9, 4), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['What']\n",
      "1 ['is']\n",
      "12 ['the', 'dependence', 'of', 'the', 'constant', 'C', 'on', '\\\\', 'eta', 'in', 'the', 'bounds']\n",
      "4 ['presented', 'in', 'Theorem', '2.1']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 6), (5, 7), (6, 12), (7, 14), (8, 7), (9, 5), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['is']\n",
      "1 ['unclear']\n",
      "19 ['what', 'trade', '-', 'off', 'comes', 'with', 'eta', 'and', 'how', 'the', 'empirical', 'risk', 'term', 'is', 'balanced', 'with', 'the', 'complexity', 'term']\n",
      "1 [',']\n",
      "11 ['since', '\\\\', 'eta', 'only', 'appears', 'next', 'to', 'the', 'empirical', 'risk', 'term']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 10), (5, 8), (6, 10), (7, 10), (8, 12), (9, 5), (10, 3), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "17 ['The', 'authors', 'demonstrate', 'via', 'a', 'concrete', 'example', 'that', 'there', 'exists', 'a', 'setting', '-LRB-', 'depending', 'on', 'epsilon', '-RRB-']\n",
      "1 [',']\n",
      "17 ['under', 'which', 'this', 'new', 'bound', '-LRB-', 'up', 'to', 'constants', '-RRB-', 'is', 'tighter', 'than', 'Bartlett', 'et', 'al', 'bound']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 4), (5, 7), (6, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['Three']\n",
      "1 ['things']\n",
      "1 ['remain']\n",
      "4 ['unclear', 'to', 'me', ':']\n",
      "1 ['-']\n",
      "5 ['How', 'do', 'the', 'constants', 'differ']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 6), (5, 3), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['Is']\n",
      "6 ['the', 'bound', 'presented', 'in', 'the', 'paper']\n",
      "1 ['tighter']\n",
      "3 ['in', 'absolute', 'terms']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 2), (5, 4), (6, 5), (7, 4), (8, 6), (9, 4), (10, 3), (11, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  5\n",
      "1 ['the']\n",
      "2 ['bound', 'tighter']\n",
      "1 ['when']\n",
      "13 ['the', 'norms', 'in', 'the', 'bounded', 'are', 'measured', 'on', 'typical', 'trained', 'neural', 'network', 'weights']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 4), (6, 4), (7, 2), (8, 3), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "1 ['Is']\n",
      "1 ['the']\n",
      "13 ['bound', 'not', 'worse', 'than', 'a', 'VC', 'bound', 'in', 'any', '-LRB-', 'reasonable', '-RRB-', 'setting']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 4), (5, 4), (6, 8), (7, 2), (8, 3), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['If', 'not']\n",
      "1 [',']\n",
      "12 ['is', 'the', 'bound', 'tighter', 'under', 'typical', 'settings', 'when', 'training', 'standard', 'vision', 'networks']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4)]\n",
      "[0, 1, 2]\n",
      "depth:  2\n",
      "1 ['Other']\n",
      "1 ['minor']\n",
      "1 ['comments']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 9), (5, 12), (6, 10), (7, 4), (8, 4), (9, 6), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "7 ['In', 'the', 'introduction', ',', 'the', 'authors', ':']\n",
      "1 ['-']\n",
      "23 ['say', 'that', 'their', 'bounds', 'are', 'size', '-', 'free', ',', 'which', 'refers', 'to', 'the', 'bounds', 'not', 'having', 'an', 'explicit', 'dependence', 'on', 'the', 'input', 'size']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 9), (5, 4), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "3 ['In', 'my', 'opinion']\n",
      "1 [',']\n",
      "1 ['this']\n",
      "11 ['comes', 'almost', '“', 'for', 'free', '”', 'when', 'using', 'convolutional', 'neural', 'network']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 2), (5, 2), (6, 6), (7, 7), (8, 8), (9, 2), (10, 2), (11, 2), (12, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['Also']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "20 ['think', 'that', 'size', '-', 'free', 'in', 'the', 'title', 'is', 'misleading', ',', 'and', 'should', 'be', 'replaced', 'with', 'input', 'size', '-', 'free']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 8), (5, 5), (6, 4), (7, 4), (8, 2), (9, 4), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "13 ['-', 'mention', 'that', 'most', 'recent', 'bounds', 'depend', 'on', 'the', 'distance', 'from', 'the', 'initialization']\n",
      "2 ['instead', 'of']\n",
      "5 ['the', 'size', 'of', 'the', 'weights']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 2), (6, 3), (7, 5), (8, 6), (9, 3), (10, 2), (11, 1), (12, 2), (13, 2), (14, 2), (15, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['idea']\n",
      "1 ['was']\n",
      "1 ['first']\n",
      "16 ['presented', 'in', 'Dziugate', 'and', 'Roy', '‘', '17', ',', 'which', 'does', 'not', 'seem', 'to', 'be', 'cited', 'there']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 5), (4, 4), (5, 4), (6, 4), (7, 4), (8, 5), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['***']\n",
      "1 ['UPDATE']\n",
      "1 ['***']\n",
      "1 ['I']\n",
      "14 [\"'ve\", 'reread', 'the', 'rebuttals', 'and', 'feel', 'that', 'most', 'of', 'my', 'concerns', 'have', 'been', 'addressed']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['increased']\n",
      "2 ['my', 'score']\n",
      "2 ['to', 'weak']\n",
      "1 ['accept']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 7), (5, 8), (6, 6), (7, 4), (8, 5), (9, 3), (10, 2), (11, 5), (12, 6), (13, 5), (14, 4), (15, 2), (16, 2), (17, 2), (18, 4), (19, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['considers']\n",
      "36 ['the', 'generalization', 'bound', 'for', 'deep', 'neural', 'networks', ',', 'specifically', ',', 'convolutional', 'neural', 'networks', ',', 'which', 'is', 'one', 'of', 'the', 'popular', 'and', 'crucial', 'topics', 'in', 'the', 'machine', 'learning', 'community', ',', 'which', 'has', 'gathered', 'a', 'lot', 'of', 'attention']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 7), (6, 6), (7, 6), (8, 8), (9, 8), (10, 17), (11, 9)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['presents']\n",
      "36 ['a', 'generalization', 'bound', 'based', 'on', 'the', 'number', 'of', 'parameters', ',', 'the', 'Lipschitz', 'constant', 'of', 'the', 'loss', 'function', 'and', 'the', 'distance', 'of', 'the', 'final', 'weights', 'from', 'the', 'initialization', ',', 'without', 'dependence', 'on', 'the', 'dimension', 'of', 'the', 'input']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 2), (6, 3), (7, 2), (8, 2), (9, 4), (10, 3), (11, 6), (12, 2), (13, 4), (14, 2), (15, 4), (16, 3), (17, 4), (18, 2), (19, 2), (20, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['bound']\n",
      "1 ['improves']\n",
      "26 ['upon', 'previous', 'bound', 'in', 'some', 'regimes', 'when', 'the', 'size', 'convolutional', 'kernel', 'is', 'much', 'less', 'than', 'the', 'width', 'of', 'the', 'network', ',', 'which', 'is', 'a', 'reasonable', 'assumption']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 4), (6, 2), (7, 3), (8, 4), (9, 4), (10, 7), (11, 2), (12, 2), (13, 2), (14, 2), (15, 2), (16, 4), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "2 ['The', 'paper']\n",
      "1 ['also']\n",
      "22 ['gives', 'another', 'bound', 'which', 'works', 'for', 'fully', 'connected', 'layers', 'with', 'an', 'additional', 'term', 'that', 'is', 'linear', 'with', 'the', 'depth', 'of', 'the', 'network']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 9), (5, 9), (6, 4), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "6 ['The', 'paper', 'has', 'some', 'nice', 'ideas']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "10 ['the', 'contribution', 'of', 'the', 'paper', 'is', 'not', 'clear', 'for', 'me']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 5), (6, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['main']\n",
      "1 ['theorems']\n",
      "1 ['are']\n",
      "8 ['based', 'on', 'previous', 'results', '-LRB-', 'Lemma', '2.3', '-RRB-']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 8), (5, 3), (6, 2), (7, 4), (8, 1), (9, 2), (10, 2), (11, 2), (12, 2), (13, 2), (14, 4), (15, 2), (16, 4), (17, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "1 ['And']\n",
      "6 ['the', 'remaining', 'work', 'of', 'the', 'paper']\n",
      "17 ['is', 'mainly', 'deriving', 'the', 'Lipchitz', 'bound', 'to', 'be', 'used', 'in', 'the', 'theorem', 'for', 'various', 'kinds', 'of', 'networks']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 3), (7, 3), (8, 3), (9, 2), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['think']\n",
      "8 ['this', 'should', 'be', 'clearly', 'stated', 'in', 'the', 'paper']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 2)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['experiment']\n",
      "1 ['part']\n",
      "1 ['is']\n",
      "1 ['not']\n",
      "2 ['quite', 'convincing']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 5), (6, 6), (7, 2), (8, 2), (9, 4), (10, 2), (11, 3), (12, 3), (13, 4), (14, 2), (15, 2), (16, 2), (17, 2), (18, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['is']\n",
      "1 ['not']\n",
      "4 ['clear', 'from', 'the', 'figures']\n",
      "19 ['that', 'the', 'norm', 'decreases', 'with', 'the', 'number', 'of', 'parameters', 'in', 'the', 'network', ',', 'which', 'is', 'claimed', 'in', 'the', 'paper']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 6), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "5 ['The', 'writing', 'of', 'the', 'paper']\n",
      "1 ['also']\n",
      "3 ['can', 'be', 'improved']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 7), (4, 5), (5, 5), (6, 3), (7, 2), (8, 2), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "6 ['presents', 'math', ',', 'which', 'is', 'nice']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "3 ['without', 'much', 'intuition']\n",
      "1 ['explained']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 2), (6, 4), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['Overall']\n",
      "1 ['I']\n",
      "7 ['would', 'not', 'recommend', 'this', 'paper', 'for', 'admission']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 7), (5, 4), (6, 10), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['Summary']\n",
      "2 ['This', 'paper']\n",
      "6 ['studied', 'the', 'generalization', 'power', 'of', 'CNNs']\n",
      "1 ['and']\n",
      "7 ['showed', 'several', 'upper', 'bounds', 'of', 'generalization', 'errors']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['Their']\n",
      "1 ['results']\n",
      "1 ['have']\n",
      "2 ['two', 'characteristics']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 2), (5, 3), (6, 5), (7, 6), (8, 7), (9, 5), (10, 2), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['First']\n",
      "1 [',']\n",
      "2 ['the', 'bounds']\n",
      "18 ['are', 'in', 'terms', 'of', 'the', 'quantity', 'that', 'is', 'independent', 'of', 'the', 'input', 'dimension', '-LRB-', 'size', '-', 'free', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 4), (5, 3), (6, 4), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['Second']\n",
      "1 [',']\n",
      "3 ['the', 'upper', 'bounds']\n",
      "8 ['involve', 'the', 'distance', 'between', 'initial', 'and', 'learned', 'parameters']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 14), (5, 19), (6, 8), (7, 5), (8, 6), (9, 6), (10, 9), (11, 11), (12, 6), (13, 4), (14, 3), (15, 6), (16, 12), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "39 ['These', 'results', 'improved', 'the', 'upper', 'bounds', 'that', 'we', 'can', 'derive', 'by', 'naively', 'applying', 'the', 'results', 'of', 'Bartlett', 'et', 'al', '-LRB-', '2017', '-RRB-', 'or', 'Neushubar', 'et', 'al', '-LRB-', '2017', '-RRB-', ',', 'because', 'the', 'dominant', 'term', 'of', 'the', 'existing', 'upper', 'bounds']\n",
      "9 ['contained', '$', 'l', '_', '{', '2', ',', '1', '}']\n",
      "18 ['$', 'or', '$', 'l_2', '$', 'norms', ',', 'which', 'could', 'depend', 'on', 'the', 'input', 'dimensions', 'in', 'the', 'worst', 'case']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 2), (6, 4), (7, 5), (8, 9), (9, 7), (10, 13), (11, 6), (12, 6), (13, 4), (14, 4), (15, 3), (16, 5), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "2 ['The', 'authors']\n",
      "1 ['empirically']\n",
      "40 ['showed', 'that', 'there', 'is', 'a', 'correlation', 'between', 'the', 'generalization', 'error', 'of', 'learned', 'CNNs', 'and', 'the', 'dominant', 'term', 'of', 'the', 'upper', 'bound', '-LRB-', 'i.e.', ',', 'the', 'product', 'of', 'the', 'parameter', 'size', 'and', 'the', 'distance', 'from', 'the', 'set', 'of', 'initial', 'parameters', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 5), (5, 9), (6, 4), (7, 2), (8, 2), (9, 5), (10, 5), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "7 ['Decision', 'To', 'the', 'best', 'of', 'my', 'knowledge']\n",
      "1 [',']\n",
      "1 ['this']\n",
      "15 ['is', 'the', 'first', 'work', 'that', 'proved', 'the', 'size', '-', 'free', 'generalization', 'bound', 'for', 'multi-layer', 'CNNs']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 3), (5, 4), (6, 10), (7, 12), (8, 8)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "22 ['think', 'the', 'assumption', 'on', 'the', 'hypothesis', 'class', 'is', 'very', 'restrictive', 'and', 'significantly', 'eases', 'the', 'problem', ',', 'as', 'I', 'discuss', 'in', 'detail', 'later']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 1), (5, 2), (6, 5), (7, 9), (8, 4), (9, 2), (10, 3), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['Therefore']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "16 ['judge', 'the', 'technical', 'contribution', 'of', 'the', 'paper', 'is', 'moderate', 'and', 'recommend', 'to', 'reject', 'the', 'paper', 'weakly']\n",
      "1 ['.']\n",
      "2020_r1e_FpNFDr 355\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 2), (5, 2), (6, 5), (7, 5), (8, 2), (9, 5), (10, 2), (11, 2), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['Therefore']\n",
      "1 [',']\n",
      "1 ['we']\n",
      "18 ['can', 'derive', 'the', 'size', '-', 'free', 'generalization', 'bound', 'if', '$', 'B$', 'does', 'not', 'depend', 'on', 'the', 'input', 'dimension']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 5), (5, 6), (6, 10), (7, 8), (8, 6), (9, 12), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "18 ['Since', 'the', 'hypothesis', 'class', '$', 'F', '_', '\\\\', 'beta', '$', 'is', 'defined', 'via', 'the', 'spectral', 'norm', 'of', 'CNNs']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "14 ['is', 'not', 'surprising', 'that', 'we', 'can', 'derive', 'the', 'size', '-', 'freeness', 'of', '$', 'B$']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 7), (5, 9), (6, 8), (7, 5), (8, 6), (9, 3), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "5 ['The', 'size', '-', 'free', 'generalization']\n",
      "1 ['bound']\n",
      "1 ['has']\n",
      "20 ['been', 'already', 'proven', 'by', 'Du', 'et', 'al', ',', '-LRB-', '2017', '-RRB-', ',', 'although', 'it', 'was', 'the', 'two', '-', 'layered', 'case']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 4)]\n",
      "[0, 1, 2, 3, 4]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['a']\n",
      "1 ['restricted']\n",
      "1 ['eigenvalue']\n",
      "1 ['assumption']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 3), (7, 2), (8, 2), (9, 4), (10, 4), (11, 4), (12, 4), (13, 2), (14, 2), (15, 6), (16, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  9\n",
      "1 ['we']\n",
      "1 ['need']\n",
      "3 ['more', 'sophisticated', 'analysis']\n",
      "13 ['if', 'we', 'do', 'not', 'assume', 'the', 'size', '-', 'freeness', 'of', 'the', 'hypothesis', 'class']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 2), (6, 2), (7, 5), (8, 5), (9, 6), (10, 7), (11, 8), (12, 8), (13, 10), (14, 4), (15, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "1 ['Comments']\n",
      "1 ['-']\n",
      "39 ['The', 'authors', 'claimed', 'that', 'Figure', '3', 'is', 'consistent', 'with', 'theorems', 'because', ',', 'according', 'to', 'the', 'upper', 'bound', 'of', 'theorems', ',', 'the', 'distance', 'from', 'the', 'initialization', 'point', 'decreases', 'when', 'the', 'generalization', 'error', 'is', 'the', 'same', 'and', 'the', 'parameter', 'size', 'increases']\n",
      "1 ['.']\n",
      "2020_r1e_FpNFDr 362\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 9), (5, 10), (6, 10), (7, 7), (8, 18), (9, 8), (10, 6), (11, 4), (12, 3), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Furthermore']\n",
      "1 [',']\n",
      "27 ['the', 'value', 'of', '$', '\\\\', '|', 'K', '-', 'K_0', '\\\\', '|', '_', '\\\\', 'sigma', '$', 'for', '$', 'W', '\\\\', 'approx', '5', '\\\\', 'times', '10', '^', '5', '$']\n",
      "19 ['is', 'approximately', 'the', 'same', 'as', 'the', 'value', 'for', '$', 'W', '\\\\', 'approx', '3', '\\\\', 'times', '10', '^', '6', '$']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 2), (4, 2), (5, 2), (6, 2), (7, 5), (8, 2), (9, 3), (10, 4), (11, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['Suggestions']\n",
      "1 ['-']\n",
      "15 ['Please', 'add', 'the', 'conclusion', 'section', 'which', 'summarizes', 'the', 'paper', 'and', 'discusses', 'the', 'possible', 'research', 'directions']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 13), (5, 9), (6, 11), (7, 10), (8, 12), (9, 8), (10, 3), (11, 2), (12, 3), (13, 3), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "12 ['Minor', 'Comments', '-', 'page', '1', ',', 'section', '1', ',', 'paragraph', '1', '-']\n",
      "1 ['...']\n",
      "34 ['with', 'roots', 'in', '-LRB-', 'Bartett', ',', '1998', '-RRB-', ',', 'is', 'that', '...', '→', 'Use', '\\\\', 'citet', '-', 'page', '2', ',', 'section', '2.1', ',', 'paragraph', '2', '-', 'Write', 'the', 'definition', 'of', '\"', 'expansive', '\"', 'activations']\n",
      "1 ['.']\n",
      "2020_r1e_FpNFDr 366\n",
      "[(0, 1), (1, 1), (2, 7), (3, 10), (4, 5), (5, 3), (6, 5), (7, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "3 ['-', 'page', '3']\n",
      "1 [',']\n",
      "2 ['section', '2.2']\n",
      "1 [',']\n",
      "9 ['definition', '2.2', '-', '$', 'N$', '→', '$', '\\\\', 'mathbb']\n",
      "3 ['{', 'N', '}']\n",
      "1 ['$']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 3), (6, 4), (7, 3), (8, 4), (9, 6), (10, 6), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['algorithm']\n",
      "1 ['is']\n",
      "20 ['evaluated', 'and', 'compared', 'to', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'on', 'various', 'image', 'classification', 'tasks', 'and', 'on', 'RNN']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 3), (5, 6), (6, 6), (7, 5), (8, 4), (9, 5), (10, 2), (11, 4), (12, 2), (13, 6), (14, 9), (15, 3), (16, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "3 ['the', 'search', 'space']\n",
      "32 ['is', 'different', 'between', 'CoNAS', 'and', 'the', 'others', 'methods', 'for', 'some', 'experiments', ',', 'making', 'it', 'difficult', 'to', 'decide', 'if', 'the', 'search', 'strategy', 'of', 'CoNAS', 'is', 'definitely', 'competitive', 'compared', 'to', 'other', 'methods', 'or', 'not']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 1), (5, 2), (6, 2), (7, 2), (8, 5), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Do']\n",
      "1 ['you']\n",
      "9 ['expect', 'to', 'reduce', 'the', 'needed', 'number', 'of', 'sampled', 'architectures']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 5), (5, 1)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "2 ['Minor', 'concerns']\n",
      "1 [':']\n",
      "5 ['Equation', '3.1', 'is', 'not', 'clear']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 7), (5, 6), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "6 ['$', 'X_S', '-LRB-', '\\\\', 'alpha_l', '-RRB-']\n",
      "1 ['$']\n",
      "1 ['does']\n",
      "1 ['not']\n",
      "3 ['depend', 'on', 'k']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 5), (5, 4), (6, 5), (7, 6), (8, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['Contributions']\n",
      "1 [':']\n",
      "17 ['This', 'paper', 'tackles', 'the', 'problem', 'of', 'One', '-', 'shot', 'Neural', 'architecture', 'search', 'by', 'proposing', 'a', 'new', 'method']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 2), (6, 5), (7, 2), (8, 5), (9, 2), (10, 2), (11, 3), (12, 4), (13, 6), (14, 4), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['method']\n",
      "1 ['consists']\n",
      "1 ['mainly']\n",
      "22 ['of', 'new', 'search', 'strategy', 'of', 'the', 'optimal', 'architecture', 'that', 'is', 'inspired', 'by', 'the', 'recovery', 'of', 'boolean', 'functions', 'from', 'their', 'sparse', 'Fourier', 'expansions']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 3), (5, 4), (6, 2), (7, 4), (8, 2), (9, 4), (10, 2), (11, 2), (12, 1), (13, 2), (14, 2), (15, 3), (16, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "2 ['As', 'such']\n",
      "1 [',']\n",
      "2 ['this', 'work']\n",
      "19 ['is', 'an', 'application', 'of', 'recent', 'progress', 'in', 'the', 'field', 'of', 'compressive', 'sensing', 'to', 'One', '-', 'shot', 'neural', 'architecture', 'search']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 10), (4, 8), (5, 7), (6, 9), (7, 5), (8, 5), (9, 6), (10, 7), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "4 ['Given', 'the', 'problem', 'formalism']\n",
      "14 [',', 'the', 'authors', 'have', 'also', 'provides', 'guarantee', 'for', 'the', 'optimality', 'of', 'their', 'method', ',']\n",
      "3 ['i.e', 'the', 'method']\n",
      "14 ['can', 'recover', 'the', 'optimal', 'sub-network', 'of', 'any', 'given', 'a', 'sufficient', 'number', 'of', 'performance', 'measurements']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 11), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "8 ['Clarity', 'Overall', ',', 'the', 'paper', 'is', 'well', 'motivated']\n",
      "1 ['and']\n",
      "5 ['the', 'technical', 'content', 'is', 'good']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 4), (7, 2), (8, 3), (9, 1), (10, 2), (11, 2), (12, 3), (13, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  6\n",
      "1 ['the']\n",
      "1 ['structure']\n",
      "1 ['could']\n",
      "11 ['be', 'enormously', 'improved', 'to', 'ease', 'the', 'reading', 'and', 'the', 'overall', 'understanding']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 5), (4, 10), (5, 15), (6, 19), (7, 13), (8, 3), (9, 4), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "8 ['For', 'example', ':', 'better', 'caption', 'for', 'Figure', '2']\n",
      "4 ['explaining', 'what', 'is', 'shown']\n",
      "1 [';']\n",
      "18 ['presenting', 'the', 'pseudo-code', 'directly', 'in', 'the', 'method', 'overview', 'and', 'spending', 'the', 'rest', 'of', 'the', 'section', 'explaining', 'the', 'method']\n",
      "1 [';']\n",
      "9 ['showing', 'the', 'related', 'work', 'before', 'the', 'experiments', ';', 'etc.']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 9), (6, 5), (7, 5), (8, 1), (9, 3), (10, 2), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['Novelty']\n",
      "6 ['The', 'main', 'novelty', 'in', 'my', 'opinion']\n",
      "1 ['is']\n",
      "11 ['the', 'application', 'of', 'compressive', 'sensing', 'methods', 'to', 'One', '-', 'shot', 'NAS']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 5), (6, 6), (7, 4), (8, 5), (9, 2), (10, 3), (11, 2), (12, 5), (13, 1), (14, 2), (15, 2), (16, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['approach']\n",
      "1 ['is']\n",
      "9 ['significantly', 'different', 'from', 'other', 'One', '-', 'shot', 'NAS', 'method']\n",
      "16 ['that', 'I', 'am', 'aware', 'of', 'mainly', 'regarding', 'the', 'search', 'strategy', 'employed', 'to', 'find', 'the', 'best', 'architecture']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 4), (5, 5), (6, 9), (7, 9), (8, 6), (9, 3), (10, 4), (11, 6), (12, 8), (13, 9), (14, 2), (15, 4), (16, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "2 ['this', 'work']\n",
      "42 ['seems', 'like', 'an', 'incremental', 'improvement', 'over', 'Hazan', 'et', 'al', '2018', '-RRB-', 'To', 'this', 'regard', ',', 'the', 'only', 'novelty', 'that', 'was', 'the', 'framing', 'of', 'One', '-', 'shot', 'NAS', 'as', 'a', 'recovery', 'of', 'boolean', 'functions', 'from', 'their', 'sparse', 'Fourier', 'expansions', 'is', 'not', 'new', 'either']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 5), (5, 2), (6, 2), (7, 2), (8, 3), (9, 3), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "3 ['That', 'is', 'said']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "9 ['am', 'open', 'to', 'be', 'proven', 'wrong', 'on', 'this', 'point']\n",
      "1 ['!']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 9), (5, 12), (6, 8), (7, 3), (8, 2), (9, 3), (10, 2), (11, 2), (12, 2), (13, 2), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "9 ['Results', 'The', 'experiment', 'section', 'is', 'not', 'self', '-', 'content']\n",
      "1 [',']\n",
      "2 ['the', 'readers']\n",
      "19 ['is', 'refered', 'a', 'couple', 'of', 'times', 'to', 'other', 'papers', 'to', 'get', 'details', 'that', 'are', 'critical', 'to', 'reproducibility', 'and', 'understanding']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 7), (5, 7)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['Overall']\n",
      "1 [',']\n",
      "5 ['the', 'search', 'strategy', 'of', 'CoNAS']\n",
      "7 ['seems', 'parameter', 'efficient', ',', 'fast', 'and', 'competitive']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 8), (5, 7), (6, 4), (7, 2), (8, 5), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Also']\n",
      "1 [',']\n",
      "12 ['small', 'ablation', 'studies', 'showing', 'the', 'effect', 'of', 'the', 'different', 'parameters', 'of', 'CoNAS']\n",
      "7 ['were', 'very', 'informative', 'and', 'well', '-', 'appreciated']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 5), (5, 5), (6, 9), (7, 10), (8, 6), (9, 6), (10, 7), (11, 4), (12, 2), (13, 3), (14, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "25 ['Points', 'of', 'improvement', ':', '1', '-', 'Structure', 'of', 'the', 'paper', '2', '-', 'Clarify', 'novelty', 'compared', 'to', 'HARMONICA', '-LRB-', 'not', 'the', 'application', 'domain', 'please', '-RRB-', '3']\n",
      "1 ['-']\n",
      "11 ['demonstrate', 'that', 'with', 'the', 'same', 'search', 'space', 'your', 'method', 'is', 'competitive']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 4), (5, 2), (6, 3), (7, 2), (8, 3), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['4', '-']\n",
      "7 ['Does', 'm', '=', '1000', 'in', 'your', 'experiments']\n",
      "3 ['satisfies', 'theorem', '3.2']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 4), (6, 2), (7, 3), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  5\n",
      "1 ['the']\n",
      "1 ['value']\n",
      "1 ['of']\n",
      "4 ['d', 'in', 'your', 'experiments']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 2), (5, 4), (6, 2), (7, 2), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['Can']\n",
      "1 ['you']\n",
      "7 ['provide', 'supporting', 'experiments', 'that', 'answer', 'those', 'questions']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 4), (6, 5), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "9 ['Preliminary', 'decision', ':', 'For', 'now', ',', 'I', 'will', 'say']\n",
      "1 ['*']\n",
      "1 ['weak']\n",
      "1 ['reject']\n",
      "1 ['*']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 5), (6, 2), (7, 7), (8, 8), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['proposes']\n",
      "16 ['a', 'new', 'algorithm', 'for', 'one', '-', 'shot', 'neural', 'architecture', 'search', '-LRB-', 'NAS', '-RRB-', 'via', 'compressive', 'sensing']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 6), (5, 12), (6, 9), (7, 5), (8, 6), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['propose']\n",
      "13 ['a', 'new', 'search', 'strategy', ',', 'as', 'well', 'as', 'a', 'slightly', 'different', 'search', 'space']\n",
      "13 ['compared', 'to', 'DARTS', '[', '1', ']', ',', 'ProxylessNAS', '[', '2', ']', ',', 'etc.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 2), (6, 7), (7, 8), (8, 7), (9, 7), (10, 6), (11, 5), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['They']\n",
      "1 ['use']\n",
      "2 ['architecture', 'samples']\n",
      "26 ['from', 'the', 'one', '-', 'shot', 'model', 'evaluated', 'with', 'the', 'search', 'parameters', 'as', 'a', 'surrogate', 'of', 'the', 'true', 'objective', 'in', 'order', 'to', 'speed', '-', 'up', 'the', 'search']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 2), (5, 1), (6, 2), (7, 2), (8, 2), (9, 4), (10, 2), (11, 3), (12, 3), (13, 1), (14, 2), (15, 3), (16, 3), (17, 6), (18, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  2\n",
      "1 ['Afterwards']\n",
      "1 [',']\n",
      "4 ['these', 'surrogate', 'function', 'evaluations']\n",
      "20 ['are', 'used', 'to', 'compute', 'Fourier', 'coefficients', 'which', 'are', 'eventually', 'used', 'to', 'optimize', 'the', 'vector', 'of', 'binary', 'parameters', 'encoding', 'the', 'architecture']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 1), (5, 2), (6, 5), (7, 3), (8, 3), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Overall']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "10 ['think', 'the', 'proposed', 'algorithm', 'is', 'interesting', 'and', 'of', 'practical', 'usefulness']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 8), (4, 6), (5, 8), (6, 8), (7, 5), (8, 9), (9, 7), (10, 2), (11, 2), (12, 2), (13, 2), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "4 ['in', 'terms', 'of', 'novelty']\n",
      "1 [',']\n",
      "2 ['this', 'work']\n",
      "26 ['seems', 'more', 'to', 'be', 'an', 'application', 'of', 'Harmonica', '[', '6', ']', 'to', 'the', 'NAS', 'problem', '-LRB-', 'with', 'small', 'modifications', 'in', 'order', 'to', 'make', 'it', 'applicable', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 6), (5, 4), (6, 2), (7, 4), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "3 ['In', 'page', '10']\n",
      "1 ['you']\n",
      "10 ['state', 'some', 'of', 'the', 'differences', 'of', 'your', 'method', 'with', 'Harmonica']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 8), (5, 4), (6, 4), (7, 7), (8, 11), (9, 10), (10, 10), (11, 6), (12, 8), (13, 13), (14, 10), (15, 7), (16, 7), (17, 6), (18, 4), (19, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  2\n",
      "27 ['I', 'agree', 'that', 'the', 'number', 'of', 'function', 'evaluations', 'you', 'use', '-LRB-', 'coming', 'from', 'the', 'one', '-', 'shot', 'model', '-RRB-', 'is', 'larger', 'and', 'computationally', 'less', 'expensive', 'to', 'obtain']\n",
      "1 [',']\n",
      "35 ['however', 'this', 'does', 'not', 'guarantee', 'that', 'these', 'are', 'a', 'good', 'surrogate', 'of', 'the', 'true', 'objective', 'that', 'NAS', 'aims', 'to', 'minimize', ',', 'ie', 'the', 'validation', '/', 'test', 'accuracy', 'of', 'final', '-LRB-', 'stand', '-', 'alone', '-RRB-', 'architectures']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 8), (4, 14), (5, 10), (6, 8), (7, 9), (8, 5), (9, 5), (10, 4), (11, 7), (12, 2), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "24 ['The', 'empirical', 'evaluations', 'of', 'their', 'algorithm', 'seem', 'to', 'outperform', '/', 'be', 'competitive', 'compared', 'to', 'other', 'NAS', 'methods', 'on', 'all', 'benchmarks', 'used', 'in', 'the', 'paper']\n",
      "1 [',']\n",
      "9 ['however', 'only', 'DARTS', 'is', 'evaluated', 'on', 'their', 'search', 'space']\n",
      "1 ['and']\n",
      "9 ['the', 'other', 'results', 'are', 'taken', 'from', 'the', 'corresponding', 'papers']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 6), (5, 5), (6, 3), (7, 6), (8, 3), (9, 5), (10, 2), (11, 3), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "7 ['The', 'paper', 'is', 'well', '-', 'written', 'and']\n",
      "1 ['-']\n",
      "9 ['structured', 'with', 'the', 'caveat', 'of', 'being', 'more', 'than', 'the']\n",
      "1 ['recommended']\n",
      "2 ['8', 'pages']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 6), (5, 9), (6, 9), (7, 10), (8, 14), (9, 21), (10, 11), (11, 15), (12, 16), (13, 7), (14, 7), (15, 6), (16, 3), (17, 3), (18, 3), (19, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "33 ['adjust', 'my', 'score', 'depending', 'on', 'the', 'authors', 'responses', 'concerning', 'the', 'following', 'questions', '/', 'issues', ':', '1', '-RRB-', 'The', 'correlation', 'between', 'the', 'architectures', 'evaluated', 'using', 'the', 'one', '-', 'shot', 'weights', 'and', 'retrained', 'from', 'scratch']\n",
      "1 [',']\n",
      "23 ['seems', 'to', 'be', 'of', 'crucial', 'importance', 'in', 'your', 'method', ',', 'since', 'you', 'directly', 'use', 'the', 'one', '-', 'shot', 'weights', 'to', 'collect', 'the', 'measurements']\n",
      "1 [',']\n",
      "3 ['similarly', 'to', 'Random']\n",
      "19 ['Search', 'with', 'weight', 'sharing', '[', '3', ']', ',', 'ENAS', '[', '4', ']', 'or', 'Bender', 'et', 'al', '[', '5', ']']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 7), (6, 5), (7, 8), (8, 2), (9, 3), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['What']\n",
      "1 ['is']\n",
      "11 ['the', 'correlation', 'of', 'these', 'measurements', 'with', 'the', 'stand', '-', 'alone', 'architectures']\n",
      "8 ['trained', 'from', 'scratch', 'using', 'the', 'final', 'evaluation', 'settings']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 4), (6, 8), (7, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['How']\n",
      "1 ['did']\n",
      "1 ['you']\n",
      "14 ['tune', 'the', 'p', 'in', 'the', 'Bernoulli', 'distribution', 'during', 'the', 'one', '-', 'shot', 'weight', 'updates']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 4), (5, 8), (6, 9), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "8 ['According', 'to', 'Bender', 'et', 'al', '[', '5', ']']\n",
      "3 ['the', 'ScheduledDropPath', 'probability']\n",
      "8 ['is', 'an', 'important', 'hyperparameter', 'affecting', 'the', 'aforementioned', 'correlation']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 5), (6, 1), (7, 3), (8, 4), (9, 6), (10, 5), (11, 9), (12, 4), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "1 ['What']\n",
      "23 ['is', 'the', 'main', 'motivation', 'for', 'using', '5', 'operations', 'in', 'the', 'operation', 'set', 'and', 'not', '8', 'as', 'in', 'DARTS', '[', '1', ']', 'for', 'example']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 1), (5, 2), (6, 4), (7, 7), (8, 6), (9, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  6\n",
      "3 ['the', 'main', 'contribution']\n",
      "4 ['in', 'the', 'competitive', 'results']\n",
      "1 ['come']\n",
      "9 ['from', 'the', 'different', 'search', 'space', 'or', 'the', 'search', 'method']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 7), (5, 4), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "2 ['3', '-RRB-']\n",
      "1 ['Is']\n",
      "5 ['there', 'any', 'reference', 'or', 'proof']\n",
      "6 ['for', 'the', 'correctness', 'of', 'Theorem', '3.2']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 1), (5, 2), (6, 3), (7, 3), (8, 5), (9, 6), (10, 6), (11, 10), (12, 12), (13, 11), (14, 4), (15, 4), (16, 2), (17, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "2 ['4', '-RRB-']\n",
      "1 ['I']\n",
      "40 ['think', 'there', 'are', 'some', 'parts', 'that', 'can', 'be', 'moved', 'in', 'the', 'Supplementary', ',', 'such', 'as', 'the', 'pseudocode', 'for', 'the', 'proposed', 'algorithm', 'or', 'Figure', '3', ',', 'and', 'some', 'other', 'parts', 'that', 'can', 'be', 'compressed', ',', 'such', 'as', 'the', 'Related', 'Work', 'section']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 8), (4, 9), (5, 1)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['References']\n",
      "3 ['[', '1', ']']\n",
      "2 ['Hanxiao', 'Liu']\n",
      "1 [',']\n",
      "2 ['Karen', 'Simonyan']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "2 ['Yiming', 'Yang']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4)]\n",
      "[0, 1, 2, 3]\n",
      "depth:  2\n",
      "1 ['DARTS']\n",
      "1 [':']\n",
      "3 ['Differentiable', 'architecture', 'search']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 1)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "2 ['In', 'ICLR']\n",
      "1 [',']\n",
      "1 ['2019']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 6), (4, 4)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "1 ['[']\n",
      "1 ['2']\n",
      "1 [']']\n",
      "2 ['Han', 'Cai']\n",
      "1 [',']\n",
      "5 ['Ligeng', 'Zhu', ',', 'Song', 'Han']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 5), (5, 6)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['ProxylessNAS']\n",
      "1 [':']\n",
      "9 ['Direct', 'Neural', 'Architecture', 'Search', 'on', 'Target', 'Task', 'and', 'Hardware']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 1)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "2 ['In', 'ICLR']\n",
      "1 [',']\n",
      "1 ['2019']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7)]\n",
      "[0, 1, 2, 3]\n",
      "depth:  2\n",
      "3 ['[', '3', ']']\n",
      "2 ['LIAM', 'LI']\n",
      "1 [',']\n",
      "2 ['AMEET', 'TALWALKAR']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 3), (5, 2), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "2 ['Random', 'Search']\n",
      "1 ['and']\n",
      "5 ['Reproducibility', 'for', 'Neural', 'Architecture', 'Search']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 11), (3, 15)]\n",
      "[0, 1, 2, 3]\n",
      "depth:  2\n",
      "3 ['[', '4', ']']\n",
      "2 ['Hieu', 'Pham']\n",
      "1 [',']\n",
      "3 ['Melody', 'Y.', 'Guan']\n",
      "1 [',']\n",
      "2 ['Barret', 'Zoph']\n",
      "1 [',']\n",
      "3 ['Quoc', 'V.', 'Le']\n",
      "1 [',']\n",
      "2 ['Jeff', 'Dean']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 5), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['Efficient']\n",
      "1 ['Neural']\n",
      "2 ['Architecture', 'Search']\n",
      "1 ['via']\n",
      "1 ['Parameter']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 11), (4, 15), (5, 3), (6, 3), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['In']\n",
      "6 ['ICML', ',', '2018', '[', '5', ']']\n",
      "2 ['Gabriel', 'Bender']\n",
      "1 [',']\n",
      "4 ['Pieter', '-', 'Jan', 'Kindermans']\n",
      "1 [',']\n",
      "2 ['Barret', 'Zoph']\n",
      "1 [',']\n",
      "2 ['Vijay', 'Vasudevan']\n",
      "1 [',']\n",
      "2 ['Quoc', 'Le']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 4), (4, 2), (5, 3), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['Understanding']\n",
      "1 ['and']\n",
      "1 ['Simplifying']\n",
      "5 ['One', '-', 'Shot', 'Architecture', 'Search']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 3), (5, 4), (6, 5), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "9 ['In', 'ICML', ',', '2018', '[', '6', ']', 'Elad', 'Hazan']\n",
      "1 [',']\n",
      "2 ['Adam', 'Klivans']\n",
      "1 [',']\n",
      "2 ['Yang', 'Yuan']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5)]\n",
      "[0, 1, 2, 3]\n",
      "depth:  3\n",
      "1 ['Hyperparameter']\n",
      "1 ['Optimization']\n",
      "1 ['A']\n",
      "1 ['Spectral']\n",
      "1 ['Approach']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 5), (5, 4), (6, 4), (7, 2), (8, 1), (9, 2), (10, 2), (11, 2), (12, 3), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "3 ['In', 'this', 'paper']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "14 ['study', 'Neural', 'Architecture', 'Search', ',', 'which', 'aims', 'to', 'automate', 'design', 'of', 'neural', 'network', 'models']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 1), (6, 2), (7, 2), (8, 4), (9, 10), (10, 8), (11, 5), (12, 2), (13, 1), (14, 2), (15, 2), (16, 4), (17, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  3\n",
      "1 ['Their']\n",
      "1 ['approach']\n",
      "1 ['consists']\n",
      "23 ['in', 'using', 'a', 'two', 'stage', 'algorithm', ':', '-', 'A', 'first', 'Neural', 'Network', '$', 'f', '$', 'is', 'trained', 'for', 'predicting', 'the', 'performances', 'of', 'sub-architectures']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 11), (5, 11), (6, 8), (7, 4), (8, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "15 ['Then', 'binary', 'sub-graphs', 'coding', 'the', 'sub-architectures', 'are', 'uniformly', 'sampled', '-LRB-', 'Bernouilli', '-LRB-', '0.5', '-RRB-', '-RRB-']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "11 ['their', 'performances', 'y', 'are', 'evaluated', 'thanks', 'to', 'the', 'first', 'Neural', 'Network']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 5), (6, 4), (7, 2), (8, 2), (9, 2), (10, 2), (11, 4), (12, 3), (13, 3), (14, 5), (15, 2), (16, 5), (17, 6), (18, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "2 ['The', 'graph']\n",
      "29 ['sampling', 'matrix', 'A', ',', 'which', 'is', 'indexed', 'by', 'the', 'm', 'sampled', 'architectures', 'and', 'the', 'Fourier', 'basis', 'of', 'size', '$', 'O', '-LRB-', 'n', '^', 'd', '-RRB-', '$', ',', 'is', 'built']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 8), (4, 9), (5, 13), (6, 8), (7, 2), (8, 2), (9, 2), (10, 1), (11, 2), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['Then']\n",
      "9 ['the', 'optimization', 'problem', '$', 'x', '^', '*', '=', '\\\\']\n",
      "15 ['arg', '\\\\', 'min_x', '|', '|', 'y', '–', 'Ax', '|', '|', '$', 'is', 'solved', 'using', 'Lasso']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 7), (5, 2), (6, 1), (7, 2), (8, 5), (9, 5), (10, 9), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "19 ['The', 'largest', 'Fourier', 'coefficients', 'are', 'chosen', 'to', 'build', 'an', 'estimate', 'g', 'of', 'f.', 'Finally', ',', 'computing', 'minimum', 'of', 'g']\n",
      "1 [',']\n",
      "2 ['the', 'architecture']\n",
      "2 ['is', 'generated']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 9), (4, 5), (5, 4), (6, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "8 ['Since', '$', 'm', '<<', 'n', '^', 'd', '$']\n",
      "1 [',']\n",
      "3 ['the', 'optimization', 'problem']\n",
      "4 ['is', 'ill', '-', 'posed']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 2), (6, 5), (7, 3), (8, 2), (9, 2), (10, 2), (11, 5), (12, 3), (13, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['Theorem']\n",
      "1 ['3.2']\n",
      "1 ['shows']\n",
      "19 ['that', 'if', 'A', 'satisfies', 'the', 'restricted', 'isometry', 'of', 'order', 's', ',', 'then', 'the', 'sparse', 'coefficients', 'x', 'can', 'be', 'recovered']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 3), (5, 3), (6, 7), (7, 11), (8, 7), (9, 5), (10, 7), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "2 ['Major', 'concern']\n",
      "1 [':']\n",
      "26 ['1', '/', 'I', 'did', 'not', 'find', 'the', 'proof', 'of', 'Theorem', '3.2', 'in', 'the', 'main', 'paper', 'and', 'in', 'the', 'appendix', ',', 'so', 'I', 'do', 'not', 'buy', 'it']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 11), (5, 9), (6, 6), (7, 10), (8, 4), (9, 6), (10, 4), (11, 6), (12, 6), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "19 ['2', '/', 'The', 'authors', 'claim', 'that', 'their', 'algorithm', 'performs', 'better', 'than', 'the', 'state', '-', 'of', '-', 'the', '-', 'art']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "18 ['according', 'to', 'tables', '1,2,3', ',', 'I', 'did', 'not', 'find', 'significant', 'differences', 'of', 'performances', 'in', 'term', 'of', 'test', 'errors']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 7), (4, 7), (5, 5)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['3']\n",
      "4 ['/', 'The', 'key', 'idea']\n",
      "3 ['of', 'the', 'algorithm']\n",
      "1 ['is']\n",
      "1 ['not']\n",
      "1 ['well']\n",
      "1 ['explained']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 4)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['A']\n",
      "3 ['one', '-', 'shot']\n",
      "1 ['NAS']\n",
      "1 ['f']\n",
      "1 ['is']\n",
      "1 ['pre-trained']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 1), (6, 2), (7, 2), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['f']\n",
      "1 ['is']\n",
      "6 ['assumed', 'to', 'be', 'well', '-', 'trained']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 2), (6, 3), (7, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['Then']\n",
      "1 ['it']\n",
      "9 ['is', 'approximated', 'with', 'a', 'Fourier', '-', 'sparse', 'Boolean', 'function']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 4), (6, 2), (7, 3), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  5\n",
      "1 ['an']\n",
      "1 ['approximation']\n",
      "1 ['if']\n",
      "3 ['f', 'is', 'perfect']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 7), (4, 7), (5, 3), (6, 4), (7, 1), (8, 3), (9, 6), (10, 2), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['Also']\n",
      "1 [',']\n",
      "2 ['in', 'general']\n",
      "1 [',']\n",
      "14 ['a', 'discussion', 'on', 'the', 'efficiency', 'of', 'training', 'the', 'proposed', 'model', 'as', 'compared', 'to', 'TreeNN']\n",
      "3 ['would', 'be', 'helpful']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 9), (5, 7), (6, 4), (7, 5), (8, 4), (9, 2), (10, 4), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "4 ['Several', 'different', 'encoding', 'benchmarks']\n",
      "4 ['of', 'the', 'entailment', 'task']\n",
      "1 ['are']\n",
      "16 ['designed', 'to', 'compare', 'against', 'the', 'performance', 'of', 'the', 'proposed', 'model', ',', 'using', 'a', 'newly', 'created', 'dataset']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 4), (6, 4), (7, 3), (8, 2), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "2 ['One', 'weakness']\n",
      "3 ['with', 'the', 'paper']\n",
      "1 ['was']\n",
      "8 ['that', 'it', 'was', 'only', 'tested', 'on', '1', 'dataset']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 12), (5, 18), (6, 8), (7, 8), (8, 5), (9, 6), (10, 2), (11, 3), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "20 ['The', 'paper', 'would', 'have', 'been', 'improved', 'through', 'testing', 'of', 'multiple', 'datasets', ',', 'and', 'not', 'just', 'on', 'there', 'self', 'generated', 'dataset']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "18 ['the', 'contribution', 'of', 'their', 'research', 'on', 'their', 'network', 'and', 'older', 'networks', 'is', 'still', 'justification', 'enough', 'for', 'this', 'paper']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 5), (4, 9), (5, 5)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['Overall']\n",
      "1 [',']\n",
      "6 ['the', 'paper', 'is', 'well', '-', 'written']\n",
      "1 ['and']\n",
      "6 ['the', 'proposed', 'model', 'is', 'quite', 'intuitive']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 1), (5, 2), (6, 3), (7, 3), (8, 2), (9, 4), (10, 2), (11, 4), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['Specifically']\n",
      "1 [',']\n",
      "2 ['the', 'idea']\n",
      "13 ['is', 'to', 'represent', 'entailment', 'as', 'a', 'product', 'of', 'continuous', 'functions', 'over', 'possible', 'worlds']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 1), (5, 2), (6, 4), (7, 4), (8, 4), (9, 4), (10, 2), (11, 3), (12, 3), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Specifically']\n",
      "1 [',']\n",
      "2 ['the', 'idea']\n",
      "16 ['is', 'to', 'generate', 'possible', 'worlds', ',', 'and', 'compute', 'the', 'functions', 'that', 'encode', 'entailment', 'in', 'those', 'worlds']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 3), (5, 3), (6, 5), (7, 2), (8, 2), (9, 3), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "2 ['The', 'functions']\n",
      "1 ['themselves']\n",
      "12 ['are', 'designed', 'as', 'tree', 'neural', 'networks', 'to', 'take', 'advantage', 'of', 'logical', 'structure']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 4), (6, 2), (7, 2), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['results']\n",
      "1 ['seem']\n",
      "7 ['very', 'impressive', 'with', '>', '99', '%', 'accuracy']\n",
      "3 ['on', 'tests', 'sets']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 5), (4, 6), (5, 3), (6, 2), (7, 2), (8, 2), (9, 2), (10, 3), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['Also']\n",
      "1 [',']\n",
      "1 ['should']\n",
      "4 ['some', 'form', 'of', 'cross-validation']\n",
      "10 ['be', 'applied', 'to', 'smooth', 'out', 'variance', 'in', 'the', 'evaluation', 'results']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 2), (7, 3), (8, 2), (9, 7), (10, 3), (11, 4), (12, 2), (13, 2), (14, 2), (15, 2), (16, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['am']\n",
      "1 ['not']\n",
      "20 ['sure', 'if', 'there', 'are', 'standard', '\"', 'shared', '\"', 'datasets', 'for', 'this', 'task', ',', 'which', 'would', 'make', 'the', 'results', 'much', 'stronger']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 9), (5, 6), (6, 6), (7, 5), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "7 ['Also', 'how', 'about', 'the', 'tradeoff', ',', 'i.e.']\n",
      "1 [',']\n",
      "12 ['does', 'training', 'time', 'significantly', 'increase', 'when', 'we', '\"', 'imagine', '\"', 'more', 'worlds']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 11), (5, 15), (6, 11), (7, 6), (8, 5), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "13 ['The', 'size', 'of', 'the', 'world', 'vectors', ',', 'I', 'would', 'believe', 'is', 'quite', 'important']\n",
      "1 [',']\n",
      "1 ['so']\n",
      "16 ['maybe', 'a', 'more', 'detailed', 'analysis', 'on', 'how', 'this', 'was', 'chosen', 'is', 'important', 'to', 'replicate', 'the', 'results']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 5), (5, 4), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "2 ['This', 'problem']\n",
      "4 [',', 'I', 'think', ',']\n",
      "6 ['is', 'quite', 'related', 'to', 'model', 'counting']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 4), (6, 6), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  5\n",
      "2 ['a', 'lot']\n",
      "2 ['of', 'work']\n",
      "1 ['on']\n",
      "2 ['model', 'counting']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 3), (6, 3), (7, 3), (8, 2), (9, 2), (10, 4), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "2 ['a', 'discussion']\n",
      "9 ['on', 'how', 'this', 'relates', 'to', 'those', 'lines', 'of', 'work']\n",
      "1 ['would']\n",
      "2 ['be', 'interesting']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 2), (6, 4), (7, 3), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "2 ['After', 'revision']\n",
      "1 ['I']\n",
      "8 ['think', 'the', 'authors', 'have', 'improved', 'the', 'experiments', 'substantially']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 9), (5, 9), (6, 3), (7, 2), (8, 3), (9, 3), (10, 2), (11, 1), (12, 2), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "13 ['SUMMARY', 'The', 'paper', 'is', 'fairly', 'broad', 'in', 'what', 'it', 'is', 'trying', 'to', 'achieve']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "6 ['the', 'approach', 'is', 'well', 'thought', 'out']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 4), (6, 4), (7, 6), (8, 6), (9, 10), (10, 7), (11, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "2 ['The', 'purpose']\n",
      "3 ['of', 'the', 'paper']\n",
      "1 ['is']\n",
      "23 ['to', 'investigate', 'the', 'effectiveness', 'of', 'prior', 'machine', 'learning', 'methods', 'with', 'predicting', 'logical', 'entailment', 'and', 'then', 'provide', 'a', 'new', 'model', 'designed', 'for', 'the', 'task']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 5), (5, 7), (6, 4), (7, 1), (8, 3), (9, 3), (10, 2), (11, 3), (12, 1), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Explicitly']\n",
      "1 [',']\n",
      "2 ['the', 'paper']\n",
      "17 ['asks', 'the', 'following', 'questions', ':', '\"', 'Can', 'neural', 'networks', 'understand', 'logical', 'formula', 'well', 'enough', 'to', 'detect', 'entailment']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 6), (5, 7), (6, 9), (7, 7), (8, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['\"']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "21 ['\"', 'Which', 'architectures', 'are', 'best', 'at', 'inferring', ',', 'encoding', ',', 'and', 'relating', 'features', 'in', 'a', 'purely', 'structural', 'sequence', '-', 'based', 'problem']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 4), (6, 3), (7, 4), (8, 8), (9, 5), (10, 2), (11, 2), (12, 1), (13, 2), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "2 ['The', 'goals']\n",
      "3 ['of', 'the', 'paper']\n",
      "1 ['is']\n",
      "16 ['to', 'understand', 'the', 'learning', 'bias', 'of', 'current', 'architectures', 'when', 'they', 'are', 'tasked', 'with', 'learning', 'logical', 'entailment']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 7), (4, 8), (5, 2), (6, 2), (7, 4), (8, 2), (9, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "4 ['The', 'proposed', 'network', 'architecture']\n",
      "1 [',']\n",
      "1 ['PossibleWorldNet']\n",
      "1 [',']\n",
      "1 ['is']\n",
      "1 ['then']\n",
      "9 ['viewed', 'as', 'an', 'improvement', 'on', 'an', 'earlier', 'architecture', 'TreeNet']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 6), (5, 4), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['POSITIVES']\n",
      "5 ['The', 'structure', 'of', 'this', 'paper']\n",
      "1 ['was']\n",
      "2 ['very', 'well']\n",
      "1 ['done']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 4), (5, 3), (6, 4), (7, 2), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "5 ['attempts', 'to', 'do', 'a', 'lot']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "4 ['succeeds', 'on', 'most', 'fronts']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 7), (5, 5), (6, 6), (7, 4), (8, 4), (9, 2), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "3 ['The', 'generated', 'dataset']\n",
      "5 ['used', 'for', 'testing', 'logical', 'entailment']\n",
      "1 ['is']\n",
      "9 ['given', 'a', 'constructive', 'description', 'which', 'allows', 'for', 'future', 'replication']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 10), (5, 5), (6, 6), (7, 4), (8, 8), (9, 5), (10, 5), (11, 2), (12, 4), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "8 ['The', 'baseline', 'benchmark', 'networks', 'are', 'covered', 'in', 'depth']\n",
      "1 ['and']\n",
      "21 ['the', 'reader', 'is', 'provided', 'with', 'a', 'deep', 'understanding', 'on', 'the', 'limitations', 'of', 'some', 'networks', 'with', 'regard', 'to', 'exploiting', 'structure', 'in', 'data']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 9), (5, 8), (6, 6), (7, 4), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "7 ['The', 'PossibleWorldNets', 'is', 'also', 'given', 'good', 'coverage']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "10 ['the', 'equations', 'provided', 'show', 'the', 'means', 'by', 'which', 'it', 'operates']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 6), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['A']\n",
      "1 ['clear']\n",
      "1 ['methodological']\n",
      "1 ['approach']\n",
      "1 ['to']\n",
      "2 ['the', 'research']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 3), (6, 6), (7, 6), (8, 5), (9, 11), (10, 4), (11, 4), (12, 4), (13, 4), (14, 5), (15, 9), (16, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['covers']\n",
      "35 ['how', 'they', 'created', 'a', 'dataset', 'which', 'can', 'be', 'used', 'for', 'logical', 'entailment', 'learning', ',', 'and', 'then', 'explains', 'clearly', 'all', 'the', 'previous', 'network', 'models', 'which', 'will', 'be', 'used', 'in', 'testing', 'as', 'well', 'as', 'their', 'proposed', 'model']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 5)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "4 ['•', 'The', 'background', 'information']\n",
      "3 ['regarding', 'each', 'model']\n",
      "1 ['was']\n",
      "2 ['exceptionally', 'thorough']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 4), (6, 3), (7, 4), (8, 9), (9, 6), (10, 2), (11, 2), (12, 1), (13, 2), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['went']\n",
      "3 ['into', 'great', 'depth']\n",
      "18 ['describing', 'the', 'pros', 'and', 'cons', 'of', 'earlier', 'network', 'models', 'and', 'why', 'they', 'may', 'struggle', 'with', 'recognizing', 'logical', 'entailment']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 7), (5, 10), (6, 8), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "3 ['•', 'The', 'section']\n",
      "6 ['describing', 'the', 'creation', 'of', 'a', 'dataset']\n",
      "1 ['captures']\n",
      "5 ['the', 'basis', 'for', 'the', 'research']\n",
      "1 [',']\n",
      "3 ['learning', 'logical', 'entailment']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 4), (5, 7), (6, 8), (7, 6), (8, 4), (9, 2), (10, 4), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "5 ['the', 'creation', 'of', 'the', 'data']\n",
      "1 [',']\n",
      "3 ['as', 'well', 'as']\n",
      "10 ['the', 'means', 'by', 'which', 'they', 'increase', 'the', 'difficulty', 'for', 'learning']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 9), (6, 5), (7, 9), (8, 7), (9, 2), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['•']\n",
      "11 ['The', 'paper', 'provides', 'an', 'in', 'depth', 'description', 'of', 'their', 'PossibleWorldNet', 'model']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "10 ['during', 'experimentation', 'we', 'see', 'clear', 'evidence', 'of', 'the', 'models', 'capabilities']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 7), (6, 7), (7, 6), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "3 ['NEGATIVES', 'One', 'issue']\n",
      "5 ['I', 'had', 'with', 'the', 'paper']\n",
      "1 ['is']\n",
      "8 ['regarding', 'the', 'creation', 'of', 'the', 'logical', 'entailment', 'dataset']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 9), (5, 12), (6, 6), (7, 9), (8, 8), (9, 11), (10, 4), (11, 1), (12, 2), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "18 ['Not', 'so', 'much', 'for', 'how', 'they', 'explained', 'the', 'process', 'of', 'creating', 'the', 'dataset', ',', 'that', 'was', 'very', 'thorough']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "21 ['the', 'fact', 'that', 'this', 'dataset', 'was', 'the', 'only', 'means', 'to', 'test', 'the', 'previous', 'network', 'models', 'and', 'their', 'new', 'proposed', 'network', 'model']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 3), (7, 2), (8, 2), (9, 2), (10, 2), (11, 2), (12, 2), (13, 4), (14, 2), (15, 2), (16, 2), (17, 2), (18, 3), (19, 2), (20, 2), (21, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "depth:  13\n",
      "1 ['non-generated']\n",
      "1 ['datasets']\n",
      "1 ['which']\n",
      "7 ['may', 'contain', 'data', 'that', 'have', 'entailment', 'relationships']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 2), (6, 4), (7, 6), (8, 4), (9, 3), (10, 2), (11, 4), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['is']\n",
      "1 ['questionable']\n",
      "14 ['if', 'their', 'hand', 'crafted', 'network', 'model', 'is', 'learned', 'best', 'on', 'their', 'hand', 'crafted', 'dataset']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 5), (6, 1), (7, 2), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['use']\n",
      "1 ['of']\n",
      "7 ['a', 'singular', 'dataset', 'for', 'learning', 'logical', 'entailment']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 3), (6, 4), (7, 6), (8, 4), (9, 5), (10, 3), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['dataset']\n",
      "1 ['was']\n",
      "1 ['also']\n",
      "17 ['created', 'by', 'the', 'researchers', 'for', 'the', 'express', 'purpose', 'of', 'testing', 'neural', 'network', 'capacity', 'to', 'learn', 'logical', 'entailment']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 1), (6, 2), (7, 2), (8, 1), (9, 2), (10, 6), (11, 5), (12, 3), (13, 6), (14, 5), (15, 6), (16, 6), (17, 3), (18, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  10\n",
      "1 ['their']\n",
      "1 ['proposed']\n",
      "1 ['network']\n",
      "1 ['is']\n",
      "3 ['an', 'incredible', 'achievement']\n",
      "16 ['since', 'PossibleWorldNet', 'effectively', 'beat', 'out', 'other', 'methods', 'on', 'a', 'dataset', 'that', 'they', 'created', 'expressly', 'for', 'it']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 4), (7, 2), (8, 5), (9, 2), (10, 1), (11, 2), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  6\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['has']\n",
      "8 ['an', 'extensive', 'section', 'dedicated', 'to', 'covering', 'related', 'work']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 7), (5, 5), (6, 8), (7, 8), (8, 11), (9, 8), (10, 2), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "9 ['I', 'would', 'say', 'the', 'research', 'involved', 'was', 'very', 'thorough']\n",
      "1 ['and']\n",
      "18 ['the', 'researchers', 'understood', 'how', 'their', 'method', 'was', 'different', 'as', 'well', 'as', 'how', 'it', 'was', 'improving', 'on', 'earlier', 'approaches']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 5), (6, 9), (7, 6), (8, 4), (9, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['CONCLUSION']\n",
      "13 ['Given', 'the', 'thorough', 'investigation', 'into', 'previous', 'networks', '’', 'capabilities', 'in', 'logical', 'entailment', 'learning']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "9 ['would', 'accept', 'this', 'paper', 'as', 'a', 'valid', 'scientific', 'contribution']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 2), (6, 4), (7, 3), (8, 5), (9, 4), (10, 2), (11, 3), (12, 3), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['performs']\n",
      "3 ['a', 'thorough', 'analysis']\n",
      "14 ['on', 'the', 'limitations', 'that', 'previous', 'networks', 'face', 'with', 'regard', 'to', 'exploiting', 'structure', 'from', 'data']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 6), (5, 7), (6, 6), (7, 6), (8, 7), (9, 4), (10, 8), (11, 2), (12, 1), (13, 2), (14, 2), (15, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "2 ['The', 'paper']\n",
      "1 ['also']\n",
      "31 ['covers', 'results', 'of', 'the', 'experiments', 'by', 'not', 'only', 'pointing', 'out', 'their', 'proposed', 'network', '’s', 'success', ',', 'but', 'by', 'analyzing', 'why', 'certain', 'earlier', 'network', 'models', 'were', 'able', 'to', 'achieve', 'competitive', 'learning', 'results']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 10), (5, 10), (6, 5), (7, 2), (8, 3), (9, 3), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "2 ['The', 'structure']\n",
      "3 ['of', 'the', 'PossibleWorldNet']\n",
      "4 ['was', 'also', 'explained', 'well']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "12 ['during', 'ex', '-', 'perimentation', 'demonstrated', 'its', 'ability', 'to', 'learn', 'structure', 'from', 'data']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 5), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  5\n",
      "1 ['a']\n",
      "1 ['wonderful']\n",
      "1 ['a']\n",
      "3 ['self', '-', 'contained']\n",
      "1 ['paper']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 7), (5, 4), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "2 ['In', 'fact']\n",
      "1 [',']\n",
      "6 ['it', 'introduces', 'a', 'very', 'important', 'problem']\n",
      "1 ['and']\n",
      "3 ['it', 'solves', 'it']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 7), (5, 4), (6, 2), (7, 3), (8, 2), (9, 1), (10, 2), (11, 3), (12, 4), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "3 ['The', 'major', 'point']\n",
      "3 ['of', 'the', 'paper']\n",
      "1 ['is']\n",
      "12 ['demonstrating', 'that', 'it', 'is', 'possible', 'to', 'model', 'logical', 'entailment', 'in', 'neural', 'networks']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 6)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "1 ['Hence']\n",
      "1 [',']\n",
      "6 ['a', 'corpus', 'and', 'a', 'NN', 'model']\n",
      "2 ['are', 'introduced']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 1), (6, 2), (7, 2), (8, 2), (9, 2), (10, 6), (11, 7), (12, 3), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['corpus']\n",
      "1 ['is']\n",
      "16 ['used', 'to', 'demonstrate', 'that', 'the', 'model', ',', 'named', 'PossibleWorld', ',', 'is', 'nearly', 'perfect', 'for', 'the', 'task']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 4), (6, 3), (7, 3), (8, 3), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['A']\n",
      "1 ['comparative']\n",
      "1 ['analysis']\n",
      "1 ['is']\n",
      "10 ['done', 'with', 'respect', 'to', 'state', 'of', 'the', 'art', 'recurrent', 'NN']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 2)]\n",
      "[0, 1, 2, 3]\n",
      "depth:  2\n",
      "2 ['So', 'far']\n",
      "1 [',']\n",
      "1 ['so']\n",
      "1 ['good']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 3), (4, 2), (5, 3), (6, 1), (7, 1), (8, 2), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Yet']\n",
      "1 [',']\n",
      "1 ['what']\n",
      "5 ['is', 'the', 'take', 'home', 'message']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 4), (5, 2), (6, 5), (7, 2), (8, 3), (9, 4), (10, 5), (11, 4), (12, 6), (13, 2), (14, 2), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "3 ['In', 'my', 'opinion']\n",
      "1 [',']\n",
      "2 ['the', 'message']\n",
      "22 ['is', 'that', 'generic', 'NN', 'should', 'not', 'be', 'used', 'for', 'specific', 'formal', 'tasks', 'whereas', 'specific', 'neural', 'networks', 'that', 'model', 'the', 'task', 'are', 'desirable']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 8), (5, 6), (6, 9), (7, 6), (8, 5), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "7 ['This', 'seems', 'to', 'be', 'a', 'trivial', 'claim']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "1 [',']\n",
      "15 ['since', 'the', 'PossibleWorld', 'nearly', 'completely', 'solves', 'the', 'task', ',', 'it', 'is', 'worth', 'to', 'be', 'investigated']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 6), (5, 5), (6, 6), (7, 3), (8, 5), (9, 2), (10, 2), (11, 2), (12, 3), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "2 ['The', 'point']\n",
      "5 ['that', 'the', 'paper', 'leaves', 'unexplained']\n",
      "1 ['is']\n",
      "1 [':']\n",
      "11 ['what', 'is', 'in', 'the', 'PossibleWorld', 'Network', 'that', 'captures', 'what', 'we', 'need']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 8), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "2 ['The', 'description']\n",
      "3 ['of', 'the', 'network']\n",
      "1 ['is']\n",
      "2 ['in', 'fact']\n",
      "2 ['very', 'criptic']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 9), (5, 3), (6, 2), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "4 ['No', 'examples', 'are', 'given']\n",
      "1 ['and']\n",
      "8 ['a', 'major', 'effort', 'is', 'required', 'to', 'the', 'reader']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 5), (5, 2), (6, 3), (7, 3), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['Can']\n",
      "1 ['you']\n",
      "11 ['provide', 'examples', 'and', 'insights', 'on', 'why', 'this', 'is', 'THE', 'needed', 'model']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 2), (5, 2), (6, 5), (7, 2), (8, 3), (9, 2), (10, 2), (11, 2), (12, 3), (13, 4), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['Finally']\n",
      "1 [',']\n",
      "2 ['the', 'paper']\n",
      "17 ['does', 'not', 'discuss', 'a', 'large', 'body', 'of', 'research', 'that', 'has', 'been', 'done', 'in', 'the', 'past', 'by', 'Plate']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 3), (7, 4), (8, 2), (9, 2), (10, 2), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  7\n",
      "1 ['symbolic']\n",
      "1 ['predicates']\n",
      "1 ['can']\n",
      "5 ['be', 'described', 'in', 'distributed', 'representations']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 2), (6, 2), (7, 3), (8, 2), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['is']\n",
      "1 ['strictly']\n",
      "7 ['related', 'to', 'the', 'problem', 'this', 'paper', 'investigates']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 5), (5, 9), (6, 10), (7, 14), (8, 12), (9, 10), (10, 8), (11, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "24 ['As', 'discussed', 'in', '\"', 'Symbolic', ',', 'Distributed', 'and', 'Distributional', 'Representations', 'for', 'Natural', 'Language', 'Processing', 'in', 'the', 'Era', 'of', 'Deep', 'Learning', ':', 'a', 'Survey', '\"']\n",
      "1 [',']\n",
      "9 ['2017', ',', 'the', 'link', 'between', 'symbolic', 'and', 'distributed', 'representations']\n",
      "12 ['has', 'to', 'be', 'better', 'investigated', 'in', 'order', 'to', 'propose', 'innovative', 'NN', 'models']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 3), (7, 2), (8, 6), (9, 2), (10, 2), (11, 2), (12, 3), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['Your']\n",
      "1 ['paper']\n",
      "1 ['can']\n",
      "14 ['be', 'one', 'of', 'the', 'first', 'NN', 'model', 'that', 'takes', 'advantage', 'of', 'this', 'strict', 'link']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 6), (6, 4), (7, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  5\n",
      "1 ['-']\n",
      "1 ['There']\n",
      "1 ['no']\n",
      "1 ['comparisons']\n",
      "1 ['with']\n",
      "6 ['baseline', 'models', 'or', 'different', 'model', 'architectures']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 1), (6, 2), (7, 6), (8, 7), (9, 13), (10, 9), (11, 10), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  7\n",
      "13 ['see', 'some', 'results', 'on', 'the', 'same', 'structure', ',', 'but', 'with', 'an', 'Linear', 'model']\n",
      "1 [',']\n",
      "7 ['MLP', 'or', 'LSTM', 'across', 'the', 'time', 'dimension']\n",
      "1 [',']\n",
      "1 ['or']\n",
      "7 ['search', 'through', 'different', 'types', 'of', 'convolutional', 'networks']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 8), (5, 9), (6, 4), (7, 2), (8, 1), (9, 3), (10, 5), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['Thousands']\n",
      "2 ['of', 'actions']\n",
      "1 ['is']\n",
      "1 ['not']\n",
      "7 ['a', 'too', 'large', 'number', '-', 'language', 'modeling']\n",
      "10 ['work', 'routinely', 'deals', 'with', 'outputting', 'many', 'more', 'classes', 'than', 'that']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 6)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['-']\n",
      "1 ['Were']\n",
      "2 ['the', 'convolutions']\n",
      "7 ['chosen', '1D', ',', '2D', ',', 'or', '3D']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 1), (5, 2), (6, 2), (7, 2), (8, 2), (9, 6), (10, 4), (11, 5), (12, 3), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['figure']\n",
      "1 ['seems']\n",
      "17 ['to', 'imply', 'that', 'the', 'convolutions', 'were', 'over', 'the', 'XZ', 'dimensions', ',', 'with', 'Y', 'as', 'the', 'channel', 'dimension']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 11), (5, 8), (6, 11), (7, 7), (8, 10), (9, 4), (10, 4), (11, 2), (12, 7), (13, 2), (14, 3), (15, 2), (16, 2), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "20 ['If', 'so', ',', 'this', 'does', \"n't\", 'make', 'too', 'much', 'sense', 'to', 'be', ',', 'since', 'the', 'Z', 'dimension', 'is', 'not', 'uniform']\n",
      "1 ['-']\n",
      "25 ['the', 'last', 'index', 'is', 'all', 'unseen', 'cards', ',', 'which', 'is', 'significantly', 'more', 'than', 'the', 'middle', 'indices', 'of', '\"', 'what', 'was', 'played', 'in', 'this', 'round', '\"']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 4), (7, 2), (8, 4), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['There']\n",
      "1 ['should']\n",
      "1 [\"n't\"]\n",
      "10 ['be', 'a', 'lot', 'of', 'translational', 'invariance', 'in', 'the', 'Z', 'dimension']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 2), (6, 2), (7, 4), (8, 2), (9, 2), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 [\"'m\"]\n",
      "1 ['also']\n",
      "1 ['not']\n",
      "10 ['convinced', 'that', 'translational', 'invariance', 'is', 'helpful', 'in', 'the', 'X', 'dimension']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 4), (6, 3), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "2 ['-', 'What']\n",
      "1 ['hyperparameters']\n",
      "1 ['were']\n",
      "6 ['searched', 'through', 'in', 'the', 'learning', 'process']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 8), (5, 9), (6, 3), (7, 2), (8, 4), (9, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "3 ['-', 'Missing', 'citations']\n",
      "2 ['for', 'MicroWe']\n",
      "1 ['being']\n",
      "15 ['the', 'best', 'CCP', 'AI', ',', 'and', 'citations', 'for', 'the', 'accomplishments', 'of', 'the', 'top', 'amateur', 'players']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 5), (6, 2), (7, 6), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  5\n",
      "1 ['far']\n",
      "1 ['away']\n",
      "1 ['are']\n",
      "7 ['the', 'top', 'amateur', 'players', 'from', 'professional', 'players']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 2), (6, 4), (7, 4), (8, 2), (9, 1), (10, 2), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['Please']\n",
      "1 ['provide']\n",
      "2 ['some', 'context']\n",
      "9 ['on', 'how', 'far', 'this', 'system', 'is', 'from', 'solving', 'CCP']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 7), (5, 3), (6, 5), (7, 4), (8, 2), (9, 2), (10, 4), (11, 2), (12, 2), (13, 4), (14, 6), (15, 4), (16, 10), (17, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  3\n",
      "3 ['-', 'Figure', '3']\n",
      "1 [',']\n",
      "1 ['4']\n",
      "1 ['should']\n",
      "1 ['just']\n",
      "30 ['say', '#of', 'games', 'instead', 'of', '\"', 'iteration', '\"', 'This', 'paper', 'shows', 'that', 'one', 'choice', 'for', 'a', 'supervised', 'learning', 'system', 'on', 'a', 'CCP', 'game', 'database', 'can', 'achieve', 'amateur', 'level', 'human', 'play']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 3), (6, 2), (7, 3), (8, 4), (9, 4), (10, 6), (11, 4), (12, 7), (13, 9), (14, 3), (15, 2), (16, 1), (17, 2), (18, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['does']\n",
      "1 ['not']\n",
      "28 ['give', 'insight', 'to', 'why', 'the', 'system', 'was', 'designed', 'this', 'way', ',', 'why', 'the', 'model', 'choices', 'were', 'made', ',', 'and', 'how', 'good', 'simpler', 'baselines', 'might', 'be', 'able', 'to', 'achieve']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 7), (5, 6), (6, 5), (7, 2), (8, 2), (9, 2), (10, 2), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "5 ['is', 'not', 'clearly', 'written', 'enough']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "12 ['does', 'not', 'provide', 'enough', 'scientific', 'value', 'to', 'be', 'accepted', 'to', 'the', 'conference']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 4), (6, 2), (7, 2), (8, 1), (9, 2), (10, 2), (11, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['propose']\n",
      "11 ['a', 'model', 'that', 'learns', 'to', 'play', 'the', 'China', 'Competitive', 'Poker', 'game']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 4), (5, 4), (6, 5), (7, 6), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['model']\n",
      "6 ['uses', 'CNN', 'to', 'predict', 'the', 'actions']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "7 ['is', 'trained', 'from', 'actual', 'human', 'game', 'records']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 1), (6, 2), (7, 2), (8, 3), (9, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['model']\n",
      "1 ['is']\n",
      "11 ['shown', 'to', 'beat', 'the', 'current', 'best', 'AI', 'and', 'human', 'amateur', 'players']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 6), (5, 2), (6, 3), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['performance']\n",
      "1 ['is']\n",
      "1 ['certainly']\n",
      "1 ['strong']\n",
      "6 ['-LRB-', 'if', 'it', 'were', 'true', '-RRB-']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 6), (3, 9), (4, 6), (5, 4), (6, 10), (7, 2), (8, 4), (9, 4), (10, 1), (11, 2), (12, 2), (13, 2), (14, 4), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "1 ['But']\n",
      "23 ['given', 'the', 'double', '-', 'blinded', 'policy', ',', 'there', 'is', 'literally', 'no', 'way', 'to', 'verify', 'the', 'correctness', 'of', 'the', 'performance', '---', 'in', 'other', 'words']\n",
      "1 [',']\n",
      "2 ['the', 'paper']\n",
      "6 ['is', 'currently', 'not', 'reproducible', 'at', 'all']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 2), (5, 2), (6, 2), (7, 6), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['So']\n",
      "3 ['the', 'following', 'comments']\n",
      "10 ['are', 'based', 'on', 'the', 'trust', '-', 'worthiness', 'of', 'the', 'paper']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 5), (5, 4), (6, 3), (7, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "3 ['-LRB-', '1', '-RRB-']\n",
      "2 ['immature', 'writing']\n",
      "1 [':']\n",
      "11 ['The', 'writing', 'lacks', 'formality', 'and', 'looks', 'like', 'a', 'final', 'project', 'report']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 8), (5, 8), (6, 2), (7, 2), (8, 2), (9, 2), (10, 5), (11, 2), (12, 3), (13, 3), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "10 ['For', 'instance', ',', 'the', 'super-short', 'Section', '2', 'is', 'rather', 'unprofessional']\n",
      "1 ['---']\n",
      "1 ['it']\n",
      "15 ['is', 'hard', 'to', 'believe', 'that', 'the', 'related', 'works', 'can', 'be', 'described', 'within', 'two', 'paragraphs', 'anyway']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 10), (5, 8), (6, 10), (7, 5), (8, 6), (9, 5), (10, 4), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "11 ['Even', 'as', 'someone', 'who', 'understands', 'the', 'game', 'of', 'China', 'Competitive', 'Poker']\n",
      "1 [',']\n",
      "9 ['I', 'find', 'it', 'very', 'hard', 'to', 'follow', 'Section', '3']\n",
      "1 ['-RRB-']\n",
      "10 ['There', 'is', 'a', 'big', 'room', 'for', 'improving', 'the', 'English', 'writing']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 9), (5, 8), (6, 8), (7, 4), (8, 2), (9, 2), (10, 2), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "3 ['-LRB-', '2', '-RRB-']\n",
      "7 ['ill', '-', 'illustrated', 'specialty', 'of', 'the', 'model']\n",
      "1 [':']\n",
      "17 ['In', 'particular', ',', 'it', 'is', 'not', 'clear', 'why', 'the', 'model', 'should', 'be', 'superior', 'than', 'other', 'modeling', 'choices']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 4), (5, 5), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "2 ['For', 'instance']\n",
      "1 [',']\n",
      "2 ['what', 'role']\n",
      "7 ['does', 'the', 'neighboring', 'connections', 'of', 'CNNs', 'play']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 6), (6, 1), (7, 2), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  5\n",
      "1 ['the']\n",
      "1 ['cons']\n",
      "1 ['and']\n",
      "1 ['pros']\n",
      "1 ['of']\n",
      "2 ['choosing', 'CNNs']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 3), (5, 2), (6, 2), (7, 2), (8, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['Are']\n",
      "1 ['there']\n",
      "8 ['strong', 'motivations', 'to', 'design', 'the', 'model', 'this', 'way']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 9), (5, 2), (6, 4), (7, 2), (8, 2), (9, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "6 ['-LRB-', '3', '-RRB-', 'many', 'unanswered', 'mysteries']\n",
      "1 [':']\n",
      "10 ['why', 'does', 'the', 'model', 'trained', 'with', 'human', 'records', 'readily', 'super-human']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 2), (5, 2), (6, 4), (7, 4), (8, 5), (9, 8), (10, 2), (11, 2), (12, 3), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  6\n",
      "1 ['this']\n",
      "1 ['is']\n",
      "5 ['controversial', 'to', 'common', 'imitation', 'learning']\n",
      "12 ['where', 'the', 'typical', 'performance', 'is', 'bound', 'by', 'the', 'human', '-', 'level', 'performance']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 6), (5, 4), (6, 5), (7, 4), (8, 4), (9, 4), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "15 ['Even', 'though', 'authors', 'claimed', 'in', 'the', 'response', 'that', 'there', 'are', '\"', 'many', 'professional', 'records', '\"']\n",
      "1 ['---']\n",
      "1 ['but']\n",
      "4 ['how', 'many', 'is', 'many']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 4), (5, 5), (6, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['Did']\n",
      "2 ['the', 'authors']\n",
      "10 ['analyze', 'the', 'records', 'and', 'separate', 'the', 'professional', 'versus', 'amateur', 'ones']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 1), (6, 2), (7, 2), (8, 2), (9, 3), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['provides']\n",
      "9 ['a', 'system', 'to', 'play', 'CCP', 'using', 'some', 'deep', 'learning']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 3), (6, 6), (7, 6), (8, 10), (9, 8), (10, 4), (11, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['system']\n",
      "1 ['consists']\n",
      "26 ['of', 'three', 'modules', '-', 'the', 'bid', 'module', ',', 'which', 'is', 'rule', 'based', ',', 'and', 'the', 'policy', 'and', 'kicker', 'networks', ',', 'which', 'are', 'simple', 'convolutional', 'neural', 'networks']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 5), (5, 6), (6, 7), (7, 2), (8, 5), (9, 4), (10, 4), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "15 ['use', 'a', 'dataset', 'of', '8', 'million', 'game', 'records', 'consisting', 'of', '80', 'million', 'state', 'action', 'pairs']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "7 ['train', 'the', 'network', 'in', 'a', 'supervised', 'fashion']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 13), (5, 11), (6, 4), (7, 4), (8, 6), (9, 9), (10, 6), (11, 9), (12, 1), (13, 2), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "31 ['The', 'resulting', 'model', 'is', 'able', 'beat', 'MicroWe', ',', 'the', 'current', 'state', 'of', 'the', 'art', 'in', 'playing', 'CCP', ',', 'and', 'even', 'are', 'able', 'to', 'beat', 'a', 'few', '\"', 'top', 'amateur', 'players', '\"']\n",
      "1 ['-']\n",
      "8 ['Why', 'is', 'the', 'bid', 'module', 'also', 'not', 'learned']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 8), (5, 8), (6, 6), (7, 6), (8, 10), (9, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "13 ['It', 'seems', 'like', 'the', 'feature', 'set', 'for', 'the', 'bid', 'module', 'is', 'fairly', 'simple']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "14 ['a', 'linear', 'or', 'MLP', 'can', 'do', 'fairly', 'well', 'compared', 'to', 'a', 'rule', 'based', 'module']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 4), (6, 4), (7, 4), (8, 6), (9, 6), (10, 1), (11, 2), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "2 ['-', 'It']\n",
      "1 [\"'s\"]\n",
      "1 ['not']\n",
      "1 ['clear']\n",
      "14 ['that', 'separating', 'the', 'policy', 'and', 'kicker', 'networks', 'would', 'be', 'more', 'advantageous', 'than', 'combining', 'them']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 5), (6, 6), (7, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['Would']\n",
      "3 ['the', 'approach', 'work']\n",
      "13 ['as', 'well', 'using', 'a', 'more', 'standard', 'encoder', '-', 'decoder', 'model', 'with', 'determinstic', 'Z']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 11), (5, 8), (6, 6), (7, 4), (8, 7), (9, 8), (10, 4), (11, 2), (12, 3), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "13 ['The', 'data', 'is', 'derived', 'from', 'an', 'online', 'repository', 'of', '~', '1500', 'Android', 'apps']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "20 ['from', 'that', 'were', 'extracted', '~', '150k', 'methods', ',', 'which', 'makes', 'the', 'data', 'very', 'respectable', 'in', 'terms', 'of', 'realisticness', 'and', 'scale']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 4), (6, 2), (7, 4), (8, 2), (9, 5), (10, 2), (11, 2), (12, 2), (13, 1), (14, 2), (15, 3), (16, 3), (17, 2), (18, 5), (19, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['introduce']\n",
      "22 ['an', 'algorithm', 'in', 'the', 'subfield', 'of', 'conditional', 'program', 'generation', 'that', 'is', 'able', 'to', 'create', 'programs', 'in', 'a', 'rich', 'java', 'like', 'programming', 'language']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 6), (5, 9), (6, 6), (7, 6), (8, 2), (9, 3), (10, 4), (11, 4), (12, 5), (13, 2), (14, 3), (15, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "3 ['In', 'this', 'setting']\n",
      "1 [',']\n",
      "1 ['they']\n",
      "27 ['propose', 'an', 'algorithm', 'based', 'on', 'sketches', '-', 'abstractions', 'of', 'programs', 'that', 'capture', 'the', 'structure', 'but', 'discard', 'program', 'specific', 'information', 'that', 'is', 'not', 'generalizable', 'such', 'as', 'variable', 'names']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 6), (5, 8), (6, 2), (7, 4), (8, 2), (9, 6), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "12 ['Conditioned', 'on', 'information', 'such', 'as', 'type', 'specification', 'or', 'keywords', 'of', 'a', 'method']\n",
      "1 ['they']\n",
      "9 ['generate', 'the', 'method', \"'s\", 'body', 'from', 'the', 'trained', 'sketches']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 10), (5, 12), (6, 12), (7, 13), (8, 9), (9, 6), (10, 2), (11, 3), (12, 3), (13, 6), (14, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['Positives']\n",
      "1 [':']\n",
      "23 ['•', 'Novel', 'algorithm', 'and', 'addition', 'of', 'rich', 'java', 'like', 'language', 'in', 'subfield', 'of', \"'\", 'conditional', 'program', 'generation', \"'\", 'proposed', '•', 'Very', 'good', 'abstract']\n",
      "1 [':']\n",
      "24 ['It', 'explains', 'high', 'level', 'overview', 'of', 'topic', 'and', 'sets', 'it', 'into', 'context', 'plus', 'gives', 'a', 'sketch', 'of', 'the', 'algorithm', 'and', 'presents', 'the', 'positive', 'results']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 4), (5, 4), (6, 4), (7, 7), (8, 5), (9, 5), (10, 4), (11, 4), (12, 6), (13, 7), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['•']\n",
      "1 ['Excellently']\n",
      "29 ['structured', 'and', 'presented', 'paper', '•', 'Motivation', 'given', 'in', 'form', 'of', 'relevant', 'applications', 'and', 'mention', 'that', 'it', 'is', 'relatively', 'unstudied', '•', 'The', 'hypothesis', '/', 'the', 'papers', 'goal', 'is', 'clearly', 'stated']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 4), (6, 6), (7, 4), (8, 5), (9, 4), (10, 3), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  5\n",
      "1 ['with']\n",
      "4 [\"'\", 'We', 'ask', \"'\"]\n",
      "1 ['followed']\n",
      "10 ['by', 'two', 'well', 'formulated', 'lines', 'that', 'make', 'up', 'the', 'hypothesis']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 4), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  5\n",
      "1 ['multiple']\n",
      "1 ['times']\n",
      "1 ['throughout']\n",
      "2 ['the', 'paper']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 4), (5, 5), (6, 6), (7, 2), (8, 3), (9, 4), (10, 5), (11, 4), (12, 2), (13, 3), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['Every']\n",
      "1 ['mention']\n",
      "1 ['introduces']\n",
      "17 ['either', 'a', 'new', 'argument', 'on', 'why', 'this', 'is', 'necessary', 'or', 'sets', 'it', 'in', 'contrast', 'to', 'other', 'learners']\n",
      "1 [',']\n",
      "3 ['clearly', 'stating', 'discrepancies']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 2), (6, 3), (7, 5), (8, 2), (9, 3), (10, 2), (11, 3), (12, 2), (13, 2), (14, 2), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  3\n",
      "2 ['•', 'Explanations']\n",
      "14 ['are', 'exceptionally', 'well', 'done', ':', 'terms', 'that', 'might', 'not', 'be', 'familiar', 'to', 'the', 'reader']\n",
      "1 ['are']\n",
      "1 ['explained']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 3), (7, 3), (8, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  8\n",
      "1 ['mathematical']\n",
      "1 ['aspects']\n",
      "1 ['as']\n",
      "1 ['well']\n",
      "1 ['as']\n",
      "1 ['program']\n",
      "1 ['generating']\n",
      "2018_HkfXMz-Ab 554\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 6), (5, 4), (6, 2), (7, 3), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['present']\n",
      "5 ['a', 'very', 'intriguing', 'novel', 'approach']\n",
      "7 ['that', 'in', 'a', 'clear', 'and', 'coherent', 'way']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 2), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['approach']\n",
      "1 ['is']\n",
      "1 ['thoroughly']\n",
      "5 ['explained', 'for', 'a', 'large', 'audience']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 3)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "2 ['The', 'task']\n",
      "1 ['itself']\n",
      "4 ['is', 'interesting', 'and', 'novel']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 11), (5, 4), (6, 7), (7, 6), (8, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "4 ['The', 'large', 'evaluation', 'section']\n",
      "5 ['that', 'discusses', 'many', 'different', 'properties']\n",
      "1 ['is']\n",
      "3 ['a', 'further', 'indication']\n",
      "11 ['that', 'this', 'approach', 'is', 'not', 'only', 'novel', 'but', 'also', 'very', 'promising']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 4), (5, 7), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "7 ['Even', 'though', 'no', 'conclusive', 'section', 'is', 'provided']\n",
      "1 [',']\n",
      "2 ['the', 'paper']\n",
      "5 ['is', 'not', 'missing', 'any', 'information']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 1), (5, 2), (6, 3), (7, 3), (8, 2), (9, 4), (10, 7), (11, 9), (12, 6), (13, 7), (14, 2), (15, 4), (16, 2), (17, 2), (18, 2), (19, 2), (20, 2), (21, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['aims']\n",
      "32 ['to', 'synthesize', 'programs', 'in', 'a', 'Java', '-', 'like', 'language', 'from', 'a', 'task', 'description', '-LRB-', 'X', '-RRB-', 'that', 'includes', 'some', 'names', 'and', 'types', 'of', 'the', 'components', 'that', 'should', 'be', 'used', 'in', 'the', 'program']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 6), (7, 5), (8, 5), (9, 8), (10, 7), (11, 9), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['argues']\n",
      "25 ['that', 'it', 'is', 'too', 'difficult', 'to', 'map', 'directly', 'from', 'the', 'description', 'to', 'a', 'full', 'program', ',', 'so', 'it', 'instead', 'formulates', 'the', 'synthesis', 'in', 'two', 'parts']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 2), (5, 2), (6, 2), (7, 6), (8, 8), (9, 8), (10, 7), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['First']\n",
      "1 [',']\n",
      "2 ['the', 'description']\n",
      "25 ['is', 'mapped', 'to', 'a', '\"', 'sketch', '\"', '-LRB-', 'Y', '-RRB-', 'containing', 'high', 'level', 'program', 'structure', 'but', 'no', 'concrete', 'details', 'about', ',', 'e.g.', ',', 'variable', 'names']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 4), (6, 6), (7, 8), (8, 3), (9, 5), (10, 2), (11, 4), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['Afterwards']\n",
      "1 [',']\n",
      "2 ['the', 'sketch']\n",
      "22 ['is', 'converted', 'into', 'a', 'full', 'program', '-LRB-', 'Prog', '-RRB-', 'by', 'stochastically', 'filling', 'in', 'the', 'abstract', 'parts', 'of', 'the', 'sketch', 'with', 'concrete', 'instantiations']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 6), (6, 15), (7, 9), (8, 11), (9, 10), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['presents']\n",
      "34 ['an', 'abstraction', 'method', 'for', 'converting', 'a', 'program', 'into', 'a', 'sketch', ',', 'a', 'stochastic', 'encoder', '-', 'decoder', 'model', 'for', 'converting', 'descriptions', 'to', 'trees', ',', 'and', 'rejection', 'sampling', '-', 'like', 'approach', 'for', 'converting', 'sketches', 'to', 'programs']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 2), (5, 2), (6, 2), (7, 3), (8, 6), (9, 5), (10, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['Experimentally']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "16 ['is', 'shown', 'that', 'using', 'sketches', 'as', 'an', 'intermediate', 'abstraction', 'outperforms', 'directly', 'mapping', 'to', 'the', 'program', 'AST']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 3), (6, 2), (7, 5), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  7\n",
      "1 ['the']\n",
      "1 ['strongest']\n",
      "1 ['points']\n",
      "1 ['of']\n",
      "2 ['the', 'paper']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 6), (6, 8), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "2 ['One', 'point']\n",
      "3 ['I', 'found', 'confusing']\n",
      "1 ['is']\n",
      "7 ['how', 'exactly', 'the', 'Combinatorial', 'Concretization', 'step', 'works']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 2), (5, 1), (6, 2), (7, 4), (8, 4), (9, 6), (10, 10), (11, 7), (12, 3), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Am']\n",
      "1 ['I']\n",
      "22 ['correct', 'in', 'understanding', 'that', 'this', 'step', 'depends', 'only', 'on', 'Y', ',', 'and', 'that', 'given', 'Y', ',', 'Prog', 'is', 'conditionally', 'independent', 'of', 'X']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 6), (6, 3), (7, 2), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "4 ['If', 'this', 'is', 'correct']\n",
      "1 [',']\n",
      "9 ['how', 'many', 'Progs', 'are', 'consistent', 'with', 'a', 'typical', 'Y']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 7), (5, 3), (6, 3), (7, 4), (8, 2), (9, 2), (10, 3), (11, 2), (12, 3), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "3 ['Some', 'additional', 'discussion']\n",
      "15 ['of', 'why', 'no', 'learning', 'is', 'required', 'for', 'the', 'P', '-LRB-', 'Prog', '|', 'Y', '-RRB-', 'step']\n",
      "1 ['would']\n",
      "2 ['be', 'appreciated']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 2), (6, 2), (7, 3), (8, 4), (9, 7), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 [\"'m\"]\n",
      "1 ['also']\n",
      "12 ['curious', 'whether', 'using', 'a', 'stochastic', 'latent', 'variable', '-LRB-', 'Z', '-RRB-', 'is', 'necessary']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 7), (5, 5), (6, 5), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "2 ['Some', 'discussion']\n",
      "9 ['of', 'Grammar', 'Variational', 'Autoencoder', '-LRB-', 'Kusner', 'et', 'al', '-RRB-']\n",
      "1 ['would']\n",
      "1 ['probably']\n",
      "2 ['be', 'appropriate']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 4), (5, 2), (6, 4), (7, 2), (8, 1), (9, 2), (10, 3), (11, 4), (12, 2), (13, 3), (14, 2), (15, 3), (16, 5), (17, 4), (18, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  2\n",
      "1 ['Overall']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "1 ['really']\n",
      "25 ['like', 'the', 'fact', 'that', 'this', 'paper', 'is', 'aiming', 'to', 'do', 'program', 'synthesis', 'on', 'programs', 'that', 'are', 'more', 'like', 'those', 'found', '\"', 'in', 'the', 'wild', '\"']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 9), (5, 5), (6, 10), (7, 15), (8, 18), (9, 13), (10, 16), (11, 14), (12, 13), (13, 11), (14, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "56 ['While', 'the', 'general', 'pattern', 'of', 'mapping', 'a', 'specification', 'to', 'abstraction', 'with', 'a', 'neural', 'net', 'and', 'then', 'mapping', 'the', 'abstraction', 'to', 'a', 'full', 'program', 'with', 'a', 'combinatorial', 'technique', 'is', 'not', 'necessarily', 'novel', ',', 'I', 'think', 'this', 'paper', 'adds', 'an', 'interesting', 'new', 'take', 'on', 'the', 'pattern', '-LRB-', 'it', 'has', 'a', 'very', 'different', 'abstraction', 'than', 'say', ',', 'DeepCoder', '-RRB-']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "21 ['this', 'paper', 'is', 'one', 'of', 'the', 'more', 'interesting', 'recent', 'papers', 'on', 'program', 'synthesis', 'using', 'machine', 'learning', 'techniques', ',', 'in', 'my', 'opinion']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 5), (6, 5), (7, 8), (8, 2), (9, 4), (10, 2), (11, 5), (12, 8), (13, 5), (14, 3), (15, 1), (16, 2), (17, 2), (18, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  5\n",
      "1 ['a']\n",
      "7 ['very', 'well', '-', 'written', 'and', 'nicely', 'structured']\n",
      "1 ['paper']\n",
      "1 ['that']\n",
      "21 ['tackles', 'the', 'problem', 'of', 'generating', '/', 'inferring', 'code', 'given', 'an', 'incomplete', 'description', '-LRB-', 'sketch', '-RRB-', 'of', 'the', 'task', 'to', 'be', 'achieved']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 5), (6, 3), (7, 1), (8, 2), (9, 3), (10, 2), (11, 2), (12, 4), (13, 2), (14, 2), (15, 2), (16, 2), (17, 1), (18, 2), (19, 2), (20, 2), (21, 5), (22, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "depth:  5\n",
      "1 ['a']\n",
      "1 ['novel']\n",
      "1 ['contribution']\n",
      "1 ['to']\n",
      "19 ['existing', 'machine', 'learning', 'approaches', 'to', 'automated', 'programming', 'that', 'is', 'achieved', 'by', 'training', 'on', 'a', 'large', 'corpus', 'of', 'Android', 'apps']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 8), (6, 7), (7, 7), (8, 4), (9, 2), (10, 4), (11, 2), (12, 2), (13, 2), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "2 ['The', 'combination']\n",
      "9 ['of', 'the', 'proposed', 'technique', 'and', 'leveraging', 'of', 'real', 'data']\n",
      "1 ['are']\n",
      "14 ['a', 'substantial', 'strength', 'of', 'the', 'work', 'compared', 'to', 'many', 'approaches', 'that', 'have', 'come', 'previously']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 11), (6, 10), (7, 19), (8, 13), (9, 13), (10, 17), (11, 18), (12, 13), (13, 7), (14, 13), (15, 9), (16, 6), (17, 7), (18, 4), (19, 4), (20, 6), (21, 3), (22, 7), (23, 3), (24, 1), (25, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['has']\n",
      "104 ['many', 'strengths', ':', '1', '-RRB-', 'The', 'writing', 'is', 'clear', ',', 'and', 'the', 'paper', 'is', 'well', '-', 'motivated', '2', '-RRB-', 'The', 'proposed', 'algorithm', 'is', 'described', 'in', 'excellent', 'detail', ',', 'which', 'is', 'essential', 'to', 'reproducibility', '3', '-RRB-', 'As', 'stated', 'previously', ',', 'the', 'approach', 'is', 'validated', 'with', 'a', 'large', 'number', 'of', 'real', 'Android', 'projects', '4', '-RRB-', 'The', 'fact', 'that', 'the', 'language', 'generated', 'is', 'non-trivial', '-LRB-', 'Java', '-', 'like', '-RRB-', 'is', 'a', 'substantial', 'plus', '5', '-RRB-', 'Good', 'discussion', 'of', 'limitations', 'Overall', ',', 'this', 'paper', 'is', 'a', 'valuable', 'addition', 'to', 'the', 'empirical', 'software', 'engineering', 'community', ',', 'and', 'a', 'nice', 'break', 'from', 'more', 'traditional', 'approaches', 'of', 'learning', 'abstract', 'syntax', 'trees']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 5), (5, 4), (6, 3), (7, 4), (8, 6), (9, 6), (10, 4), (11, 5), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "9 ['To', 'demonstrate', 'that', 'this', 'is', 'a', 'broadly', 'applicable', 'family']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "14 ['would', 'also', 'be', 'good', 'to', 'do', 'experiments', 'on', 'a', 'more', 'standard', 'datasets', 'like', 'MNIST']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 10), (5, 7), (6, 7), (7, 6), (8, 2), (9, 2), (10, 3), (11, 3), (12, 2), (13, 2), (14, 2), (15, 6), (16, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "9 ['The', 'experimental', 'results', 'are', 'very', 'good', 'for', 'document', 'modeling']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "24 ['without', 'ablation', 'analysis', 'against', 'the', 'baseline', 'it', 'is', 'hard', 'to', 'see', 'why', 'they', 'should', 'be', 'with', 'such', 'a', 'small', 'modification', 'in', 'G', '-', 'NVDM']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 8), (5, 9), (6, 6), (7, 10), (8, 4), (9, 2), (10, 5), (11, 2), (12, 5), (13, 3), (14, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "17 ['Overall', ',', 'the', 'very', 'strong', 'improvements', 'on', 'the', 'text', 'modeling', 'task', 'over', 'NVDM', 'seem', 'hard', 'to', 'understand']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "19 ['I', 'would', 'like', 'to', 'see', 'an', 'ablation', 'analysis', 'of', 'all', 'the', 'differences', 'between', 'that', 'model', 'and', 'the', 'proposed', 'one']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 7), (5, 6), (6, 7), (7, 5), (8, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "10 ['As', 'the', 'prior', 'is', 'learned', 'together', 'with', 'the', 'variational', 'posterior']\n",
      "1 [',']\n",
      "4 ['a', 'more', 'flexible', 'prior']\n",
      "9 ['would', 'alleviate', 'the', 'regularisation', 'imposed', 'by', 'the', 'KL', 'term']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 4), (5, 4), (6, 2), (7, 6), (8, 4), (9, 3), (10, 5), (11, 10), (12, 9), (13, 9), (14, 3), (15, 2), (16, 1), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "1 ['Certainly']\n",
      "1 [',']\n",
      "2 ['more', 'parameters']\n",
      "35 ['are', 'applied', 'as', 'well', ',', 'so', 'a', 'fair', 'comparison', 'would', 'at', 'least', 'be', 'z', '=', '<', 'z_gaussian', ',', 'z_piecewise', '>', 'and', 'z', '=', '<', 'z_gaussian1', ',', 'z_gaussian2', '>', 'which', 'equals', 'to', 'a', 'double', 'sized', 'z_gaussian']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 5), (5, 2), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['--']\n",
      "6 ['The', 'results', 'shown', 'in', 'Table', '3']\n",
      "2 ['are', 'implausible']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 5), (7, 2), (8, 2), (9, 2), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['can']\n",
      "1 ['not']\n",
      "9 ['believe', 'the', 'author', 'used', 'gradients', 'to', 'evaluate', 'the', 'model']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 2), (5, 2), (6, 1), (7, 2), (8, 5), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['--']\n",
      "2 ['Equation', '5']\n",
      "9 ['is', 'confusing', ',', 'adding', 'a', 'multiplication', 'sign', 'might', 'help']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 2), (5, 2), (6, 2), (7, 2), (8, 4), (9, 5), (10, 3), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['--']\n",
      "1 ['3.1']\n",
      "11 ['can', 'be', 'deleted', 'because', 'people', 'attending', 'ICLR', 'are', 'familiar', 'with', 'VAEs']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 5), (5, 4), (6, 3), (7, 7), (8, 7), (9, 4), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['as']\n",
      "1 ['well']\n",
      "1 ['as']\n",
      "2 ['the', 'well']\n",
      "12 ['as', 'the', 'generated', 'prior', '-', '>', 'as', 'well', 'as', 'the', 'generated', 'prior']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 3), (5, 2), (6, 4), (7, 5), (8, 8), (9, 2), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['UPDATE']\n",
      "1 [':']\n",
      "17 ['I', 'have', 'read', 'the', 'authors', \"'\", 'rebuttal', 'and', 'also', 'the', 'other', 'comments', 'in', 'this', 'paper', \"'s\", 'thread']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 1)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['My']\n",
      "1 ['thoughts']\n",
      "1 ['have']\n",
      "1 ['not']\n",
      "1 ['changed']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 1), (5, 3), (6, 4), (7, 3), (8, 2), (9, 5), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['propose']\n",
      "14 ['using', 'a', 'mixture', 'prior', 'rather', 'than', 'a', 'uni-modal', 'prior', 'for', 'variational', 'auto', '-', 'encoders']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 2), (5, 3), (6, 6), (7, 2), (8, 5), (9, 2), (10, 4), (11, 3), (12, 2), (13, 5), (14, 3), (15, 2), (16, 4), (17, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "1 ['They']\n",
      "28 ['argue', 'that', 'the', 'simple', 'uni-modal', 'prior', '\"', 'hinders', 'the', 'overall', 'expressivity', 'of', 'the', 'learned', 'model', 'as', 'it', 'can', 'not', 'possibly', 'capture', 'more', 'complex', 'aspects', 'of', 'the', 'data', 'distribution']\n",
      "1 ['.']\n",
      "1 ['\"']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 7), (6, 12), (7, 5), (8, 4), (9, 2), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['find']\n",
      "6 ['the', 'motivation', 'of', 'the', 'paper', 'suspicious']\n",
      "14 ['because', 'while', 'the', 'prior', 'may', 'be', 'uni-modal', ',', 'the', 'posterior', 'distribution', 'is', 'certainly', 'not']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 9), (5, 6), (6, 2), (7, 4), (8, 3), (9, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Furthermore']\n",
      "1 [',']\n",
      "8 ['a', 'uni-modal', 'distribution', 'on', 'the', 'latent', 'variable', 'space']\n",
      "13 ['can', 'certainly', 'still', 'lead', 'to', 'the', 'capturing', 'of', 'complex', ',', 'multi-modal', 'data', 'distributions']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 8), (5, 9), (6, 7), (7, 6), (8, 5), (9, 5), (10, 2), (11, 1), (12, 3), (13, 5), (14, 5), (15, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "1 ['-LRB-']\n",
      "16 ['As', 'the', 'most', 'trivial', 'case', ',', 'take', 'the', 'latent', 'variable', 'space', 'to', 'be', 'a', 'uniform', 'distribution']\n",
      "1 [';']\n",
      "21 ['take', 'the', 'likelihood', 'to', 'be', 'a', 'point', 'mass', 'given', 'by', 'applying', 'the', 'true', 'data', 'distribution', \"'s\", 'inverse', 'CDF', 'to', 'the', 'uniform']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "3 ['Such', 'a', 'model']\n",
      "4 ['can', 'capture', 'any', 'distribution']\n",
      "1 ['.']\n",
      "1 ['-RRB-']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 6), (6, 3), (7, 4), (8, 3), (9, 8), (10, 8), (11, 4), (12, 4), (13, 5), (14, 8), (15, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "2 ['In', 'addition']\n",
      "1 [',']\n",
      "1 ['multi-modality']\n",
      "35 ['is', 'arguably', 'an', 'overfocused', 'concept', 'in', 'the', 'literature', ',', 'where', 'the', '-LRB-', 'latent', 'variable', '-RRB-', 'space', 'is', 'hardly', 'anymore', 'worth', 'capturing', 'from', 'a', 'mixture', 'of', 'simple', 'distributions', 'when', 'it', 'is', 'often', 'a', 'complex', 'nonlinear', 'space']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 6), (6, 8), (7, 8), (8, 9), (9, 8), (10, 6), (11, 5), (12, 7), (13, 6), (14, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['is']\n",
      "4 ['unclear', 'from', 'the', 'experiments']\n",
      "39 ['how', 'much', 'the', 'influence', 'of', 'the', 'prior', \"'s\", 'multimodality', 'influences', 'the', 'posterior', 'to', 'capture', 'more', 'complex', 'phenomena', ',', 'and', 'whether', 'this', 'is', 'any', 'better', 'than', 'considering', 'a', 'more', 'complex', '-LRB-', 'but', 'still', 'reparameterizable', '-RRB-', 'distribution', 'on', 'the', 'latent', 'space']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 8), (7, 9), (8, 6), (9, 4), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  6\n",
      "1 ['this']\n",
      "1 ['paper']\n",
      "2 ['be', 'rejected']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "4 ['encourage', 'the', 'authors', 'to']\n",
      "2 ['more', 'extensively']\n",
      "6 ['study', 'the', 'effect', 'of', 'different', 'priors']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 11), (5, 9), (6, 7), (7, 6), (8, 9), (9, 7), (10, 1), (11, 2), (12, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "9 ['I', \"'d\", 'also', 'like', 'to', 'make', 'two', 'additional', 'comments']\n",
      "1 [':']\n",
      "25 ['While', 'there', 'is', 'no', 'length', 'restriction', 'at', 'ICLR', ',', 'the', '14', 'page', 'document', 'can', 'be', 'significantly', 'condensed', 'without', 'loss', 'of', 'describing', 'their', 'innovation', 'or', 'clarity']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 4), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  6\n",
      "1 ['the']\n",
      "1 ['authors']\n",
      "1 ['do']\n",
      "1 ['so']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 1), (5, 2), (6, 3), (7, 2), (8, 1), (9, 2), (10, 3), (11, 4), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['Finally']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "11 ['think', 'it', \"'s\", 'important', 'to', 'note', 'the', 'controversy', 'in', 'this', 'paper']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 4), (7, 7), (8, 9), (9, 10), (10, 4), (11, 6), (12, 4), (13, 3), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  6\n",
      "4 ['many', 'significant', 'incomplete', 'details']\n",
      "1 ['-LRB-']\n",
      "24 ['e.g.', ',', 'no', 'experiments', ',', 'many', 'missing', 'citations', ',', 'a', 'figure', 'placed', 'inside', 'that', 'was', 'pencilled', 'in', 'by', 'hand', ',', 'and', 'several', 'missing', 'paragraphs']\n",
      "1 ['-RRB-']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 5), (4, 3), (5, 5), (6, 3), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['These']\n",
      "1 ['details']\n",
      "1 ['were']\n",
      "1 ['not']\n",
      "8 ['completed', 'until', 'roughly', 'a', 'week', '-LRB-', '?', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 2)]\n",
      "[0, 1, 2]\n",
      "Depth to split: None\n",
      "depth:  None\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 5), (7, 3), (8, 2), (9, 3), (10, 2), (11, 2), (12, 2), (13, 2), (14, 2), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  6\n",
      "1 ['the']\n",
      "1 ['chairs']\n",
      "1 ['discuss']\n",
      "1 ['this']\n",
      "9 ['in', 'light', 'of', 'what', 'should', 'be', 'allowed', 'next', 'year']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 10), (6, 7), (7, 6), (8, 4), (9, 4), (10, 3), (11, 4), (12, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['introduce']\n",
      "7 ['some', 'new', 'prior', 'and', 'approximate', 'posterior', 'families']\n",
      "21 ['for', 'variational', 'autoencoders', ',', 'which', 'are', 'compatible', 'with', 'the', 'reparameterization', 'trick', ',', 'as', 'well', 'as', 'being', 'capable', 'of', 'expressing', 'multiple', 'modes']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 5), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['They']\n",
      "1 ['also']\n",
      "8 ['introduce', 'a', 'gating', 'mechanism', 'between', 'prior', 'and', 'posterior']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 2), (6, 3), (7, 4), (8, 5), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['They']\n",
      "1 ['show']\n",
      "1 ['improvements']\n",
      "11 ['on', 'bag', 'of', 'words', 'document', 'modeling', ',', 'and', 'dialogue', 'response', 'generation']\n",
      "2017_BJ9fZNqle 602\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 4), (5, 8), (6, 9), (7, 5), (8, 4), (9, 6), (10, 1), (11, 2), (12, 2), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "16 ['While', 'the', 'assertion', 'that', 'a', 'unimodal', 'latent', 'prior', 'is', 'necessary', 'to', 'model', 'multimodal', 'observations', 'is', 'false']\n",
      "1 [',']\n",
      "1 ['there']\n",
      "10 ['are', 'sensible', 'motivations', 'for', 'the', 'piecewise', 'constant', 'prior', 'and', 'posterior']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 8), (3, 8), (4, 7), (5, 9), (6, 8), (7, 9), (8, 10), (9, 6), (10, 3), (11, 4), (12, 6), (13, 7), (14, 10), (15, 11), (16, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "2 ['For', 'example']\n",
      "1 [',']\n",
      "27 ['if', 'we', 'think', 'of', 'a', 'VAE', 'as', 'a', 'sort', 'of', 'regularized', 'autoencoder', 'where', 'codes', 'are', 'constrained', 'to', '\"', 'fill', 'up', '\"', 'parts', 'of', 'the', 'prior', 'latent', 'space']\n",
      "1 [',']\n",
      "1 ['then']\n",
      "1 ['there']\n",
      "24 ['is', 'a', 'sphere', '-', 'packing', 'argument', 'to', 'be', 'made', 'that', 'filling', 'a', 'Gaussian', 'prior', 'with', 'Gaussian', 'posteriors', 'is', 'a', 'bad', 'use', 'of', 'code', 'space']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 5), (6, 2), (7, 2), (8, 7), (9, 6), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['Although']\n",
      "17 ['the', 'authors', 'do', \"n't\", 'explore', 'this', 'much', ',', 'a', 'hypercube', '-', 'based', 'tiling', 'of', 'latent', 'code', 'space']\n",
      "1 ['is']\n",
      "3 ['a', 'sensible', 'idea']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 4), (5, 7), (6, 5), (7, 5), (8, 4), (9, 6), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "2 ['As', 'stated']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "18 ['found', 'the', 'message', 'of', 'the', 'paper', 'to', 'be', 'quite', 'sloppy', 'with', 'respect', 'to', 'the', 'concept', 'of', '\"', 'multi-modality']\n",
      "1 ['.']\n",
      "1 ['\"']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 11), (5, 10), (6, 17), (7, 12), (8, 22), (9, 20), (10, 12), (11, 9), (12, 12), (13, 16), (14, 12), (15, 2), (16, 4), (17, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "7 ['3', 'types', 'of', 'multimodality', 'at', 'play', 'here']\n",
      "1 [':']\n",
      "21 ['multimodality', 'in', 'the', 'observed', 'marginal', 'distribution', 'p', '-LRB-', 'x', '-RRB-', ',', 'which', 'can', 'be', 'captured', 'by', 'any', 'deep', 'latent', 'Gaussian', 'model']\n",
      "1 [',']\n",
      "17 ['multimodality', 'in', 'the', 'prior', 'p', '-LRB-', 'z', '-RRB-', ',', 'which', 'makes', 'sense', 'in', 'some', 'situations', '-LRB-', 'eg']\n",
      "1 [':']\n",
      "18 ['a', 'model', 'of', 'MNIST', 'digits', 'could', 'have', '10', 'prior', 'modes', 'corresponding', 'to', 'latent', 'codes', 'for', 'each', 'digit', 'class']\n",
      "1 ['-RRB-']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "17 ['multimodality', 'in', 'the', 'posterior', 'z', 'for', 'a', 'given', 'observation', 'x_i', ',', 'q', '-LRB-', 'z_i', '|', 'x_i', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 9), (5, 5), (6, 7), (7, 5), (8, 6), (9, 4), (10, 4), (11, 4), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "3 ['The', 'final', 'type']\n",
      "2 ['of', 'multimodality']\n",
      "1 ['is']\n",
      "1 ['harder']\n",
      "8 ['to', 'argue', 'for', ',', 'except', 'in', 'so', 'far']\n",
      "13 ['as', 'it', 'allows', 'the', 'expression', 'of', 'flexibly', 'shaped', 'distributions', 'without', 'highly', 'separated', 'modes']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 7), (5, 3), (6, 3), (7, 7), (8, 5), (9, 2), (10, 4), (11, 4), (12, 5), (13, 4), (14, 5), (15, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "18 ['I', 'believe', 'flexible', 'posterior', 'approximations', 'are', 'important', 'to', 'enable', 'fine', '-', 'grained', 'and', 'efficient', 'tiling', 'of', 'latent', 'space']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "11 ['I', 'do', \"n't\", 'think', 'these', 'need', 'to', 'have', 'multiple', 'strong', 'modes']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 1), (7, 2), (8, 2), (9, 2), (10, 4), (11, 3), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  10\n",
      "1 ['experiments']\n",
      "1 ['demonstrating']\n",
      "1 ['otherwise']\n",
      "4 ['for', 'real', 'world', 'data']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 3), (6, 5), (7, 8), (8, 6), (9, 6), (10, 4), (11, 2), (12, 5), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  6\n",
      "2 ['this', 'paper']\n",
      "10 ['should', 'be', 'more', 'clear', 'about', 'the', 'different', 'types', 'of', 'multi-modality']\n",
      "1 ['which']\n",
      "4 ['parts', 'of', 'their', 'analysis']\n",
      "3 ['demonstrate', 'which', 'ones']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 4), (5, 2), (6, 7), (7, 2), (8, 4), (9, 7), (10, 8), (11, 10), (12, 6), (13, 1), (14, 2), (15, 2), (16, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "1 ['I']\n",
      "1 ['also']\n",
      "34 ['found', 'it', 'unsatisfactory', 'that', 'the', 'piecewise', 'variable', 'analysis', 'did', 'not', 'show', 'different', 'components', 'of', 'the', 'multi-modal', 'prior', 'corresponding', 'to', 'different', 'words', ',', 'but', 'rather', 'just', 'a', 'separation', 'between', 'the', 'Gaussian', 'and', 'the', 'piecewise', 'variables']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 6), (5, 7), (6, 7), (7, 11), (8, 7), (9, 11), (10, 14), (11, 8), (12, 6), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "7 ['As', 'I', 'mention', 'in', 'my', 'earlier', 'questions']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "37 ['found', 'it', 'surprising', 'that', 'the', 'learned', 'variance', 'and', 'mean', 'for', 'the', 'Gaussian', 'prior', 'helps', 'so', 'dramatically', 'with', 'G', '-', 'NVDM', 'likelihood', 'when', 'the', 'powerful', 'networks', 'transforming', 'to', 'and', 'from', 'latent', 'space', 'should', 'make', 'it', 'scale', '-', 'invariant']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 10), (5, 11), (6, 6), (7, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "10 ['Explicitly', 'separating', 'out', 'the', 'contributions', 'of', 'a', 'reimplemented', 'base', 'model']\n",
      "1 [',']\n",
      "9 ['prior', '-', 'posterior', 'interpolation', 'and', 'the', 'learned', 'prior', 'parameters']\n",
      "4 ['would', 'strengthen', 'these', 'experiments']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 7), (5, 7), (6, 4), (7, 4), (8, 6), (9, 7), (10, 2), (11, 5), (12, 2), (13, 3), (14, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "13 ['The', 'fact', 'that', 'adding', 'more', 'constant', 'components', 'helps', 'for', 'document', 'modeling', 'is', 'interesting']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "15 ['it', 'would', 'be', 'nice', 'to', 'see', 'more', 'qualitative', 'analysis', 'of', 'what', 'the', 'prior', 'modes', 'represent']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 3), (5, 5), (6, 4), (7, 4), (8, 8), (9, 5), (10, 6), (11, 3), (12, 2), (13, 1), (14, 2), (15, 2), (16, 2), (17, 1), (18, 2), (19, 2), (20, 2), (21, 2), (22, 3), (23, 1), (24, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "depth:  2\n",
      "1 ['I']\n",
      "1 ['also']\n",
      "30 ['would', 'be', 'surprised', 'if', 'posterior', 'modes', 'were', 'highly', 'separated', ',', 'and', 'if', 'they', 'were', 'it', 'would', 'be', 'interesting', 'to', 'explore', 'if', 'they', 'corresponded', 'to', 'eg', ':', 'ambiguous', 'word', '-', 'senses']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 7), (5, 4)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "2 ['The', 'experiments']\n",
      "3 ['on', 'dialog', 'modeling']\n",
      "1 ['are']\n",
      "3 ['mostly', 'negative', 'results']\n",
      "1 [',']\n",
      "1 ['quantitatively']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 8), (5, 6), (6, 7), (7, 12), (8, 7), (9, 9), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "2 ['The', 'observation']\n",
      "17 ['that', 'the', 'the', 'piecewise', 'constant', 'variables', 'encode', 'time', '-', 'related', 'words', 'and', 'the', 'Gaussian', 'variables', 'encode', 'sentiment']\n",
      "1 ['is']\n",
      "1 ['interesting']\n",
      "1 [',']\n",
      "9 ['especially', 'since', 'it', 'occurs', 'in', 'both', 'sets', 'of', 'experiments']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 7), (5, 5), (6, 2), (7, 2), (8, 1), (9, 2), (10, 2), (11, 3), (12, 2), (13, 3), (14, 3), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "5 ['This', 'is', 'actually', 'quite', 'interesting']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "13 ['I', 'would', 'be', 'interested', 'in', 'seeing', 'analysis', 'of', 'why', 'this', 'is', 'the', 'case']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 1), (6, 2), (7, 2), (8, 2), (9, 4), (10, 2), (11, 4), (12, 3), (13, 4), (14, 6), (15, 8), (16, 4), (17, 4), (18, 5), (19, 3), (20, 3), (21, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "depth:  2\n",
      "2 ['As', 'above']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "32 ['would', 'like', 'to', 'see', 'an', 'analysis', 'of', 'the', 'sorts', 'of', 'words', 'that', 'are', 'encoded', 'in', 'the', 'different', 'prior', 'modes', 'and', 'whether', 'they', 'correspond', 'to', 'eg', ':', 'groups', 'of', 'similar', 'holidays', 'or', 'days']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 2), (5, 2), (6, 9), (7, 5), (8, 2), (9, 4), (10, 2), (11, 5), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "2 ['In', 'conclusion']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "21 ['think', 'the', 'piecewise', 'constant', 'variational', 'family', 'is', 'a', 'good', 'idea', ',', 'although', 'it', 'is', 'not', 'well', '-', 'motivated', 'by', 'the', 'paper']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 6), (5, 2), (6, 5), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "2 ['The', 'fact']\n",
      "6 ['that', 'H', '-', 'NVDM', 'performs', 'better']\n",
      "1 ['is']\n",
      "1 ['interesting']\n",
      "1 [',']\n",
      "1 ['though']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 5), (5, 6), (6, 6), (7, 7), (8, 11), (9, 4), (10, 2), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "10 ['should', 'better', 'motivate', 'the', 'need', 'for', 'different', 'types', 'of', 'multi-modality']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "13 ['demonstrate', 'that', 'those', 'sorts', 'of', 'things', 'are', 'actually', 'being', 'captured', 'by', 'the', 'model']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 15), (5, 8), (6, 8), (7, 2), (8, 3), (9, 2), (10, 3), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "20 ['As', 'it', 'is', ',', 'the', 'paper', 'introduces', 'an', 'interesting', 'variational', 'family', 'and', 'shows', 'that', 'it', 'performs', 'better', 'for', 'some', 'tasks']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "8 ['the', 'motivation', 'and', 'analysis', 'is', 'not', 'clearly', 'focused']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 6), (5, 11), (6, 6), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "8 ['Even', 'without', 'an', 'absolute', 'log', '-', 'likelihood', 'improvement']\n",
      "1 [',']\n",
      "7 ['if', 'the', 'method', 'yielded', 'interpretable', 'multiple', 'modes']\n",
      "1 ['this']\n",
      "5 ['would', 'be', 'a', 'valuable', 'contribution']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 9), (5, 5), (6, 3), (7, 3), (8, 4), (9, 5), (10, 6), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['proposes']\n",
      "4 ['a', 'piecewise', 'constant', 'parameterisation']\n",
      "4 ['for', 'neural', 'variational', 'models']\n",
      "17 ['so', 'that', 'it', 'could', 'explore', 'the', 'multi-modality', 'of', 'the', 'latent', 'variables', 'and', 'develop', 'more', 'powerful', 'neural', 'models']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 5), (6, 12), (7, 9), (8, 11), (9, 3), (10, 4), (11, 4), (12, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "2 ['The', 'experiments']\n",
      "13 ['of', 'neural', 'variational', 'document', 'models', 'and', 'variational', 'hierarchical', 'recurrent', 'encoder', '-', 'decoder', 'models']\n",
      "1 ['show']\n",
      "22 ['that', 'the', 'introduction', 'of', 'the', 'piecewise', 'constant', 'distribution', 'helps', 'achieve', 'better', 'perplexity', 'on', 'modelling', 'documents', 'and', 'seemly', 'better', 'performance', 'on', 'modelling', 'dialogues']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 8), (5, 15), (6, 12), (7, 8), (8, 7), (9, 8), (10, 6), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "13 ['The', 'idea', 'of', 'having', 'a', 'piecewise', 'constant', 'prior', 'for', 'latent', 'variables', 'is', 'interesting']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "27 ['the', 'paper', 'is', 'not', 'well', '-', 'written', '-LRB-', 'even', '14', 'pages', 'long', '-RRB-', 'and', 'the', 'design', 'of', 'the', 'experiments', 'fails', 'to', 'demonstrate', 'the', 'most', 'of', 'the', 'claims']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 16), (5, 8), (6, 11), (7, 10), (8, 10), (9, 16), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "7 ['The', 'detailed', 'comments', 'are', 'as', 'follows', ':']\n",
      "1 ['--']\n",
      "26 ['The', 'author', 'explains', 'the', 'limitations', 'of', 'the', 'VAEs', 'with', 'standard', 'Gaussian', 'prior', 'in', 'the', 'last', 'paragraph', 'of', '3.1', 'and', 'the', 'last', 'paragraph', 'of', '5.1', '-RRB-', 'Hence']\n",
      "1 [',']\n",
      "12 ['a', 'multimodal', 'prior', 'would', 'help', 'the', 'VAEs', 'overcome', 'the', 'issues', 'of', 'optimisation']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 2), (5, 4), (6, 2), (7, 3), (8, 1), (9, 2), (10, 4), (11, 6), (12, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "1 ['there']\n",
      "15 ['is', 'a', 'lack', 'of', 'evidence', 'showing', 'the', 'multimodality', 'of', 'the', 'prior', 'helps', 'break', 'the', 'bottleneck']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 3), (5, 7), (6, 8), (7, 3), (8, 2), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['--']\n",
      "6 ['In', 'the', 'last', 'paragraph', 'of', '6.1']\n",
      "1 [',']\n",
      "2 ['the', 'author']\n",
      "12 ['claimed', 'the', 'decoder', 'parameter', 'matrix', 'is', 'directly', 'affected', 'by', 'the', 'latent', 'variables']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 5), (5, 7), (6, 7), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['But']\n",
      "5 ['what', 'the', 'connects', 'the', 'decoder']\n",
      "11 ['is', 'a', 'combination', 'of', 'a', 'piecewise', 'constant', 'and', 'Gaussian', 'latent', 'variables']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 8), (4, 8), (5, 4), (6, 5), (7, 3), (8, 4), (9, 5), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "8 ['No', 'matter', 'what', 'is', 'discovered', 'in', 'the', 'experiments']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "1 ['only']\n",
      "10 ['shows', 'z', '=', '<', 'z_gaussian', ',', 'z_piecewise', '>', 'is', 'multimodal']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 8), (5, 7), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "7 ['z', '=', '<', 'z_gaussian1', ',', 'z_gaussian2', '>']\n",
      "5 ['can', 'be', 'multimodal', 'as', 'well']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 2), (6, 4), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  6\n",
      "1 ['the']\n",
      "1 ['claims']\n",
      "1 ['in']\n",
      "2 ['this', 'paragraph']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 9), (5, 9), (6, 7), (7, 2), (8, 3), (9, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['--']\n",
      "6 ['In', 'the', 'quantitative', 'evaluation', 'of', 'NVDM']\n",
      "1 [',']\n",
      "1 ['there']\n",
      "16 ['is', 'an', 'incremental', 'model', 'from', 'z', '=', 'z_gaussian', 'to', 'z', '=', '<', 'z_gaussian', ',', 'z_piecewise', '>']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 1), (6, 2), (7, 2), (8, 2), (9, 2), (10, 6), (11, 3), (12, 2), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  10\n",
      "1 ['the']\n",
      "1 ['proposed']\n",
      "1 ['method']\n",
      "1 ['is']\n",
      "1 ['also']\n",
      "4 ['effective', 'for', 'image', 'classification']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 7), (5, 3), (6, 4), (7, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "2 ['More', 'datasets']\n",
      "2 ['for', 'evaluation']\n",
      "1 ['are']\n",
      "9 ['needed', ',', 'even', 'only', 'for', 'the', 'object', 'detection', 'application']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 6), (5, 1), (6, 2), (7, 2), (8, 2), (9, 4), (10, 4), (11, 4), (12, 5), (13, 2), (14, 5), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "3 ['-LRB-', '-', '-RRB-']\n",
      "4 ['More', 'than', 'two', 'datasets']\n",
      "20 ['are', 'necessary', 'to', 'show', 'the', 'effectiveness', 'of', 'the', 'methods', 'comments', '-RRB-', '-', 'What', 'is', 'the', 'higher', 'level', 'feature', 'map', 'P_m']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 4), (6, 2), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['should']\n",
      "8 ['discuss', 'their', 'difference', 'with', 'self', '-', 'paced', 'learning']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 4), (6, 4), (7, 5), (8, 2), (9, 2), (10, 4), (11, 3), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['do']\n",
      "1 ['not']\n",
      "16 ['know', 'the', 'reason', 'why', 'the', 'authors', 'only', 'apply', 'for', 'object', 'detection', 'on', 'a', 'very', 'specific', 'dataset']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 7), (5, 4), (6, 5), (7, 2), (8, 3), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['proposes']\n",
      "3 ['a', 'novel', 'approach']\n",
      "3 ['with', 'the', 'hypothesis']\n",
      "10 ['that', 'the', 'reliable', 'features', 'can', 'guide', 'the', 'less', 'reliable', 'ones']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 3), (7, 8)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['approach']\n",
      "1 ['is']\n",
      "11 ['applied', 'to', 'the', 'object', 'detection', 'task', 'and', 'show', 'consistent', 'performance', 'improvements']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 6), (6, 5), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['pros']\n",
      "1 ['-RRB-']\n",
      "13 ['-LRB-', '+', '-RRB-', 'This', 'paper', 'is', 'well', '-', 'written', 'and', 'easy', 'to', 'follow']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 8), (4, 9), (5, 5), (6, 4), (7, 9), (8, 12), (9, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "3 ['-LRB-', '+', '-RRB-']\n",
      "4 ['The', 'base', 'idea', 'that']\n",
      "23 ['divides', 'the', 'learned', 'features', 'into', 'two', 'sets', ';', 'the', 'reliable', 'feature', 'set', 'and', 'the', 'less', 'reliable', 'one', 'is', 'very', 'interesting', 'and', 'looks', 'novel']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 6), (5, 2), (6, 2), (7, 2), (8, 2), (9, 4), (10, 3), (11, 4), (12, 3), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Plus']\n",
      "1 [',']\n",
      "17 ['the', 'hypothesis', ',', 'which', 'is', 'that', 'reliable', 'features', 'can', 'guide', 'the', 'features', 'in', 'the', 'less', 'reliable', 'set']\n",
      "3 ['is', 'also', 'interesting']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 8), (4, 3)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "3 ['-LRB-', '+', '-RRB-']\n",
      "3 ['The', 'performance', 'improvements']\n",
      "3 ['are', 'quite', 'large']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 8), (4, 3), (5, 1), (6, 2), (7, 3), (8, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "3 ['-LRB-', '+', '-RRB-']\n",
      "3 ['Extensive', 'ablative', 'studies']\n",
      "8 ['are', 'provided', 'to', 'support', 'the', 'proposed', 'method', 'well']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 6), (5, 8), (6, 10), (7, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['cons']\n",
      "1 ['-RRB-']\n",
      "16 ['-LRB-', '-', '-RRB-', 'The', 'method', 'of', 'obtaining', 'the', 'representative', 'in', 'buffer', 'B', 'is', 'not', 'clearly', 'presented']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 11), (4, 6)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "3 ['-LRB-', '-', '-RRB-']\n",
      "6 ['The', 'overall', 'training', 'and', 'inference', 'procedure']\n",
      "4 ['are', 'not', 'clearly', 'presented']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 9), (4, 4)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "3 ['-LRB-', '-', '-RRB-']\n",
      "4 ['Some', 'notations', 'and', 'descriptions']\n",
      "4 ['are', 'vague', 'and', 'confusing']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 10), (6, 18), (7, 8)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['and']\n",
      "1 ['How']\n",
      "26 ['did', 'you', 'choose', 'the', 'higher', 'level', 'feature', 'map', 'at', 'the', 'm', '-', 'th', 'level', 'in', 'option', '-LRB-', 'b', '-RRB-', 'and', '-LRB-', 'c', '-RRB-', 'in', 'Section', '3.3']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 6), (6, 3), (7, 7), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  5\n",
      "1 ['-']\n",
      "1 ['What']\n",
      "1 ['the']\n",
      "1 ['meaning']\n",
      "1 ['of']\n",
      "8 ['the', '\"', 'past', '\"', 'features', 'in', 'Section', '3.2']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 4), (6, 3), (7, 2), (8, 5), (9, 3), (10, 8)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "2 ['-', 'It']\n",
      "1 ['is']\n",
      "1 ['better']\n",
      "15 ['to', 'show', 'the', 'exact', 'architecture', 'of', 'the', 'make', '-', 'up', 'module', 'and', 'the', 'critic', 'module']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 4), (6, 3), (7, 6), (8, 3), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['-']\n",
      "1 ['Can']\n",
      "2 ['this', 'method']\n",
      "12 ['apply', 'to', 'the', 'other', 'backbones', 'such', 'as', 'VGG', 'or', 'ResNets', 'without', 'FPN']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 7), (5, 5), (6, 5), (7, 3), (8, 2), (9, 4), (10, 4), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "7 ['The', 'sentences', 'at', 'the', 'bottom', 'of', 'p.']\n",
      "11 ['4', 'starting', 'with', '\"', 'Note', 'that', 'only', '~', '\"', 'looks', 'ambiguous']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 5), (5, 4), (6, 2), (7, 2), (8, 3), (9, 3), (10, 2), (11, 6), (12, 10), (13, 5), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['-']\n",
      "1 ['f_critic']\n",
      "1 ['^']\n",
      "1 ['j']\n",
      "18 ['may', 'be', 'the', 'j', '-', 'th', 'element', 'of', 'F_critic', ',', 'please', 'denote', 'what', 'f_critic', '^', 'j', 'stands', 'for']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 3), (5, 6), (6, 6), (7, 6), (8, 6), (9, 10), (10, 6), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "11 ['Even', 'if', 'the', 'paper', 'needs', 'to', 'be', 'revised', 'for', 'better', 'readability']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "16 ['think', 'this', 'paper', 'is', 'above', 'the', 'standard', 'of', 'ICLR', 'because', 'the', 'idea', 'is', 'interesting', 'and', 'novel']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 6), (5, 5)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['Furthermore']\n",
      "1 [',']\n",
      "3 ['the', 'experimental', 'studies']\n",
      "9 ['are', 'properly', 'designed', 'and', 'well', 'support', 'the', 'main', 'idea']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 6), (5, 4), (6, 3), (7, 3), (8, 2), (9, 2), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "5 ['I', 'am', 'leaning', 'toward', 'acceptance']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "10 ['I', 'would', 'like', 'to', 'see', 'the', 'other', 'reviewers', \"'\", 'comments']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 1), (5, 2), (6, 3), (7, 4), (8, 5), (9, 5), (10, 3), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['aims']\n",
      "13 ['to', 'facilitate', 'feature', 'learning', 'in', 'NN', 'models', 'by', 'exploiting', 'more', 'from', 'reliable', 'examples']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 5), (6, 7), (7, 8), (8, 5), (9, 6), (10, 2), (11, 2), (12, 2), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['is']\n",
      "7 ['very', 'similar', 'to', 'self', '-', 'paced', 'learning']\n",
      "19 ['where', 'the', 'model', 'learns', 'from', 'the', 'easier', 'samples', 'at', 'first', 'and', 'proceeds', 'to', 'learn', 'from', 'difficult', 'and', 'challenging', 'samples']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 2), (7, 5), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['method']\n",
      "1 ['is']\n",
      "8 ['positioned', 'as', 'a', 'general', 'one', 'for', 'feature', 'learning']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 2), (6, 4), (7, 1), (8, 3), (9, 5), (10, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['OVERVIEW']\n",
      "1 [':']\n",
      "15 ['The', 'authors', 'tackle', 'the', 'problem', 'of', 'detecting', 'small', '/', 'low', 'resolution', 'objects', 'in', 'an', 'image']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 2), (6, 4), (7, 6), (8, 7), (9, 2), (10, 1), (11, 2), (12, 2), (13, 2), (14, 4), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  3\n",
      "1 ['Their']\n",
      "1 ['key']\n",
      "1 ['idea']\n",
      "1 ['is']\n",
      "19 ['that', 'detecting', 'bigger', 'objects', 'is', 'an', 'easier', 'task', 'and', 'can', 'be', 'used', 'to', 'guide', 'the', 'detection', 'of', 'smaller', 'objects']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 1), (6, 2), (7, 2), (8, 7), (9, 2), (10, 2), (11, 2), (12, 3), (13, 4), (14, 3), (15, 3), (16, 6), (17, 8), (18, 8), (19, 9), (20, 6), (21, 3), (22, 2), (23, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "depth:  8\n",
      "1 ['the']\n",
      "1 ['\"']\n",
      "1 ['Feature']\n",
      "1 ['Intertwiner']\n",
      "1 ['\"']\n",
      "1 ['which']\n",
      "32 ['consists', 'of', 'two', 'branches', ',', 'one', 'for', 'the', 'larger', 'objects', '-LRB-', 'more', 'reliable', 'set', 'that', 'is', 'also', 'easier', 'to', 'detect', '-RRB-', 'and', 'one', 'for', 'the', 'smaller', 'objects', '-LRB-', 'less', 'reliable', 'set', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 1), (5, 2), (6, 5), (7, 3), (8, 3), (9, 9), (10, 6), (11, 4), (12, 4), (13, 4), (14, 5), (15, 6), (16, 4), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['second']\n",
      "1 ['branch']\n",
      "1 ['contains']\n",
      "27 ['a', 'make', '-', 'up', 'layer', 'learned', 'during', 'training', '-LRB-', 'which', 'acts', 'as', 'the', 'guidance', 'from', 'the', 'more', 'reliable', 'set', '-RRB-', 'that', 'helps', 'compensate', 'details', 'needed', 'for', 'detection']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 5), (6, 2), (7, 3), (8, 4), (9, 7), (10, 6), (11, 10), (12, 6), (13, 3), (14, 4), (15, 8), (16, 8), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['define']\n",
      "40 ['a', 'class', 'buffer', 'that', 'contains', 'representative', 'elements', 'of', 'object', 'features', 'from', 'the', 'reliable', 'set', 'for', 'every', 'category', '&', 'scale', 'and', 'an', 'intertwiner', 'loss', 'that', 'computes', 'the', 'L2', 'loss', 'between', 'the', 'features', 'from', 'the', 'less', 'reliable', 'set', '&', 'the', 'class', 'buffer']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 6), (5, 2), (6, 6), (7, 2), (8, 4), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['They']\n",
      "1 ['also']\n",
      "16 ['use', 'an', 'Optimal', 'Transport', 'procedure', 'with', 'a', 'Sinkhorn', 'divergence', 'loss', 'between', 'object', 'features', 'from', 'both', 'sets']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 8), (5, 6), (6, 5), (7, 10)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "3 ['The', 'overall', 'loss']\n",
      "3 ['of', 'the', 'system']\n",
      "1 ['is']\n",
      "1 ['now']\n",
      "15 ['a', 'sum', 'of', 'the', 'detection', 'loss', ',', 'the', 'intertwiner', 'loss', 'and', 'the', 'optimal', 'transport', 'loss']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 2), (6, 7), (7, 2), (8, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['They']\n",
      "1 ['evaluate']\n",
      "2 ['their', 'model']\n",
      "15 ['on', 'the', 'COCO', 'Object', 'detection', 'challenge', 'showing', 'state', '-', 'of', '-', 'the', '-', 'art', 'performance']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 5), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['They']\n",
      "1 ['also']\n",
      "8 ['provide', 'thorough', 'ablation', 'analysis', 'of', 'various', 'design', 'choices']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 8), (5, 2), (6, 5), (7, 4), (8, 4), (9, 6), (10, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "3 ['The', 'qualitative', 'result']\n",
      "18 ['in', 'Fig.', '1', 'showing', 'well', 'clustered', 'features', 'for', 'both', 'high', '&', 'low', 'resolution', 'objects', 'via', 't', '-', 'SNE']\n",
      "1 ['is']\n",
      "3 ['a', 'nice', 'touch']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 7), (5, 7), (6, 1), (7, 2), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['COMMENTS']\n",
      "1 [':']\n",
      "4 ['Clarity', '-', 'The', 'paper']\n",
      "1 ['is']\n",
      "6 ['well', 'written', 'and', 'easy', 'to', 'follow']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 8), (5, 9)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['Originality']\n",
      "1 ['&']\n",
      "4 ['Significance', '-', 'The', 'paper']\n",
      "4 ['tackles', 'an', 'important', 'problem']\n",
      "1 ['and']\n",
      "4 ['provides', 'a', 'novel', 'solution']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 2), (6, 2), (7, 2), (8, 2), (9, 6), (10, 7), (11, 10), (12, 6), (13, 1), (14, 2), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "1 ['Quality']\n",
      "1 ['-']\n",
      "27 ['The', 'paper', 'is', 'complete', 'in', 'that', 'it', 'tackles', 'an', 'important', 'problem', ',', 'provides', 'a', 'novel', 'solution', 'and', 'demonstrates', 'via', 'thorough', 'experiments', 'the', 'improvement', 'achieved', 'using', 'their', 'approach']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 7), (5, 3), (6, 2), (7, 1), (8, 2), (9, 2), (10, 4), (11, 7), (12, 4), (13, 4), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['QUESTIONS']\n",
      "1 [':']\n",
      "22 ['1', '-RRB-', 'The', 'Class', 'Buffer', 'seems', 'very', 'restricted', 'in', 'having', 'a', 'single', 'element', 'per', 'object', 'category', 'per', 'scale', 'to', 'represent', 'all', 'features']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 5), (6, 5), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "2 ['The', 'advantage']\n",
      "5 ['of', 'forcing', 'such', 'a', 'representation']\n",
      "1 ['is']\n",
      "6 ['tight', 'clustering', 'in', 'the', 'feature', 'space']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 9), (5, 12), (6, 9), (7, 6), (8, 2), (9, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['But']\n",
      "1 [',']\n",
      "8 ['would', \"n't\", 'a', 'dictionary', 'approach', 'with', 'multiple', 'elements']\n",
      "22 ['give', 'more', 'flexibility', 'to', 'the', 'model', 'and', 'learn', 'a', 'richer', 'feature', 'representation', 'at', 'the', 'cost', 'of', 'not', '-', 'so', '-', 'good', 'clustering']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 2), (6, 3), (7, 3), (8, 2), (9, 3), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['2']\n",
      "1 ['-RRB-']\n",
      "2 ['Any', 'comment']\n",
      "7 ['on', 'why', 'you', 'drop', 'performance', 'for', 'couch']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 10), (4, 11), (5, 14), (6, 9), (7, 8), (8, 9), (9, 5), (10, 9), (11, 8), (12, 6), (13, 1), (14, 2), (15, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "9 ['-LRB-', 'and', 'baseball', 'bat', '+', 'bedroll', '-RRB-', '3', '-RRB-']\n",
      "17 ['In', 'Table', '4', 'of', 'Appendix', 'where', 'you', 'compare', 'with', 'more', 'object', 'detection', 'results', ',', 'I', 'find', 'it']\n",
      "24 ['interesting', 'that', 'Mask', 'RCNN', ',', 'updated', 'results', 'has', 'a', 'might', 'higher', 'AP_S', '-LRB-', '43.5', '-RRB-', 'compared', 'to', 'you', '-LRB-', '27.2', '-RRB-', 'and', 'everyone', 'else']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 2), (6, 2), (7, 2), (8, 2), (9, 4), (10, 2), (11, 5), (12, 2), (13, 5), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  9\n",
      "1 ['the']\n",
      "1 ['best']\n",
      "1 ['under']\n",
      "10 ['that', 'metric', 'due', 'to', 'the', 'explicit', 'design', 'for', 'small', 'objects']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 10), (5, 8), (6, 5), (7, 2), (8, 4), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['They']\n",
      "6 ['-LRB-', 'MaskRCNN', ',', 'updated', 'results', '-RRB-']\n",
      "1 ['are']\n",
      "1 ['also']\n",
      "9 ['significantly', 'better', 'than', 'the', 'rest', 'under', 'AP_M', 'but', 'worse']\n",
      "2 ['under', 'AP_L']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 2)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "1 ['Can']\n",
      "1 ['you']\n",
      "3 ['explain', 'this', 'behavior']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 3), (6, 4), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['Is']\n",
      "3 ['the', 'ResNeXt', 'backbone']\n",
      "6 ['that', 'much', 'better', 'for', 'small', 'objects']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 4), (5, 7), (6, 8), (7, 16), (8, 6), (9, 10), (10, 6), (11, 9), (12, 12), (13, 7), (14, 2), (15, 3), (16, 4), (17, 5), (18, 2), (19, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  2\n",
      "2 ['Paper', 'Strengths']\n",
      "1 [':']\n",
      "54 ['--', 'Elegant', 'use', 'of', 'MoE', 'for', 'expanding', 'model', 'capacity', 'and', 'enabling', 'training', 'large', 'models', 'necessary', 'for', 'exploiting', 'very', 'large', 'datasets', 'in', 'a', 'computationally', 'feasible', 'manner', '--', 'The', 'effective', 'batch', 'size', 'for', 'training', 'the', 'MoE', 'drastically', 'increased', 'also', '--', 'Interesting', 'experimental', 'results', 'on', 'the', 'effects', 'of', 'increasing', 'the', 'number', 'of', 'MoEs', ',', 'which', 'is', 'expected']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 8), (5, 9), (6, 10), (7, 12), (8, 5), (9, 6), (10, 7), (11, 3), (12, 2), (13, 2), (14, 4), (15, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "10 ['There', 'are', 'also', 'some', 'glitches', 'in', 'the', 'writing', ',', 'eg']\n",
      "1 [':']\n",
      "31 ['the', 'end', 'of', 'Section', '3.1', '-RRB-', '-', 'The', 'paper', 'is', 'missing', 'some', 'important', 'references', 'in', 'conditional', 'computation', '-LRB-', 'eg', ':', 'https://arxiv.org/pdf/1308.3432.pdf', '-RRB-', 'which', 'deal', 'with', 'very', 'similar', 'issues', 'in', 'deep', 'learning']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 2), (6, 4), (7, 4), (8, 5), (9, 5), (10, 8), (11, 12), (12, 3), (13, 3), (14, 1), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['proposes']\n",
      "2 ['a', 'method']\n",
      "26 ['for', 'significantly', 'increasing', 'the', 'number', 'of', 'parameters', 'in', 'a', 'single', 'layer', 'while', 'keeping', 'computation', 'in', 'par', 'with', '-LRB-', 'or', 'even', 'less', 'than', '-RRB-', 'current', 'SOTA', 'models']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 4), (5, 5), (6, 6), (7, 8), (8, 8), (9, 7), (10, 6), (11, 7), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['idea']\n",
      "1 ['is']\n",
      "16 ['based', 'on', 'using', 'a', 'large', 'mixture', 'of', 'experts', '-LRB-', 'MoE', '-RRB-', '-LRB-', 'ie', 'small', 'networks', '-RRB-']\n",
      "1 [',']\n",
      "13 ['where', 'only', 'a', 'few', 'of', 'them', 'are', 'adaptively', 'activated', 'via', 'a', 'gating', 'network']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 9), (5, 7), (6, 7), (7, 8), (8, 10), (9, 13), (10, 7), (11, 2), (12, 2), (13, 1), (14, 2), (15, 2), (16, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "5 ['While', 'the', 'idea', 'seems', 'intuitive']\n",
      "1 [',']\n",
      "6 ['the', 'main', 'novelty', 'in', 'the', 'paper']\n",
      "33 ['is', 'in', 'designing', 'the', 'gating', 'network', 'which', 'is', 'encouraged', 'to', 'achieve', 'two', 'objectives', ':', 'utilizing', 'all', 'available', 'experts', '-LRB-', 'aka', 'importance', '-RRB-', ',', 'and', 'distributing', 'computation', 'fairly', 'across', 'them', '-LRB-', 'aka', 'load', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 4), (5, 1), (6, 5), (7, 6), (8, 4), (9, 3), (10, 6), (11, 2), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['Additionally']\n",
      "1 [',']\n",
      "2 ['the', 'paper']\n",
      "20 ['introduces', 'two', 'techniques', 'for', 'increasing', 'the', 'batch', '-', 'size', 'passed', 'to', 'each', 'expert', ',', 'and', 'hence', 'maximizing', 'parallelization', 'in', 'GPUs']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 7), (6, 5), (7, 6), (8, 6), (9, 5), (10, 4), (11, 4), (12, 2), (13, 2), (14, 4), (15, 2), (16, 3), (17, 2), (18, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  3\n",
      "1 ['Experiments']\n",
      "10 ['applying', 'the', 'proposed', 'approach', 'on', 'RNNs', 'in', 'language', 'modelling', 'task']\n",
      "1 ['show']\n",
      "21 ['that', 'it', 'can', 'beat', 'SOTA', 'results', 'with', 'significantly', 'less', 'computation', ',', 'which', 'is', 'a', 'result', 'of', 'selectively', 'using', 'much', 'more', 'parameters']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 4), (6, 4), (7, 4), (8, 7), (9, 5), (10, 5), (11, 6), (12, 2), (13, 3), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  5\n",
      "2 ['machine', 'translation']\n",
      "1 ['show']\n",
      "1 ['that']\n",
      "19 ['a', 'model', 'with', 'more', 'than', '30x', 'number', 'of', 'parameters', 'can', 'beat', 'SOTA', 'while', 'incurring', 'half', 'of', 'the', 'effective', 'computation']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 7), (5, 4), (6, 7), (7, 6), (8, 3), (9, 5), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "8 ['I', 'have', 'the', 'several', 'comments', 'on', 'the', 'paper']\n",
      "1 [':']\n",
      "14 ['-', 'I', 'believe', 'that', 'the', 'authors', 'can', 'do', 'a', 'better', 'job', 'in', 'their', 'presentation']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 8), (5, 4), (6, 4), (7, 11), (8, 7), (9, 7), (10, 10), (11, 4), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "16 ['The', 'paper', 'currently', 'is', 'at', '11', 'pages', '-LRB-', 'which', 'is', 'too', 'long', 'in', 'my', 'opinion', '-RRB-']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "18 ['I', 'find', 'that', 'Section', '3.2', '-LRB-', 'the', 'crux', 'of', 'the', 'paper', '-RRB-', 'needs', 'better', 'motivation', 'and', 'intuitive', 'explanation']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 5), (5, 2), (6, 3), (7, 2), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "2 ['For', 'example']\n",
      "1 [',']\n",
      "2 ['equation', '8']\n",
      "8 ['deserves', 'more', 'description', 'than', 'currently', 'devoted', 'to', 'it']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 3), (6, 2), (7, 1), (8, 3), (9, 4), (10, 5), (11, 8), (12, 9), (13, 5), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['Additional']\n",
      "1 ['space']\n",
      "1 ['can']\n",
      "25 ['be', 'easily', 'regained', 'by', 'moving', 'details', 'in', 'the', 'experiments', 'section', '-LRB-', 'eg', ':', 'architecture', 'and', 'training', 'details', '-RRB-', 'to', 'the', 'appendix', 'for', 'the', 'curious', 'readers']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 3), (6, 2), (7, 1), (8, 3), (9, 5), (10, 2), (11, 2), (12, 2), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['Experiment']\n",
      "1 ['section']\n",
      "1 ['can']\n",
      "14 ['be', 'better', 'organized', 'by', 'finishing', 'on', 'experiment', 'completely', 'before', 'moving', 'to', 'the', 'other', 'one']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 11), (6, 14), (7, 6), (8, 12), (9, 9), (10, 3), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "5 ['-', 'One', 'very', 'important', 'lesson']\n",
      "5 ['from', 'the', 'conditional', 'computation', 'literature']\n",
      "1 ['is']\n",
      "29 ['that', 'while', 'we', 'can', 'in', 'theory', 'incur', 'much', 'less', 'computation', ',', 'in', 'practice', '-LRB-', 'especially', 'with', 'the', 'current', 'GPU', 'architectures', '-RRB-', 'the', 'actual', 'time', 'does', 'not', 'match', 'the', 'theory']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 2), (7, 2), (8, 4), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  8\n",
      "1 ['inefficient']\n",
      "1 ['branching']\n",
      "1 ['in']\n",
      "1 ['GPUs']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 5), (6, 4), (7, 5), (8, 8), (9, 5), (10, 5), (11, 2), (12, 7), (13, 6), (14, 2), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  5\n",
      "1 ['nice']\n",
      "19 ['if', 'the', 'paper', 'includes', 'a', 'discussion', 'of', 'how', 'their', 'model', '-LRB-', 'and', 'perhaps', 'implementation', '-RRB-', 'deal', 'with', 'this', 'problem']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "6 ['why', 'it', 'scales', 'well', 'in', 'practice']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 8), (5, 7), (6, 5), (7, 3), (8, 2), (9, 3), (10, 5), (11, 3), (12, 4), (13, 4), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "9 ['-', 'Table', '1', 'and', 'Table', '3', 'contain', 'repetitive', 'information']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "16 ['I', 'think', 'they', 'should', 'be', 'combined', 'in', 'one', '-LRB-', 'maybe', 'moving', 'Table', '3', 'to', 'appendix', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 4), (6, 6), (7, 2), (8, 2), (9, 4), (10, 6), (11, 5), (12, 3), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "2 ['One', 'thing']\n",
      "4 ['I', 'do', 'not', 'understand']\n",
      "1 ['is']\n",
      "13 ['how', 'does', 'the', 'number', 'of', 'ops', '/', 'timestep', 'relate', 'to', 'the', 'training', 'time']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['This']\n",
      "1 ['also']\n",
      "5 ['related', 'to', 'the', 'pervious', 'comment']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 1), (3, 5), (4, 5), (5, 4), (6, 6), (7, 4), (8, 8), (9, 2), (10, 5), (11, 5), (12, 5), (13, 6), (14, 10), (15, 7), (16, 5), (17, 5), (18, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  3\n",
      "2 ['Paper', 'Weaknesses']\n",
      "1 [':']\n",
      "1 ['---']\n",
      "40 ['there', 'are', 'many', 'different', 'ways', 'of', 'increasing', 'model', 'capacity', 'to', 'enable', 'the', 'exploitation', 'of', 'very', 'large', 'datasets', ';', 'it', 'would', 'be', 'very', 'nice', 'to', 'discuss', 'the', 'use', 'of', 'MoE', 'and', 'other', 'alternatives', 'in', 'terms', 'of', 'computational', 'efficiency', 'and', 'other', 'factors']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 4), (5, 5), (6, 4), (7, 9), (8, 11), (9, 8), (10, 8), (11, 6), (12, 7), (13, 6), (14, 4), (15, 2), (16, 2), (17, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['describes']\n",
      "24 ['a', 'method', 'for', 'greatly', 'expanding', 'network', 'model', 'size', '-LRB-', 'in', 'terms', 'of', 'number', 'of', 'stored', 'parameters', '-RRB-', 'in', 'the', 'context', 'of', 'a', 'recurrent', 'net']\n",
      "1 [',']\n",
      "17 ['by', 'applying', 'a', 'Mixture', 'of', 'Experts', 'between', 'recurrent', 'net', 'layers', 'that', 'is', 'shared', 'between', 'all', 'time', 'steps']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 10), (4, 14), (5, 14), (6, 11), (7, 6), (8, 10), (9, 13), (10, 9), (11, 7), (12, 6), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "31 ['By', 'process', 'features', 'from', 'all', 'timesteps', 'at', 'the', 'same', 'time', ',', 'the', 'effective', 'batch', 'size', 'to', 'the', 'MoE', 'is', 'increased', 'by', 'a', 'factor', 'of', 'the', 'number', 'of', 'steps', 'in', 'the', 'model']\n",
      "1 [';']\n",
      "23 ['thus', 'even', 'for', 'sparsely', 'assigned', 'experts', ',', 'each', 'expert', 'can', 'be', 'used', 'on', 'a', 'large', 'enough', 'sub-batch', 'of', 'inputs', 'to', 'remain', 'computationally', 'efficient']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 9), (5, 4), (6, 7), (7, 5), (8, 3), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "3 ['Another', 'second', 'technique']\n",
      "7 ['that', 'redistributes', 'elements', 'within', 'a', 'distributed', 'model']\n",
      "1 ['is']\n",
      "1 ['also']\n",
      "9 ['described', ',', 'further', 'increasing', 'per', '-', 'expert', 'batch', 'sizes']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 3), (7, 9), (8, 11), (9, 11), (10, 7), (11, 4), (12, 2), (13, 4), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  7\n",
      "6 ['language', 'modeling', 'and', 'machine', 'translation', 'tasks']\n",
      "1 [',']\n",
      "14 ['showing', 'significant', 'gains', 'by', 'increasing', 'the', 'number', 'of', 'experts', ',', 'compared', 'to', 'both', 'SoA']\n",
      "1 ['as']\n",
      "1 ['well']\n",
      "1 ['as']\n",
      "4 ['explicitly', 'computationally', '-', 'matched']\n",
      "1 ['baseline']\n",
      "1 ['systems']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 3), (6, 5), (7, 7), (8, 6), (9, 3), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "2 ['An', 'area']\n",
      "5 ['that', 'falls', 'a', 'bit', 'short']\n",
      "1 ['is']\n",
      "13 ['in', 'presenting', 'plots', 'or', 'statistics', 'on', 'the', 'real', 'computational', 'load', 'and', 'system', 'behavior']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 4), (5, 7), (6, 5), (7, 1), (8, 2), (9, 2), (10, 2), (11, 4), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "12 ['While', 'two', 'loss', 'terms', 'were', 'employed', 'to', 'balance', 'the', 'use', 'of', 'experts']\n",
      "1 [',']\n",
      "1 ['these']\n",
      "7 ['are', 'not', 'explored', 'in', 'the', 'experiments', 'section']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 2), (7, 1), (8, 2), (9, 2), (10, 4), (11, 6), (12, 4), (13, 2), (14, 4), (15, 1), (16, 2), (17, 7), (18, 8), (19, 5), (20, 5), (21, 8), (22, 6), (23, 7), (24, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "depth:  10\n",
      "2 ['the', 'effects']\n",
      "3 ['of', 'these', 'more']\n",
      "1 [',']\n",
      "35 ['along', 'with', 'the', 'effects', 'of', 'increasing', 'effective', 'batch', 'sizes', ',', 'eg', ':', 'measurements', 'of', 'the', 'losses', 'over', 'the', 'course', 'of', 'training', ',', 'compared', 'to', 'the', 'counts', '/', 'histogram', 'distributions', 'of', 'per', '-', 'expert', 'batch', 'sizes']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 1), (5, 2), (6, 3), (7, 2), (8, 5), (9, 5), (10, 4), (11, 3), (12, 3), (13, 5), (14, 2), (15, 4), (16, 2), (17, 2), (18, 2), (19, 2), (20, 3), (21, 3), (22, 2), (23, 2), (24, 4), (25, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "depth:  2\n",
      "1 ['Overall']\n",
      "1 ['I']\n",
      "32 ['think', 'this', 'is', 'a', 'well', '-', 'described', 'system', 'that', 'achieves', 'good', 'results', ',', 'using', 'a', 'nifty', 'placement', 'for', 'the', 'MoE', 'that', 'can', 'overcome', 'what', 'otherwise', 'might', 'be', 'a', 'disadvantage', 'for', 'sparse', 'computation']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 4), (5, 8), (6, 6), (7, 2), (8, 3), (9, 2), (10, 2), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "2 ['Small', 'comment']\n",
      "1 [':']\n",
      "19 ['I', 'like', 'Figure', '3', ',', 'but', 'it', \"'s\", 'not', 'entirely', 'clear', 'whether', 'datapoints', 'coincide', 'between', 'left', 'and', 'right', 'plots']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 6), (5, 4), (6, 7), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "3 ['H', '-', 'H']\n",
      "1 ['line']\n",
      "1 ['has']\n",
      "9 ['3', 'points', 'on', 'left', 'but', '5', 'on', 'the', 'right']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 3), (6, 2), (7, 4), (8, 2), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  7\n",
      "1 ['the']\n",
      "1 ['colors']\n",
      "1 ['matched']\n",
      "3 ['between', 'corresponding', 'lines']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 13), (5, 12), (6, 16), (7, 8), (8, 11), (9, 8), (10, 9), (11, 6), (12, 2), (13, 10), (14, 5), (15, 3), (16, 2), (17, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "45 ['A', 'much', 'simpler', 'version', 'of', 'a', 'similar', 'trick', 'may', 'have', 'been', 'proposed', 'in', 'the', 'context', 'of', 'memory', 'networks', ',', 'also', 'for', 'ICLR', \"'17\", '-LRB-', 'see', 'match', 'type', 'in', '\"', 'LEARNING', 'END', '-', 'TO', '-', 'END', 'GOAL', '-', 'ORIENTED', 'DIALOG', '\"', 'by', 'Bordes', 'et', 'al', '-RRB-']\n",
      "13 ['The', 'authors', 'also', 'mention', 'the', 'time', 'and', 'size', 'needed', 'to', 'train', 'the', 'model']\n",
      "1 [':']\n",
      "10 ['is', 'the', 'issue', 'arising', 'for', 'learning', ',', 'inference', 'or', 'both']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 4), (5, 3), (6, 4), (7, 4), (8, 6), (9, 4), (10, 5), (11, 4), (12, 4), (13, 4), (14, 2), (15, 2), (16, 2), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "19 ['proposes', 'learning', 'on', 'the', 'fly', 'to', 'represent', 'a', 'dialog', 'as', 'a', 'graph', '-LRB-', 'which', 'acts', 'as', 'the', 'memory', '-RRB-']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "7 ['is', 'first', 'demonstrated', 'on', 'the', 'bAbI', 'tasks']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 7), (5, 5), (6, 7), (7, 8), (8, 10), (9, 7), (10, 2), (11, 3), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "10 ['Graph', 'learning', 'is', 'part', 'of', 'the', 'inference', 'process', ',', 'though']\n",
      "1 ['there']\n",
      "20 ['is', 'long', 'term', 'representation', 'learning', 'to', 'learn', 'graph', 'transformation', 'parameters', 'and', 'the', 'encoding', 'of', 'sentences', 'as', 'input', 'to', 'the', 'graph']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 9), (5, 8), (6, 12), (7, 16), (8, 17), (9, 16), (10, 10), (11, 10), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "13 ['This', 'seems', 'to', 'be', 'the', 'first', 'implementation', 'of', 'a', 'differentiable', 'memory', 'as', 'graph']\n",
      "1 [':']\n",
      "43 ['it', 'is', 'much', 'more', 'complex', 'than', 'previous', 'approaches', 'like', 'memory', 'networks', 'without', 'significant', 'gain', 'in', 'performance', 'in', 'bAbI', 'tasks', ',', 'but', 'it', 'is', 'still', 'very', 'preliminary', 'work', ',', 'and', 'the', 'representation', 'of', 'memory', 'as', 'a', 'graph', 'seems', 'much', 'more', 'powerful', 'than', 'a', 'stack']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 9), (5, 9), (6, 8), (7, 3), (8, 2), (9, 4), (10, 4), (11, 2), (12, 2), (13, 4), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "5 ['Clarity', 'is', 'a', 'major', 'issue']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "25 ['from', 'an', 'initial', 'version', 'that', 'was', 'constructive', 'and', 'better', 'read', 'by', 'a', 'computer', 'than', 'a', 'human', ',', 'the', 'author', 'proposed', 'a', 'hugely', 'improved', 'later', 'version']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 7), (5, 8), (6, 6), (7, 3), (8, 1), (9, 2), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "5 ['This', 'original', ',', 'technically', 'accurate']\n",
      "1 ['-LRB-']\n",
      "9 ['within', 'what', 'I', 'understood', '-RRB-', 'and', 'thought', 'provoking', 'paper']\n",
      "1 ['is']\n",
      "2 ['worth', 'publishing']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 4), (5, 4), (6, 2), (7, 6), (8, 5), (9, 7), (10, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['preliminary']\n",
      "1 ['results']\n",
      "1 ['do']\n",
      "1 ['not']\n",
      "21 ['tell', 'us', 'yet', 'if', 'the', 'highly', 'complex', 'graph', '-', 'based', 'differentiable', 'memory', 'has', 'more', 'learning', 'or', 'generalization', 'capacity', 'than', 'other', 'approaches']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 11), (5, 9), (6, 6), (7, 8), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "2 ['The', 'performance']\n",
      "4 ['on', 'the', 'bAbI', 'task']\n",
      "1 ['is']\n",
      "15 ['comparable', 'to', 'the', 'best', 'memory', 'networks', ',', 'but', 'still', 'worse', 'than', 'more', 'traditional', 'rule', 'induction']\n",
      "4 ['-LRB-', 'see', 'http://www.public.asu.edu/~cbaral/papers/aaai2016-sub.pdf', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['is']\n",
      "1 ['still']\n",
      "2 ['clearly', 'promising']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 7), (5, 4), (6, 8), (7, 4), (8, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "2 ['The', 'sequence']\n",
      "5 ['of', 'transformation', 'in', 'algorithm', '1']\n",
      "1 ['looks']\n",
      "1 ['sensible']\n",
      "1 [',']\n",
      "10 ['though', 'the', 'authors', 'do', 'not', 'discuss', 'any', 'other', 'operation', 'ordering']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 5), (5, 4), (6, 5), (7, 8), (8, 2), (9, 4), (10, 3), (11, 5), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "2 ['In', 'particular']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "25 ['is', 'not', 'clear', 'to', 'me', 'that', 'you', 'need', 'the', 'node', 'state', 'update', 'step', 'T_h', 'if', 'you', 'have', 'the', 'direct', 'reference', 'update', 'step', 'T_h', ',', 'direct']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 4), (6, 4), (7, 4), (8, 7), (9, 10), (10, 11), (11, 7), (12, 9), (13, 10), (14, 15), (15, 6), (16, 5), (17, 2), (18, 2), (19, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  2\n",
      "42 ['It', 'is', 'striking', 'that', 'the', 'only', 'trick', 'that', 'is', 'essential', 'for', 'proper', 'performance', 'is', 'the', '‘', 'direct', 'reference', '’', ',', 'which', 'actually', 'has', 'nothing', 'to', 'do', 'with', 'the', 'graph', 'building', 'process', ',', 'but', 'is', 'rather', 'an', 'attention', 'mechanism', 'for', 'the', 'graph', 'input']\n",
      "1 [':']\n",
      "17 ['attention', 'is', 'focused', 'on', 'words', 'that', 'are', 'relevant', 'to', 'the', 'node', 'type', 'rather', 'than', 'the', 'whole', 'sentence']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 4), (5, 3), (6, 3), (7, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['So']\n",
      "11 ['the', 'question', '“', 'how', 'useful', 'are', 'all', 'these', 'graph', 'operations', '”']\n",
      "1 ['remain']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 9), (5, 7), (6, 6), (7, 2), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "8 ['A', 'description', 'of', 'the', 'actual', 'implementation', 'would', 'help']\n",
      "1 ['-LRB-']\n",
      "8 ['no', 'pointer', 'to', 'open', 'source', 'code', 'is', 'provide']\n",
      "1 ['-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 7), (4, 5), (5, 6), (6, 8), (7, 6), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['author']\n",
      "1 ['mentions']\n",
      "1 ['Theano']\n",
      "5 ['in', 'one', 'of', 'my', 'questions']\n",
      "1 [':']\n",
      "9 ['how', 'are', 'the', 'transformations', 'compiled', 'in', 'advance', 'as', 'units']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 2), (6, 2), (7, 3), (8, 1), (9, 2), (10, 2), (11, 3), (12, 1), (13, 3), (14, 3), (15, 2), (16, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  3\n",
      "1 ['How']\n",
      "1 ['is']\n",
      "2 ['the', 'gradient']\n",
      "14 ['back', '-', 'propagated', 'through', 'the', 'graph', 'is', 'this', 'one', 'is', 'only', 'described', 'at', 'runtime']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 6), (5, 11), (6, 10), (7, 11), (8, 4), (9, 5), (10, 5), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['Typo']\n",
      "1 [':']\n",
      "30 ['in', 'the', 'appendices', 'B.', '2', 'and', 'B.', '2.1', ',', 'the', 'right', 'side', 'of', 'the', 'equation', 'that', 'applies', 'the', 'update', 'gate', 'has', 'h', '’', '_', 'nu', 'while', 'it', 'should', 'be', 'h_nu']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 5), (5, 4), (6, 6), (7, 5), (8, 3), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "3 ['In', 'the', 'references']\n",
      "1 [',']\n",
      "2 ['the', 'author']\n",
      "13 ['could', 'mention', 'the', 'pioneering', 'work', 'of', 'Lee', 'Giles', 'on', 'representing', 'graphs', 'with', 'RNNs']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 8), (5, 6), (6, 6), (7, 7), (8, 4), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "12 ['Revision', ':', 'I', 'have', 'improved', 'my', 'rating', 'for', 'the', 'following', 'reasons', ':']\n",
      "1 ['-']\n",
      "10 ['Pointers', 'to', 'an', 'highly', 'readable', 'and', 'well', 'structured', 'Theano', 'source']\n",
      "2 ['is', 'provided']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 4), (4, 7), (5, 10), (6, 6), (7, 5), (8, 3), (9, 2), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "13 ['The', 'delta', 'improvement', 'of', 'the', 'paper', 'has', 'been', 'impressive', 'over', 'the', 'review', 'process']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "9 ['I', 'am', 'confident', 'this', 'will', 'be', 'an', 'impactful', 'paper']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 5), (5, 6), (6, 5), (7, 4), (8, 5), (9, 2), (10, 2), (11, 4), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "19 ['-', 'Much', 'simpler', 'alternatives', 'approaches', 'such', 'as', 'Memory', 'Networks', 'seem', 'to', 'be', 'plateauing', 'for', 'problems', 'such', 'as', 'dialog', 'modeling']\n",
      "1 [',']\n",
      "1 ['we']\n",
      "2 ['need', 'alternatives']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 6), (5, 10), (6, 13), (7, 8), (8, 4), (9, 3), (10, 3), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "2 ['The', 'architecture']\n",
      "12 ['is', 'this', 'work', 'is', 'still', 'too', 'complex', ',', 'but', 'this', 'is', 'often']\n",
      "14 ['as', 'we', 'start', 'with', 'DNNs', ',', 'and', 'then', 'find', 'simplifications', 'that', 'actually', 'improve', 'performance']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 4), (6, 2), (7, 2), (8, 4), (9, 2), (10, 4), (11, 2), (12, 5), (13, 2), (14, 2), (15, 2), (16, 2), (17, 3), (18, 3), (19, 3), (20, 5), (21, 5), (22, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "depth:  3\n",
      "3 ['The', 'main', 'contribution']\n",
      "3 ['of', 'this', 'paper']\n",
      "1 ['seems']\n",
      "26 ['to', 'be', 'an', 'introduction', 'of', 'a', 'set', 'of', 'differential', 'graph', 'transformations', 'which', 'will', 'allow', 'you', 'to', 'learn', 'graph', '-', '>', 'graph', 'classification', 'tasks', 'using', 'gradient', 'descent']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 2), (6, 4), (7, 1), (8, 2), (9, 2), (10, 5), (11, 2), (12, 2), (13, 3), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['maps']\n",
      "1 ['naturally']\n",
      "13 ['to', 'a', 'task', 'of', 'learning', 'a', 'cellular', 'automaton', 'represented', 'as', 'sequence', 'of', 'graphs']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 10), (5, 5), (6, 4), (7, 4), (8, 6), (9, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "3 ['In', 'that', 'task']\n",
      "1 [',']\n",
      "4 ['the', 'graph', 'of', 'nodes']\n",
      "17 ['grows', 'at', 'each', 'iteration', ',', 'with', 'nodes', 'pointing', 'to', 'neighbors', 'and', 'special', 'nodes', '0/1', 'representing', 'the', 'values']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 2), (6, 4), (7, 4), (8, 8), (9, 8), (10, 10), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['Proposed']\n",
      "1 ['architecture']\n",
      "1 ['allows']\n",
      "1 ['one']\n",
      "22 ['to', 'learn', 'this', 'sequence', 'of', 'graphs', ',', 'although', 'in', 'the', 'experiment', ',', 'this', 'task', '-LRB-', 'Rule', '30', '-RRB-', 'was', 'far', 'from', 'solved']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 3), (6, 4), (7, 6), (8, 6), (9, 5), (10, 7), (11, 5), (12, 6), (13, 9), (14, 4), (15, 2), (16, 2), (17, 2), (18, 3), (19, 2), (20, 2), (21, 3), (22, 2), (23, 4), (24, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['idea']\n",
      "1 ['is']\n",
      "43 ['combined', 'with', 'ideas', 'from', 'previous', 'papers', '-LRB-', 'GGS', '-', 'NN', '-RRB-', 'to', 'allow', 'the', 'model', 'to', 'produce', 'textual', 'output', 'rather', 'than', 'graph', 'output', ',', 'and', 'use', 'graphs', 'as', 'intermediate', 'representation', ',', 'which', 'allows', 'it', 'to', 'beat', 'state', 'of', 'the', 'art', 'on', 'BaBi', 'tasks']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 5), (6, 5), (7, 11), (8, 3), (9, 2), (10, 2), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['proposes']\n",
      "9 ['an', 'extension', 'of', 'the', 'Gated', 'Graph', 'Sequence', 'Neural', 'Network']\n",
      "12 ['by', 'including', 'in', 'this', 'model', 'the', 'ability', 'to', 'produce', 'complex', 'graph', 'transformations']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 1), (5, 2), (6, 2), (7, 2), (8, 4), (9, 2), (10, 2), (11, 3), (12, 7), (13, 5), (14, 2), (15, 5), (16, 8), (17, 6), (18, 10), (19, 2), (20, 2), (21, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['underlying']\n",
      "1 ['idea']\n",
      "1 ['is']\n",
      "35 ['to', 'propose', 'a', 'method', 'that', 'will', 'be', 'able', 'build', '/', 'modify', 'a', 'graph', '-', 'structure', 'as', 'an', 'internal', 'representation', 'for', 'solving', 'a', 'problem', ',', 'and', 'particularly', 'for', 'solving', 'question', '-', 'answering', 'problems', 'in', 'this', 'paper']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 7), (6, 2), (7, 2), (8, 2), (9, 4), (10, 5), (11, 6), (12, 5), (13, 3), (14, 4), (15, 6), (16, 4), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['author']\n",
      "1 ['proposes']\n",
      "30 ['5', 'different', 'possible', 'differentiable', 'transformations', 'that', 'will', 'be', 'learned', 'on', 'a', 'training', 'set', ',', 'typically', 'in', 'a', 'supervised', 'fashion', 'where', 'the', 'state', 'of', 'the', 'graph', 'is', 'given', 'at', 'each', 'timestep']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 7), (5, 4), (6, 1), (7, 3), (8, 4), (9, 8), (10, 4), (11, 5), (12, 8), (13, 11), (14, 6), (15, 7), (16, 3), (17, 2), (18, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  3\n",
      "3 ['A', 'particular', 'occurence']\n",
      "3 ['of', 'the', 'model']\n",
      "1 ['is']\n",
      "37 ['presented', 'that', 'takes', 'a', 'sequence', 'as', 'an', 'input', 'a', 'iteratively', 'update', 'an', 'internal', 'graph', 'state', 'to', 'a', 'final', 'prediction', ',', 'and', 'which', 'can', 'be', 'applied', 'for', 'solving', 'QA', 'tasks', '-LRB-', 'eg', ':', 'BaBi', '-RRB-', 'with', 'interesting', 'results']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 8), (5, 10), (6, 8), (7, 6), (8, 4), (9, 3), (10, 5), (11, 7), (12, 8), (13, 10), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "26 ['The', 'approach', 'in', 'this', 'paper', 'is', 'really', 'interesting', 'since', 'the', 'proposed', 'model', 'is', 'able', 'to', 'maintain', 'a', 'representation', 'of', 'its', 'current', 'state', 'as', 'a', 'complex', 'graph']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "16 ['still', 'keeping', 'the', 'property', 'of', 'being', 'differentiable', 'and', 'thus', 'easily', 'learnable', 'through', 'gradient', '-', 'descent', 'techniques']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 3), (6, 3), (7, 5), (8, 2), (9, 2), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  7\n",
      "1 ['a']\n",
      "1 ['succesfull']\n",
      "1 ['attempt']\n",
      "1 ['to']\n",
      "5 ['mix', 'continuous', 'and', 'symbolic', 'representations']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 4), (5, 2), (6, 5), (7, 1), (8, 2), (9, 2), (10, 2), (11, 7), (12, 7), (13, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['It']\n",
      "1 ['moreover']\n",
      "25 ['seems', 'more', 'general', 'that', 'the', 'recent', 'attempts', 'made', 'to', 'add', 'some', \"'\", 'symbolic', \"'\", 'stuffs', 'in', 'differentiable', 'models', '-LRB-', 'Memory', 'networks', ',', 'NTM', ',', 'etc.']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 2), (6, 3), (7, 2), (8, 6), (9, 5), (10, 5), (11, 5), (12, 5), (13, 5), (14, 5), (15, 4), (16, 2), (17, 4), (18, 2), (19, 4), (20, 2), (21, 2), (22, 2), (23, 3), (24, 7), (25, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "depth:  3\n",
      "1 ['My']\n",
      "1 ['main']\n",
      "1 ['concerns']\n",
      "1 ['is']\n",
      "42 ['about', 'the', 'way', 'the', 'model', 'is', 'trained', 'i.e', 'by', 'providing', 'the', 'state', 'of', 'the', 'graph', 'at', 'each', 'timestep', 'which', 'can', 'be', 'done', 'for', 'particular', 'tasks', '-LRB-', 'eg', ':', 'Babi', '-RRB-', 'only', ',', 'and', 'can', 'not', 'be', 'the', 'solution', 'for', 'more', 'complex', 'problems']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 2), (6, 5), (7, 2), (8, 4), (9, 2), (10, 4), (11, 6), (12, 5), (13, 9), (14, 3), (15, 6), (16, 2), (17, 2), (18, 2), (19, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  3\n",
      "1 ['My']\n",
      "1 ['other']\n",
      "1 ['concern']\n",
      "1 ['is']\n",
      "32 ['about', 'the', 'whole', 'content', 'of', 'the', 'paper', 'that', 'would', 'perhaps', 'best', 'fit', 'a', 'journal', 'format', 'and', 'not', 'a', 'conference', 'format', ',', 'making', 'the', 'article', 'still', 'difficult', 'to', 'read', 'due', 'to', 'its', 'density']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 8), (5, 11), (6, 9), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "7 ['Even', 'if', 'the', 'assumption', 'does', 'not', 'hold']\n",
      "1 [',']\n",
      "9 ['relatively', 'close', 'values', 'for', 'average', 'activation', 'between', 'the', 'networks']\n",
      "6 ['would', 'make', 'the', 'comparison', 'more', 'convincing']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 3), (4, 5), (5, 2), (6, 2), (7, 3), (8, 3), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['-LRB-']\n",
      "1 ['Nit']\n",
      "1 [':']\n",
      "12 ['I', 'do', \"n't\", 'see', 'measurements', 'for', 'the', 'full', '-', 'precision', 'baseline', '-RRB-']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 1), (7, 2), (8, 3), (9, 4), (10, 6), (11, 11), (12, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  9\n",
      "5 ['a', 'SOTA', 'result', 'on', 'ImageNet']\n",
      "1 ['and']\n",
      "7 ['a', 'result', 'on', 'a', 'strong', 'LSTM', 'baseline']\n",
      "4 ['to', 'be', 'fully', 'convinced']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 4), (6, 6), (7, 6), (8, 5), (9, 2), (10, 6), (11, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['goes']\n",
      "1 ['on']\n",
      "15 ['to', 'provide', 'an', 'exhaustive', 'analysis', 'of', 'performance', '-LRB-', 'essentially', 'no', 'loss', '-RRB-', 'on', 'real', 'benchmarks']\n",
      "9 ['-LRB-', 'this', 'paper', 'is', 'remarkably', 'MNIST', '-', 'free', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 3), (6, 2), (7, 3), (8, 4), (9, 7), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['is']\n",
      "1 ['\"']\n",
      "14 ['what', 'is', 'new', 'in', 'the', 'topic', '\"', 'update', ',', 'not', 'really', 'a', 'standalone', 'paper']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 12), (5, 10), (6, 4), (7, 5), (8, 4), (9, 4), (10, 5), (11, 6), (12, 8), (13, 7), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "2 ['The', 'description']\n",
      "4 ['of', 'the', 'main', 'algorithm']\n",
      "8 ['is', 'very', 'concise', ',', 'to', 'say', 'the', 'least']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "26 ['is', 'probably', 'clear', 'to', 'those', 'who', 'read', 'some', 'of', 'the', 'previous', 'work', 'on', 'this', 'narrow', 'subject', ',', 'but', 'is', 'unsuitable', 'for', 'a', 'broader', 'deep', 'learning', 'audience']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 5), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "2 ['3', '-RRB-']\n",
      "1 ['There']\n",
      "7 ['is', 'no', 'convincing', 'motivation', 'for', 'the', 'work']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 4), (6, 4), (7, 5), (8, 9), (9, 3), (10, 6), (11, 4), (12, 4), (13, 6), (14, 4), (15, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  5\n",
      "4 ['is', 'an', 'engineering', 'gimmick']\n",
      "1 [',']\n",
      "1 ['that']\n",
      "20 ['would', 'be', 'cool', 'and', 'valuable', 'if', 'it', 'really', 'is', 'used', 'in', 'production', ',', 'but', 'is', 'that', 'really', 'needed', 'for', 'anything']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 5), (5, 2), (6, 2), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['Are']\n",
      "1 ['there']\n",
      "7 ['any', 'practical', 'applications', 'that', 'require', 'this', 'refinement']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 10), (5, 5), (6, 4), (7, 2), (8, 3), (9, 2), (10, 2), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "12 ['I', 'do', 'not', 'find', 'the', 'motivation', '\"', 'it', 'is', 'related', 'to', 'mobile']\n",
      "1 [',']\n",
      "6 ['therefore', 'it', 'is', 'cool', '\"', 'sufficient']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 10), (5, 9), (6, 9), (7, 10), (8, 6), (9, 8), (10, 3), (11, 6), (12, 8), (13, 5), (14, 5), (15, 2), (16, 5), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "11 ['This', 'paper', 'is', 'a', 'small', 'step', 'further', 'in', 'a', 'niche', 'research']\n",
      "1 [',']\n",
      "43 ['as', 'long', 'as', 'the', 'authors', 'do', 'not', 'provide', 'a', 'sufficient', 'practical', 'motivation', 'for', 'pursuing', 'this', 'particular', 'topic', 'with', 'the', 'next', 'step', 'on', 'a', 'long', 'list', 'of', 'small', 'refinements', ',', 'I', 'do', 'not', 'think', 'it', 'belongs', 'in', 'ICLR', 'with', 'a', 'broad', 'and', 'diversified', 'audience']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 5), (5, 1), (6, 3), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['Also']\n",
      "1 ['-']\n",
      "5 ['the', 'code', 'was', 'not', 'released']\n",
      "3 ['is', 'my', 'understanding']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 1), (6, 2), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['presents']\n",
      "2 ['new', 'way']\n",
      "4 ['for', 'compressing', 'CNN', 'weights']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 3), (5, 7), (6, 4), (7, 3), (8, 4), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['In', 'particular']\n",
      "2 ['this', 'paper']\n",
      "14 ['uses', 'a', 'new', 'neural', 'network', 'quantization', 'method', 'that', 'compresses', 'network', 'weights', 'to', 'ternary', 'values']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 10), (5, 7), (6, 8), (7, 4), (8, 3), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "10 ['The', 'group', 'has', 'recently', 'published', 'multiple', 'paper', 'on', 'this', 'topic']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "10 ['this', 'one', 'offers', 'possibly', 'the', 'lowest', 'returns', 'I', 'have', 'seen']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 5), (5, 1)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['Only']\n",
      "4 ['a', 'fraction', 'of', 'percentage']\n",
      "2 ['in', 'ImageNet']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 8), (5, 5), (6, 5), (7, 5), (8, 1), (9, 2), (10, 4), (11, 6), (12, 4), (13, 5), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['Results']\n",
      "2 ['on', 'AlexNet']\n",
      "1 ['are']\n",
      "5 ['of', 'very', 'little', 'interest', 'now']\n",
      "1 [',']\n",
      "18 ['given', 'the', 'group', 'already', 'showed', 'this', 'kind', 'of', 'older', 'style', '-', 'network', 'can', 'be', 'compressed', 'by', 'large', 'amounts']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 5), (5, 6), (6, 9), (7, 8), (8, 7), (9, 12), (10, 13), (11, 5), (12, 1), (13, 2), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['I']\n",
      "1 ['also']\n",
      "39 ['would', 'have', 'liked', 'to', 'see', 'this', 'group', 'release', 'code', 'for', 'the', 'compression', ',', 'and', 'also', 'report', 'data', 'on', 'the', 'amount', 'of', 'effort', 'required', 'to', 'compress', ':', 'flops', ',', 'time', ',', 'number', 'of', 'passes', ',', 'required', 'original', 'dataset', ',', 'etc.']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 1), (6, 2), (7, 2), (8, 2), (9, 2), (10, 4), (11, 2), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['data']\n",
      "1 ['is']\n",
      "10 ['important', 'to', 'decide', 'if', 'a', 'compression', 'is', 'worth', 'the', 'effort']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 4), (6, 8), (7, 4), (8, 2), (9, 2), (10, 4), (11, 5), (12, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['work']\n",
      "1 ['presents']\n",
      "20 ['a', 'novel', 'ternary', 'weight', 'quantization', 'approach', 'which', 'quantizes', 'weights', 'to', 'either', '0', 'or', 'one', 'of', 'two', 'layer', 'specific', 'learned', 'values']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 6), (5, 3), (6, 2), (7, 2), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "3 ['Unlike', 'past', 'work']\n",
      "1 [',']\n",
      "3 ['these', 'quantized', 'values']\n",
      "10 ['are', 'separate', 'and', 'learned', 'stochastically', 'alongside', 'all', 'other', 'network', 'parameters']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 1), (6, 4), (7, 2), (8, 5), (9, 6), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['approach']\n",
      "1 ['achieves']\n",
      "3 ['impressive', 'quantization', 'results']\n",
      "13 ['while', 'retaining', 'or', 'surpassing', 'corresponding', 'full', '-', 'precision', 'networks', 'on', 'CIFAR10', 'and', 'ImageNet']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 5), (6, 3), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "6 ['Strengths', ':', '-', 'Overall', 'well', 'written']\n",
      "1 ['and']\n",
      "1 ['algorithm']\n",
      "1 ['is']\n",
      "2 ['presented', 'clearly']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 4), (6, 7), (7, 6), (8, 3), (9, 3), (10, 6), (11, 5), (12, 4), (13, 3), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  5\n",
      "11 ['-', 'Approach', 'appears', 'to', 'work', 'well', 'in', 'the', 'experiments', ',', 'resulting']\n",
      "3 ['in', 'good', 'compression']\n",
      "1 ['without']\n",
      "4 ['loss', '-LRB-', 'and', 'sometimes']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 1)]\n",
      "[0, 1, 2, 3]\n",
      "depth:  3\n",
      "1 ['performance']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 7), (5, 11), (6, 8), (7, 6), (8, 5), (9, 3), (10, 2), (11, 5), (12, 2), (13, 2), (14, 2), (15, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "1 ['I']\n",
      "30 ['enjoyed', 'the', 'analysis', 'of', 'sparsity', '-LRB-', 'and', 'how', 'it', 'changes', '-RRB-', 'over', 'the', 'course', 'of', 'training', ',', 'though', 'it', 'is', 'uncertain', 'if', 'any', 'useful', 'conclusion', 'can', 'be', 'drawn', 'from', 'it']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 5), (5, 9), (6, 4), (7, 2), (8, 4), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['Some', 'points']\n",
      "1 [':']\n",
      "1 ['-']\n",
      "16 ['The', 'energy', 'analysis', 'in', 'Table', '3', 'assumes', 'dense', 'activations', 'due', 'to', 'the', 'unpredictability', 'of', 'sparse', 'activations']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 5), (5, 1), (6, 3), (7, 3), (8, 7), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Can']\n",
      "11 ['the', 'authors', 'provide', 'average', 'activation', 'sparsity', 'for', 'each', 'network', 'to', 'help']\n",
      "3 ['verify', 'this', 'assumption']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 3), (5, 2), (6, 2), (7, 5), (8, 11), (9, 10), (10, 8), (11, 8), (12, 6), (13, 5), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "3 ['In', 'section', '5.1.1']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "35 ['suggest', 'having', 'a', 'fixed', 't', '-LRB-', 'threshold', 'parameter', 'set', 'at', '0.05', '-RRB-', 'for', 'all', 'layers', 'allows', 'for', 'varying', 'sparsity', '-LRB-', 'owed', 'to', 'the', 'relative', 'magnitude', 'of', 'different', 'layer', 'weights', 'with', 'respect', 'to', 'the', 'maximum', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 6), (5, 4), (6, 1), (7, 2), (8, 1), (9, 2), (10, 4), (11, 2), (12, 2), (13, 2), (14, 1), (15, 3), (16, 4), (17, 5), (18, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  2\n",
      "5 ['In', 'Section', '5.1.2', 'paragraph', '2']\n",
      "1 [',']\n",
      "1 ['this']\n",
      "19 ['is', 'further', 'developed', 'by', 'suggesting', 'additional', 'sparsity', 'can', 'be', 'achieved', 'by', 'allowing', 'each', 'layer', 'a', 'different', 'values', 'of', 't']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 7), (5, 2), (6, 2), (7, 3), (8, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['Does']\n",
      "4 ['this', 'multiple', 'threshold', 'style']\n",
      "1 ['network']\n",
      "8 ['appear', 'in', 'any', 'of', 'the', 'tables', 'or', 'figures']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 1)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "1 ['Can']\n",
      "1 ['it']\n",
      "2 ['be', 'added']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 5), (5, 1), (6, 2), (7, 2), (8, 2), (9, 4), (10, 6), (11, 6), (12, 1), (13, 2), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "2 ['The', 'authors']\n",
      "15 ['claim', '\"', 'ii', '-RRB-', 'Quantized', 'weights', 'play', 'the', 'role', 'of', '\"', 'learning', 'rate', 'multipliers', '\"']\n",
      "1 ['during']\n",
      "2 ['back', 'propagation']\n",
      "1 ['.']\n",
      "1 ['\"']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 4), (5, 1), (6, 2), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['a']\n",
      "1 ['benefit']\n",
      "1 ['of']\n",
      "4 ['using', 'trained', 'quantization', 'factors']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['Why']\n",
      "1 ['is']\n",
      "3 ['this', 'a', 'benefit']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 6)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "2 ['-', 'Figure']\n",
      "1 ['and']\n",
      "2 ['table', 'captions']\n",
      "1 ['are']\n",
      "1 ['not']\n",
      "2 ['very', 'descriptive']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 3), (5, 1), (6, 2), (7, 4), (8, 5), (9, 5), (10, 7), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "2 ['Preliminary', 'Rating']\n",
      "1 [':']\n",
      "16 ['I', 'think', 'this', 'is', 'an', 'interesting', 'paper', 'with', 'convincing', 'results', 'but', 'is', 'somewhat', 'lacking', 'in', 'novelty']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 3), (5, 2), (6, 4), (7, 3), (8, 2), (9, 3), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "2 ['Minor', 'notes']\n",
      "1 [':']\n",
      "13 ['-', 'Table', '3', 'lists', 'FLOPS', 'rather', 'than', 'Energy', 'for', 'the', 'full', 'precision', 'model']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 1)]\n",
      "[0, 1, 2, 3]\n",
      "Depth to split: None\n",
      "depth:  None\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 10), (5, 4), (6, 1), (7, 2), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "2 ['-', 'Section']\n",
      "5 ['5', \"'\", 'speeding', 'up', \"'\"]\n",
      "4 ['5.1.1', 'figure', 'reference', 'error']\n",
      "2 ['last', 'line']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 11), (5, 13), (6, 2), (7, 4), (8, 5), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "15 ['This', 'paper', 'studies', 'in', 'depth', 'the', 'idea', 'of', 'quantizing', 'down', 'convolutional', 'layers', 'to', '3', 'bits']\n",
      "1 [',']\n",
      "10 ['with', 'a', 'different', 'positive', 'and', 'negative', 'per', '-', 'layer', 'scale']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 4), (6, 9), (7, 9), (8, 9), (9, 10), (10, 8), (11, 6), (12, 9), (13, 7), (14, 9), (15, 3), (16, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  3\n",
      "2 ['The', 'relevance']\n",
      "3 ['of', 'this', 'paper']\n",
      "1 ['is']\n",
      "45 ['that', 'it', 'likely', 'provides', 'a', 'lower', 'bound', 'on', 'quantization', 'approaches', 'that', 'do', \"n't\", 'sacrifice', 'any', 'performance', ',', 'and', 'hence', 'can', 'plausibly', 'become', 'the', 'approach', 'of', 'choice', 'for', 'resource', '-', 'constrained', 'inference', ',', 'and', 'might', 'suggest', 'new', 'hardware', 'designs', 'to', 'take', 'advantage', 'of', 'the', 'proposed', 'structure']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 4), (6, 2), (7, 4), (8, 6), (9, 2), (10, 4), (11, 5), (12, 3), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Furthermore']\n",
      "1 [',']\n",
      "2 ['the', 'paper']\n",
      "19 ['provides', 'power', 'measurements', ',', 'which', 'is', 'really', 'the', 'main', 'metric', 'that', 'anyone', 'working', 'seriously', 'in', 'that', 'space', 'cares', 'about']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 3), (6, 1), (7, 2), (8, 3), (9, 3), (10, 5), (11, 5), (12, 1), (13, 2), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  10\n",
      "1 ['discussion']\n",
      "1 ['of']\n",
      "3 ['the', 'wall', 'time']\n",
      "1 ['to']\n",
      "5 ['result', 'using', 'this', 'training', 'procedure']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 2), (6, 5), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['shows']\n",
      "3 ['a', 'different', 'approach']\n",
      "6 ['to', 'a', 'ternary', 'quantization', 'of', 'weights']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 10), (5, 15), (6, 8), (7, 11), (8, 10), (9, 4), (10, 2), (11, 3), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['Strengths']\n",
      "1 [':']\n",
      "12 ['1', '-RRB-', 'The', 'paper', 'shows', 'performance', 'improvements', 'over', 'existing', 'solutions', '2', '-RRB-']\n",
      "14 ['The', 'idea', 'of', 'learning', 'the', 'quantization', 'instead', 'of', 'using', 'pre-defined', 'human', '-', 'made', 'algorithm']\n",
      "12 ['is', 'nice', 'and', 'very', 'much', 'in', 'the', 'spirit', 'of', 'modern', 'machine', 'learning']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['Weaknesses']\n",
      "1 [':']\n",
      "7 ['1', '-RRB-', 'The', 'paper', 'is', 'very', 'incremental']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 2), (5, 2), (6, 3), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "2 ['The', 'paper']\n",
      "7 ['is', 'addressed', 'to', 'a', 'very', 'narrow', 'audience']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 2), (5, 2), (6, 4), (7, 2), (8, 2), (9, 2), (10, 5), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "2 ['The', 'paper']\n",
      "2 ['very', 'clearly']\n",
      "14 ['assumes', 'that', 'the', 'reader', 'is', 'familiar', 'with', 'the', 'previous', 'work', 'on', 'the', 'ternary', 'quantization']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 4), (5, 4), (6, 3), (7, 2), (8, 4), (9, 2), (10, 3), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['I']\n",
      "1 ['also']\n",
      "13 ['agree', 'with', 'Reviewer', '2', 'that', 'there', 'is', 'a', 'lack', 'of', 'comparison', 'against', 'baselines']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 3), (5, 5), (6, 3), (7, 5), (8, 7), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  5\n",
      "1 ['the']\n",
      "1 ['method']\n",
      "1 ['work']\n",
      "1 ['on']\n",
      "11 ['established', 'semantic', 'segmentation', 'datasets', 'with', 'many', 'classes', ',', 'such', 'as', 'PASCAL']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 8), (4, 13), (5, 12), (6, 3), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "4 ['Even', 'the', 'ADE20K', 'dataset']\n",
      "1 [',']\n",
      "5 ['from', 'which', 'this', 'paper', 'samples']\n",
      "1 [',']\n",
      "3 ['is', 'substantially', 'larger']\n",
      "1 ['and']\n",
      "5 ['has', 'an', 'established', 'benchmarking', 'methodology']\n",
      "4 ['-LRB-', 'see', 'http://placeschallenge.csail.mit.edu/', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 2), (6, 4), (7, 2), (8, 2), (9, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['An']\n",
      "1 ['additional']\n",
      "1 ['problem']\n",
      "1 ['is']\n",
      "10 ['that', 'performance', 'is', 'not', 'compared', 'to', 'any', 'external', 'prior', 'work']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 7), (5, 8), (6, 6), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "10 ['Only', 'simple', 'baselines', '-LRB-', 'eg', ':', 'autoencoder', ',', 'kmeans', '-RRB-']\n",
      "4 ['implemented', 'by', 'this', 'paper']\n",
      "1 ['are']\n",
      "1 ['included']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 4), (6, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['How']\n",
      "1 ['well']\n",
      "1 ['does']\n",
      "2 ['the', 'approach']\n",
      "9 ['compare', 'to', 'supervised', 'CNNs', 'on', 'an', 'established', 'segmentation', 'task']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 2), (5, 3), (6, 6), (7, 8), (8, 15), (9, 9), (10, 7), (11, 2), (12, 2), (13, 4), (14, 2), (15, 4), (16, 2), (17, 4), (18, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  6\n",
      "9 ['the', 'proposed', 'method', 'need', 'not', 'necessarily', 'outperform', 'supervised', 'approaches']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "18 ['the', 'reader', 'should', 'be', 'provided', 'with', 'some', 'idea', 'of', 'the', 'size', 'of', 'the', 'gap', 'between', 'this', 'unsupervised', 'method']\n",
      "8 ['the', 'state', '-', 'of', '-', 'the', '-', 'art']\n",
      "2 ['supervised', 'approach']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 11), (4, 8), (5, 9), (6, 9), (7, 11), (8, 2), (9, 3), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "3 ['-LRB-', '3', '-RRB-']\n",
      "6 ['Regarding', 'the', 'fine', '-', 'tuning', 'baselines']\n",
      "1 [',']\n",
      "2 ['the', 'comparison']\n",
      "24 ['is', 'a', 'bit', 'unfair', 'since', 'the', 'proposed', 'method', 'performs', 'pooling', 'over', 'images', ',', 'while', 'the', 'baseline', '-LRB-', 'average', 'mask', '-RRB-', 'is', 'not', 'translation', 'invariant']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 9), (5, 9), (6, 6), (7, 6), (8, 5), (9, 5), (10, 2), (11, 4), (12, 2), (13, 5), (14, 5), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "7 ['Even', 'if', 'it', 'does', 'work', 'as', 'advertised']\n",
      "1 [',']\n",
      "5 ['the', 'utilization', 'of', 'implicit', 'labels']\n",
      "24 ['would', 'make', 'it', 'subject', 'to', 'comparisons', 'with', 'a', 'lot', 'of', 'weakly', '-', 'supervised', 'learning', 'papers', 'with', 'far', 'better', 'results', 'than', 'shown', 'in', 'this', 'paper']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 8), (6, 6), (7, 3), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['creates']\n",
      "5 ['a', 'layered', 'representation', 'in', 'order']\n",
      "7 ['to', 'better', 'learn', 'segmentation', 'from', 'unlabeled', 'images']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 3), (6, 6), (7, 4), (8, 4), (9, 7), (10, 5), (11, 7), (12, 2), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['is']\n",
      "2 ['well', 'motivated']\n",
      "1 [',']\n",
      "23 ['as', 'Figure', '1', 'clearly', 'shows', 'the', 'idea', 'that', 'if', 'the', 'segmentation', 'was', 'removed', 'properly', ',', 'the', 'result', 'would', 'still', 'be', 'a', 'natural', 'image']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 8), (5, 8), (6, 5), (7, 3), (8, 2), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "8 ['the', 'method', 'itself', 'as', 'described', 'in', 'the', 'paper']\n",
      "11 ['leaves', 'many', 'questions', 'about', 'whether', 'they', 'can', 'achieve', 'the', 'proposed', 'goal']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 4), (6, 4), (7, 2), (8, 2), (9, 5), (10, 2), (11, 3), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['can']\n",
      "1 ['not']\n",
      "13 ['see', 'from', 'the', 'formulation', 'why', 'would', 'this', 'model', 'work', 'as', 'it', 'is', 'advertised']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 10), (5, 8), (6, 8), (7, 4), (8, 8), (9, 6), (10, 8), (11, 5), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "2 ['The', 'formulation']\n",
      "5 ['-LRB-', '3', '-', '4', '-RRB-']\n",
      "1 ['looks']\n",
      "17 ['like', 'a', 'standard', 'GAN', ',', 'with', 'some', 'twist', 'about', 'measuring', 'the', 'GAN', 'loss', 'in', 'the', 'z', 'space']\n",
      "14 ['-LRB-', 'this', 'has', 'been', 'used', 'in', 'eg', ':', 'PPGN', 'and', 'CVAE', '-', 'GAN', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 10), (5, 5), (6, 2), (7, 4), (8, 2), (9, 2), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "9 ['I', 'do', \"n't\", 'see', 'any', 'term', 'that', 'would', 'guarantee']\n",
      "1 [':']\n",
      "8 ['1', '-RRB-', 'Each', 'layer', 'is', 'a', 'natural', 'image']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 9), (5, 5), (6, 6), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "6 ['This', 'was', 'advertised', 'in', 'the', 'paper']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "10 ['the', 'loss', 'function', 'is', 'only', 'on', 'the', 'final', 'product', 'G_K']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 8), (5, 8), (6, 6), (7, 4), (8, 4), (9, 4), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "8 ['The', 'way', 'it', 'is', 'written', 'in', 'the', 'paper']\n",
      "1 [',']\n",
      "13 ['the', 'result', 'of', 'each', 'layer', 'does', 'not', 'need', 'to', 'go', 'through', 'a', 'discriminator']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 2), (7, 2), (8, 2), (9, 1), (10, 2), (11, 2), (12, 2), (13, 2), (14, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  14\n",
      "1 ['each']\n",
      "1 ['layer']\n",
      "1 ['outputs']\n",
      "1 ['a']\n",
      "1 ['natural']\n",
      "1 ['image']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 4), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "4 ['None', 'of', 'the', 'layers']\n",
      "2 ['is', 'degenerate']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 1), (6, 2), (7, 2), (8, 2), (9, 7), (10, 5), (11, 5), (12, 6), (13, 4), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['There']\n",
      "1 ['does']\n",
      "1 ['not']\n",
      "20 ['seem', 'to', 'be', 'any', 'constraint', 'either', 'regularizing', 'the', 'content', 'in', 'each', 'layer', ',', 'or', 'preventing', 'any', 'layer', 'to', 'be', 'non-degenerate']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 1)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['3']\n",
      "1 ['-RRB-']\n",
      "2 ['The', 'mask']\n",
      "2 ['being', 'contiguous']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 8), (5, 7), (6, 4), (7, 5), (8, 7), (9, 8), (10, 4), (11, 2), (12, 4), (13, 2), (14, 4), (15, 4), (16, 2), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "11 ['I', 'do', \"n't\", 'see', 'any', 'term', 'ensuring', 'the', 'mask', 'being', 'contiguous']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "24 ['imagine', 'normally', 'without', 'such', 'terms', 'doing', 'such', 'kinds', 'of', 'optimization', 'would', 'lead', 'to', 'a', 'lot', 'of', 'fragmented', 'small', 'areas', 'being', 'considered', 'as', 'the', 'mask']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 2), (6, 4), (7, 2), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "2 ['The', 'claim']\n",
      "8 ['that', 'this', 'paper', 'is', 'for', 'unsupervised', 'semantic', 'segmentation']\n",
      "1 ['is']\n",
      "1 ['overblown']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 5), (6, 6), (7, 7), (8, 11), (9, 5), (10, 3), (11, 2), (12, 2), (13, 2), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['A']\n",
      "1 ['major']\n",
      "1 ['problem']\n",
      "1 ['is']\n",
      "26 ['that', 'when', 'conducting', 'experiments', ',', 'all', 'the', 'images', 'seem', 'to', 'be', 'taken', 'from', 'a', 'single', 'category', ',', 'this', 'implicitly', 'uses', 'the', 'label', 'information', 'of', 'the', 'category']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 4), (5, 2), (6, 2), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "3 ['In', 'that', 'regard']\n",
      "1 [',']\n",
      "1 ['this']\n",
      "8 ['can', 'not', 'be', 'viewed', 'as', 'an', 'unsupervised', 'algorithm']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 5), (5, 2), (6, 2), (7, 2), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "3 ['Even', 'with', 'that']\n",
      "1 [',']\n",
      "2 ['the', 'results']\n",
      "1 ['definitely']\n",
      "6 ['looked', 'too', 'good', 'to', 'be', 'true']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 5), (6, 4), (7, 3), (8, 7), (9, 7), (10, 5), (11, 5), (12, 5), (13, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  5\n",
      "1 ['a']\n",
      "2 ['really', 'difficult']\n",
      "1 ['time']\n",
      "1 ['believing']\n",
      "22 ['why', 'such', 'a', 'standard', 'GAN', 'optimization', 'would', 'not', 'generate', 'any', 'of', 'the', 'aforementioned', 'artifacts', 'and', 'would', 'perform', 'exactly', 'as', 'the', 'authors', 'advertised']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 3), (5, 2), (6, 2), (7, 5), (8, 3), (9, 2), (10, 4), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['Hence']\n",
      "1 ['I']\n",
      "13 ['am', 'pretty', 'sure', 'that', 'this', 'is', 'not', 'up', 'to', 'the', 'standards', 'of', 'ICLR']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 4), (5, 5), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "3 ['read', 'the', 'rebuttal']\n",
      "1 ['and']\n",
      "1 ['still']\n",
      "2 ['not', 'convinced']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 1), (6, 2), (7, 4), (8, 1), (9, 2), (10, 3), (11, 3), (12, 2), (13, 4), (14, 2), (15, 2), (16, 3), (17, 2), (18, 3), (19, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['do']\n",
      "1 [\"n't\"]\n",
      "17 ['think', 'the', 'authors', 'managed', 'to', 'convince', 'me', 'that', 'this', 'method', 'would', 'work', 'the', 'way', 'it', \"'s\", 'advertised']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 4), (5, 2), (6, 5), (7, 2), (8, 3), (9, 3), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "2 ['Paper', 'summary']\n",
      "1 [':']\n",
      "12 ['The', 'paper', 'proposes', 'a', 'generative', 'model', 'that', 'decomposes', 'images', 'into', 'multiple', 'layers']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 7), (4, 5), (5, 4), (6, 4), (7, 5), (8, 4), (9, 3), (10, 4), (11, 2), (12, 3), (13, 2), (14, 1), (15, 2), (16, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['proposed']\n",
      "1 ['approach']\n",
      "1 ['is']\n",
      "3 ['GAN', '-', 'based']\n",
      "1 [',']\n",
      "18 ['where', 'the', 'objective', 'of', 'the', 'GAN', 'is', 'to', 'distinguish', 'real', 'images', 'from', 'images', 'formed', 'by', 'combining', 'the', 'layers']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 4), (6, 3), (7, 2), (8, 2), (9, 2), (10, 2), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['Some']\n",
      "3 ['of', 'the', 'layers']\n",
      "1 ['correspond']\n",
      "9 ['to', 'objects', 'that', 'are', 'common', 'in', 'specific', 'scene', 'categories']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 2), (7, 2), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['method']\n",
      "1 ['has']\n",
      "7 ['been', 'tested', 'on', 'kitchen', 'and', 'bedroom', 'scenes']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 7), (5, 5), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "2 ['Paper', 'Strengths']\n",
      "1 [':']\n",
      "1 ['+']\n",
      "5 ['The', 'idea', 'of', 'the', 'paper']\n",
      "2 ['is', 'interesting']\n",
      "[(0, 1), (1, 1), (2, 2)]\n",
      "[0, 1, 2]\n",
      "Depth to split: None\n",
      "depth:  None\n",
      "[(0, 1), (1, 1), (2, 2)]\n",
      "[0, 1, 2]\n",
      "Depth to split: None\n",
      "depth:  None\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 6), (5, 11), (6, 7), (7, 4), (8, 1), (9, 2), (10, 4), (11, 8), (12, 2), (13, 2), (14, 2), (15, 3), (16, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "2 ['Paper', 'Weaknesses']\n",
      "1 [':']\n",
      "31 ['-', 'The', 'evaluation', 'of', 'the', 'model', 'is', 'not', 'great', ':', '-LRB-', '1', '-RRB-', 'It', 'would', 'be', 'interesting', 'to', 'combine', 'bedroom', 'and', 'kitchen', 'images', 'and', 'train', 'jointly', 'to', 'see', 'what', 'it', 'learns']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 2), (5, 2), (6, 1), (7, 2), (8, 2), (9, 2), (10, 2), (11, 2), (12, 5), (13, 2), (14, 4), (15, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "3 ['-LRB-', '2', '-RRB-']\n",
      "1 ['It']\n",
      "14 ['would', 'be', 'good', 'to', 'see', 'how', 'the', 'performance', 'changes', 'for', 'different', 'number', 'of', 'layers']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 5), (6, 8), (7, 9), (8, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "2 ['-', 'It']\n",
      "1 ['is']\n",
      "1 ['unclear']\n",
      "14 ['why', '\"', 'contiguous', '\"', 'masks', 'are', 'generated', '-LRB-', 'e.g.', ',', 'in', 'figure', '4', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['Is']\n",
      "1 ['there']\n",
      "5 ['any', 'constraint', 'in', 'the', 'optimization']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 2), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['should']\n",
      "5 ['be', 'explained', 'in', 'the', 'rebuttal']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 3), (6, 3), (7, 5), (8, 3), (9, 2), (10, 4), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "2 ['The', 'method']\n",
      "16 ['should', 'not', 'be', 'called', '\"', 'unsupervised', '\"', 'since', 'it', 'knows', 'the', 'label', 'for', 'the', 'scene', 'category']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 2), (5, 3), (6, 6), (7, 2), (8, 3), (9, 2), (10, 4), (11, 2), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['Also']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "17 ['should', 'not', 'be', 'called', '\"', 'semantic', 'segmentation', '\"', 'since', 'there', 'is', 'no', 'semantics', 'associated', 'to', 'the', 'object']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['is']\n",
      "1 ['just']\n",
      "6 ['a', 'binary', 'foreground', '/', 'background', 'mask']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 4)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "5 ['The', 'plots', 'in', 'Figure', '5']\n",
      "4 ['are', 'a', 'bit', 'strange']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 3), (6, 6), (7, 3), (8, 2), (9, 2), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['precision']\n",
      "1 ['increases']\n",
      "10 ['uniformly', 'as', 'the', 'recall', 'goes', 'up', ',', 'which', 'is', 'weird']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 2), (7, 2), (8, 4), (9, 3), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  8\n",
      "1 ['the']\n",
      "1 ['rebuttal']\n",
      "1 ['why']\n",
      "2 ['that', 'happens']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 5), (5, 4), (6, 8), (7, 5), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "2 ['-', 'Similar']\n",
      "10 ['to', 'most', 'GAN', '-', 'based', 'models', ',', 'the', 'generated', 'images']\n",
      "1 ['are']\n",
      "1 ['not']\n",
      "1 ['that']\n",
      "1 ['appealing']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 4), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "5 ['The', 'claim', 'about', 'object', 'removal']\n",
      "4 ['should', 'be', 'toned', 'down']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 1), (6, 2), (7, 3), (8, 4), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['method']\n",
      "1 ['is']\n",
      "1 ['not']\n",
      "8 ['able', 'to', 'remove', 'any', 'object', 'from', 'a', 'scene']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 3), (5, 2), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['Only']\n",
      "1 [',']\n",
      "3 ['the', 'learned', 'layers']\n",
      "3 ['can', 'be', 'removed']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 6), (5, 2), (6, 4), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['proposes']\n",
      "4 ['a', 'neural', 'network', 'architecture']\n",
      "7 ['around', 'the', 'idea', 'of', 'layered', 'scene', 'composition']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 12), (5, 6), (6, 5), (7, 8), (8, 5), (9, 7), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "8 ['Training', 'is', 'cast', 'in', 'the', 'generative', 'adversarial', 'framework']\n",
      "1 [';']\n",
      "17 ['a', 'subnetwork', 'is', 'reused', 'to', 'generate', 'and', 'compose', '-LRB-', 'via', 'an', 'output', 'mask', '-RRB-', 'multiple', 'image', 'layers']\n",
      "1 [';']\n",
      "8 ['the', 'resulting', 'image', 'is', 'fed', 'to', 'a', 'discriminator']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 2), (6, 5), (7, 6), (8, 6), (9, 4), (10, 6), (11, 4), (12, 7), (13, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['An']\n",
      "1 ['encoder']\n",
      "1 ['is']\n",
      "1 ['later']\n",
      "26 ['trained', 'to', 'map', 'real', 'images', 'into', 'the', 'space', 'of', 'latent', 'codes', 'for', 'the', 'generator', ',', 'allowing', 'the', 'system', 'to', 'be', 'applied', 'to', 'real', 'image', 'segmentation', 'tasks']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 2), (6, 4), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['idea']\n",
      "1 ['is']\n",
      "3 ['interesting', 'and', 'different']\n",
      "5 ['from', 'established', 'approaches', 'to', 'segmentation']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 3), (6, 7), (7, 7), (8, 10), (9, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['Visualization']\n",
      "13 ['of', 'learned', 'layers', 'for', 'several', 'scene', 'types', '-LRB-', 'Figures', '3', ',', '7', '-RRB-']\n",
      "1 ['shows']\n",
      "10 ['that', 'the', 'network', 'does', 'learn', 'a', 'reasonable', 'compositional', 'scene', 'model']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 2), (6, 7), (7, 8), (8, 9), (9, 4), (10, 8), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['Experiments']\n",
      "1 ['evaluate']\n",
      "2 ['the', 'ability']\n",
      "24 ['to', 'port', 'the', 'model', 'learned', 'in', 'an', 'unsupervised', 'manner', 'to', 'semantic', 'segmentation', 'tasks', ',', 'using', 'a', 'limited', 'amount', 'of', 'supervision', 'for', 'the', 'end', 'task']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 4), (5, 1), (6, 2), (7, 2), (8, 2), (9, 4), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "3 ['the', 'included', 'experiments']\n",
      "12 ['are', 'not', 'nearly', 'sufficient', 'to', 'establish', 'the', 'effectiveness', 'of', 'the', 'proposed', 'method']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 8), (4, 8), (5, 14), (6, 6), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['Only']\n",
      "1 ['two']\n",
      "9 ['scene', 'types', '-LRB-', 'bedroom', ',', 'kitchen', '-RRB-', 'and', 'four']\n",
      "1 ['object']\n",
      "1 ['classes']\n",
      "9 ['-LRB-', 'bed', ',', 'window', ',', 'appliance', ',', 'counter', '-RRB-']\n",
      "1 ['are']\n",
      "3 ['used', 'for', 'evaluation']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 4), (6, 7), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['is']\n",
      "4 ['far', 'below', 'the', 'norm']\n",
      "7 ['for', 'semantic', 'segmentation', 'work', 'in', 'computer', 'vision']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 2), (6, 4), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "2 ['The', 'range']\n",
      "6 ['of', 'prior', 'work', 'on', 'semantic', 'segmentation']\n",
      "1 ['is']\n",
      "1 ['extensive']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 11), (5, 6), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "9 ['In', 'summary', ',', 'the', 'proposed', 'method', 'may', 'be', 'promising']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "5 ['far', 'more', 'experiments', 'are', 'needed']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 4), (6, 7), (7, 4), (8, 3), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "10 ['present', 'a', 'convincing', 'set', 'of', 'results', 'over', 'many', 'translation', 'tasks']\n",
      "1 ['and']\n",
      "5 ['compare', 'with', 'very', 'competitive', 'baselines']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 4), (5, 7), (6, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "12 ['ACL', ',', 'EMNLP', 'and', 'other', 'NLP', 'conferences', 'would', 'be', 'a', 'better', 'venue']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "1 ['think']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 8), (5, 4), (6, 3), (7, 2), (8, 2), (9, 5), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "11 ['The', 'system', 'described', 'works', 'comparably', 'to', 'bi-directional', 'LSTM', 'baseline', 'for', 'NMT']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "5 ['CNN', \"'s\", 'are', 'naturally', 'parallelizable']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 4), (5, 7), (6, 6), (7, 6), (8, 5), (9, 6), (10, 4), (11, 3), (12, 1), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['Key']\n",
      "1 ['ideas']\n",
      "1 ['include']\n",
      "18 ['the', 'use', 'of', 'two', 'stacked', 'CNN', \"'s\", '-LRB-', 'one', 'for', 'each', 'of', 'encoding', 'and', 'decoding', '-RRB-', 'for', 'translation']\n",
      "1 [',']\n",
      "6 ['with', 'res', 'connections', 'and', 'position', 'embeddings']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 9), (5, 10), (6, 12), (7, 8), (8, 7), (9, 7), (10, 12), (11, 12), (12, 5), (13, 6), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "18 ['The', 'use', 'of', 'CNN', \"'s\", 'for', 'translation', 'has', 'been', 'attempted', 'previously', '-LRB-', 'as', 'described', 'by', 'the', 'authors', '-RRB-']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "34 ['presumably', 'it', 'is', 'the', 'authors', \"'\", 'combination', 'of', 'various', 'architectural', 'choices', '-LRB-', 'attention', ',', 'position', 'embeddings', ',', 'etc', '-RRB-', 'that', 'make', 'the', 'present', 'system', 'competitive', 'with', 'RNN', \"'s\", ',', 'whereas', 'earlier', 'attempts', 'were', 'not']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 7), (6, 5), (7, 4), (8, 5), (9, 2), (10, 2), (11, 4), (12, 2), (13, 3), (14, 2), (15, 3), (16, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  3\n",
      "1 ['They']\n",
      "1 ['describe']\n",
      "3 ['system', \"'s\", 'sensitivity']\n",
      "22 ['to', 'some', 'of', 'these', 'choices', '-LRB-', 'eg', ':', 'experiments', 'to', 'choose', 'appropriate', 'number', 'of', 'layers', 'in', 'each', 'of', 'the', 'CNN', \"'s\", '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 3), (5, 2), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['experimental']\n",
      "1 ['results']\n",
      "1 ['are']\n",
      "1 ['well']\n",
      "3 ['reported', 'in', 'detail']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 6), (5, 2), (6, 1), (7, 2), (8, 2), (9, 1), (10, 2), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['One']\n",
      "1 ['or']\n",
      "2 ['two', 'figures']\n",
      "1 ['would']\n",
      "1 ['definitely']\n",
      "7 ['be', 'required', 'to', 'help', 'clarify', 'the', 'architecture']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 5), (6, 6), (7, 7), (8, 13), (9, 6), (10, 6), (11, 6), (12, 6), (13, 4), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['is']\n",
      "22 ['less', 'about', 'new', 'ways', 'of', 'learning', 'representations', 'than', 'about', 'the', 'combination', 'of', 'choices', 'made', '-LRB-', 'over', 'the', 'set', 'of', 'existing', 'techniques', '-RRB-']\n",
      "15 ['in', 'order', 'to', 'get', 'the', 'good', 'results', 'that', 'they', 'do', 'on', 'the', 'reported', 'NMT', 'tasks']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 8), (4, 6), (5, 8), (6, 5), (7, 6), (8, 5), (9, 4), (10, 2), (11, 4), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "3 ['In', 'this', 'respect']\n",
      "1 [',']\n",
      "14 ['while', 'I', 'am', 'fairly', 'confident', 'that', 'the', 'paper', 'represents', 'good', 'work', 'in', 'machine', 'learning']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "12 ['am', 'not', 'quite', 'as', 'confident', 'about', 'its', 'fit', 'for', 'this', 'particular', 'conference']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 6), (6, 3), (7, 4), (8, 3), (9, 6), (10, 4), (11, 4), (12, 5), (13, 4), (14, 2), (15, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['is']\n",
      "23 ['the', 'first', '-LRB-', 'I', 'believe', '-RRB-', 'to', 'establish', 'a', 'simple', 'yet', 'important', 'result', 'that', 'Convnets', 'for', 'NMT', 'encoders', 'can', 'be', 'competitive', 'to', 'RNNs']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 5), (6, 2), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['I']\n",
      "1 ['also']\n",
      "9 ['appreciate', 'the', 'detailed', 'report', 'on', 'training', 'and', 'generation', 'speed']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 8), (5, 7), (6, 5), (7, 11), (8, 10), (9, 10), (10, 13), (11, 10), (12, 5), (13, 10), (14, 7), (15, 4), (16, 2), (17, 4), (18, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  2\n",
      "20 ['I', 'find', 'it', \"'s\", 'very', 'interesting', 'when', 'position', 'embeddings', 'turn', 'out', 'to', 'be', 'hugely', 'important', '-LRB-', 'beside', 'residual', 'connections', '-RRB-']\n",
      "1 [';']\n",
      "38 ['unfortunately', ',', 'there', 'is', 'little', 'analysis', 'to', 'shed', 'more', 'lights', 'on', 'this', 'aspect', 'and', 'perhaps', 'compare', 'other', 'ways', 'of', 'capturing', 'positions', '-LRB-', 'a', 'wild', 'guess', 'might', 'be', 'to', 'use', 'embeddings', 'that', 'represent', 'some', 'form', 'of', 'relative', 'positions', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 5), (6, 8), (7, 3), (8, 6), (9, 5), (10, 2), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "3 ['The', 'only', 'concern']\n",
      "9 ['I', 'have', '-LRB-', 'similar', 'to', 'the', 'other', 'reviewer', '-RRB-']\n",
      "1 ['is']\n",
      "10 ['that', 'this', 'paper', 'perhaps', 'fits', 'better', 'in', 'an', 'NLP', 'conference']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 4), (5, 6), (6, 8), (7, 6), (8, 5), (9, 6), (10, 10), (11, 11), (12, 13), (13, 8), (14, 10), (15, 8), (16, 5), (17, 8), (18, 6), (19, 4), (20, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "55 ['One', 'minor', 'comment', ':', 'it', \"'s\", 'slight', 'strange', 'that', 'this', 'well', '-', 'executed', 'paper', 'does', \"n't\", 'have', 'a', 'single', 'figure', 'on', 'the', 'proposed', 'architecture', ':-RRB-', 'It', 'will', 'also', 'be', 'even', 'better', 'to', 'draw', 'a', 'figure', 'for', 'the', 'biLSTM', 'architecture', 'as', 'well', '-LRB-', 'it', 'does', 'take', 'some', 'effort', 'to', 'understand', 'the', 'last', 'paragraph', 'in', 'Section', '2']\n",
      "1 [',']\n",
      "1 ['especially']\n",
      "9 ['the', 'part', 'on', 'having', 'a', 'linear', 'layer', 'to', 'compute']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 3), (6, 4), (7, 1), (8, 2), (9, 3), (10, 3), (11, 2), (12, 5), (13, 2), (14, 2), (15, 3), (16, 2), (17, 5), (18, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['reports']\n",
      "24 ['a', 'very', 'clear', 'and', 'easy', 'to', 'understand', 'result', 'that', 'a', 'convolutional', 'network', 'can', 'be', 'used', 'instead', 'of', 'the', 'recurrent', 'encoder', 'for', 'neural', 'machine', 'translation']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 10), (5, 13), (6, 11), (7, 8), (8, 7), (9, 6), (10, 3), (11, 2), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "12 ['Apart', 'from', 'the', 'known', 'architectural', 'elements', ',', 'such', 'as', 'convolution', ',', 'pooling']\n",
      "1 [',']\n",
      "8 ['residual', 'connections', ',', 'position', 'embeddings', ',', 'the', 'paper']\n",
      "21 ['features', 'one', 'unexpected', 'architectural', 'twist', ':', 'two', 'stacks', 'of', 'convolutions', ',', 'one', 'for', 'computing', 'alignment', 'and', 'another', 'for', 'computing', 'the', 'representations']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 7), (5, 10), (6, 8), (7, 9), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "16 ['The', 'empirical', 'evidence', 'that', 'this', 'was', 'necessary', 'is', 'provided', ',', 'however', 'the', 'question', 'of', '*', 'why']\n",
      "1 ['*']\n",
      "5 ['it', 'is', 'necessary', 'remains', 'open']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 5), (5, 6), (6, 2), (7, 5), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['experimental']\n",
      "1 ['evaluation']\n",
      "3 ['is', 'very', 'extensive']\n",
      "1 ['and']\n",
      "9 ['leaves', 'no', 'doubt', 'that', 'the', 'proposed', 'approach', 'works', 'well']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 11), (5, 10), (6, 4), (7, 2), (8, 4), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "9 ['The', 'convnet', '-', 'based', 'model', 'was', 'faster', 'at', 'evaluation']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "13 ['it', 'is', 'not', 'very', 'clear', 'what', 'is', 'the', 'main', 'speed', '-', 'up', 'factor']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 2), (6, 3), (7, 4), (8, 4), (9, 4), (10, 7), (11, 2), (12, 2), (13, 2), (14, 2), (15, 2), (16, 5), (17, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['’s']\n",
      "1 ['however']\n",
      "1 ['hard']\n",
      "22 ['to', 'argue', 'against', 'the', 'fact', 'that', 'the', 'speed', 'advantage', 'of', 'convnets', 'is', 'likely', 'to', 'increase', 'if', 'a', 'more', 'parallel', 'implementation', 'is', 'considered']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 2), (6, 6), (7, 4), (8, 4), (9, 5), (10, 3), (11, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['My']\n",
      "1 ['main']\n",
      "1 ['concern']\n",
      "1 ['is']\n",
      "21 ['whether', 'or', 'not', 'the', 'paper', 'is', 'appropriate', 'for', 'ICLR', ',', 'because', 'the', 'contribution', 'is', 'quite', 'incremental', 'and', 'rather', 'application', '-', 'specific']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 5), (7, 4), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  6\n",
      "1 ['the']\n",
      "1 ['authors']\n",
      "1 ['compare']\n",
      "2 ['their', 'model']\n",
      "3 ['with', 'these', 'approaches']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 5), (5, 3), (6, 2), (7, 2), (8, 6), (9, 8), (10, 12), (11, 6), (12, 2), (13, 2), (14, 2), (15, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "3 ['In', 'this', 'case']\n",
      "1 [',']\n",
      "2 ['the', 'approach']\n",
      "29 ['should', 'be', 'also', 'compared', 'to', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'tracking', 'approaches', '-LRB-', 'that', 'are', 'cheaper', 'to', 'acquire', '-RRB-', 'in', 'terms', 'of', 'computational', 'efficiency', 'and', 'performance']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 5), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['should']\n",
      "8 ['make', 'more', 'comprehensive', 'evaluation', 'on', 'the', 'larger', 'dataset']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 7), (5, 9), (6, 9), (7, 8), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "19 ['The', 'authors', 'term', 'this', 'as', 'dynamic', 'convolution', 'and', 'evaluate', 'this', 'method', 'on', 'the', 'SSD', 'architecture', 'across', 'datasets', 'like', 'PETS']\n",
      "1 [',']\n",
      "3 ['AVSS', ',', 'VIRAT']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 9), (5, 6), (6, 3), (7, 4), (8, 4), (9, 2), (10, 9), (11, 7), (12, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "2 ['Paper', 'weaknesses']\n",
      "1 ['-']\n",
      "23 ['A', 'simple', 'baseline', 'that', 'only', 'processes', 'a', 'frame', 'if', '\\\\', 'sum', '_', '{', 'ij', '}', 'D', '_', '{', 'ij', '}', 'exceeds', 'a', 'threshold']\n",
      "1 ['is']\n",
      "1 ['never']\n",
      "4 ['mentioned', 'or', 'compared', 'against']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 3), (5, 2), (6, 5), (7, 2), (8, 2), (9, 2), (10, 2), (11, 2), (12, 3), (13, 5), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "2 ['In', 'general']\n",
      "1 [',']\n",
      "2 ['the', 'paper']\n",
      "18 ['does', 'not', 'compare', 'against', 'any', 'other', 'existing', 'work', 'which', 'reduces', 'compute', 'for', 'video', 'analysis', ',', 'e.g.', ',', 'tracking']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 7), (5, 7), (6, 5), (7, 4), (8, 3), (9, 1), (10, 3), (11, 5), (12, 5), (13, 6), (14, 5), (15, 2), (16, 2), (17, 5), (18, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  3\n",
      "4 ['-', 'Overall', ',', 'I']\n",
      "3 ['think', 'this', 'paper']\n",
      "1 ['can']\n",
      "24 ['be', 'substantially', 'improved', 'in', 'terms', 'of', 'providing', 'details', 'on', 'the', 'proposed', 'approach', 'and', 'comparing', 'against', 'baselines', 'to', 'demonstrate', 'that', 'Dynamic', '-', 'Convolutions', 'are', 'helpful']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 6), (5, 7), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "2 ['-', 'State']\n",
      "3 ['of', 'the', 'art']\n",
      "1 ['is']\n",
      "1 ['not']\n",
      "6 ['well', '-', 'studied', 'in', 'the', 'paper']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 8), (5, 1), (6, 2), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "3 ['In', 'this', 'paper']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "10 ['propose', 'a', 'dynamic', 'convolution', 'model', 'by', 'exploiting', 'the', 'inter-scene', 'similarity']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 3), (6, 1), (7, 2), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['computation']\n",
      "1 ['cost']\n",
      "1 ['is']\n",
      "7 ['reduced', 'significantly', 'by', 'reusing', 'the', 'feature', 'map']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 12), (5, 5)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "8 ['In', 'general', ',', 'the', 'paper', 'is', 'present', 'clearly']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "6 ['the', 'technical', 'contribution', 'is', 'rather', 'incremental']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 10), (5, 6), (6, 4), (7, 2), (8, 5), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "4 ['I', 'have', 'several', 'concerns']\n",
      "1 [':']\n",
      "15 ['1', '-RRB-', 'The', 'authors', 'should', 'further', 'clarify', 'their', 'advantages', 'over', 'the', 'popular', 'framework', 'of', 'CNN+LSTM']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 2), (5, 1)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['Actually']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "4 ['did', 'not', 'see', 'it']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 4), (6, 3), (7, 4), (8, 3), (9, 4), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "1 ['What']\n",
      "13 ['is', 'the', 'difference', 'between', 'the', 'proposed', 'method', 'and', 'applying', 'incremental', 'learning', 'on', 'CNN']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 8), (4, 4), (5, 2), (6, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "2 ['3', '-RRB-']\n",
      "3 ['The', 'proposed', 'method']\n",
      "10 ['reduced', 'the', 'computation', 'in', 'which', 'phase', ',', 'training', 'or', 'tesing']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 2)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "2 ['4', '-RRB-']\n",
      "3 ['The', 'experimental', 'section']\n",
      "3 ['is', 'rather', 'weak']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 5), (5, 3), (6, 4), (7, 2), (8, 2), (9, 2), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['Currently']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "1 ['only']\n",
      "13 ['use', 'some', 'small', 'dataset', 'with', 'short', 'videos', ',', 'which', 'makes', 'the', 'acceleration', 'unnecessary']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 6), (5, 1), (6, 2), (7, 3), (8, 5), (9, 3), (10, 4), (11, 5), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['Summary']\n",
      "1 ['-']\n",
      "2 ['This', 'paper']\n",
      "1 ['proposes']\n",
      "14 ['a', 'technique', 'to', 'reduce', 'the', 'compute', 'cost', 'when', 'applying', 'recognition', 'models', 'in', 'surveillance', 'models']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 1), (5, 2), (6, 5), (7, 7), (8, 7), (9, 6), (10, 2), (11, 2), (12, 2), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['core']\n",
      "1 ['idea']\n",
      "1 ['is']\n",
      "18 ['to', 'analytically', 'compute', 'the', 'pixels', 'that', 'changed', 'across', 'frames', 'and', 'only', 'apply', 'the', 'convolution', 'operation', 'to', 'those', 'pixels']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 4), (5, 1), (6, 2), (7, 2), (8, 4), (9, 3), (10, 3), (11, 5), (12, 3), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "2 ['Paper', 'strengths']\n",
      "1 ['-']\n",
      "15 ['The', 'problem', 'of', 'reducing', 'computational', 'requirements', 'when', 'using', 'CNNs', 'for', 'video', 'analysis', 'is', 'well', 'motivated']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 5), (5, 2), (6, 4), (7, 2), (8, 2), (9, 3), (10, 3), (11, 2), (12, 5), (13, 4), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "2 ['The', 'authors']\n",
      "19 ['analyze', 'a', 'standard', 'model', 'on', 'benchmark', 'datasets', 'which', 'makes', 'it', 'easier', 'to', 'understand', 'and', 'place', 'their', 'results', 'in', 'context']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 3), (6, 2), (7, 2), (8, 3), (9, 4), (10, 4), (11, 1), (12, 2), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  9\n",
      "1 ['the']\n",
      "1 ['contribution']\n",
      "2 ['practical', 'benefit']\n",
      "4 ['of', 'using', 'this', 'method']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 5), (6, 5), (7, 8)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "2 ['The', 'paper']\n",
      "15 ['has', 'many', 'spelling', 'and', 'grammar', 'mistakes', '-', '\"', 'siliarlity', '\"', ',', '\"', 'critiria', '\"', 'etc.']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 7), (5, 7), (6, 5), (7, 2), (8, 2), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "5 ['-', 'Continuous', 'convolutions', '-', 'It']\n",
      "1 ['is']\n",
      "1 ['not']\n",
      "3 ['clear', 'to', 'me']\n",
      "6 ['what', 'is', 'meant', 'by', 'this', 'term']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 11), (5, 13), (6, 14), (7, 12), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "19 ['It', 'is', 'used', 'many', 'times', 'and', 'there', 'is', 'an', 'entire', 'section', 'of', 'results', 'on', 'it', '-LRB-', 'Table', '6', '-RRB-']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "13 ['without', 'clearly', 'understanding', 'this', 'concept', ',', 'I', 'can', 'not', 'fully', 'appreciate', 'the', 'results']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 5), (6, 4), (7, 1), (8, 2), (9, 2), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['-']\n",
      "1 ['Section']\n",
      "2 ['5.2', '-']\n",
      "1 ['what']\n",
      "9 ['criteria', 'or', 'metric', 'is', 'used', 'to', 'compute', 'scene', 'similarity']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 8), (5, 5), (6, 9), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "3 ['-', 'Design', 'decisions']\n",
      "10 ['such', 'as', 'cell', '-', 'based', 'convolution', '-LRB-', 'Figure', '3', '-RRB-']\n",
      "1 ['are']\n",
      "1 ['never']\n",
      "2 ['evaluated', 'empirically']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 1), (5, 2), (6, 3), (7, 4), (8, 2), (9, 4), (10, 2), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['addresses']\n",
      "10 ['the', 'problem', 'of', 'computational', 'inefficiency', 'in', 'video', 'surveillance', 'understanding', 'approaches']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 4), (7, 1), (8, 2), (9, 4), (10, 2), (11, 6), (12, 5), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  6\n",
      "1 ['an']\n",
      "1 ['approach']\n",
      "1 ['called']\n",
      "14 ['Dynamic', 'Convolution', 'consists', 'of', 'Frame', 'differencing', ',', 'Prediction', ',', 'and', 'Dyn', '-', 'Convolution', 'steps']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 1), (5, 2), (6, 2), (7, 4), (8, 4), (9, 7), (10, 8), (11, 4), (12, 2), (13, 5), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['idea']\n",
      "1 ['is']\n",
      "22 ['to', 'reuse', 'some', 'of', 'the', 'convolutional', 'feature', 'maps', ',', 'and', 'frame', 'features', 'particularly', 'when', 'there', 'is', 'a', 'significant', 'similarity', 'among', 'the', 'frames']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 4), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['evaluates']\n",
      "6 ['the', 'results', 'on', '4', 'public', 'datasets']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 4), (5, 3), (6, 4), (7, 2), (8, 3), (9, 4), (10, 3), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "1 ['just']\n",
      "15 ['compares', 'the', 'approach', 'to', 'a', 'baseline', ',', 'which', 'is', 'indeed', 'applying', 'convnet', 'on', 'all', 'frames']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 7), (5, 4), (6, 4), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "2 ['Video', 'understanding']\n",
      "2 ['approaches', 'usually']\n",
      "1 ['are']\n",
      "1 ['not']\n",
      "1 ['just']\n",
      "5 ['applying', 'convnet', 'on', 'all', 'frames']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 9), (5, 11), (6, 11), (7, 8), (8, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "7 ['Many', 'of', 'the', 'approaches', 'on', 'video', 'analysis']\n",
      "1 [',']\n",
      "23 ['select', 'a', 'random', 'set', 'of', 'frames', '-LRB-', 'or', 'just', 'a', 'single', 'frame', '-RRB-', '[', '5', ']', ',', 'and', 'extract', 'the', 'features', 'for', 'them']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 6), (5, 9), (6, 4), (7, 4), (8, 7), (9, 9), (10, 3), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['There']\n",
      "1 ['is']\n",
      "4 ['another', 'set', 'of', 'work']\n",
      "2 ['on', 'attention']\n",
      "1 [',']\n",
      "21 ['that', 'try', 'to', 'extracts', 'the', 'most', 'important', 'spatio', '-', 'temporal', '[', '1', '-', '4', ']', 'information', 'to', 'solve', 'a', 'certain', 'task']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 5), (5, 2), (6, 1), (7, 3), (8, 3), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['These']\n",
      "1 ['approaches']\n",
      "1 ['are']\n",
      "1 ['usually']\n",
      "1 ['computationally']\n",
      "9 ['less', 'expensive', 'than', 'applying', 'convnet', 'on', 'all', 'video', 'frames']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 6), (5, 3), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "2 ['In', 'addition']\n",
      "1 [',']\n",
      "5 ['car', 'and', 'pedestrian', 'detection', 'performance']\n",
      "6 ['is', 'part', 'of', 'the', 'evaluation', 'process']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 7), (5, 3), (6, 2), (7, 3), (8, 3), (9, 7), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "5 ['The', 'writing', 'of', 'the', 'paper']\n",
      "13 ['should', 'also', 'improve', 'to', 'make', 'the', 'paper', 'more', 'understandable', 'and', 'easier', 'to', 'follow']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3)]\n",
      "[0, 1, 2, 3]\n",
      "depth:  2\n",
      "2 ['Some', 'examples']\n",
      "1 [':']\n",
      "1 ['1']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 1)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['Unnecessary']\n",
      "1 ['information']\n",
      "1 ['can']\n",
      "2 ['be', 'summarized']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 9), (5, 4), (6, 7), (7, 4), (8, 5), (9, 4), (10, 2), (11, 5), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "2 ['For', 'example']\n",
      "1 [',']\n",
      "11 ['many', 'details', 'on', 'the', 'computational', 'costs', 'in', 'abstract', 'and', 'the', 'introduction']\n",
      "15 ['can', 'just', 'simply', 'be', 'replaced', 'by', 'stating', 'that', '“', 'these', 'approaches', 'are', 'computationally', 'costly', '”']\n",
      "1 ['.']\n",
      "2019_HyVxPsC9tm 938\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 4), (5, 2), (6, 4), (7, 2), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['Using']\n",
      "6 ['present', 'tense', 'for', 'the', 'SoTA', 'approaches']\n",
      "1 ['more']\n",
      "1 ['common']\n",
      "2019_HyVxPsC9tm 940\n",
      "[(0, 1), (1, 1), (2, 7), (3, 10), (4, 17), (5, 17), (6, 21), (7, 16), (8, 10), (9, 3), (10, 5), (11, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "6 ['Long', 'sentences', 'are', 'difficult', 'to', 'follow']\n",
      "1 [':']\n",
      "1 ['“']\n",
      "36 ['In', 'real', 'surveillance', 'video', 'application', ',', 'although', 'the', 'calculation', 'reduction', 'on', 'convolution', 'is', 'the', 'main', 'concern', 'of', 'speeding', 'up', 'the', 'overall', 'processing', 'time', ',', 'the', 'data', 'transfer', 'is', 'another', 'important', 'factor', 'which', 'contributes', 'to', 'the', 'time']\n",
      "1 ['”']\n",
      "17 ['+', 'The', 'problem', 'of', 'large', '-', 'scale', 'video', 'understanding', 'is', 'an', 'important', 'and', 'interesting', 'problem', 'to', 'tackle']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 5), (4, 1), (5, 2), (6, 4), (7, 2), (8, 2), (9, 5), (10, 4), (11, 4), (12, 8), (13, 7), (14, 2), (15, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "2 ['Minor', 'comments']\n",
      "1 [':']\n",
      "1 ['-']\n",
      "1 ['I']\n",
      "25 ['believe', 'one', 'should', 'not', 'compare', 'the', 'distance', 'shown', 'between', 'the', 'left', 'and', 'right', 'columns', 'of', 'Figure', '3', 'as', 'they', 'are', 'obtained', 'from', 'two', 'different', 'models']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 7), (5, 8), (6, 12), (7, 5), (8, 9), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Can']\n",
      "1 ['authors']\n",
      "26 ['provide', 'empirical', 'comparison', 'between', 'blind', '-', 'spot', 'attacks', 'and', 'the', 'work', 'by', 'Song', 'et', 'al', '-LRB-', '2018', '-RRB-', ',', 'e.g.', ',', 'attack', 'success', 'rate', '&', 'distortion']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 4), (5, 2), (6, 5), (7, 2), (8, 5), (9, 3), (10, 4), (11, 8), (12, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "3 ['In', 'this', 'paper']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "22 ['associated', 'with', 'the', 'generalization', 'gap', 'of', 'robust', 'adversarial', 'training', 'with', 'the', 'distance', 'between', 'the', 'test', 'point', 'and', 'the', 'manifold', 'of', 'training', 'data']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 8), (5, 7), (6, 2), (7, 2), (8, 2), (9, 4), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "4 ['A', 'so', '-', 'called']\n",
      "6 [\"'\", 'blind', '-', 'spot', 'attack', \"'\"]\n",
      "1 ['is']\n",
      "9 ['proposed', 'to', 'show', 'the', 'weakness', 'of', 'robust', 'adversarial', 'training']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 4), (5, 8), (6, 6), (7, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "9 ['Although', 'the', 'paper', 'contains', 'interesting', 'ideas', 'and', 'empirical', 'results']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "7 ['have', 'several', 'concerns', 'about', 'the', 'current', 'version']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 8), (4, 4), (5, 5), (6, 4), (7, 8), (8, 3), (9, 2), (10, 5), (11, 2), (12, 2), (13, 2), (14, 4), (15, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "2 ['a', '-RRB-']\n",
      "3 ['In', 'the', 'paper']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "23 ['mentioned', 'that', '\"', 'This', 'simple', 'metric', 'is', 'non-parametric', 'and', 'we', 'found', 'that', 'the', 'results', 'are', 'not', 'sensitive', 'to', 'the', 'selection', 'of', 'k', '\"']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 6), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['Can']\n",
      "1 ['authors']\n",
      "11 ['provide', 'more', 'details', ',', 'e.g.', ',', 'empirical', 'results', ',', 'about', 'it']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['What']\n",
      "1 ['is']\n",
      "2 ['its', 'rationale']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 7), (3, 7), (4, 4), (5, 2), (6, 5), (7, 7), (8, 4), (9, 4), (10, 5), (11, 2), (12, 3), (13, 4), (14, 5), (15, 2), (16, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "2 ['b', '-RRB-']\n",
      "3 ['In', 'the', 'paper']\n",
      "1 [',']\n",
      "1 ['\"']\n",
      "1 ['We']\n",
      "27 ['find', 'that', 'these', 'blind', '-', 'spots', 'are', 'prevalent', 'and', 'can', 'be', 'easily', 'found', 'without', 'resorting', 'to', 'complex', 'generative', 'models', 'like', 'in', 'Song', 'et', 'al', '-LRB-', '2018', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 6), (5, 7), (6, 5), (7, 8), (8, 18), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "18 ['For', 'the', 'MNIST', 'dataset', 'which', 'Madry', 'et', 'al', '-LRB-', '2018', '-RRB-', 'demonstrate', 'the', 'strongest', 'defense', 'results', 'so', 'far']\n",
      "1 [',']\n",
      "1 ['we']\n",
      "13 ['propose', 'a', 'simple', 'transformation', 'to', 'find', 'the', 'blind', '-', 'spots', 'in', 'this', 'model']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 8), (5, 2), (6, 5), (7, 2), (8, 4), (9, 2), (10, 5), (11, 5), (12, 2), (13, 2), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "2 ['c', '-RRB-']\n",
      "6 ['The', 'linear', 'transformation', 'x', '^', '\\\\']\n",
      "20 ['prime', '=', '\\\\', 'alpha', 'x', '+', '\\\\', 'beta', 'yields', 'a', 'blind', '-', 'spot', 'attack', 'which', 'can', 'defeat', 'robust', 'adversarial', 'training']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 6), (5, 3), (6, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "4 ['given', 'the', 'linear', 'transformation']\n",
      "1 [',']\n",
      "1 ['one']\n",
      "8 ['can', 'further', 'modify', 'the', 'inner', 'maximization', '-LRB-', 'adv.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 11), (5, 8), (6, 8), (7, 5), (8, 6), (9, 4), (10, 4), (11, 7), (12, 10), (13, 2), (14, 6), (15, 5), (16, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "34 ['generation', '-RRB-', 'in', 'robust', 'training', 'framework', 'so', 'that', 'the', '$', '\\\\', 'ell_infty', '$', 'attack', 'satisfies', 'max', '_', '{', '\\\\', 'alpha', ',', '\\\\', 'beta', '}', 'f', '-LRB-', '\\\\', 'alpha', 'x', '+', '\\\\', 'beta', '-RRB-', 'subject']\n",
      "4 ['to', '\\\\', '|', '\\\\']\n",
      "2 ['alpha', 'x']\n",
      "1 ['+']\n",
      "1 ['\\\\']\n",
      "2 ['beta', '\\\\']\n",
      "1 ['|']\n",
      "1 ['\\\\']\n",
      "1 ['leq']\n",
      "1 ['\\\\']\n",
      "1 ['epsilon']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 8), (5, 4), (6, 2), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "13 ['In', 'this', 'case', ',', 'robust', 'training', 'framework', 'can', 'defend', 'blind', '-', 'spot', 'attacks']\n",
      "1 [',']\n",
      "1 ['right']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 8), (5, 4), (6, 4), (7, 6), (8, 7), (9, 10), (10, 6), (11, 4), (12, 2), (13, 4), (14, 2), (15, 3), (16, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "21 ['I', 'agree', 'with', 'the', 'authors', 'that', 'the', 'generalization', 'error', 'is', 'due', 'to', 'the', 'mismatch', 'between', 'training', 'data', 'and', 'test', 'data', 'distribution']\n",
      "1 [',']\n",
      "17 ['however', ',', 'I', 'am', 'not', 'convinced', 'that', 'blind', '-', 'spot', 'attacks', 'are', 'effective', 'enough', 'to', 'robust', 'training']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 5), (5, 8), (6, 9), (7, 7), (8, 11), (9, 9), (10, 5), (11, 8), (12, 4), (13, 2), (14, 2), (15, 2), (16, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['d']\n",
      "1 ['-RRB-']\n",
      "23 ['\"', 'Because', 'we', 'scale', 'the', 'image', 'by', 'a', 'factor', 'of', '\\\\', 'alpha', ',', 'we', 'also', 'set', 'a', 'stricter', 'criterion', 'of', 'success', ',', '...']\n",
      "1 [',']\n",
      "16 ['perturbation', 'must', 'be', 'less', 'than', '\\\\', 'alpha', '\\\\', 'epsilon', 'to', 'be', 'counted', 'as', 'a', 'successful', 'attack']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['did']\n",
      "1 ['not']\n",
      "3 ['get', 'the', 'point']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 10), (4, 6), (5, 6), (6, 11), (7, 10), (8, 16), (9, 11)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "19 ['Even', 'if', 'you', 'have', 'a', 'scaling', 'factor', 'in', 'x', '^', '\\\\', 'prime', '=', '\\\\', 'alpha', 'x', '+', '\\\\', 'beta']\n",
      "1 [',']\n",
      "4 ['the', 'universal', 'perturbation', 'rule']\n",
      "18 ['should', 'still', 'be', '|', 'x', '-', 'x', '^', '\\\\', 'prime', '|', '_', '\\\\', 'infty', '\\\\', 'leq', '\\\\', 'epsilon']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 4), (6, 6), (7, 5), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "2 ['The', 'metric']\n",
      "3 ['the', 'authors', 'used']\n",
      "1 ['would']\n",
      "9 ['result', 'in', 'a', 'higher', 'attack', 'success', 'rate', ',', 'right']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 6), (6, 5), (7, 5), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['provides']\n",
      "7 ['some', 'insights', 'on', 'influence', 'of', 'data', 'distribution']\n",
      "5 ['on', 'robustness', 'of', 'adversarial', 'training']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 4), (6, 9), (7, 10), (8, 4), (9, 9), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['demonstrates']\n",
      "5 ['through', 'a', 'number', 'of', 'analysis']\n",
      "20 ['that', 'the', 'distance', 'between', 'the', 'training', 'an', 'test', 'data', 'sets', 'plays', 'an', 'important', 'role', 'on', 'the', 'effectiveness', 'of', 'adversarial', 'training']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 5), (5, 3), (6, 4), (7, 3), (8, 3), (9, 6), (10, 6), (11, 3), (12, 6), (13, 7), (14, 10), (15, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "4 ['To', 'show', 'the', 'latter']\n",
      "1 [',']\n",
      "2 ['the', 'paper']\n",
      "32 ['proposes', 'an', 'approach', 'to', 'measure', 'the', 'distance', 'between', 'the', 'two', 'data', 'sets', 'using', 'combination', 'of', 'nonlinear', 'projection', '-LRB-', 'eg', ':', 't', '-', 'SNE', '-RRB-', ',', 'KDE', ',', 'and', 'K', '-', 'L', 'divergence']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 4), (6, 6), (7, 8), (8, 8), (9, 6), (10, 11), (11, 3), (12, 3), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "2 ['The', 'paper']\n",
      "1 ['also']\n",
      "32 ['shows', 'that', 'under', 'simple', 'transformation', 'to', 'the', 'test', 'dataset', '-LRB-', 'eg', ':', 'scaling', '-RRB-', ',', 'performance', 'of', 'adversarial', 'training', 'reduces', 'significantly', 'due', 'to', 'the', 'large', 'gap', 'between', 'training', 'and', 'test', 'data', 'set']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 2), (7, 1), (8, 2), (9, 5), (10, 6), (11, 5), (12, 6), (13, 5), (14, 4), (15, 3), (16, 8), (17, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  9\n",
      "2 ['high', 'dimensional']\n",
      "1 ['data']\n",
      "1 ['sets']\n",
      "6 ['more', 'than', 'low', 'dimensional', 'data', 'sets']\n",
      "17 ['since', 'it', 'is', 'much', 'harder', 'to', 'cover', 'the', 'whole', 'ground', 'truth', 'data', 'distribution', 'in', 'the', 'training', 'dataset']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 4), (6, 3), (7, 4), (8, 2), (9, 4), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['Pros']\n",
      "1 [':']\n",
      "13 ['-', 'Provides', 'insights', 'on', 'why', 'adversarial', 'training', 'is', 'less', 'effective', 'on', 'some', 'datasets']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 4), (6, 2), (7, 2), (8, 1), (9, 3), (10, 3), (11, 2), (12, 2), (13, 4), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  5\n",
      "1 ['a']\n",
      "1 ['metric']\n",
      "1 ['that']\n",
      "10 ['seems', 'to', 'strongly', 'correlate', 'with', 'the', 'effectiveness', 'of', 'adversarial', 'training']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['Cons']\n",
      "1 [':']\n",
      "5 ['-', 'Lack', 'of', 'theoretical', 'analysis']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 3), (6, 3), (7, 2), (8, 4), (9, 4), (10, 6), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  8\n",
      "1 ['the']\n",
      "1 ['authors']\n",
      "1 ['could']\n",
      "9 ['show', 'the', 'observed', 'phenomenon', 'analytically', 'on', 'some', 'simple', 'distribution']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 14), (5, 6), (6, 3), (7, 4), (8, 3), (9, 3), (10, 3), (11, 2), (12, 2), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "8 ['The', 'marketing', 'phrase', '\"', 'the', 'blind', '-', 'spot']\n",
      "16 ['attach', '\"', 'falls', 'short', 'in', 'delivering', 'what', 'one', 'may', 'expect', 'from', 'the', 'paper', 'after', 'reading', 'it']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 3), (6, 6), (7, 4), (8, 6), (9, 4), (10, 7), (11, 3), (12, 2), (13, 4), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['would']\n",
      "23 ['read', 'much', 'better', 'if', 'the', 'authors', 'better', 'describe', 'the', 'phenomena', 'based', 'on', 'the', 'gap', 'between', 'the', 'two', 'distribution', 'than', 'using', 'bling', '-', 'spot']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 9), (4, 7), (5, 5), (6, 6), (7, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "3 ['For', 'some', 'dataset']\n",
      "7 [',', 'this', 'is', 'beyond', 'a', 'spot', ',']\n",
      "1 ['it']\n",
      "9 ['could', 'actually', 'be', 'huge', 'portion', 'of', 'the', 'input', 'space']\n",
      "1 ['!']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 4), (5, 6), (6, 4), (7, 3), (8, 3), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "7 ['Though', 'the', 'paper', 'is', 'not', 'suggesting', 'that']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "8 ['would', 'help', 'to', 'clarify', 'it', 'in', 'the', 'paper']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 2), (5, 2), (6, 2), (7, 4), (8, 2), (9, 3), (10, 6), (11, 9), (12, 8), (13, 8), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['Furthermore']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "28 ['would', 'help', 'if', 'the', 'paper', 'elaborates', 'why', 'the', 'distance', 'between', 'the', 'test', 'and', 'training', 'dataset', 'is', 'smaller', 'in', 'an', 'adversarially', 'trained', 'network', 'compared', 'to', 'a', 'naturally', 'trained', 'network']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 4), (6, 2), (7, 4), (8, 3), (9, 6), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  5\n",
      "1 ['the']\n",
      "1 ['results']\n",
      "1 ['in']\n",
      "12 ['Table', '1', 'for', 'an', 'adversarially', 'trained', 'network', 'or', 'a', 'naturally', 'trained', 'network']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 3), (6, 1), (7, 2), (8, 2), (9, 3), (10, 4), (11, 11), (12, 10), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "2 ['Either', 'way']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "24 ['could', 'be', 'also', 'interesting', 'to', 'see', 'the', 'average', 'K', '-', 'L', 'divergence', 'between', 'an', 'adversarially', 'and', 'a', 'naturally', 'trained', 'network', 'on', 'the', 'same', 'dataset']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 2), (4, 6), (5, 7), (6, 4), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "4 ['Please', 'provide', 'more', 'visualization']\n",
      "7 ['similarly', 'to', 'those', 'shown', 'in', 'Figure', '4']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 10), (5, 9), (6, 1), (7, 2), (8, 2), (9, 3), (10, 4), (11, 9), (12, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "5 ['The', 'paper', 'is', 'well', 'written']\n",
      "1 ['and']\n",
      "23 ['the', 'main', 'contribution', ',', 'a', 'methodology', 'to', 'find', '“', 'blind', '-', 'spot', 'attacks', '”', 'well', 'motivated', 'and', 'differences', 'to', 'prior', 'work', 'stated', 'clearly']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 7), (5, 2), (6, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "3 ['The', 'empirical', 'results']\n",
      "6 ['presented', 'in', 'Figure', '1', 'and', '2']\n",
      "1 ['are']\n",
      "2 ['very', 'convincing']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 7), (4, 6), (5, 4), (6, 6), (7, 7), (8, 7), (9, 2), (10, 2), (11, 5), (12, 3), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "2 ['The', 'gain']\n",
      "18 ['of', 'using', 'a', 'sufficiently', 'more', 'complicated', 'approach', 'to', 'assess', 'the', 'overall', 'distance', 'between', 'the', 'test', 'and', 'training', 'dataset']\n",
      "1 ['is']\n",
      "1 ['not']\n",
      "1 ['clear']\n",
      "1 [',']\n",
      "7 ['comparing', 'it', 'to', 'the', 'very', 'insightful', 'histograms']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 6), (6, 2), (7, 5), (8, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['Why']\n",
      "2 ['for', 'example']\n",
      "1 ['not']\n",
      "14 ['using', 'a', 'simple', 'score', 'based', 'on', 'the', 'histogram', ',', 'or', 'even', 'the', 'mean', 'distance']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 9), (5, 4), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "2 ['Of', 'course']\n",
      "4 ['providing', 'a', 'single', 'measure']\n",
      "5 ['would', 'allow', 'to', 'leverage', 'that']\n",
      "3 ['information', 'during', 'training']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 7), (5, 6), (6, 4), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "4 ['in', 'its', 'current', 'form']\n",
      "1 ['this']\n",
      "11 ['seems', 'rather', 'complicated', 'and', 'computationally', 'expensive', '-LRB-', 'KL', '-', 'based', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 4), (5, 4), (6, 5), (7, 4), (8, 7), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "8 ['As', 'stated', 'later', 'in', 'the', 'paper', 'the', 'histograms']\n",
      "1 ['themselves']\n",
      "11 ['are', 'not', 'informative', 'enough', 'to', 'detect', 'such', 'blind', '-', 'spot', 'transformation']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 6), (5, 6), (6, 6), (7, 4), (8, 7), (9, 3), (10, 2), (11, 4), (12, 2), (13, 2), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['Intuitively']\n",
      "1 ['this']\n",
      "25 ['makes', 'a', 'lot', 'of', 'sense', 'given', 'that', 'the', 'distance', 'is', 'based', 'on', 'the', 'network', 'embedding', 'and', 'is', 'therefore', 'also', 'susceptible', 'to', 'this', 'kind', 'of', 'data']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 2), (5, 1), (6, 1), (7, 2), (8, 2), (9, 3), (10, 6), (11, 8), (12, 6), (13, 4), (14, 4), (15, 3), (16, 2), (17, 4), (18, 3), (19, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "30 ['is', 'not', 'further', 'discussed', 'how', 'the', 'overall', 'KL', '-', 'based', 'data', 'similarity', 'measure', 'would', 'help', 'in', 'this', 'case', 'since', 'it', 'seems', 'likely', 'that', 'it', 'would', 'also', 'exhibit', 'the', 'same', 'issue']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 9), (4, 12), (5, 9), (6, 6), (7, 4), (8, 5), (9, 6), (10, 9), (11, 2), (12, 5), (13, 5), (14, 10), (15, 25), (16, 17), (17, 18), (18, 10), (19, 11), (20, 9), (21, 15), (22, 4), (23, 2), (24, 2), (25, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "depth:  2\n",
      "3 ['-LRB-', '1', '-RRB-']\n",
      "93 ['A', 'deep', 'BNN', 'to', 'show', 'that', 'the', 'cumulative', 'error', 'is', 'negligible', 'as', 'the', 'number', 'of', 'the', 'hidden', 'layers', 'increases', '-LRB-', '2', '-RRB-', 'Small', 'latent', 'dimension', 'since', 'CLT', 'may', 'not', 'hold', '-LRB-', '3', '-RRB-', 'A', 'heavy', '-', 'tailed', 'variational', 'distribution', 'since', 'the', 'second', 'moment', 'may', 'not', 'be', 'finite', '-LRB-', '4', '-RRB-', 'Other', 'nonlinear', 'activations', 'since', 'the', 'Gaussian', 'approximation', 'may', 'not', 'be', 'accurate', 'due', 'to', '-LRB-', 'generalized', '-RRB-', 'Berry', '-', 'Esseen', 'theorem', '-LRB-', '5', '-RRB-', 'A', 'BNN', 'with', 'skip', 'connections', 'since', 'a', 'Bayesian', 'multiplayer', 'perceptron', 'with', 'skip', 'connections', 'is', 'also', 'a', 'feed', '-', 'forward', 'BNN']\n",
      "16 ['Among', 'these', 'cases', ',', 'I', 'am', 'eager', 'to', 'see', 'some', 'results', 'on', 'a', 'deep', 'thin', 'BNN']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 2), (5, 1), (6, 2), (7, 3), (8, 7), (9, 10), (10, 10), (11, 9), (12, 12), (13, 4), (14, 2), (15, 3), (16, 1), (17, 2), (18, 2), (19, 5), (20, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "depth:  2\n",
      "1 ['Furthermore']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "43 ['would', 'like', 'to', 'see', 'some', 'empirical', 'comparison', 'on', 'real', '-', 'world', 'datasets', 'between', 'DVI', 'and', 'MCVI', 'under', 'a', '*', 'fixed', '*', 'prior', 'since', 'such', 'comparison', 'demonstrates', 'the', 'approximation', 'accuracy', 'of', 'DVI', 'and', 'rule', 'out', 'the', 'confounding', 'factor', 'introduced', 'by', 'the', 'empirical', 'Bayes', 'approach']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 5), (6, 4), (7, 3), (8, 4), (9, 5), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['Why']\n",
      "1 ['do']\n",
      "1 ['you']\n",
      "13 ['not', 'also', 'compare', 'against', 'this', ',', 'and', 'show', 'it', 'really', 'does', 'not', 'work']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 6), (6, 8), (7, 3), (8, 3), (9, 5), (10, 5), (11, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['experiments']\n",
      "1 ['are']\n",
      "9 ['for', 'rather', 'small', 'datasets', 'and', 'for', 'the', 'DVI', 'method']\n",
      "13 ['if', 'I', 'understand', 'correctly', 'only', 'models', 'with', 'a', 'single', 'hidden', 'layer', 'are', 'considered']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 2), (4, 4), (5, 6), (6, 3), (7, 3), (8, 2), (9, 3), (10, 3), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['If']\n",
      "1 ['so']\n",
      "14 ['a', 'comparison', 'of', 'DVI', 'with', 'MCVI', 'in', 'a', 'more', 'complex', 'example', 'is', 'of', 'interest']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 4), (6, 4), (7, 7), (8, 5), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  5\n",
      "2 ['at', 'least']\n",
      "1 ['dDVI']\n",
      "1 ['with']\n",
      "12 ['diagonal', 'q', '-LRB-', 'w', '-RRB-', 'on', 'some', 'much', 'larger', 'models', 'and', 'datasets']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 7), (5, 6), (6, 7), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "2 ['-', 'Experiments']\n",
      "1 ['are']\n",
      "1 ['OK']\n",
      "1 [',']\n",
      "12 ['but', 'on', 'pretty', 'small', 'datasets', ',', 'and', 'for', 'single', 'hidden', 'layer', 'NNs']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 5), (5, 9), (6, 7), (7, 3), (8, 4), (9, 2), (10, 2), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "17 ['On', 'such', 'data', 'and', 'models', ',', 'the', 'Barber', '&', 'Bishop', '98', 'method', 'could', 'be', 'run', 'as', 'well']\n",
      "1 ['-']\n",
      "5 ['Was', 'MCVI', 'run', 'with', 're-parameterization']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 4), (5, 4), (6, 3), (7, 8), (8, 4), (9, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['Why']\n",
      "1 ['not']\n",
      "5 ['show', 'the', 'PBP', '-', '1']\n",
      "10 ['results', ',', 'comparing', 'to', 'dDVI', ',', 'in', 'the', 'main', 'text']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 4), (6, 2), (7, 6), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['PBP']\n",
      "1 ['also']\n",
      "11 ['has', 'the', 'property', 'that', 'a', 'DL', 'system', 'gives', 'you', 'the', 'gradients']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 3), (5, 4), (6, 4), (7, 2), (8, 2), (9, 4), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "3 ['Having', 'said', 'that']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "8 ['think', 'dDVI', 'may', 'be', 'more', 'useful', 'than', 'PBP']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 4), (5, 8), (6, 7), (7, 3), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "7 ['While', 'Barber', '&', 'BIshop', '98', 'is', 'cited']\n",
      "1 [',']\n",
      "1 ['they']\n",
      "8 ['miss', 'the', 'expression', 'for', '<h_j', 'h_l>', 'in', 'there']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 5), (4, 5), (5, 5), (6, 3), (7, 4), (8, 2), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Now']\n",
      "1 [',']\n",
      "4 ['what', 'is', 'done', 'here']\n",
      "1 [',']\n",
      "9 ['is', 'more', 'elegant', ',', 'does', 'not', 'need', '1D', 'quadrature']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 10), (5, 9), (6, 5), (7, 4), (8, 6), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "16 ['-', 'Significance', ':', 'Judging', 'from', 'the', 'existing', 'experiments', ',', 'the', 'significance', 'may', 'be', 'rather', 'small', ',']\n",
      "1 ['*']\n",
      "9 ['if', 'one', 'only', 'looks', 'at', 'test', 'log', 'likelihood', '*']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 3), (6, 4), (7, 3), (8, 4), (9, 2), (10, 4), (11, 5), (12, 5), (13, 2), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 [\"'d\"]\n",
      "1 ['still']\n",
      "21 ['give', 'this', 'the', 'benefit', 'of', 'the', 'doubt', ',', 'as', 'in', 'particular', 'dDVI', 'could', 'be', 'really', 'interesting', 'at', 'large', 'scale', 'as', 'well']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 3), (5, 3), (6, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['But']\n",
      "2 ['the', 'authors']\n",
      "7 ['may', 'tone', 'down', 'their', 'language', 'a', 'bit']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 3), (5, 4), (6, 3), (7, 3), (8, 4), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "3 ['To', 'increase', 'significance']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "9 ['recommend', 'to', 'comment', 'beyond', 'just', 'test', 'log', 'likelihood', 'scores']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 4), (6, 3), (7, 4), (8, 3), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['For', 'example']\n",
      "1 [':']\n",
      "1 ['-']\n",
      "12 ['Does', 'the', 'optimization', 'become', 'simpler', ',', 'less', 'tuning', 'required', ',', 'more', 'automatic']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 1), (6, 2), (7, 4), (8, 5), (9, 2), (10, 1), (11, 2), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['Would']\n",
      "1 ['one']\n",
      "1 ['not']\n",
      "13 ['expect', 'so', ',', 'given', 'you', 'make', 'a', 'big', 'point', 'out', 'of', 'reducing', 'variance']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 1), (5, 2), (6, 3), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['Does']\n",
      "3 ['it', 'converge', 'faster']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 5), (6, 4), (7, 6), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['-']\n",
      "1 ['Can']\n",
      "1 ['you']\n",
      "12 ['do', 'something', 'with', 'your', 'posterior', 'that', 'normal', 'DNN', 'methods', 'can', 'not', 'do']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 5)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "2 ['Better', 'decisions']\n",
      "1 ['-LRB-']\n",
      "6 ['bandits', ',', 'active', 'learning', ',', 'HPO']\n",
      "1 ['-RRB-']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3)]\n",
      "[0, 1, 2]\n",
      "depth:  3\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 5), (5, 2), (6, 2), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "3 ['In', 'the', 'end']\n",
      "1 [',']\n",
      "1 ['who']\n",
      "6 ['really', 'cares', 'about', 'test', 'log', 'likelihood']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 3), (6, 2), (7, 8), (8, 2), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['Experiments']\n",
      "1 [':']\n",
      "2 ['-', 'What']\n",
      "10 ['is', 'the', 'q', '-LRB-', 'w', '-RRB-', 'family', 'being', 'used', 'here']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3)]\n",
      "[0, 1, 2, 3]\n",
      "depth:  2\n",
      "1 ['Fully']\n",
      "1 ['factorized']\n",
      "1 ['Gaussian']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 1)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['suppose']\n",
      "1 ['so']\n",
      "2 ['for', 'dDVI']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 1)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['for']\n",
      "1 ['DVI']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 6), (5, 5), (6, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "7 ['Not', 'said', 'anywhere', ',', 'in', 'main', 'paper']\n",
      "1 ['or']\n",
      "5 ['Appendix', '-', 'A', 'bit', 'disappointing']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 5), (6, 6), (7, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  5\n",
      "1 ['numbers']\n",
      "4 ['on', 'speed', 'and', 'robustness']\n",
      "2 ['of', 'learning']\n",
      "1 [',']\n",
      "1 ['etc']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 2), (5, 4), (6, 4), (7, 2), (8, 1), (9, 2), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  5\n",
      "1 ['what']\n",
      "1 ['you']\n",
      "1 ['really']\n",
      "5 ['gain', 'by', 'reducing', 'the', 'variance']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['is']\n",
      "2 ['really', 'important']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 3), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "2 ['If', 'not']\n",
      "1 [',']\n",
      "1 ['this']\n",
      "6 ['would', 'be', 'an', 'important', 'missing', 'comparison']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 6), (5, 4), (6, 3), (7, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['Please']\n",
      "10 ['be', 'clear', 'in', 'the', 'main', 'text', '-', 'Advantages', 'over', 'MCVI']\n",
      "1 ['are']\n",
      "1 ['not']\n",
      "2 ['very', 'large']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 2), (6, 2), (7, 2), (8, 2), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['At', 'least']\n",
      "1 [',']\n",
      "1 ['dDVI']\n",
      "7 ['should', 'be', 'faster', 'to', 'converge', 'than', 'MCVI']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 3), (5, 2), (6, 3), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['Can']\n",
      "1 ['you']\n",
      "6 ['say', 'something', 'about', 'robustness', 'of', 'training']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 3), (4, 2), (5, 3), (6, 3), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['Is']\n",
      "1 ['it']\n",
      "1 ['easier']\n",
      "5 ['to', 'train', 'dDVI', 'than', 'MCVI']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 2), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['Are']\n",
      "1 ['they']\n",
      "5 ['obtained', 'with', 'the', 'same', 'model']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 1)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['dDVI']\n",
      "1 ['is']\n",
      "2 ['doing', 'better']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 3), (5, 5), (6, 5), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "2 ['Other', 'points']\n",
      "1 [':']\n",
      "12 ['-', 'Please', 'acknowledge', 'the', '<h_j', 'h_l>', 'expression', 'in', 'Barber', '&', 'Bishop', '98']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 7), (5, 7), (6, 7), (7, 9), (8, 5), (9, 4), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['Yours']\n",
      "5 ['is', 'more', 'elegant', 'and', 'faster']\n",
      "1 ['-LRB-']\n",
      "10 ['does', 'not', 'need', '1D', 'quadrature', '-RRB-', '-', 'Relation', 'to', 'PBP']\n",
      "1 [':']\n",
      "8 ['Note', 'that', 'dDVI', 'has', 'an', 'advantage', 'in', 'practice']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 2), (5, 2), (6, 3), (7, 3), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "2 ['With', 'PBP']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "7 ['need', 'to', 'compute', 'gradients', 'for', 'every', 'datapoint']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "2 ['In', 'dDVI']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "4 ['can', 'do', 'mini-batch', 'updates']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 9)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "1 ['I']\n",
      "10 ['just', '*', 'love', '*', 'the', 'header', '\"', 'Wild', 'approximations', '\"']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 3), (7, 4), (8, 6), (9, 4), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  7\n",
      "1 ['to']\n",
      "4 ['this', 'kind', 'of', 'work']\n",
      "1 ['as']\n",
      "4 ['\"', 'weak', 'analogies', '\"']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 2), (6, 4), (7, 6), (8, 12), (9, 2), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['propose']\n",
      "3 ['a', 'new', 'approach']\n",
      "19 ['to', 'perform', 'deterministic', 'variational', 'inference', 'for', 'feed', '-', 'forward', 'BNN', 'with', 'specific', 'nonlinear', 'activation', 'functions', 'by', 'approximating', 'layerwise', 'moments']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 4), (5, 2), (6, 6), (7, 4), (8, 3), (9, 2), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "3 ['Under', 'certain', 'conditions']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "14 ['show', 'that', 'the', 'proposed', 'method', 'achieves', 'better', 'performance', 'than', 'existing', 'Monte', 'Carlo', 'variational', 'inference']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 2), (6, 4), (7, 5), (8, 6), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['is']\n",
      "1 ['interesting']\n",
      "12 ['since', 'most', 'of', 'the', 'existing', 'works', 'focus', 'on', 'Monte', 'Carlo', 'variational', 'inference']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 4), (6, 2), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "3 ['The', 'main', 'contribution']\n",
      "3 ['of', 'this', 'paper']\n",
      "1 ['is']\n",
      "4 ['to', 'perform', 'Gaussian', 'approximation']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 4), (6, 7), (7, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['show']\n",
      "11 ['that', 'for', 'specific', 'activation', 'functions', ',', 'the', 'Gaussian', 'approximation', 'is', 'reasonable']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 5), (5, 2), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['main']\n",
      "1 ['concern']\n",
      "1 ['is']\n",
      "3 ['the', 'cumulative', 'error']\n",
      "5 ['due', 'to', 'the', 'Gaussian', 'approximation']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 5), (5, 5), (6, 4), (7, 4), (8, 7), (9, 7), (10, 4), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "16 ['Since', 'the', 'authors', 'argue', 'that', 'the', 'proposed', 'method', 'fixes', 'the', 'issues', 'of', 'stochastic', 'VI', 'for', 'BNN']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "8 ['should', 'also', 'investigate', '/', 'clarify', 'the', 'following', 'cases']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 6), (5, 6), (6, 5), (7, 3), (8, 5), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['For', 'example']\n",
      "1 [',']\n",
      "14 ['a', 'BNN', 'with', '5', 'hidden', 'layers', ',', 'where', 'the', 'latent', 'dimension', 'at', 'each', 'layer']\n",
      "4 ['is', 'less', 'than', '32']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 6), (5, 1), (6, 3), (7, 5), (8, 2), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['considers']\n",
      "4 ['a', 'purely', 'deterministic', 'approach']\n",
      "9 ['to', 'learning', 'variational', 'posterior', 'approximations', 'for', 'Bayesian', 'neural', 'networks']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 2), (6, 1), (7, 4), (8, 5), (9, 7), (10, 5), (11, 6), (12, 7), (13, 9), (14, 6), (15, 9), (16, 5), (17, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  3\n",
      "1 ['Variational']\n",
      "2 ['lower', 'bound']\n",
      "1 ['gradients']\n",
      "1 ['are']\n",
      "43 ['obtained', 'by', 'approximating', 'the', 'lower', 'bound', 'using', 'Gaussian', 'approximations', 'and', 'moment', 'propagation', 'for', 'network', 'activations', ',', 'and', 'using', 'a', 'closed', 'form', 'expression', 'for', 'the', 'variational', 'expectation', 'of', 'the', 'log', '-', 'likelihood', ',', 'the', 'latter', 'being', 'available', 'for', 'the', 'models', 'considered', 'in', 'the', 'paper']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['is']\n",
      "3 ['an', 'interesting', 'paper']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 11), (5, 5), (6, 5), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "3 ['The', 'Gaussian', 'approximations']\n",
      "1 ['and']\n",
      "3 ['moment', 'propagation', 'approximations']\n",
      "1 ['are']\n",
      "4 ['clever', 'and', 'highly', 'original']\n",
      "6 ['although', 'the', 'derivation', 'is', 'rather', 'heuristic']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 2), (6, 4), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['There']\n",
      "1 ['is']\n",
      "3 ['some', 'empirical', 'support']\n",
      "5 ['that', 'the', 'approximations', 'work', 'well']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 5), (6, 2), (7, 2), (8, 4), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['is']\n",
      "1 ['generally']\n",
      "12 ['well', 'written', 'and', 'clearly', 'motivated', 'in', 'the', 'context', 'of', 'the', 'existing', 'literature']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 2), (6, 4), (7, 2), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['approximations']\n",
      "1 ['work']\n",
      "1 ['well']\n",
      "7 ['for', 'the', 'examples', 'presented', 'in', 'the', 'paper']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 6), (7, 8), (8, 2), (9, 3), (10, 3), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  6\n",
      "2 ['the', 'Gaussian']\n",
      "1 ['and']\n",
      "3 ['moment', 'propagation', 'approximations']\n",
      "1 ['cause']\n",
      "1 ['difficulty']\n",
      "6 ['when', 'applied', 'repeatedly', 'in', 'deeper', 'networks']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 6), (5, 5), (6, 7), (7, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['Are']\n",
      "8 ['the', 'problems', 'with', 'MCVI', 'and', 'high', 'gradient', 'variance']\n",
      "9 ['most', 'serious', 'for', 'large', 'datasets', 'and', 'more', 'complex', 'models']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 2), (5, 6), (6, 4), (7, 7), (8, 7), (9, 7), (10, 4), (11, 2), (12, 4), (13, 2), (14, 2), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['empirical']\n",
      "1 ['Bayes']\n",
      "1 ['approximations']\n",
      "1 ['are']\n",
      "25 ['interesting', '-', 'I', 'would', 'have', 'thought', 'similar', 'approximations', 'been', 'used', 'in', 'the', 'literature', 'before', ',', 'in', 'addition', 'to', 'the', 'work', 'you', 'mention', 'in', 'Section', '5']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 1), (6, 2), (7, 5), (8, 4), (9, 3), (10, 6), (11, 8), (12, 6), (13, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['do']\n",
      "1 [\"n't\"]\n",
      "23 ['feel', 'there', 'is', 'much', 'to', 'compare', 'the', 'proposed', 'EB', 'approximations', 'to', ',', 'although', 'a', 'comparison', 'with', 'manual', 'tuning', 'is', 'given', 'in', 'Section', '6']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 2), (6, 1), (7, 2), (8, 4), (9, 6), (10, 6), (11, 5), (12, 3), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Summary']\n",
      "1 [':']\n",
      "19 ['This', 'work', 'is', 'tackling', 'two', 'difficulties', 'in', 'current', 'VB', 'applied', 'to', 'DNNs', '-LRB-', '\"', 'Bayes', 'by', 'backprop', '\"', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 6), (5, 4), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['First']\n",
      "1 [',']\n",
      "5 ['MC', 'approximations', 'of', 'intractable', 'expectations']\n",
      "5 ['are', 'replaced', 'by', 'deterministic', 'approximations']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 5), (5, 6), (6, 2), (7, 2), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "6 ['While', 'this', 'has', 'been', 'done', 'before']\n",
      "1 [',']\n",
      "2 ['the', 'solution']\n",
      "1 ['here']\n",
      "5 ['is', 'new', 'and', 'very', 'interesting']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 8), (5, 6), (6, 6), (7, 5), (8, 2), (9, 3), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['Second']\n",
      "1 [',']\n",
      "6 ['a', 'Gaussian', 'prior', 'with', 'length', 'scales']\n",
      "16 ['is', 'learned', 'by', 'VB', 'empirical', 'Bayes', 'alongside', 'the', 'normal', 'training', ',', 'which', 'is', 'also', 'very', 'useful']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 7), (4, 10), (5, 6), (6, 6), (7, 9), (8, 7), (9, 8), (10, 4), (11, 6), (12, 5), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "6 ['The', 'term', '\"', 'fixing', 'VB', '\"']\n",
      "1 ['and']\n",
      "4 ['some', 'of', 'the', 'intro']\n",
      "1 ['is']\n",
      "1 ['not']\n",
      "1 ['really']\n",
      "27 ['supported', 'by', 'the', 'rather', 'weak', 'experiments', ',', 'done', 'on', 'small', 'datasets', 'and', 'networks', ',', 'where', 'much', 'older', 'work', 'like', 'Barber', '&', 'Bishop', 'would', 'apply', 'without', 'any', 'problems']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 8), (5, 5), (6, 8), (7, 5), (8, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "15 ['While', 'interesting', 'and', 'potentially', 'very', 'useful', 'novelties', 'are', 'presented', ',', 'and', 'the', 'writing', 'is', 'excellent']\n",
      "1 [',']\n",
      "4 ['both', 'experiments', 'and', 'motivation']\n",
      "3 ['can', 'be', 'improved']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 8), (5, 7), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "7 ['-', 'Quality', ':', 'Extremely', 'well', 'written', 'paper']\n",
      "1 [',']\n",
      "6 ['I', 'learned', 'a', 'lot', 'from', 'it']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 2), (6, 4), (7, 2), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  6\n",
      "1 ['great']\n",
      "1 ['figures']\n",
      "1 ['to']\n",
      "2 ['explain', 'things']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 10), (5, 4), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['And']\n",
      "11 ['the', 'major', 'technical', 'novelty', ',', 'the', 'expression', 'for', '<h_j', 'h_l>', ',']\n",
      "5 ['is', 'really', 'interesting', 'and', 'useful']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 4), (5, 2), (6, 3), (7, 2), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "2 ['-', 'Clarity']\n",
      "1 [':']\n",
      "8 ['Excellent', 'writing', 'until', 'it', 'comes', 'to', 'the', 'experiments']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 9), (5, 5), (6, 2), (7, 2), (8, 3), (9, 3), (10, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "16 ['Here', ',', 'important', 'details', 'are', 'just', 'missing', ',', 'for', 'example', 'what', 'q', '-LRB-', 'w', '-RRB-', 'is']\n",
      "1 ['-LRB-']\n",
      "3 ['fully', 'factorized', 'Gaussian']\n",
      "1 ['?']\n",
      "1 ['-RRB-']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "4 ['Very', 'nice', 'literature', 'review']\n",
      "1 [',']\n",
      "2 ['also', 'historical']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 10), (5, 11), (6, 6), (7, 5), (8, 5), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "2 ['-', 'Originality']\n",
      "1 [':']\n",
      "10 ['The', 'idea', 'of', 'matching', 'Gaussian', 'moments', 'along', 'the', 'network', 'graph']\n",
      "1 ['is']\n",
      "1 ['previously']\n",
      "12 ['done', 'in', 'PBP', '-LRB-', 'Lobato', ',', 'Adams', '-RRB-', ',', 'as', 'acknowledged', 'here']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 5), (5, 5), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['Porting']\n",
      "1 ['this']\n",
      "2 ['from', 'ADF']\n",
      "2 ['to', 'VB']\n",
      "1 ['dDVI']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 2), (5, 1), (6, 2), (7, 2), (8, 2), (9, 5), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "11 ['was', 'hoping', 'to', 'see', 'a', 'direct', 'comparison', 'between', 'FP16', 'and', 'INT16']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 10), (4, 13), (5, 7), (6, 3), (7, 6), (8, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "7 ['For', 'reference', ',', 'please', 'include', 'wallclock', 'time']\n",
      "1 ['and']\n",
      "17 ['actual', 'overall', 'memory', 'consumption', 'comparisons', 'of', 'the', 'proposed', 'methods', 'and', 'other', 'methods', 'as', 'well', 'as', 'the', 'baseline']\n",
      "5 ['-LRB-', 'default', 'FP32', 'training', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 4), (6, 5), (7, 1), (8, 2), (9, 4), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['describes']\n",
      "13 ['an', 'implementation', 'of', 'reduced', 'precision', 'deep', 'learning', 'using', 'a', '16', 'bit', 'integer', 'representation']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 2), (6, 4), (7, 2), (8, 4), (9, 3), (10, 2), (11, 2), (12, 2), (13, 4), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['field']\n",
      "1 ['has']\n",
      "1 ['recently']\n",
      "16 ['seen', 'a', 'lot', 'of', 'publications', 'proposing', 'various', 'methods', 'to', 'reduce', 'the', 'precision', 'of', 'weights', 'and', 'activations']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 1), (6, 2), (7, 5), (8, 4), (9, 6), (10, 7), (11, 7), (12, 2), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['These']\n",
      "1 ['schemes']\n",
      "1 ['have']\n",
      "1 ['generally']\n",
      "19 ['achieved', 'close', '-', 'to', '-', 'SOTA', 'accuracy', 'for', 'small', 'networks', 'on', 'datasets', 'such', 'as', 'MNIST', 'and', 'CIFAR', '-', '10']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 9), (4, 3), (5, 6), (6, 9), (7, 5), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "16 ['for', 'larger', 'networks', '-LRB-', 'ResNET', ',', 'Vgg', ',', 'etc', '-RRB-', 'on', 'large', 'dataset', 'such', 'as', 'ImageNET']\n",
      "1 [',']\n",
      "4 ['a', 'significant', 'accuracy', 'drop']\n",
      "2 ['are', 'reported']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 4), (5, 2), (6, 4), (7, 8), (8, 6), (9, 6), (10, 8), (11, 3), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "3 ['In', 'this', 'work']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "26 ['show', 'that', 'a', 'careful', 'implementation', 'of', 'mixed', '-', 'precision', 'dynamic', 'fixed', 'point', 'computation', 'can', 'achieve', 'SOTA', 'on', '4', 'large', 'networks', 'on', 'the', 'ImageNET', '-', '1K', 'datasets']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 4), (5, 6), (6, 7), (7, 3), (8, 5), (9, 9), (10, 6), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['Using']\n",
      "8 ['a', 'INT16', '-LRB-', 'as', 'opposed', 'to', 'FP16', '-RRB-']\n",
      "2 ['the', 'advantage']\n",
      "14 ['of', 'enabling', 'the', 'use', 'of', 'new', 'SIMD', 'mul', '-', 'acc', 'instructions', 'such', 'as', 'QVNNI16']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 3), (5, 3), (6, 6), (7, 2), (8, 3), (9, 4), (10, 4), (11, 3), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['reported']\n",
      "1 ['accuracy']\n",
      "1 ['numbers']\n",
      "1 ['show']\n",
      "16 ['convincingly', 'that', 'INT16', 'weights', 'and', 'activations', 'can', 'be', 'used', 'without', 'loss', 'of', 'accuracy', 'in', 'large', 'CNNs']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 8), (5, 3), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "5 ['The', 'paper', 'is', 'written', 'clearly']\n",
      "1 ['and']\n",
      "4 ['the', 'English', 'is', 'fine']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 5), (6, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['is']\n",
      "7 ['about', 'low', '-', 'precision', 'training', 'for', 'ConvNets']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 5), (6, 8), (7, 4), (8, 6), (9, 7), (10, 4), (11, 3), (12, 4), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  5\n",
      "5 ['a', '\"', 'dynamic', 'fixed', 'point']\n",
      "1 ['\"']\n",
      "1 ['scheme']\n",
      "1 ['that']\n",
      "18 ['shares', 'the', 'exponent', 'part', 'for', 'a', 'tensor', ',', 'and', 'developed', 'procedures', 'to', 'do', 'NN', 'computing', 'with', 'this', 'format']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 1), (6, 2), (7, 4), (8, 6), (9, 8), (10, 13), (11, 8), (12, 7), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['proposed']\n",
      "1 ['method']\n",
      "1 ['is']\n",
      "31 ['shown', 'to', 'achieve', 'matching', 'performance', 'against', 'their', 'FP32', 'counter-parts', 'with', 'the', 'same', 'number', 'of', 'training', 'iterations', 'on', 'several', 'state', '-', 'of', '-', 'the', '-', 'art', 'ConvNets', 'architectures', 'on', 'Imagenet', '-', '1K']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 4), (5, 6), (6, 2), (7, 4), (8, 6), (9, 3), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "4 ['According', 'to', 'the', 'paper']\n",
      "1 [',']\n",
      "1 ['this']\n",
      "14 ['is', 'the', 'first', 'time', 'such', 'kind', 'of', 'performance', 'are', 'demonstrated', 'for', 'limited', 'precision', 'training']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 6), (6, 6), (7, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "2 ['Potential', 'improvements']\n",
      "1 [':']\n",
      "13 ['-', 'Please', 'define', 'the', 'terms', 'like', 'FPROP', 'and', 'WTGRAD', 'at', 'the', 'first', 'occurance']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 6), (6, 2), (7, 2), (8, 2), (9, 5), (10, 2), (11, 2), (12, 3), (13, 4), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['work']\n",
      "1 ['presents']\n",
      "16 ['a', 'CNN', 'training', 'setup', 'that', 'uses', 'half', 'precision', 'implementation', 'that', 'can', 'get', '2X', 'speedup', 'for', 'training']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 9), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "5 ['The', 'work', 'is', 'clearly', 'presented']\n",
      "1 ['and']\n",
      "4 ['the', 'evaluations', 'seem', 'convincing']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 7), (4, 4), (5, 4), (6, 4), (7, 5), (8, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['presented']\n",
      "1 ['implementations']\n",
      "1 ['are']\n",
      "5 ['competitive', 'in', 'terms', 'of', 'accuracy']\n",
      "1 [',']\n",
      "6 ['when', 'compared', 'to', 'the', 'FP32', 'representation']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 10), (5, 6), (6, 7), (7, 5), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "8 ['I', \"'m\", 'not', 'an', 'expert', 'in', 'this', 'area']\n",
      "1 ['but']\n",
      "6 ['the', 'contribution', 'seems', 'relevant', 'to', 'me']\n",
      "1 [',']\n",
      "5 ['and', 'enough', 'for', 'being', 'published']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 4), (5, 2), (6, 4), (7, 2), (8, 4), (9, 2), (10, 4), (11, 2), (12, 3), (13, 4), (14, 2), (15, 2), (16, 2), (17, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "1 ['Comments']\n",
      "1 [':']\n",
      "20 ['Earlier', 'I', 'had', 'some', 'concern', 'about', 'the', 'correctness', 'of', 'a', 'claim', 'made', 'by', 'the', 'authors', ',', 'which', 'is', 'resolved', 'now']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 1), (6, 2), (7, 6), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  7\n",
      "1 ['their']\n",
      "1 ['proposed']\n",
      "1 ['sharpness']\n",
      "1 ['criterion']\n",
      "1 ['is']\n",
      "2 ['scale', 'invariance']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 4), (6, 3), (7, 2), (8, 4), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['They']\n",
      "1 ['took']\n",
      "3 ['care', 'of', 'it']\n",
      "8 ['by', 'removing', 'this', 'claim', 'in', 'the', 'revised', 'version']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 4), (7, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  6\n",
      "1 ['the']\n",
      "1 ['paper']\n",
      "1 ['is']\n",
      "4 ['quite', 'interesting', 'and', 'useful']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 4), (5, 4), (6, 4), (7, 5), (8, 1), (9, 2), (10, 4), (11, 8), (12, 4), (13, 6), (14, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['benefit']\n",
      "5 ['from', 'additional', 'investigations', ',', 'e.g.']\n",
      "1 [',']\n",
      "20 ['by', 'adding', 'some', 'rescaled', 'Gaussian', 'noise', 'to', 'gradients', 'during', 'the', 'LB', 'regime', 'one', 'can', 'get', 'advantages', 'of', 'the', 'SB', 'regime']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 7), (4, 8), (5, 3), (6, 2), (7, 2), (8, 3), (9, 5), (10, 6), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "2 ['Interesting', 'paper']\n",
      "1 [',']\n",
      "1 ['definitely']\n",
      "1 ['provides']\n",
      "1 ['value']\n",
      "3 ['to', 'the', 'community']\n",
      "12 ['by', 'discussing', 'why', 'large', 'batch', 'gradient', 'descent', 'does', 'not', 'work', 'too', 'well']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 10), (5, 9), (6, 7), (7, 5), (8, 3), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "9 ['The', 'paper', 'is', 'an', 'empirical', 'study', 'to', 'justify', 'that']\n",
      "1 [':']\n",
      "13 ['1', '-RRB-', 'SGD', 'with', 'smaller', 'batch', 'sizes', 'converges', 'to', 'flatter', 'minima', ',', '2']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 5), (5, 3), (6, 5), (7, 6), (8, 6), (9, 4), (10, 4), (11, 1), (12, 3), (13, 3), (14, 2), (15, 5), (16, 2), (17, 3), (18, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  2\n",
      "3 ['Pros', 'and', 'Cons']\n",
      "1 [':']\n",
      "29 ['Although', 'there', 'is', 'little', 'novelty', 'in', 'the', 'paper', ',', 'I', 'think', 'the', 'work', 'is', 'of', 'great', 'value', 'in', 'shedding', 'light', 'into', 'some', 'interesting', 'questions', 'around', 'generalization', 'of', 'deep', 'networks']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 3), (5, 1), (6, 2), (7, 4), (8, 4), (9, 7), (10, 9), (11, 8), (12, 4), (13, 6), (14, 9), (15, 6), (16, 6), (17, 4), (18, 2), (19, 4), (20, 1), (21, 2), (22, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "depth:  2\n",
      "1 ['Significance']\n",
      "1 [':']\n",
      "44 ['I', 'think', 'such', 'results', 'may', 'have', 'impact', 'on', 'both', 'theory', 'and', 'practice', ',', 'respectively', 'by', 'suggesting', 'what', 'assumptions', 'are', 'legitimate', 'for', 'real', 'scenarios', 'for', 'building', 'new', 'theories', ',', 'or', 'be', 'used', 'heuristically', 'to', 'develop', 'new', 'algorithms', 'with', 'generalization', 'by', 'smart', 'manipulation', 'of', 'mini-batch', 'sizes']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 7), (5, 10), (6, 13), (7, 13), (8, 9), (9, 9), (10, 6), (11, 2), (12, 1), (13, 2), (14, 3), (15, 6), (16, 2), (17, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "21 ['There', 'have', 'also', 'been', 'a', 'number', 'of', 'papers', 'on', '“', 'automatic', 'post', 'editing', '”', ',', 'including', 'the', 'shared', 'task', 'at', 'WMT2016']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "29 ['there', 'are', 'not', 'only', 'standard', 'test', 'sets', 'and', 'baselines', ',', 'but', 'also', 'datasets', 'that', 'could', 'actually', 'be', 'used', 'to', 'train', 'a', 'post', 'editing', 'model', 'with', 'human', '-', 'generated', 'data']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 3), (6, 2), (7, 2), (8, 1), (9, 2), (10, 2), (11, 2), (12, 3), (13, 4), (14, 2), (15, 2), (16, 5), (17, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  3\n",
      "2 ['Specific', 'comments']\n",
      "1 [':']\n",
      "2 ['-', 'It']\n",
      "17 ['would', 'be', 'interesting', 'to', 'see', 'what', 'the', 'improvements', 'are', 'if', 'the', 'baseline', 'model', 'is', 'a', 'neural', 'system']\n",
      "[(0, 1), (1, 1), (2, 8), (3, 10), (4, 13), (5, 11), (6, 13), (7, 13), (8, 19), (9, 12), (10, 6), (11, 4), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['Indeed']\n",
      "1 [',']\n",
      "24 ['unsurprisingly', ',', 'the', 'authors', 'note', 'that', '\"', 'the', 'probability', 'of', 'correctly', 'labelling', 'a', 'word', 'as', 'a', 'mistake', 'remains', 'low', '-LRB-', '62', '%', '-RRB-', '\"']\n",
      "1 ['-']\n",
      "17 ['this', 'admittedly', 'beats', 'a', 'random', '-', 'chance', 'baseline', ',', 'but', 'is', 'not', 'compared', 'to', 'something', 'more', 'meaningful']\n",
      "1 [',']\n",
      "19 ['such', 'as', 'simply', 'contrasting', 'the', 'existing', 'system', 'with', 'a', 'more', 'powerful', 'convolutional', 'model', 'and', 'labelling', 'all', 'discrepancies', 'as', 'mistakes']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 8), (3, 6), (4, 6), (5, 10), (6, 5), (7, 6), (8, 10), (9, 6), (10, 3), (11, 6), (12, 6), (13, 4), (14, 2), (15, 7), (16, 8), (17, 8), (18, 11), (19, 8), (20, 13), (21, 15), (22, 12), (23, 12), (24, 9), (25, 10), (26, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "depth:  2\n",
      "20 ['Although', 'I', 'do', 'like', 'the', 'paper', 'on', 'the', 'whole', ',', 'to', 'really', 'convince', 'me', 'that', 'main', 'objective', '--', 'ie', 'that']\n",
      "1 ['**']\n",
      "1 ['iterative']\n",
      "1 ['**']\n",
      "3 ['improvement', 'is', 'beneficial']\n",
      "1 ['--']\n",
      "75 ['has', 'been', 'satifactorily', 'demonstrated', 'it', 'would', 'be', 'necessary', 'to', 'include', 'stronger', 'baselines', '-', 'and', 'in', 'particular', ',', 'to', 'show', 'that', 'an', 'iterative', 'refinement', 'scheme', 'can', 'really', 'improve', 'over', 'a', 'system', 'closely', 'matched', 'to', 'the', 'attention', '-', 'based', 'model', ',', 'both', 'when', 'used', 'in', 'isolation', 'and', 'when', 'used', 'in', 'system', 'combination', 'with', 'a', 'PBMT', 'system', ',', 'and', 'to', 'demonstrate', 'that', 'the', 'PBMT', 'system', 'is', 'not', 'simply', 'acting', 'as', 'a', 'regulariser', 'for', 'the', 'attention', '-', 'based', 'model']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 5), (5, 4), (6, 2), (7, 2), (8, 4), (9, 3), (10, 3), (11, 3), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['How']\n",
      "1 ['does']\n",
      "2 ['the', 'approach']\n",
      "12 ['compare', 'to', 'a', 'model', 'that', 'simply', 're-ranks', 'the', 'k', '-', 'best', 'output']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 6), (6, 11), (7, 5), (8, 2), (9, 2), (10, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  5\n",
      "3 ['some', 'minor', 'typos']\n",
      "1 [':']\n",
      "3 ['-', 'p.', '2']\n",
      "1 [':']\n",
      "3 ['a', 'lookup', 'table']\n",
      "8 ['that', 'replace', '*', 'S', '*', 'each', 'word', '...']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 5), (5, 7), (6, 6), (7, 4), (8, 3), (9, 2), (10, 2), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "3 ['-', 'p.', '3']\n",
      "1 [':']\n",
      "17 ['I', 'might', 'be', 'mistanken', 'but', 'it', 'seems', 'to', 'me', 'that', 'j', 'is', 'used', 'for', 'two', 'different', 'things']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['is']\n",
      "1 ['confusing']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 7), (5, 4), (6, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "3 ['-', 'p.', '3']\n",
      "1 [':']\n",
      "1 ['...']\n",
      "11 ['takes', 'as', 'input', 'these', 'representation', '*', 'S', '*', 'and', 'outputs', '...']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 3), (7, 2), (8, 4), (9, 2), (10, 2), (11, 2), (12, 2), (13, 2), (14, 4), (15, 5), (16, 2), (17, 3), (18, 3), (19, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['work']\n",
      "1 ['proposes']\n",
      "24 ['to', 'iteratively', 'improve', 'a', 'sentence', 'that', 'has', 'been', 'generated', 'from', 'another', 'MT', 'system', '-LRB-', 'in', 'this', 'case', ',', 'a', 'phrase', '-', 'based', 'system', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 5), (6, 2), (7, 4), (8, 5), (9, 8), (10, 7), (11, 5), (12, 4), (13, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['use']\n",
      "29 ['a', 'neural', 'net', 'that', 'takes', 'in', 'the', 'source', 'sentence', 'and', 'a', 'window', 'of', '-LRB-', 'gold', '-RRB-', 'words', 'around', 'the', 'current', 'target', 'word', ',', 'and', 'predicts', 'the', 'current', 'target', 'word']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 3), (5, 2), (6, 3), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "2 ['During', 'testing']\n",
      "1 [',']\n",
      "3 ['the', 'gold', 'words']\n",
      "6 ['are', 'replaced', 'with', 'the', 'generated', 'words']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 10), (5, 5), (6, 5), (7, 5), (8, 5), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "17 ['While', 'this', 'is', 'an', 'interesting', 'area', 'of', 'research', ',', 'I', 'am', 'not', 'convinced', 'by', 'the', 'proposed', 'approach']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "4 ['experimental', 'evidence', 'is', 'lacking']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 5), (5, 3), (6, 5), (7, 9), (8, 8), (9, 10), (10, 3), (11, 6), (12, 5), (13, 5), (14, 4), (15, 5), (16, 5), (17, 5), (18, 5), (19, 4), (20, 6), (21, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "depth:  2\n",
      "4 ['Under', 'the', 'current', 'framework']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "51 ['is', 'all', 'but', 'impossible', 'for', 'the', 'model', 'to', 'do', 'anything', 'more', 'than', 'a', 'rudimentary', 'word', 'replacement', '-LRB-', 'eg', ':', 'it', 'can', 'not', 'change', '\"', 'I', 'went', 'to', 'the', 'fridge', 'even', 'though', 'I', 'was', 'not', 'hungry', '\"', 'to', '\"', 'Although', 'I', 'was', 'not', 'hungry', ',', 'I', 'went', 'to', 'the', 'fridge', '\"', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 5), (6, 2), (7, 2), (8, 2), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['fact']\n",
      "1 ['that']\n",
      "9 ['only', '0.6', 'words', 'are', 'edited', 'on', 'average', 'supports', 'this']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 10), (6, 10), (7, 12), (8, 5), (9, 8), (10, 4), (11, 4), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "2 ['-', 'It']\n",
      "1 ['seems']\n",
      "7 ['strange', '-LRB-', 'to', 'me', 'at', 'least', '-RRB-']\n",
      "24 ['that', 'T', '^', 'i', 'and', 'L', '-LRB-', 'y', '^', '{', '-', 'i', '|', 'k', '}', '-RRB-', 'only', 'look', 'at', 'a', 'window', 'of', '2k', 'words']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 4), (6, 7), (7, 4), (8, 5), (9, 5), (10, 4), (11, 4), (12, 5), (13, 6), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  5\n",
      "11 ['when', 'making', 'the', 'decision', 'to', 'change', 'the', 'i', '-', 'th', 'word']\n",
      "1 [',']\n",
      "2 ['the', 'model']\n",
      "10 ['does', 'not', 'know', 'what', 'was', 'generated', 'outside', 'of', 'the', 'window']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 10), (5, 10), (6, 2), (7, 3), (8, 4), (9, 2), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "12 ['-', 'Relatedly', ',', 'the', 'idea', 'of', 'changing', 'individual', 'words', 'based', 'on', 'local']\n",
      "7 ['-LRB-', 'ie', 'word', '-', 'level', '-RRB-', 'scores']\n",
      "1 ['seems']\n",
      "1 ['counterintuitive']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 6), (5, 3), (6, 4), (7, 3), (8, 3), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "11 ['Given', 'that', 'we', 'have', 'the', 'full', 'generated', 'sentence', ',', 'do', \"n't\"]\n",
      "1 ['we']\n",
      "4 ['want', 'a', 'global', 'score']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 5), (6, 7), (7, 5), (8, 2), (9, 3), (10, 3), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "6 ['Scoring', 'at', 'the', 'sentence', '-', 'level']\n",
      "1 ['could']\n",
      "1 ['also']\n",
      "13 ['make', 'room', 'for', 'non-greedy', 'search', 'strategies', ',', 'which', 'could', 'potentially', 'facilitate', 'richer', 'edits']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 4), (5, 2), (6, 2), (7, 6), (8, 2), (9, 4), (10, 5), (11, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "4 ['-', 'Instead', 'of', 'editing']\n",
      "1 [',']\n",
      "1 ['did']\n",
      "1 ['you']\n",
      "16 ['consider', 'learning', 'an', 'encoder', '-', 'decoder', 'that', 'takes', 'in', 'x', ',', 'y_g', ',', 'and', 'generates', 'y_ref']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 4), (6, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['When']\n",
      "2 ['decoding', 'you']\n",
      "1 ['can']\n",
      "6 ['attend', 'to', 'both', 'x', 'and', 'y_g']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 8), (5, 8), (6, 8), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "2 ['Minor', 'comments']\n",
      "1 [':']\n",
      "6 ['-', 'Iteratively', 'improving', 'a', 'generated', 'text']\n",
      "1 ['was']\n",
      "1 ['also']\n",
      "8 ['explored', 'in', 'https://arxiv.org/pdf/1510.09202v1.pdf', 'from', 'a', 'reinforcement', 'learning', 'angle']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "1 ['I']\n",
      "5 ['do', \"n't\", 'understand', 'footnote', '1']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 2), (6, 6), (7, 6), (8, 7), (9, 11), (10, 7), (11, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['proposes']\n",
      "2 ['a', 'method']\n",
      "28 ['for', 'iteratively', 'improving', 'the', 'output', 'of', 'an', 'existing', 'machine', 'translation', 'by', 'identifying', 'potential', 'mistakes', 'and', 'proposing', 'a', 'substitution', ',', 'in', 'this', 'case', 'using', 'an', 'attention', '-', 'based', 'model']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 2), (7, 5), (8, 7), (9, 6), (10, 3), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  7\n",
      "1 ['the']\n",
      "1 ['method']\n",
      "2 ['in', 'which']\n",
      "5 ['-LRB-', 'it', 'is', 'assumed', '-RRB-']\n",
      "3 ['human', 'translators', 'operate']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['is']\n",
      "3 ['interesting', 'and', 'imaginative']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 8), (4, 10), (5, 10), (6, 7), (7, 11), (8, 12), (9, 9), (10, 8), (11, 5), (12, 9), (13, 4), (14, 9), (15, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "46 ['in', 'general', 'terms', ',', 'I', 'am', 'somewhat', 'sceptical', 'of', 'this', 'kind', 'of', 'approach', '--', 'whereby', 'a', 'machine', 'learning', 'method', 'is', 'used', 'to', 'identify', 'and', 'correct', 'the', 'predictions', 'of', 'another', 'method', ',', 'or', 'itself', '--', 'because', 'in', 'the', 'first', 'case', ',', 'if', 'the', 'new', 'method', 'is', 'better']\n",
      "1 [',']\n",
      "1 ['why']\n",
      "12 ['not', 'use', 'it', 'from', 'the', 'outset', 'in', 'place', 'of', 'the', 'other', 'method']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 6), (5, 8), (6, 4), (7, 7), (8, 11), (9, 16), (10, 10), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['And']\n",
      "15 ['in', 'the', 'second', 'case', ',', 'since', 'the', 'method', 'has', 'no', 'new', 'information', 'compared', 'to', 'previously']\n",
      "1 [',']\n",
      "1 ['why']\n",
      "24 ['is', 'it', 'more', 'likely', 'to', 'identify', 'more', 'past', 'mistakes', 'and', 'correct', 'them', ',', 'than', 'identify', 'past', 'correct', 'terms', 'and', 'turn', 'them', 'into', 'new', 'errors']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 4), (7, 5), (8, 2), (9, 5), (10, 2), (11, 2), (12, 1), (13, 2), (14, 3), (15, 4), (16, 5), (17, 2), (18, 2), (19, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  6\n",
      "1 ['there']\n",
      "1 ['is']\n",
      "3 ['a', 'specific', 'reason']\n",
      "18 ['that', 'an', 'iterative', 'approach', 'can', 'be', 'shown', 'to', 'converge', 'to', 'a', 'better', 'solution', 'when', 'run', 'over', 'several', 'epochs']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 3), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['does']\n",
      "1 ['not']\n",
      "5 ['convince', 'me', 'on', 'these', 'points']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 7), (4, 7), (5, 8), (6, 4), (7, 3), (8, 4), (9, 1), (10, 2), (11, 2), (12, 2), (13, 2), (14, 4), (15, 6), (16, 5), (17, 5), (18, 2), (19, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['oracle']\n",
      "1 ['experiments']\n",
      "1 ['are']\n",
      "1 ['rather']\n",
      "25 ['meaningless', '-', 'they', 'just', 'serve', 'to', 'confirm', 'that', 'improving', 'a', 'translation', 'is', 'very', 'easy', 'when', 'the', 'existing', 'mistakes', 'have', 'been', 'identified', ',', 'but', 'much', 'harder']\n",
      "4 ['when', 'they', 'are', 'not']\n",
      "[(0, 1), (1, 1), (2, 8), (3, 8), (4, 5), (5, 4), (6, 10), (7, 16), (8, 18), (9, 19), (10, 14), (11, 8), (12, 4), (13, 3), (14, 4), (15, 4), (16, 4), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "2 ['Minor', 'comments']\n",
      "1 [':']\n",
      "10 ['I', 'find', 'the', 'notation', 'excessively', 'fiddly', 'at', 'times', '-', 'eg']\n",
      "1 [':']\n",
      "1 ['F']\n",
      "1 ['^']\n",
      "49 ['i', '=', '-LRB-', 'F', '^', '{', 'i', ',1', '}', ',', 'F', '^', '{', 'i', ',', '|', 'F', '^', 'i', '|', '}', '-RRB-', '-', 'why', 'use', '|', 'F', '^', 'i', '|', 'here', 'when', 'F', 'is', 'a', 'matrix', ',', 'so', 'surely', 'the', 'length', 'of', 'the', 'slice', 'is', 'not', 'dependent', 'on', 'i']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 8), (5, 6), (6, 9), (7, 7), (8, 4), (9, 4), (10, 3), (11, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "21 ['In', 'the', 'discussion', 'in', 'section', '4', '-', 'it', 'seems', 'that', 'this', 'still', 'creates', 'a', 'mismatch', 'between', 'the', 'training', 'and', 'test', 'conditions']\n",
      "1 ['-']\n",
      "6 ['could', 'anything', 'be', 'done', 'about', 'this']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 2), (6, 3), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['proposes']\n",
      "2 ['a', 'model']\n",
      "5 ['for', 'iteratively', 'refining', 'translation', 'hypotheses']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 4), (6, 1), (7, 5), (8, 7), (9, 12), (10, 12), (11, 14)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  5\n",
      "1 ['several']\n",
      "1 ['benefits']\n",
      "1 ['including']\n",
      "32 ['enabling', 'the', 'translation', 'model', 'to', 'condition', 'not', 'only', 'on', '“', 'left', 'context', '”', ',', 'but', 'also', 'on', '“', 'right', 'context', '”', ',', 'and', 'potentially', 'enabling', 'more', 'rapid', 'and', '/', 'or', 'accurate', 'decoding']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 7), (6, 7), (7, 8), (8, 2), (9, 3), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "2 ['The', 'motivation']\n",
      "1 ['given']\n",
      "1 ['is']\n",
      "17 ['that', 'often', 'translators', '-LRB-', 'and', 'text', 'generators', 'generally', '-RRB-', 'use', 'a', 'process', 'of', 'refinement', 'in', 'generating', 'outputs']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 7), (5, 5), (6, 5), (7, 2), (8, 4), (9, 3), (10, 2), (11, 3), (12, 2), (13, 4), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "18 ['This', 'is', 'an', 'important', 'idea', 'that', 'is', 'not', 'currently', 'playing', 'much', 'of', 'a', 'role', 'in', 'neural', 'net', 'models']\n",
      "1 [',']\n",
      "1 ['so']\n",
      "6 ['this', 'paper', 'is', 'a', 'welcome', 'contribution']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 6), (4, 4), (5, 5), (6, 3), (7, 6), (8, 8), (9, 8), (10, 7), (11, 3), (12, 2), (13, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "9 ['while', 'I', 'think', 'this', 'is', 'an', 'important', 'first', 'step']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "21 ['do', 'feel', 'that', 'the', 'lack', 'of', 'in', 'depth', 'analysis', 'suggests', 'this', 'paper', 'is', 'not', 'quite', 'ready', 'for', 'a', 'final', 'publication', 'version']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 6), (5, 5), (6, 10), (7, 7), (8, 7), (9, 8), (10, 4), (11, 4), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "2 ['For', 'example']\n",
      "1 [',']\n",
      "1 ['there']\n",
      "28 ['are', 'many', 'possible', 'connections', 'to', 'prior', 'work', 'in', 'NLP', ',', 'MT', ',', 'and', 'other', 'parts', 'of', 'ML', 'that', 'could', 'better', 'contextualize', 'this', 'work', '-LRB-', 'see', 'specifics', 'below', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 8), (5, 4), (6, 2), (7, 2), (8, 8), (9, 5), (10, 6), (11, 5), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "2 ['More', 'substantively']\n",
      "1 [',']\n",
      "5 ['the', 'model', 'in', 'Section', '3']\n",
      "20 ['could', 'be', 'interpreted', 'as', 'a', 'globally', 'normalized', ',', 'undirected', '-LRB-', '~', 'CRF', '-RRB-', 'translation', 'model', 'trained', 'using', 'a', 'pseudo-likelihood', 'objective']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 8), (4, 13), (5, 5), (6, 5), (7, 8), (8, 16), (9, 13), (10, 13), (11, 7), (12, 1), (13, 2), (14, 2), (15, 3), (16, 3), (17, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "3 ['In', 'this', 'analysis']\n",
      "1 [',']\n",
      "18 ['the', 'model', 'squarely', 'back', 'in', 'the', 'context', 'of', 'traditional', 'discriminative', 'translation', 'models', 'which', 'used', '“', 'undirected', '”', 'features']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "34 ['the', 'decoding', 'algorithm', 'then', 'looks', 'more', 'like', 'a', 'standard', 'greedy', 'hill', '-', 'climbing', 'algorithm', '-LRB-', 'albeit', 'with', 'an', 'extra', 'heuristic', 'model', 'for', 'selecting', 'which', 'variable', 'to', 'update', '-RRB-', ',', 'which', 'is', 'also', 'nothing', 'unfamiliar']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 7), (5, 4), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "3 ['My', 'second', 'criticism']\n",
      "5 ['the', 'limitations', 'of', 'the', 'model']\n",
      "1 ['are']\n",
      "1 ['not']\n",
      "1 ['well']\n",
      "1 ['discussed']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 10), (4, 7), (5, 4), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "2 ['For', 'example']\n",
      "1 [',']\n",
      "4 ['the', 'proposed', 'editing', 'procedure']\n",
      "11 ['can', 'not', 'obviously', 'remove', 'or', 'insert', 'a', 'word', 'from', 'a', 'translation']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 5), (5, 5), (6, 4), (7, 7), (8, 4), (9, 5), (10, 1), (11, 2), (12, 2), (13, 2), (14, 2), (15, 2), (16, 4), (17, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "17 ['While', 'I', 'think', 'this', 'is', 'a', 'reasonable', 'assumption', 'than', 'can', 'be', 'made', 'for', 'the', 'sake', 'of', 'tractability']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "10 ['is', 'very', 'unfortunate', 'since', 'missing', 'or', 'extra', 'words', '-LRB-', 'esp']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 5), (6, 2), (7, 5), (8, 2), (9, 2), (10, 2), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "2 ['function', 'words']\n",
      "1 ['-RRB-']\n",
      "12 ['are', 'a', 'common', 'problem', 'in', 'the', 'baseline', 'models', 'that', 'are', 'being', 'used']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 11), (5, 8), (6, 10), (7, 8), (8, 5), (9, 6), (10, 3), (11, 2), (12, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['Second']\n",
      "1 [',']\n",
      "13 ['the', 'standard', 'objections', 'to', 'absolute', 'positional', 'models', '-LRB-', 'vs.', 'relative', 'positional', 'models', '-RRB-']\n",
      "24 ['seem', 'particularly', 'crucial', 'to', 'bring', 'up', 'in', 'this', 'work', ',', 'especially', 'since', 'they', 'might', 'make', 'some', 'of', 'the', 'design', 'decisions', 'a', 'bit', 'more', 'justifiable']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 8), (5, 6), (6, 7), (7, 5), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "11 ['Overall', ',', 'this', 'is', 'an', 'initial', 'step', 'in', 'an', 'interesting', 'direction']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "9 ['it', 'needs', 'more', 'thorough', 'analysis', 'to', 'demonstrate', 'its', 'value']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 7), (4, 16), (5, 4), (6, 4), (7, 6), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "4 ['A', 'more', 'thorough', 'analysis']\n",
      "12 ['will', 'also', 'likely', 'suggest', 'some', 'important', 'model', 'variants', '-LRB-', 'for', 'example', ':']\n",
      "1 ['is']\n",
      "4 ['a', 'global', 'translation', 'model']\n",
      "1 ['really']\n",
      "2 ['the', 'goal']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 4), (4, 2), (5, 5), (6, 2), (7, 4), (8, 4), (9, 2), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['is']\n",
      "12 ['a', 'post-editing', 'model', 'that', 'fixes', 'outputs', 'with', 'more', 'complex', 'operations', 'more', 'ideal']\n",
      "1 ['?']\n",
      "1 ['-RRB-']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 3), (5, 2), (6, 2), (7, 3), (8, 2), (9, 2), (10, 1), (11, 2), (12, 3), (13, 4), (14, 2), (15, 4), (16, 3), (17, 4), (18, 4), (19, 5), (20, 6), (21, 4), (22, 2), (23, 4), (24, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "depth:  2\n",
      "2 ['Related', 'work']\n",
      "1 [':']\n",
      "30 ['I', 'think', 'that', 'more', 'could', 'be', 'done', 'to', 'put', 'this', 'work', 'in', 'the', 'context', 'of', 'what', 'has', 'come', 'before', 'and', 'what', 'is', 'currently', 'going', 'on', 'in', 'other', 'parts', 'of', 'ML']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 5), (6, 5), (7, 4), (8, 6), (9, 4), (10, 9), (11, 11), (12, 10), (13, 9), (14, 8), (15, 19), (16, 6), (17, 10), (18, 11), (19, 15), (20, 13), (21, 9), (22, 11), (23, 15), (24, 5), (25, 2), (26, 4), (27, 3), (28, 4), (29, 6), (30, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "depth:  3\n",
      "2 ['The', 'idea']\n",
      "3 ['of', 'iterative', 'refinement']\n",
      "1 ['has']\n",
      "111 ['been', 'proposed', 'in', 'other', 'problems', 'that', 'have', 'complex', 'output', 'spaces', ',', 'for', 'example', 'the', 'DRAW', 'model', 'of', 'Gregor', 'et', 'al', 'and', 'the', 'conditional', 'adversarial', 'network', 'models', 'used', 'to', 'refine', 'images', 'proposed', 'recently', 'by', 'Isola', 'et', 'al', 'In', 'NLP', ',', 'there', 'have', 'been', 'several', '-LRB-', 'stochastic', '-RRB-', 'hill', 'climbing', 'approaches', 'that', 'have', 'been', 'proposed', ',', 'such', 'as', 'the', 'work', 'on', 'parsing', 'by', 'Zhang', 'and', 'Lei', 'et', 'al', '-LRB-', '2014', '-RRB-', 'who', 'use', 'random', 'initial', 'guesses', 'and', 'then', 'do', 'greedy', 'hill', 'climbing', 'using', 'a', 'series', 'of', 'local', 'refinements', ',', 'the', 'structured', 'prediction', 'cascades', 'of', 'Weiss', 'and', 'Taskar', '-LRB-', '2009', '-RRB-', '-LRB-', 'not', 'to', 'mention', 'general', 'coarse', '-', 'to', '-', 'fine', 'modeling', 'strategies', '-RRB-']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 6), (3, 5), (4, 6), (5, 8), (6, 4), (7, 4), (8, 2), (9, 3), (10, 4), (11, 2), (12, 3), (13, 3), (14, 3), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "1 ['Finally']\n",
      "1 [',']\n",
      "2 ['in', 'MT']\n",
      "1 [',']\n",
      "24 ['Arun', 'et', 'al', '-LRB-', '2009', '-RRB-', 'who', 'use', 'a', 'Gibbs', 'sampler', 'to', 'refine', 'an', 'initial', 'guess', 'to', 'do', 'decoding', 'with', 'a', 'more', 'complex', 'model']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 7), (5, 10), (6, 9), (7, 7), (8, 13), (9, 19), (10, 16), (11, 8), (12, 6), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "15 ['The', 'use', 'of', 'an', 'explicit', 'error', 'model', 'is', 'rather', 'novel', 'in', 'the', 'context', 'of', 'correction']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "42 ['I', 'would', 'point', 'out', 'that', 'although', 'the', 'proposed', 'architecture', 'is', 'different', ',', 'the', 'discriminative', 'word', 'lexicon', 'models', 'of', 'Mauser', 'et', 'al', '-LRB-', '2009', '-RRB-', 'and', 'the', 'neural', 'version', 'of', 'the', 'same', 'by', 'Ha', 'et', 'al', '-LRB-', '2014', '-RRB-', 'are', 'similar', 'in', 'spirit']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 3), (6, 2), (7, 2), (8, 2), (9, 5), (10, 2), (11, 4), (12, 2), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Minimally']\n",
      "3 ['using', 'the', 'techniques']\n",
      "1 ['they']\n",
      "13 ['described', 'could', 'be', 'a', 'useful', 'foil', 'for', 'the', 'models', 'presented', 'in', 'this', 'paper']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 5), (4, 7), (5, 5), (6, 8), (7, 4), (8, 6), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['“']\n",
      "13 ['the', 'target', 'sentence', 'is', 'also', 'embedded', 'in', 'distributional', 'space', 'via', 'a', 'lookup', 'table']\n",
      "1 ['”']\n",
      "1 ['I']\n",
      "9 ['think', '“', 'distributional', 'space', '”', 'is', 'a', 'bit', 'unclear']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 2), (5, 2), (6, 2), (7, 3), (8, 2), (9, 5), (10, 4), (11, 5), (12, 3), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Maybe']\n",
      "1 ['“']\n",
      "3 ['the', 'target', 'sentence']\n",
      "17 ['is', 'represented', 'in', 'terms', 'of', 'distributed', 'word', 'representations', 'via', 'a', 'lookup', 'table', '”', 'or', 'something', 'like', 'that']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 2), (6, 4), (7, 2), (8, 2), (9, 2), (10, 3), (11, 4), (12, 4), (13, 4), (14, 4), (15, 3), (16, 3), (17, 4), (18, 2), (19, 4), (20, 2), (21, 3), (22, 4), (23, 6), (24, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "depth:  3\n",
      "1 ['“']\n",
      "1 ['distributional']\n",
      "1 ['”']\n",
      "1 ['suggests']\n",
      "34 ['that', 'the', 'representations', 'are', 'derived', 'from', 'how', 'the', 'words', 'are', 'distributed', 'in', 'the', 'corpus', ',', 'whereas', 'you', 'are', 'learning', 'these', 'representations', 'on', 'this', 'task', 'which', 'is', 'n’t', 'modeling', 'their', 'distribution', 'except', 'only', 'very', 'indirectly']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 9), (5, 6), (6, 7), (7, 8), (8, 10), (9, 9), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "3 ['Section', '3', 'Model']\n",
      "1 [':']\n",
      "33 ['In', 'Section', '3', ',', 'the', 'model', 'computes', 'the', 'distribution', 'over', 'target', 'word', 'types', 'at', 'an', 'absolute', 'position', 'i', 'in', 'the', 'output', 'sentence', ',', 'given', 'the', 'target', 'language', 'context', 'and', 'the', 'source', 'language', 'context']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 10), (5, 11), (6, 8), (7, 6), (8, 8), (9, 14), (10, 9), (11, 9), (12, 6), (13, 1), (14, 2), (15, 2), (16, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "40 ['It', 'is', 'introduced', 'as', 'the', 'model', 'that', 'is', 'used', 'to', 'refine', 'an', 'existing', 'hypothesis', ',', 'but', 'it', 'is', 'not', 'immediately', 'clear', 'that', 'the', 'training', 'data', 'for', 'this', 'model', '-LRB-', 'at', 'least', 'in', 'this', 'section', '-RRB-', 'are', 'the', 'gold', 'standard', 'translations']\n",
      "1 ['-']\n",
      "11 ['“', 'training', 'set', '”', 'could', 'be', 'interpreted', 'in', 'variety', 'of', 'ways']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 7), (5, 6), (6, 8), (7, 5), (8, 5), (9, 4), (10, 2), (11, 4), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "9 ['This', 'becomes', 'clearer', 'when', 'reading', 'later', 'in', 'the', 'paper']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "15 ['it', '’s', 'a', 'bit', 'less', 'clear', 'when', 'reading', 'from', 'the', 'beginning', 'for', 'the', 'first', 'time']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 5), (5, 4), (6, 10), (7, 6), (8, 8), (9, 14), (10, 12), (11, 8)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "14 ['The', 'use', 'of', 'a', 'fixed', 'sized', 'window', 'for', 'representing', 'the', 'target', 'word', 'in', 'context']\n",
      "1 ['also']\n",
      "30 ['seems', 'to', 'make', 'something', 'like', 'a', 'model', '1', 'assumption', 'since', 'only', 'the', 'lexical', 'features', '-LRB-', 'and', 'not', 'any', '“', 'alignment', '”', 'or', '“', 'positional', '”', 'features', '-RRB-', 'determine', 'the', 'attention']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 4), (6, 6), (7, 9), (8, 10), (9, 11), (10, 17), (11, 12), (12, 10), (13, 11), (14, 5), (15, 9), (16, 7), (17, 11), (18, 9), (19, 6), (20, 7), (21, 10), (22, 5), (23, 1), (24, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "depth:  5\n",
      "1 ['clarified']\n",
      "74 ['since', 'it', 'will', 'make', 'the', 'assumptions', 'of', 'the', 'model', 'more', 'transparent', '-LRB-', 'and', 'also', 'suggest', 'possible', 'refinements', 'to', 'the', 'model', ',', 'e.g.', ',', 'including', '-LRB-', 'representations', '-RRB-', 'of', 'i', 'and', 'j', 'as', 'components', 'of', 'S', '^', 'j', 'and', 'T', '^', 'i', ',', 'which', 'would', 'allow', 'model', '2', '/', '3', '-', 'like', 'responses', 'to', 'be', 'learned', '-', 'although', 'by', 'leaving', 'them', 'out', ',', 'the', 'model', 'might', 'behave', 'a', 'bit', 'more', 'like', 'a', 'relative', 'positional', 'model']\n",
      "10 ['than', 'an', 'absolute', 'positional', 'model', ',', 'which', 'is', 'probably', 'attractive']\n",
      "1 ['-RRB-']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 6), (5, 4), (6, 7), (7, 7), (8, 8), (9, 4), (10, 3), (11, 4), (12, 5), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Finally']\n",
      "1 [',']\n",
      "14 ['some', 'discussion', 'for', 'why', 'a', 'fixed', 'window', 'is', 'used', 'to', 'represent', 'the', 'target', 'sentence']\n",
      "17 ['is', 'worth', 'including', '-LRB-', 'since', 'a', 'global', 'context', 'is', 'apparently', 'used', 'to', 'represent', 'the', 'source', 'sentence', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 14), (5, 8), (6, 4), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "2 ['The', 'relationship']\n",
      "7 ['between', 'this', 'training', 'objective', 'and', 'pseudo', 'likelihood']\n",
      "7 ['-LRB-', 'PL', ';', 'Besag', ',', '1975', '-RRB-']\n",
      "1 ['might']\n",
      "3 ['be', 'worth', 'mentioning']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 6), (5, 9), (6, 7), (7, 6), (8, 4), (9, 3), (10, 5), (11, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "14 ['Since', 'I', 'believe', 'this', 'is', 'just', 'a', 'PL', 'objective', 'for', 'a', 'certain', 'global', 'model']\n",
      "1 [',']\n",
      "1 ['this']\n",
      "15 ['suggests', 'alternative', 'decoding', 'algorithms', ',', 'or', 'certainly', 'a', 'different', 'analysis', 'of', 'the', 'proposed', 'decoding', 'objective']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 2), (6, 5), (7, 2), (8, 4), (9, 6), (10, 9)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['section']\n",
      "3 ['4', 'model', 'conditions']\n",
      "20 ['on', 'the', 'true', 'context', 'of', 'a', 'position', 'in', 'the', 'true', 'target', ',', 'the', 'current', 'target', 'guess', ',', 'and', 'the', 'source']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 9), (5, 11), (6, 9), (7, 10), (8, 8), (9, 7), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "21 ['I', 'do', 'n’t', 'completely', 'understand', 'the', 'rationale', 'for', 'this', 'model', 'since', 'at', 'test', 'time', 'only', 'two', 'of', 'these', 'variables', 'are', 'available']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "10 ['the', 'replacement', 'of', 'y_ref', 'with', 'y_g', 'seems', 'hard', 'to', 'justify']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 2), (6, 4), (7, 2), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['Disclosure']\n",
      "1 [':']\n",
      "9 ['I', 'am', 'not', 'an', 'expert', 'in', 'machine', 'translation', 'algorithms']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 6), (5, 4), (6, 5), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['Summary']\n",
      "1 [':']\n",
      "13 ['A', 'human', 'translator', 'does', 'not', 'come', 'up', 'with', 'the', 'final', 'translation', 'right', 'away']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 4), (5, 5), (6, 8), (7, 2), (8, 2), (9, 2), (10, 1), (11, 2), (12, 2), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Instead']\n",
      "1 [',']\n",
      "8 ['-LRB-', 's', '-RRB-', 'he', 'uses', 'an', 'iterative', 'process']\n",
      "1 [',']\n",
      "11 ['starting', 'with', 'a', 'rough', 'draft', 'which', 'is', 'corrected', 'little', 'by', 'little']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 4), (6, 2), (7, 2), (8, 5), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "2 ['The', 'idea']\n",
      "3 ['behind', 'this', 'paper']\n",
      "1 ['is']\n",
      "9 ['to', 'implement', 'a', 'similar', 'framework', 'for', 'an', 'automated', 'system']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['is']\n",
      "3 ['generally', 'well', 'written']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 3), (6, 4), (7, 5), (8, 3), (9, 2), (10, 2), (11, 3), (12, 5), (13, 2), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['is']\n",
      "2 ['my', 'opinion']\n",
      "17 ['however', 'that', 'drawings', 'illustrating', 'the', 'architectures', 'would', 'help', 'understanding', 'how', 'the', 'different', 'algorithms', 'relate', 'to', 'one', 'another']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 4), (6, 3), (7, 4), (8, 3), (9, 5), (10, 2), (11, 2), (12, 4), (13, 2), (14, 4), (15, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  5\n",
      "1 ['a']\n",
      "1 ['lot']\n",
      "1 ['that']\n",
      "16 ['you', 'report', 'on', 'a', 'preliminary', 'experiment', 'to', 'give', 'an', 'intuition', 'of', 'how', 'difficult', 'the', 'task', 'is']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 4), (7, 2), (8, 4), (9, 1), (10, 2), (11, 3), (12, 4), (13, 8), (14, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  6\n",
      "1 ['the']\n",
      "1 ['links']\n",
      "1 ['between']\n",
      "16 ['the', 'task', 'of', 'finding', 'the', 'errors', 'in', 'a', 'guess', 'translation', 'and', 'the', 'task', 'of', 'iterative', 'refinement']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 3), (5, 2), (6, 2), (7, 5), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['Could']\n",
      "1 ['you']\n",
      "11 ['use', 'post-edited', 'text', 'to', 'have', 'a', 'more', 'solid', 'ground', '-', 'truth']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 7), (5, 5), (6, 7), (7, 4), (8, 2), (9, 2), (10, 2), (11, 2), (12, 5), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "3 ['My', 'main', 'concern']\n",
      "3 ['with', 'this', 'paper']\n",
      "1 ['is']\n",
      "18 ['that', 'in', 'the', 'experimental', 'section', 'the', 'iterative', 'approach', 'tries', 'to', 'improve', 'upon', 'only', 'one', 'type', 'of', 'machine', 'translation']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 7), (5, 9), (6, 6), (7, 2), (8, 2), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "5 ['Which', 'immediately', 'prompts', 'these', 'questions']\n",
      "1 [':']\n",
      "1 ['-']\n",
      "9 ['why', 'did', 'they', 'choose', 'that', 'approach', 'to', 'improve', 'on']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 4), (6, 2), (7, 4), (8, 2), (9, 3), (10, 5), (11, 5), (12, 8), (13, 6), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "1 ['what']\n",
      "24 ['is', 'the', 'part', 'of', 'the', 'improvement', 'that', 'comes', 'from', 'the', 'choice', 'of', 'the', 'initial', 'draft', '-LRB-', 'maybe', 'it', 'was', 'a', 'very', 'bad', 'draft', '-RRB-']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 8), (5, 11), (6, 4), (7, 4), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "2 ['Experimental', 'results']\n",
      "5 ['on', 'two', 'popular', 'benchmark', 'datasets']\n",
      "1 ['validate']\n",
      "4 ['the', 'advantage', 'of', 'DMPN']\n",
      "10 ['over', 'other', 'state', '-', 'of', '-', 'the', '-', 'art', 'methods']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 2), (5, 2), (6, 2), (7, 2), (8, 8), (9, 5), (10, 4), (11, 10), (12, 4), (13, 5), (14, 2), (15, 6), (16, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "1 ['I']\n",
      "34 ['am', 'concerned', 'about', 'whether', 'the', 'proposed', 'method', 'works', 'well', 'with', 'harder', 'datasets', 'such', 'as', 'Office', '-', 'Home', 'dataset', ',', 'because', 'each', 'class', 'data', 'are', 'modeled', 'by', 'a', 'simple', 'Gaussian', 'distribution', 'in', 'the', 'proposed', 'method']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 7), (5, 5), (6, 5), (7, 6), (8, 3), (9, 1), (10, 2), (11, 2), (12, 6), (13, 2), (14, 8), (15, 7), (16, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "2 ['The', 'paper']\n",
      "34 ['is', 'a', 'bit', 'hard', 'to', 'follow', ',', 'and', 'would', 'be', 'improved', 'by', 'giving', 'a', 'more', 'explicit', 'comparison', 'of', 'the', 'methods', 'used', 'here', 'to', 'past', 'work', ',', 'especially', '[', '1', ']', 'and', '[', '3', ']']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 5), (5, 2), (6, 2), (7, 2), (8, 7), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['proposed']\n",
      "1 ['Distribution']\n",
      "10 ['Matching', 'Prototypical', 'Network', '-LRB-', 'DMPN', '-RRB-', 'for', 'unsupervised', 'domain', 'adaptation']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 8), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['DMPN']\n",
      "1 ['extracts']\n",
      "1 ['features']\n",
      "6 ['from', 'the', 'input', 'data', 'and', 'models']\n",
      "5 ['them', 'as', 'Gaussian', 'mixture', 'distributions']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 10), (5, 11), (6, 6), (7, 8), (8, 5), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "9 ['By', 'explicitly', 'modeling', 'the', 'distributions', 'that', 'the', 'features', 'follow']\n",
      "1 [',']\n",
      "13 ['the', 'discrepancy', 'between', 'the', 'distribution', 'of', 'source', 'data', 'and', 'that', 'of', 'target', 'data']\n",
      "4 ['can', 'be', 'easily', 'evaluated']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 2), (7, 3), (8, 2), (9, 4), (10, 3), (11, 3), (12, 2), (13, 2), (14, 2), (15, 4), (16, 2), (17, 6), (18, 5), (19, 5), (20, 2), (21, 2), (22, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "depth:  9\n",
      "1 ['two']\n",
      "1 ['kinds']\n",
      "1 ['of']\n",
      "21 ['loss', ',', 'which', 'are', 'classification', 'loss', 'on', 'the', 'source', 'data', 'and', 'domain', 'discrepancy', 'loss', 'that', 'is', 'calculated', 'via', 'the', 'explicit', 'models']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 9), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "2 ['<Review', 'summary>']\n",
      "3 ['The', 'proposed', 'method']\n",
      "2 ['seems', 'simple']\n",
      "1 ['but']\n",
      "3 ['empirically', 'performs', 'well']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 9), (5, 7), (6, 5), (7, 1), (8, 2), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "9 ['The', 'paper', 'is', 'well', 'written', 'and', 'easy', 'to', 'follow']\n",
      "1 [',']\n",
      "1 ['so']\n",
      "6 ['we', 'can', 'maybe', 'easily', 'implement', 'it']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 5), (5, 2), (6, 6), (7, 3), (8, 5), (9, 2), (10, 2), (11, 2), (12, 4), (13, 4), (14, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "25 ['have', 'several', 'concerns', 'mainly', 'about', 'the', 'details', 'and', 'theories', 'of', 'the', 'proposed', 'method', ',', 'which', 'makes', 'my', 'score', 'a', 'bit', 'lower', 'than', 'the', 'border', 'line']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 4), (5, 5), (6, 4), (7, 2), (8, 2), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "6 ['Given', 'clarifications', 'in', 'an', 'author', 'response']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "7 ['would', 'be', 'willing', 'to', 'increase', 'the', 'score']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 6), (6, 4), (7, 2), (8, 2), (9, 3), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['<Details>']\n",
      "1 ['*']\n",
      "12 ['Strength', '+', 'The', 'motivation', 'of', 'using', 'ProtoNet', 'for', 'domain', 'adaptation', 'seems', 'reasonable']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 2)]\n",
      "[0, 1, 2]\n",
      "Depth to split: None\n",
      "depth:  None\n",
      "[(0, 1), (1, 1), (2, 2)]\n",
      "[0, 1, 2]\n",
      "Depth to split: None\n",
      "depth:  None\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 7), (5, 9), (6, 3), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['*']\n",
      "15 ['Weakness', 'and', 'concerns', '-', 'Several', 'points', 'on', 'the', 'proposed', 'loss', '-LRB-', 'GCMM', 'and', 'PDM', '-RRB-']\n",
      "4 ['are', 'not', 'sufficiently', 'discussed']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 3), (5, 2), (6, 4), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['--']\n",
      "1 ['Why']\n",
      "7 ['do', 'we', 'need', 'two', 'kinds', 'of', 'loss']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 1), (5, 2), (6, 2), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['These']\n",
      "1 ['losses']\n",
      "1 ['seem']\n",
      "5 ['to', 'play', 'almost', 'same', 'role']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 6), (5, 7), (6, 10), (7, 4), (8, 2), (9, 1), (10, 2), (11, 3), (12, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "18 ['Since', 'PDM', 'loss', 'corresponds', 'to', 'target', '-', 'side', 'log', 'likelihood', 'regularization', 'term', '-LRB-', 'Equation', '-LRB-', '3', '-RRB-', '-RRB-']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "7 ['wonder', 'if', 'we', 'really', 'need', 'GCMM', 'loss']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 5), (4, 5), (5, 8), (6, 6), (7, 6), (8, 6), (9, 4), (10, 6), (11, 4), (12, 5), (13, 7), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['--']\n",
      "14 ['Since', 'the', 'authors', 'explicitly', 'model', 'the', 'feature', 'distributions', 'by', 'Gaussian', 'mixtures', '-LRB-', 'GMs', '-RRB-']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "20 ['might', 'be', 'possible', 'to', 'calculate', 'a', 'standard', 'divergence', 'between', 'source', 'and', 'target', 'data', 'distributions', 'by', 'using', 'the', 'parameters', 'of', 'GMs']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 8), (5, 7), (6, 2), (7, 2), (8, 1), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "6 ['Compared', 'with', 'such', 'a', 'straightforward', 'approach']\n",
      "1 [',']\n",
      "3 ['the', 'proposed', 'method']\n",
      "11 ['seems', 'to', 'be', 'ad', '-', 'hoc', 'and', 'is', 'not', 'theoretically', 'validated']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 6), (5, 2), (6, 4), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "2 ['What', 'term']\n",
      "6 ['of', 'divergence', '-LRB-', 'or', 'distance', '-RRB-']\n",
      "1 ['does']\n",
      "1 ['it']\n",
      "1 ['minimize']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 6), (5, 4), (6, 4), (7, 7), (8, 3), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['--']\n",
      "1 ['When']\n",
      "17 ['a', 'certain', 'class', 'does', 'not', 'appear', 'in', 'pseudo-labeled', 'target', 'data', ',', 'how', 'can', 'we', 'calculate', 'GCMM', 'loss']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 6), (4, 7), (5, 8), (6, 5), (7, 6), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['-LRB-']\n",
      "1 ['specifically']\n",
      "1 [',']\n",
      "9 ['\\\\', 'mu', '^', '{', 'et', '}', '_', 'c', '-RRB-']\n",
      "1 ['--']\n",
      "11 ['Are', 'Equation', '-LRB-', '3', '-RRB-', 'and', 'Equation', '-LRB-', '6', '-RRB-', 'correct']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 4), (6, 6), (7, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  5\n",
      "1 ['as']\n",
      "6 ['total', 'loss', ',', 'not', 'average', ',']\n",
      "1 ['over']\n",
      "2 ['each', 'domain']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 8), (5, 9), (6, 6), (7, 5), (8, 4), (9, 5), (10, 6), (11, 6), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "19 ['It', 'means', 'that', 'the', 'scale', 'of', 'the', 'coefficients', 'for', 'these', 'terms', 'changes', 'according', 'to', 'the', 'number', 'of', 'training', 'data']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "11 ['the', 'sensitivity', 'analysis', 'in', 'Figure', '2', 'does', 'not', 'show', 'such', 'effect']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 5), (4, 6), (5, 10), (6, 6), (7, 8), (8, 7), (9, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['--']\n",
      "14 ['Since', 'the', 'proposed', 'losses', 'heavily', 'depend', 'on', 'the', 'pseudo', 'labels', 'on', 'the', 'target', 'data']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "12 ['should', 'be', 'important', 'to', 'carefully', 'set', 'a', 'proper', 'threshold', 'for', 'the', 'confidence']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 2), (6, 4), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['Is']\n",
      "3 ['the', 'proposed', 'method']\n",
      "7 ['sensitive', 'against', 'the', 'change', 'of', 'this', 'threshold']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 4), (5, 1)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "2 ['If', 'so']\n",
      "1 [',']\n",
      "1 ['how']\n",
      "4 ['can', 'we', 'tune', 'it']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 3), (5, 2), (6, 6), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['--']\n",
      "1 ['How']\n",
      "9 ['can', 'we', 'know', 'p', '-LRB-', 'c', '-RRB-', 'in', 'advance']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 4), (5, 2), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "2 ['The', 'theory']\n",
      "3 ['shown', 'in', '3.5']\n",
      "4 ['is', 'not', 'sufficiently', 'validated']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 6), (5, 4), (6, 10), (7, 9), (8, 3), (9, 7), (10, 9), (11, 3), (12, 5), (13, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['--']\n",
      "3 ['The', 'authors', 'state']\n",
      "39 ['``', '``', 'we', 'minimize', 'the', 'first', 'term', 'through', 'minimizing', 'the', 'domain', 'discrepancy', 'losses', ',', '\"', 'but', 'it', 'is', 'not', 'sufficiently', 'supported', ',', 'because', 'the', 'relationship', 'between', 'the', 'proposed', 'losses', 'and', 'H', '-', 'delta', '-', 'H', 'divergence', 'is', 'not', 'clear']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 3), (5, 3), (6, 2), (7, 2), (8, 4), (9, 2), (10, 5), (11, 4), (12, 10), (13, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['*']\n",
      "2 ['Minor', 'concerns']\n",
      "25 ['that', 'do', 'not', 'have', 'an', 'impact', 'on', 'the', 'score', '-', 'Using', 'both', 'f', '^', 's_i', 'and', 'F', '-LRB-', 'x', '^', 's_i', ';', '\\\\', 'theta', '-RRB-']\n",
      "2 ['is', 'confusing']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 7), (5, 3), (6, 3), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "2 ['-', 'Typo']\n",
      "5 ['in', 'Equation', '-LRB-', '7', '-RRB-']\n",
      "1 ['PMD']\n",
      "1 ['-']\n",
      "2 ['>', 'PDM']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 1), (6, 3), (7, 4), (8, 5), (9, 6), (10, 5), (11, 4), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['develops']\n",
      "3 ['a', 'new', 'method']\n",
      "18 ['for', 'adapting', 'models', 'trained', 'on', 'labeled', 'data', 'from', 'some', 'source', 'domain', 'to', 'unlabeled', 'data', 'in', 'a', 'target', 'domain']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 11), (5, 8), (6, 5), (7, 8), (8, 12), (9, 2), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "12 ['The', 'authors', 'accomplish', 'this', 'by', 'adapting', 'a', 'technique', 'from', '[', '1', ']']\n",
      "1 ['and']\n",
      "17 ['[', '2', ']', 'enforcing', 'that', 'the', 'deep', 'features', 'learned', 'during', 'training', 'approximately', 'follow', 'a', 'Gaussian', 'mixture', 'distribution']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 6), (5, 5), (6, 5), (7, 4), (8, 4), (9, 2), (10, 4), (11, 2), (12, 3), (13, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "7 ['With', 'the', 'learned', 'features', 'in', 'this', 'form']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "17 ['ensure', 'domain', 'adaptation', 'by', 'minimizing', 'the', 'discrepancy', 'between', 'the', 'distributions', 'arising', 'from', 'the', 'source', 'and', 'target', 'datasets']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 8), (5, 6), (6, 6), (7, 4), (8, 9), (9, 7), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['Strengths']\n",
      "1 [':']\n",
      "1 ['+']\n",
      "4 ['The', 'paper', \"'s\", 'experiments']\n",
      "20 ['show', 'an', 'improvement', 'in', 'the', 'model', \"'s\", 'performance', 'relative', 'to', 'past', 'work', ',', 'utilizing', 'a', 'large', 'number', 'of', 'comparison', 'models']\n",
      "[(0, 1), (1, 1), (2, 2)]\n",
      "[0, 1, 2]\n",
      "Depth to split: None\n",
      "depth:  None\n",
      "[(0, 1), (1, 1), (2, 6), (3, 8), (4, 10), (5, 11), (6, 10), (7, 5), (8, 5), (9, 2), (10, 6), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['Weaknesses']\n",
      "1 [':']\n",
      "1 ['-']\n",
      "14 ['The', 'proposed', 'method', 'for', 'unsupervised', 'domain', 'adaptation', 'is', 'very', 'similar', 'to', 'the', 'prototypical', 'networks']\n",
      "21 ['approach', 'in', '[', '3', ']', ',', 'with', 'the', 'primary', 'difference', 'being', 'a', 'loss', 'term', 'incentivizing', 'a', 'Gaussian', 'mixture', 'distribution', 'over', 'features']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 8), (4, 11), (5, 10), (6, 13), (7, 8)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "10 ['While', 'the', 'authors', 'achieve', 'improved', 'performance', 'over', '[', '3', ']']\n",
      "1 [',']\n",
      "9 ['the', 'gains', 'in', 'classification', 'accuracy', 'on', 'the', 'target', 'dataset']\n",
      "11 ['are', \"n't\", 'especially', 'huge', '-LRB-', '~', '1', '-', '3', '%', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 5), (4, 7), (5, 5)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['[']\n",
      "1 ['1']\n",
      "1 [']']\n",
      "2 ['Weitao', 'Wan']\n",
      "1 [',']\n",
      "9 ['Yuanyi', 'Zhong', ',', 'Tianpeng', 'Li', ',', 'and', 'Jiansheng', 'Chen']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 4), (5, 2), (6, 4), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['feature']\n",
      "1 ['distribution']\n",
      "1 ['for']\n",
      "5 ['loss', 'functions', 'in', 'image', 'classification']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 6), (5, 3), (6, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['2018']\n",
      "10 ['IEEE', '/', 'CVF', 'Conference', 'on', 'Computer', 'Vision', 'and', 'Pattern', 'Recognition']\n",
      "1 [',']\n",
      "1 ['pp']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "1 ['9117']\n",
      "1 ['–']\n",
      "3 ['9126', ',', '2018']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 11), (4, 12)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['[']\n",
      "1 ['2']\n",
      "1 [']']\n",
      "4 ['Hong', '-', 'Ming', 'Yang']\n",
      "1 [',']\n",
      "4 ['Xu', '-', 'Yao', 'Zhang']\n",
      "1 [',']\n",
      "2 ['Fangying', 'Yin']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "2 ['Chenglin', 'Liu']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['Robust']\n",
      "1 ['classification']\n",
      "1 ['with']\n",
      "3 ['convolutional', 'prototype', 'learning']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 6), (5, 3), (6, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['2018']\n",
      "10 ['IEEE', '/', 'CVF', 'Conference', 'on', 'Computer', 'Vision', 'and', 'Pattern', 'Recognition']\n",
      "1 [',']\n",
      "1 ['pp']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "1 ['3474']\n",
      "1 ['–']\n",
      "3 ['3482', ',', '2018']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 5), (4, 11), (5, 11)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['[']\n",
      "1 ['3']\n",
      "1 [']']\n",
      "2 ['Yingwei', 'Pan']\n",
      "1 [',']\n",
      "17 ['Ting', 'Yao', ',', 'Yehao', 'Li', ',', 'Yu', 'Wang', ',', 'Chong', '-', 'Wah', 'Ngo', ',', 'and', 'Tao', 'Mei']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['Transferrable']\n",
      "1 ['prototypical']\n",
      "1 ['networks']\n",
      "1 ['for']\n",
      "3 ['unsupervised', 'domain', 'adaptation']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 2), (5, 3), (6, 2), (7, 5), (8, 3), (9, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "12 ['In', 'Proceedings', 'of', 'the', 'IEEE', 'Conference', 'on', 'Computer', 'Vision', 'and', 'Pattern', 'Recognition']\n",
      "1 [',']\n",
      "1 ['pp']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "1 ['2239']\n",
      "1 ['–']\n",
      "3 ['2247', ',', '2019']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 4), (6, 2), (7, 7), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['introduces']\n",
      "11 ['Distribution', 'Matching', 'Prototypical', 'Network', '-LRB-', 'DMPN', '-RRB-', 'for', 'Unsupervised', 'Domain', 'Adaptation']\n",
      "3 ['-LRB-', 'UDA', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 8), (4, 6), (5, 6), (6, 2), (7, 6), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "3 ['The', 'proposed', 'method']\n",
      "1 ['explicitly']\n",
      "15 ['models', 'the', 'feature', 'distribution', 'as', 'a', 'Gaussian', 'mixture', 'model', 'in', 'both', 'source', 'and', 'target', 'domains']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 7), (5, 7), (6, 5), (7, 3), (8, 3), (9, 2), (10, 2), (11, 2), (12, 2), (13, 5), (14, 3), (15, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "1 ['Then']\n",
      "2 ['the', 'method']\n",
      "29 ['aligns', 'the', 'target', 'distribution', 'with', 'the', 'source', 'distribution', 'by', 'minimizing', 'losses', ',', 'which', 'are', 'called', 'Gaussian', 'Component', 'Mean', 'Matching', '-LRB-', 'GCMM', '-RRB-', 'and', 'Pseudo', 'Distribution', 'Matching', '-LRB-', 'PDM', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 12), (5, 9), (6, 5), (7, 2), (8, 2), (9, 4), (10, 6), (11, 4), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "17 ['This', 'paper', 'should', 'be', 'rejected', 'because', '-LRB-', '1', '-RRB-', 'the', 'novelty', 'of', 'the', 'main', 'idea', 'is', 'marginal']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "13 ['-LRB-', '2', '-RRB-', 'the', 'performance', 'gain', 'over', 'the', 'baseline', 'methods', 'is', 'also', 'marginal']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 2), (5, 4), (6, 1), (7, 3), (8, 4), (9, 6), (10, 6), (11, 2), (12, 4), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "3 ['Pan', 'et', 'al']\n",
      "1 ['already']\n",
      "19 ['proposed', 'the', 'idea', 'of', 'transferring', 'the', 'knowledge', 'from', 'the', 'source', 'to', 'the', 'target', 'using', 'the', 'prototype', 'of', 'each', 'class']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 1), (6, 2), (7, 2), (8, 2), (9, 3), (10, 5), (11, 4), (12, 7), (13, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  10\n",
      "1 ['explicit']\n",
      "1 ['modeling']\n",
      "1 ['performs']\n",
      "6 ['better', 'than', 'implicit', 'modeling', 'of', 'prototypes']\n",
      "4 ['by', 'theory', 'or', 'practice']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 12), (4, 8), (5, 10), (6, 8), (7, 8), (8, 7), (9, 6), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "3 ['In', 'table', '2']\n",
      "21 [',', 'the', 'proposed', 'method', 'seems', 'better', 'than', 'TPN', ',', 'but', 'in', 'the', 'appendix', ',', 'by', 'comparing', 'then', 'in', 'each', 'category', ',']\n",
      "3 ['the', 'proposed', 'method']\n",
      "10 ['wins', 'six', 'categories', ',', 'whereas', 'TPN', 'also', 'wins', 'six', 'categories']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 2), (5, 1), (6, 2), (7, 2), (8, 1), (9, 2), (10, 5), (11, 2), (12, 4), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['Therefore']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "13 ['is', 'hard', 'to', 'say', 'the', 'proposed', 'DMPN', 'is', 'more', 'effective', 'than', 'another', 'method']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 1), (6, 2), (7, 3), (8, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['Each']\n",
      "1 ['prototype']\n",
      "1 ['is']\n",
      "8 ['modeled', 'using', 'a', 'mean', 'and', 'a', 'covariance', 'matrix']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 5), (5, 2), (6, 2), (7, 6), (8, 3), (9, 4), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['the']\n",
      "1 ['authors']\n",
      "1 ['do']\n",
      "1 [\"n't\"]\n",
      "12 ['use', 'the', 'estimated', 'covariance', 'matrix', 'to', 'measure', 'the', 'distance', 'in', 'eq', '.5']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 4), (5, 7), (6, 6), (7, 5), (8, 5), (9, 1), (10, 2), (11, 3), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "10 ['Because', 'the', 'proposed', 'method', 'uses', 'pseudo-labeling', 'for', 'the', 'target', 'domain']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "10 ['seems', 'that', 'the', 'weights', 'to', 'determine', 'unreliable', 'examples', 'are', 'crucial']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 3), (6, 6), (7, 3), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['should']\n",
      "9 ['show', 'the', 'sensitivity', 'of', 'ways', 'to', 'determine', 'the', 'weights']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 2), (5, 2), (6, 2), (7, 4), (8, 5), (9, 5), (10, 2), (11, 7), (12, 6), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  7\n",
      "1 ['values']\n",
      "4 ['of', '0.1', 'and', '0.9']\n",
      "1 ['are']\n",
      "12 ['changed', 'in', '-LRB-', 'pi', '-', '0.1', '-RRB-', '/', '0.9', 'on', 'page', '6']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 4), (4, 10), (5, 12), (6, 6), (7, 8), (8, 2), (9, 6), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "11 ['The', 'effectiveness', 'of', 'the', 'contributions', 'is', 'validated', 'on', 'multiple', 'UDA', 'tasks']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "19 ['the', 'ablative', 'analysis', 'supports', 'the', 'claims', '-LRB-', 'that', 'prototype', '-', 'level', 'alignment', 'and', 'within', '-', 'class', 'compactness', 'helps', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 9), (5, 4), (6, 6), (7, 5), (8, 5), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "13 ['Recently', 'there', 'is', 'a', 'paper', '[', '2', ']', 'about', 'model', 'selection', 'for', 'UDA']\n",
      "1 [',']\n",
      "6 ['maybe', 'the', 'authors', 'should', 'try', 'it']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 5), (4, 11), (5, 14), (6, 15), (7, 14), (8, 7), (9, 10), (10, 11), (11, 5), (12, 4), (13, 2), (14, 3), (15, 2), (16, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  3\n",
      "25 ['Summary', ':', '-', 'key', 'problem', ':', 'address', '\"', 'class', 'mismatch', '\"', 'in', 'adversarial', 'learning', 'methods', 'for', 'unsupervised', 'domain', 'adaptation', '-LRB-', 'UDA', '-RRB-', ';', '-', 'contributions']\n",
      "1 [':']\n",
      "6 ['1', '-RRB-', 'extension', 'of', 'the', 'domain']\n",
      "3 ['adversarial', 'learning', 'objective']\n",
      "25 ['to', 'leverage', 'class', 'prototypes', '-LRB-', 'exponential', 'moving', 'average', 'of', 'features', 'weighted', 'by', 'predicted', 'class', 'probabilities', '-RRB-', 'in', 'addition', 'to', 'pseudo-labels', 'and', 'intermediate', 'representations', '-LRB-', 'cf.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 9), (5, 11), (6, 14), (7, 7), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "4 ['eqs.5', '-', '11', '-RRB-']\n",
      "1 [',']\n",
      "27 ['2', '-RRB-', 'state', '-', 'of', '-', 'the', '-', 'art', 'results', 'on', 'several', 'UDA', 'tasks', '-LRB-', 'Office', '-', 'Home', ',', 'ImageCLEF', '-', 'DA', ',', 'sim2real', 'on', 'Cityscapes', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 2), (5, 2), (6, 4), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['Recommendation']\n",
      "1 [':']\n",
      "8 ['weak', 'accept', '-LRB-', 'with', 'some', 'reservations', 'below', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 4), (5, 5), (6, 3), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "2 ['Key', 'reason']\n",
      "1 [':']\n",
      "8 ['interesting', 'and', 'effective', 'use', 'of', 'prototypes', 'for', 'UDA']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 12), (5, 10), (6, 7), (7, 3), (8, 2), (9, 1), (10, 2), (11, 2), (12, 3), (13, 6), (14, 8), (15, 19), (16, 19), (17, 21), (18, 16), (19, 9), (20, 4), (21, 2), (22, 2), (23, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "8 ['The', 'formulation', 'of', 'the', 'prototypes', 'and', 'additional', 'learning']\n",
      "85 ['objectives', 'for', 'UDA', 'are', 'clear', 'and', 'seem', 'novel', ',', 'although', 'I', 'would', 'like', 'to', 'see', 'a', 'discussion', 'of', 'additional', 'related', 'works', ':', '--', '\"', 'Mean', 'teachers', 'are', 'better', 'role', 'models', ':', 'Weight', '-', 'averaged', 'consistency', 'targets', 'improve', 'semi-supervised', 'deep', 'learning', 'results', '\"', ',', 'Tarvainen', 'and', 'Valpola', ',', 'NeurIPS', \"'\", '17', ';', '--', '\"', 'Unsupervised', 'Domain', 'Adaptation', 'with', 'Similarity', 'Learning', '\"', ',', 'Pinheiro', ',', 'CVPR', \"'\", '18', ';', '--', '\"', 'Transferable', 'Prototypical', 'Networks', 'for', 'Unsupervised', 'Domain', 'Adaptation', '\"', ',', 'Pan', 'et', 'al', ',', 'CVPR', \"'\", '19']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 7), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "2 ['Main', 'reservation']\n",
      "1 [':']\n",
      "7 ['the', 'specific', 'problem', 'is', 'not', 'clearly', 'formalized']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 11), (6, 5), (7, 4), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  5\n",
      "1 ['-']\n",
      "1 ['What']\n",
      "1 ['the']\n",
      "6 ['often', 'mentioned', 'but', 'not', 'clearly', 'described']\n",
      "1 ['\"']\n",
      "1 ['class']\n",
      "1 ['mismatch']\n",
      "1 ['\"']\n",
      "1 ['problem']\n",
      "1 ['in']\n",
      "1 ['UDA']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 7), (4, 10), (5, 8), (6, 5), (7, 5), (8, 6), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "6 ['To', 'the', 'best', 'of', 'my', 'knowledge']\n",
      "1 [',']\n",
      "5 ['this', 'not', 'a', 'standard', 'problem']\n",
      "1 ['-LRB-']\n",
      "17 ['could', 'not', 'find', 'any', 'mention', 'in', 'the', 'previous', 'literature', ',', 'no', 'citations', 'or', 'definitions', 'in', 'the', 'submission']\n",
      "1 ['-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 2), (5, 8), (6, 8), (7, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['Is']\n",
      "1 ['it']\n",
      "18 ['that', 'the', 'target', 'label', 'space', 'is', 'different', 'than', 'the', 'source', 'label', 'space', '-LRB-', 'e.g.', ',', 'different', 'ontologies', '-RRB-']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 4), (5, 4), (6, 2), (7, 6), (8, 2), (9, 3), (10, 4), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "3 ['In', 'this', 'case']\n",
      "1 [',']\n",
      "1 ['what']\n",
      "16 ['is', 'the', 'information', 'on', 'the', 'target', 'label', 'space', 'that', 'enables', 'unsupervised', 'adaptation', 'from', 'the', 'source', 'one']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 3), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['What']\n",
      "1 ['is']\n",
      "7 ['the', 'inductive', 'bias', '/', 'prior', '/', 'assumptions']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 3), (4, 2), (5, 5), (6, 5), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "1 ['Alternatively']\n",
      "1 [',']\n",
      "10 ['is', 'the', 'tackled', 'problem', 'only', 'the', 'noise', 'in', 'the', 'pseudo-labels']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 13), (5, 12), (6, 7), (7, 12), (8, 5), (9, 2), (10, 3), (11, 2), (12, 4), (13, 1), (14, 3), (15, 4), (16, 2), (17, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "14 ['In', 'any', 'case', ',', 'the', 'submission', 'would', 'greatly', 'benefit', 'from', 'a', 'clearer', 'mathematical', 'formalism']\n",
      "1 ['and']\n",
      "29 ['experimental', 'characterization', 'of', 'the', 'specific', 'problem', 'tackled', 'here', ',', 'especially', 'in', 'light', 'of', 'claims', 'like', '\"', 'conditioning', 'the', 'alignment', 'on', 'pseudo', 'labels', 'can', 'not', 'well', 'address', 'the', 'mismatch', 'problem']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 7), (5, 5), (6, 3), (7, 3), (8, 1), (9, 2), (10, 2), (11, 4), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "5 ['Compared', 'with', 'the', 'pseudo', 'labels']\n",
      "1 [',']\n",
      "3 ['the', 'class', 'prototypes']\n",
      "15 ['are', 'more', 'robust', 'and', 'reliable', 'in', 'terms', 'of', 'representing', 'the', 'distribution', 'of', 'different', 'semantic', 'classes']\n",
      "1 ['.']\n",
      "2020_Byg79h4tvB 1253\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 7), (5, 6), (6, 8), (7, 11), (8, 7), (9, 5), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "2 ['The', 'clarifications']\n",
      "8 ['with', 'respect', 'to', 'related', 'works', 'and', 'missing', 'references']\n",
      "1 ['is']\n",
      "1 ['helpful']\n",
      "1 [',']\n",
      "19 ['although', 'a', 'bit', 'high', '-', 'level', '-LRB-', 'ie', 'not', 'necessarily', 'describing', 'the', 'relative', 'advantages', 'of', 'the', 'proposed', 'method', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 10), (5, 4), (6, 2), (7, 2), (8, 3), (9, 2), (10, 6), (11, 3), (12, 7), (13, 7), (14, 4), (15, 6), (16, 10), (17, 4), (18, 7), (19, 5), (20, 8)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "depth:  2\n",
      "1 ['Nonetheless']\n",
      "1 [',']\n",
      "5 ['the', 'expected', 'benefits', 'of', 'prototypes']\n",
      "49 ['is', 'still', 'not', 'entirely', 'clear', 'enough', 'here', ',', 'for', 'instance', 'regarding', 'the', 'main', 'statistical', 'assumptions', 'that', 'the', 'method', 'needs', 'to', 'make', 'to', 'get', 'robust', 'prototypes', '-LRB-', 'e.g.', ',', 'in', 'the', 'presence', 'of', 'outliers', 'or', 'specific', 'forms', 'of', '\"', 'inaccuracies', '\"', 'in', 'the', 'pseudo-labels', 'or', '\"', 'domain', 'misalignment', '\"', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 7), (4, 9), (5, 4), (6, 6), (7, 8), (8, 4), (9, 7), (10, 7), (11, 2), (12, 3), (13, 3), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "1 ['Therefore']\n",
      "1 [',']\n",
      "18 ['due', 'to', 'the', 'overall', 'lack', 'of', 'mathematical', 'clarity', 'in', 'the', 'text', 'and', 'rebuttal', ',', 'my', 'main', 'reservation', 'remains']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "14 ['I', 'will', 'change', 'my', '\"', 'weak', 'accept', '/', 'borderline', '\"', 'score', 'to', 'weak', 'reject']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 3), (6, 2), (7, 7), (8, 8), (9, 7), (10, 6), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['encourage']\n",
      "2 ['the', 'authors']\n",
      "20 ['to', 'formalize', 'the', 'problem', 'in', 'a', 'clearer', ',', 'non-ambiguous', 'way', ',', 'discussing', 'more', 'explicitly', 'the', 'limitations', 'of', 'the', 'proposed', 'method']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 4), (6, 3), (7, 5), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['proposes']\n",
      "3 ['to', 'leverage', 'prototypes']\n",
      "9 ['to', 'solve', 'the', 'mismatch', 'problem', 'in', 'unsupervised', 'domain', 'adaptation']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 3), (5, 2), (6, 2), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['It']\n",
      "1 ['further']\n",
      "7 ['imposes', 'intra-class', 'compactness', 'to', 'help', 'ambiguous', 'classes']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 3), (7, 2), (8, 5), (9, 5), (10, 4), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  8\n",
      "1 ['new']\n",
      "7 ['state', '-', 'of', '-', 'the', '-', 'art']\n",
      "1 ['results']\n",
      "1 ['in']\n",
      "2 ['several', 'datasets']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 3), (5, 3), (6, 2), (7, 5), (8, 4), (9, 2), (10, 4), (11, 2), (12, 2), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "1 ['pros']\n",
      "1 [':']\n",
      "16 ['+', 'intra-class', 'compactness', 'to', 'help', 'ambiguous', 'classes', 'concerns', ':', '--', 'Prototypes', 'does', 'not', 'come', 'from', 'nowhere']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 1)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['They']\n",
      "1 ['come']\n",
      "2 ['from', 'predictions']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 8), (4, 7), (5, 4), (6, 5), (7, 4), (8, 4), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "13 ['If', 'you', 'worry', 'about', 'the', 'quality', 'of', 'target', 'predictions', '-LRB-', 'pseudo', 'labels', '-RRB-']\n",
      "1 [',']\n",
      "1 ['then']\n",
      "5 ['Equation', '8', 'and', 'Equation', '9']\n",
      "2 ['are', 'questionable']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 7), (4, 3), (5, 1)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['intra-class']\n",
      "1 ['compactness']\n",
      "1 ['relies']\n",
      "2 ['on', 'p_t']\n",
      "1 [',']\n",
      "1 ['too']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 3), (7, 5), (8, 6), (9, 3), (10, 2), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['should']\n",
      "12 ['explain', 'why', 'prototypes', 'are', 'superior', 'than', 'pseudo', 'labels', 'in', '[', '1', ']']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 4), (5, 2), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['--']\n",
      "1 ['How']\n",
      "7 ['does', 'the', 'authors', 'select', 'hyper', '-', 'parameters']\n",
      "1 ['?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 4), (5, 7), (6, 6), (7, 10), (8, 4), (9, 2), (10, 2), (11, 2), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['lots']\n",
      "6 ['of', 'magic', 'numbers', 'in', 'Section', '4.1']\n",
      "4 ['about', 'hyper', '-', 'parameters']\n",
      "8 ['but', 'no', 'clues', 'about', 'how', 'to', 'tune', 'them']\n",
      "2020_Byg79h4tvB 1269\n",
      "[(0, 1), (1, 1), (2, 19), (3, 35), (4, 18), (5, 10), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "6 ['-', 'Implementation', 'Details', ':', 'Section', '4.1']\n",
      "1 [',']\n",
      "1 ['paragraph']\n",
      "9 ['4', ':', '\\\\', 'lambda', '^', '{', 'f', '}', '_']\n",
      "3 ['{', 'adv', '}']\n",
      "1 ['=']\n",
      "6 ['5e', '-', '3', ',', '\\\\', 'lambda']\n",
      "1 ['^']\n",
      "3 ['{', 'f', '}']\n",
      "1 ['_']\n",
      "3 ['{', 'adv', '}']\n",
      "3 ['and', '\\\\', 'lambda']\n",
      "1 ['^']\n",
      "3 ['{', 'p', '}']\n",
      "1 ['_']\n",
      "3 ['{', 'adv', '}']\n",
      "4 ['increase', 'from', '0', 'to']\n",
      "1 ['1']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 2), (6, 3), (7, 8), (8, 9), (9, 6), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  7\n",
      "7 ['\\\\', 'lambda', '^', '{', 'f', '}', '_']\n",
      "1 ['{']\n",
      "1 ['adv']\n",
      "1 ['}']\n",
      "1 ['both']\n",
      "1 ['is']\n",
      "4 ['a', 'constant', 'and', 'changes']\n",
      "1 ['continuously']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 9), (5, 14), (6, 12), (7, 12)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "17 ['[', '1', ']', 'Conditional', 'adversarial', 'domain', 'adaptation', ',', 'Long', 'et.al', ',', 'in', 'NeurIPS', '2018', '[', '2', ']']\n",
      "9 ['Towards', 'Accurate', 'Model', 'Selection', 'in', 'Deep', 'Unsupervised', 'Domain', 'Adaptation']\n",
      "2 ['You', 'et.al']\n",
      "1 [',']\n",
      "3 ['in', 'ICML', '2019']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 2), (6, 1), (7, 2), (8, 2), (9, 2), (10, 4), (11, 1), (12, 3), (13, 5), (14, 4), (15, 4), (16, 9), (17, 7), (18, 3), (19, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  3\n",
      "1 ['Significant']\n",
      "1 ['work']\n",
      "1 ['could']\n",
      "1 ['also']\n",
      "28 ['be', 'done', 'to', 'explore', 'the', 'effect', 'of', 'using', 'different', 'neural', 'network', 'structures', 'for', 'the', 'NRT', '-', 'in', 'this', 'paper', 'only', 'a', 'fairly', 'simple', '3', 'layer', 'architecture', 'is', 'used']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 10), (5, 6), (6, 10), (7, 8), (8, 6), (9, 6), (10, 8), (11, 7), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "10 ['The', 'proposed', 'approach', 'is', 'an', 'elaborate', 'extension', 'of', 'this', 'approach']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "31 ['if', 'we', 'want', 'prediction', 'performance', 'for', 'regression', ',', 'we', 'would', 'use', 'some', 'ensembles', 'of', 'regression', 'trees', 'such', 'as', 'Random', 'forest', ',', 'GBDT', ',', 'ExtraTrees', ',', '...', 'instead', 'of', 'a', 'single', 'CART']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 9), (4, 6), (5, 6), (6, 6), (7, 3), (8, 6), (9, 4), (10, 4), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['Over']\n",
      "1 ['all']\n",
      "1 ['the']\n",
      "1 ['paper']\n",
      "7 ['is', 'well', 'written', 'and', 'easy', 'to', 'follow']\n",
      "1 ['but']\n",
      "12 ['is', 'limited', 'by', 'its', 'lack', 'of', 'well', 'detailed', 'motivation', 'and', 'insufficient', 'baselines']\n",
      "1 ['and']\n",
      "2 ['applied', 'tasks']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 6), (6, 2), (7, 2), (8, 5), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "2 ['The', 'experiments']\n",
      "4 ['against', 'CART', 'and', 'SVR']\n",
      "1 ['would']\n",
      "10 ['be', 'too', 'naive', 'in', 'the', 'current', 'context', 'of', 'supervised', 'learning']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 14), (5, 9), (6, 8), (7, 7), (8, 6), (9, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "7 ['The', 'presented', 'experiments', 'are', 'also', 'not', 'thorough']\n",
      "1 [',']\n",
      "11 ['there', 'are', 'stronger', 'and', 'simpler', 'baselines', 'for', 'regression', 'like', 'random', 'forests']\n",
      "1 [',']\n",
      "13 ['gradient', 'boosted', 'trees', 'or', 'kernel', 'ridge', 'regression', 'which', 'are', 'not', 'evaluated', 'and', 'compared']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 4), (6, 4), (7, 4), (8, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "12 ['The', 'experiments', 'on', 'single', 'datasets', 'of', 'a', 'very', 'specific', 'speaker', 'profiling', 'problem']\n",
      "4 ['would', 'be', 'somewhat', 'misleading']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 9), (3, 9), (4, 14), (5, 15), (6, 10), (7, 13), (8, 4), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "4 ['On', 'the', 'other', 'hand']\n",
      "1 [',']\n",
      "6 ['if', 'this', 'is', 'for', 'benchmarking', 'purpose']\n",
      "1 [',']\n",
      "20 ['a', 'regression', 'by', 'neural', 'nets', 'and', 'tree', 'ensemble', '-LRB-', 'random', 'forest', 'or', 'something', '-RRB-', 'can', 'be', 'included', 'as', 'other', 'baselines']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "9 ['also', 'other', 'types', 'of', 'regression', 'problems', 'can', 'be', 'tested']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 2), (6, 5), (7, 2), (8, 4), (9, 2), (10, 2), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['would']\n",
      "1 ['also']\n",
      "13 ['be', 'significantly', 'more', 'compelling', 'if', 'the', 'strategy', 'was', 'applied', 'to', 'more', 'varied', 'tasks']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 10), (5, 10), (6, 3), (7, 5), (8, 4), (9, 4), (10, 7), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "13 ['Furthermore', 'the', 'two', 'baseline', 'models', 'used', 'are', '11', 'and', '34', 'years', 'old', 'respectively']\n",
      "1 ['and']\n",
      "16 ['i', 'do', 'not', 'believe', 'they', 'represent', 'a', 'thorough', 'review', 'of', 'the', 'potential', 'approaches', 'to', 'this', 'problem']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 7), (5, 2), (6, 2), (7, 4), (8, 2), (9, 2), (10, 2), (11, 2), (12, 4), (13, 2), (14, 1), (15, 2), (16, 2), (17, 4), (18, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  2\n",
      "4 ['Section', '4.4', 'is', 'interesting']\n",
      "1 ['and']\n",
      "20 ['i', 'believe', 'the', 'paper', 'would', 'be', 'improved', 'if', 'more', 'time', 'was', 'spent', 'exploring', 'the', 'explanability', 'of', 'this', 'new', 'proposed', 'model']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 7), (5, 4), (6, 7), (7, 4), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['Finally']\n",
      "7 ['the', 'scan', 'method', 'mentioned', 'in', 'the', 'conclusion']\n",
      "10 ['could', 'have', 'more', 'emphasis', 'placed', 'on', 'it', 'in', 'the', 'text']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 2), (6, 7), (7, 4), (8, 4), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Summary']\n",
      "1 [':']\n",
      "15 ['This', 'paper', 'presents', 'a', 'neural', 'network', 'based', 'tree', 'model', 'for', 'the', 'regression', 'via', 'classification', 'problem']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 7), (5, 3), (6, 3), (7, 5), (8, 4), (9, 2), (10, 4), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "6 ['The', 'paper', 'is', 'easy', 'to', 'follow']\n",
      "1 ['but']\n",
      "11 ['it', 'failed', 'to', 'give', 'motivations', 'for', 'the', 'significance', 'of', 'this', 'work']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 3), (6, 4), (7, 7), (8, 7), (9, 7), (10, 3), (11, 6), (12, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['I']\n",
      "1 ['do']\n",
      "1 ['not']\n",
      "24 ['understand', 'why', 'regression', 'via', 'classification', 'is', 'any', 'useful', 'and', 'what', 'value', 'it', 'brings', 'to', 'the', 'well', 'studied', 'regression', 'problem', 'with', 'many', 'different', 'function', 'approximators']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 3), (6, 4), (7, 6), (8, 6), (9, 9), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "9 ['neither', 'explain', 'why', 'regression', 'via', 'classification', 'is', 'any', 'useful']\n",
      "1 ['nor']\n",
      "9 ['does', 'it', 'motivates', 'the', 'need', 'for', 'the', 'presented', 'model']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 5), (7, 3), (8, 5), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  6\n",
      "1 ['this']\n",
      "1 ['work']\n",
      "1 ['do']\n",
      "1 ['not']\n",
      "7 ['pass', 'the', 'acceptance', 'bar', 'at', 'ICLR', 'conference']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 2), (6, 2), (7, 3), (8, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['Comments']\n",
      "1 [':']\n",
      "13 ['1', '-RRB-', 'I', 'was', 'not', 'aware', 'of', 'this', 'age', 'and', 'height', 'estimation', 'tasks']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 3), (6, 1), (7, 2), (8, 2), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['i', '-']\n",
      "1 ['vectors']\n",
      "7 ['are', 'the', 'standard', 'features', 'for', 'speaker', 'recognition']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 4), (5, 6), (6, 7), (7, 5), (8, 2), (9, 2), (10, 2), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['Can']\n",
      "2 ['the', 'authors']\n",
      "1 ['please']\n",
      "18 ['elaborate', 'in', 'a', 'line', 'or', 'two', 'why', 'i', '-', 'vectors', 'would', 'be', 'suitable', 'for', 'age', 'and', 'height', 'estimation']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 10), (5, 5), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "7 ['The', 'regressor', 'function', 'r', '-LRB-', '-RRB-', 'simply']\n",
      "8 ['gives', 'out', 'the', 'mean', 'value', 'of', 'the', 'bin']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 2), (7, 2), (8, 3), (9, 2), (10, 2), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['could']\n",
      "8 ['have', 'provided', 'on', 'details', 'on', 'why', 'this', 'choice']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 3), (5, 1)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['and']\n",
      "1 ['how']\n",
      "3 ['it', 'affects', 'MAE']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 7), (5, 4), (6, 2), (7, 2), (8, 5), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['3', '-RRB-']\n",
      "5 ['Each', 'node', 'in', 'the', 'NRT']\n",
      "10 ['is', 'successively', 'being', 'trained', 'on', 'a', 'lesser', 'amount', 'of', 'data']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 6), (6, 11)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  5\n",
      "3 ['all', 'the', 'node']\n",
      "1 ['-']\n",
      "3 ['specific', 'neural', 'networks']\n",
      "1 ['need']\n",
      "4 ['the', 'same', 'parameter', 'size']\n",
      "1 ['then']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 10), (4, 5), (5, 6), (6, 5), (7, 8), (8, 2), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['4', '-RRB-']\n",
      "2 ['In', 'Conclusion']\n",
      "2 ['the', 'authors']\n",
      "19 ['say', ',', '\"', 'In', 'addition', ',', 'we', 'proposed', 'a', 'scan', 'method', 'and', 'a', 'gradient', 'method', 'to', 'optimize', 'the', 'tree']\n",
      "1 ['.']\n",
      "1 ['\"']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 8), (5, 7), (6, 4), (7, 2), (8, 5), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "16 ['The', 'authors', 'do', 'not', 'very', 'clearly', 'mention', 'these', 'two', 'methods', 'in', 'the', 'text', ',', 'neither', 'are']\n",
      "2 ['the', 'results']\n",
      "4 ['demonstrated', 'in', 'that', 'way']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 6), (5, 2), (6, 2), (7, 3), (8, 6), (9, 5), (10, 11), (11, 10), (12, 7), (13, 5), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "2 ['Miscellaneous', 'comments']\n",
      "1 [':']\n",
      "38 ['1', '-RRB-', 'This', 'line', 'seems', 'incomplete', 'in', 'Section', '1', ':', '\"', 'Traditional', 'methods', 'for', 'defining', 'the', 'partition', 'T', 'by', 'prior', 'knowledge', ',', 'such', 'as', 'equally', 'probable', 'intervals', ',', 'equal', 'width', 'intervals', ',', 'k', '-', 'means', 'clustering', ',', 'etc.']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 8), (3, 3)]\n",
      "[0, 1, 2, 3]\n",
      "depth:  2\n",
      "1 ['[']\n",
      "1 ['4']\n",
      "1 [',']\n",
      "1 ['5']\n",
      "1 [',']\n",
      "1 ['3']\n",
      "1 [']']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 6), (5, 4), (6, 4), (7, 6), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "9 ['The', 'notations', 'used', 'inside', 'the', 'nodes', 'in', 'Figure', '1']\n",
      "7 ['has', 'not', 'been', 'defined', 'in', 'the', 'paper']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 8), (4, 5), (5, 1)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "2 ['3', '-RRB-']\n",
      "5 ['Figure', '2', 'and', '3', 'axes']\n",
      "4 ['do', \"n't\", 'have', 'labels']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 7), (5, 6), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "5 ['Figure', '3', 'caption', 'says', 'age']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "4 ['it', 'is', 'for', 'heights']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 8), (3, 9), (4, 7), (5, 3), (6, 4), (7, 3), (8, 5), (9, 6), (10, 5), (11, 6), (12, 3), (13, 2), (14, 2), (15, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "2 ['4', '-RRB-']\n",
      "2 ['In', 'Section']\n",
      "1 ['4.4']\n",
      "1 [':']\n",
      "16 ['Figure', '4.4', 'should', 'be', 'Figure', '4', 'and', 'at', 'one', 'point', '\"', 'This', 'is', 'visible', 'in', '4.4']\n",
      "1 ['\"']\n",
      "9 ['should', 'be', '\"', 'This', 'is', 'visible', 'in', 'Figure', '4']\n",
      "1 ['\"']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 2), (6, 5), (7, 5), (8, 5), (9, 3), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['Summary']\n",
      "1 [':']\n",
      "17 ['The', 'paper', 'presents', 'a', 'novel', 'supervised', '-', 'learning', 'method', 'for', 'regression', 'using', 'decision', 'trees', 'and', 'neural', 'nets']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 2), (6, 2), (7, 6), (8, 6), (9, 5), (10, 6), (11, 6), (12, 10), (13, 7), (14, 8), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['core']\n",
      "1 ['idea']\n",
      "1 ['is']\n",
      "36 ['based', 'on', 'a', '90s', 'technique', 'called', '\"', 'regression', 'via', 'classification', '\"', 'by', 'first', 'apply', 'discretization', 'of', 'target', 'response', 'y', 'by', 'some', 'clustering', ',', 'and', 'apply', 'any', '\"', 'classification', '\"', 'to', 'those', 'discretized', 'values', 'as', 'class', 'labels']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 8), (6, 11), (7, 5), (8, 3), (9, 5), (10, 3), (11, 5), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "11 ['Because', 'real', '-', 'valued', 'y', 'is', 'one', '-', 'dimensional', 'and', 'ordered']\n",
      "1 [',']\n",
      "1 ['discretization']\n",
      "16 ['means', 'setting', 'up', 'any', 'thresholds', 'to', 'give', 'N', '-', 'partitions', 'of', 'training', '{', 'y_i', '}', 's']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 8), (5, 7), (6, 3), (7, 3), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "10 ['Because', 'each', 'node', 'splitters', 'are', 'given', 'by', 'neural', 'nets', 'here']\n",
      "1 [',']\n",
      "5 ['probability', 'outputs', 'for', 'binary', 'classification']\n",
      "3 ['are', 'also', 'available']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 10), (5, 8), (6, 6), (7, 4), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "9 ['Regarding', 'these', 'probabilities', 'as', 'probabilistic', 'splitting', 'at', 'each', 'node']\n",
      "1 [',']\n",
      "9 ['response', 'y', 'weighted', 'by', 'the', 'path', 'probabilities', 'to', 'leaves']\n",
      "4 ['is', 'the', 'final', 'prediction']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 4), (6, 11), (7, 4), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['learning']\n",
      "1 ['is']\n",
      "9 ['in', 'a', 'greedy', 'manner', 'as', 'in', 'standard', 'tree', 'learning']\n",
      "7 ['because', 'exact', 'joint', 'learning', 'is', 'computationally', 'hard']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 8), (5, 4), (6, 6), (7, 2), (8, 3), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['Experiments']\n",
      "3 ['on', 'speaker', 'profiling']\n",
      "1 ['illustrate']\n",
      "3 ['the', 'performance', 'improvements']\n",
      "10 ['against', 'standard', 'nonlinear', 'regression', 'such', 'as', 'SVR', 'and', 'regression', 'trees']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 4), (5, 7), (6, 5), (7, 6), (8, 4), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Comment']\n",
      "1 [':']\n",
      "18 ['This', 'is', 'a', 'technically', 'very', 'interesting', 'contribution', ',', 'but', 'several', 'points', 'can', 'be', 'considered', 'more', 'carefully', 'as', 'below']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 4), (5, 5), (6, 5), (7, 7), (8, 6), (9, 3), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "4 ['-', 'To', 'be', 'honest']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "17 ['would', 'be', 'unconvincing', 'that', 'the', 'approach', '\"', 'regression', 'via', 'classification', '-LRB-', 'RvC', '-RRB-', '\"', 'is', 'still', 'valid']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 3), (5, 3), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['Or']\n",
      "1 ['even']\n",
      "1 ['we']\n",
      "7 ['can', 'directly', 'use', 'deep', 'learning', 'based', 'regression']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 7), (5, 4), (6, 4), (7, 1), (8, 2), (9, 2), (10, 2), (11, 4), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "4 ['On', 'the', 'other', 'hand']\n",
      "1 [',']\n",
      "2 ['single', 'CARTs']\n",
      "17 ['are', 'well', 'interpretable', 'and', 'can', 'be', 'a', 'nice', 'tool', 'to', 'get', 'some', 'interpretations', 'of', 'the', 'given', 'data']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 1), (5, 2), (6, 3), (7, 4), (8, 6), (9, 2), (10, 3), (11, 4), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['But']\n",
      "3 ['the', 'proposed', 'method']\n",
      "15 ['seems', 'to', 'lose', 'this', 'type', 'of', 'interpretability', 'because', 'of', 'introducing', 'node', 'splitters', 'by', 'neural', 'nets']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 6), (5, 5)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['So']\n",
      "6 ['the', 'merits', 'of', 'the', 'proposed', 'approach']\n",
      "4 ['would', 'be', 'somewhat', 'unclear']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 6), (3, 5), (4, 3), (5, 6), (6, 4), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['-']\n",
      "6 ['In', 'the', 'context', 'of', 'tree', 'learning']\n",
      "1 [',']\n",
      "1 ['we']\n",
      "5 ['need', 'to', 'consider', 'two', 'things']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 13), (5, 14), (6, 11), (7, 3), (8, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "16 ['First', 'of', 'all', ',', 'node', 'splitting', 'by', 'general', 'binary', 'splitters', 'are', 'called', '\"', 'multivariate', 'trees', '\"']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "17 ['interestingly', 'this', 'does', 'not', 'always', 'bring', 'the', 'good', 'prediction', 'performance', 'on', 'current', 'quite', 'high', '-', 'dimensional', 'data']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 2), (6, 6), (7, 8), (8, 6), (9, 6), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['So']\n",
      "1 ['I']\n",
      "22 ['guess', 'that', 'both', 'optimizing', '\"', 'threshold', 'for', 'RvC', '\"', 'and', '\"', 'nonlinear', 'node', 'splitters', '\"', 'can', 'not', 'always', 'bring', 'the', 'prediction', 'performance']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 2), (5, 1), (6, 2), (7, 2), (8, 2), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['Limitations']\n",
      "1 ['and']\n",
      "1 ['conditions']\n",
      "1 ['would']\n",
      "6 ['need', 'to', 'be', 'clarified', 'more', 'carefully']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 5), (5, 5), (6, 6), (7, 10), (8, 7), (9, 14), (10, 9), (11, 5), (12, 5), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['Second']\n",
      "14 ['of', 'all', ',', 'probabilistic', 'consideration', 'of', 'decision', 'trees', 'such', 'as', 'eq', '-LRB-', '4', '-RRB-']\n",
      "1 ['is']\n",
      "25 ['almost', 'like', 'so', '-', 'called', '\"', 'probabilistic', 'decision', 'trees', '\"', 'also', 'known', 'as', '\"', 'hierarchical', 'mixtures', 'of', 'experts', '-LRB-', 'HME', '-RRB-', '\"', 'in', 'machine', 'learning']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 2), (5, 5), (6, 4), (7, 4), (8, 4), (9, 8)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  5\n",
      "1 ['famous']\n",
      "1 ['widely']\n",
      "1 ['-']\n",
      "1 ['cited']\n",
      "11 ['papers', 'of', 'Jordan', '&', 'Jacobs', '1994', 'and', 'Bishop', '&', 'Svensen', '2003']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 3), (4, 10), (5, 5), (6, 6), (7, 5), (8, 9), (9, 11), (10, 9), (11, 7), (12, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "38 ['This', 'can', 'bring', 'joint', 'learning', 'of', 'probabilistic', 'node', 'splitter', '-LRB-', 'gating', 'networks', '-RRB-', 'and', 'decision', 'functions', 'at', 'leaves', '-LRB-', 'expert', 'networks', '-RRB-', ',', 'and', 'is', 'also', 'known', 'to', 'bring', 'the', 'smoothing', 'effect', 'into', 'discrete', 'and', 'unstable', 'regression', 'trees']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "5 ['hence', 'the', 'improved', 'prediction', 'performance']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 10), (5, 15), (6, 20), (7, 10), (8, 11), (9, 7), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "14 ['So', 'which', 'of', 'probabilistic', 'consideration', 'or', 'RvC', 'contributes', 'to', 'the', 'observed', 'improvement', 'is', 'unclear']\n",
      "1 ['...']\n",
      "30 ['-', 'The', 'target', 'joint', 'optimization', 'of', 'eq', '-LRB-', '3', '-RRB-', 'is', 'actually', 'optimized', 'by', 'a', 'number', 'of', 'heuristic', 'ways', ',', 'and', 'it', 'is', 'quite', 'unclear', 'how', 'it', 'is', 'truly', 'optimized']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 6), (5, 7), (6, 8), (7, 7), (8, 4), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['In', 'contrast']\n",
      "1 [',']\n",
      "2 ['HME', 'learning']\n",
      "22 ['is', 'formulated', 'as', 'a', 'joint', 'optimization', '-LRB-', 'and', 'solved', 'by', 'EM', 'in', 'the', 'case', 'of', 'Jordan', '&', 'Jacobs', ',', 'for', 'example', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 6), (4, 5), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['Probably']\n",
      "1 [',']\n",
      "4 ['for', 'this', 'specific', 'problem']\n",
      "1 [',']\n",
      "1 ['there']\n",
      "5 ['would', 'be', 'other', 'existing', 'methods']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 2), (6, 3), (7, 2), (8, 4), (9, 2), (10, 5), (11, 3), (12, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['presents']\n",
      "3 ['a', 'new', 'approach']\n",
      "17 ['to', 'regression', 'via', 'classification', 'problem', 'utilizing', 'a', 'hybrid', 'model', 'between', 'a', 'neural', 'network', 'and', 'a', 'decision', 'tree']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 4), (6, 3), (7, 2), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['is']\n",
      "7 ['very', 'well', 'written', 'and', 'easy', 'to', 'follow']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 3), (6, 6), (7, 6), (8, 2), (9, 5), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['presents']\n",
      "1 ['results']\n",
      "15 ['on', 'two', 'very', 'similar', 'regression', 'tasks', 'and', 'claims', 'state', 'of', 'the', 'art', 'performance', 'on', 'both']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 6), (5, 6), (6, 7), (7, 2), (8, 2), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['The', 'paper']\n",
      "1 ['however']\n",
      "18 ['does', 'not', 'motivate', 'its', 'contributions', 'sufficiently', ',', 'and', 'does', 'not', 'provide', 'enough', 'experimental', 'results', 'to', 'justify', 'their', 'method']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 4), (5, 4), (6, 1), (7, 3), (8, 3), (9, 2), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['could']\n",
      "1 ['significantly']\n",
      "10 ['improve', 'the', 'paper', 'by', 'spending', 'more', 'time', 'motivating', 'their', 'work']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 5), (5, 4), (6, 6), (7, 9), (8, 4), (9, 7), (10, 5), (11, 3), (12, 2), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "2 ['For', 'example']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "24 ['is', 'unclear', 'why', 'RvC', 'is', 'the', 'best', 'strategy', 'for', 'the', 'tasks', 'they', 'study', 'and', 'what', 'other', 'tasks', 'one', 'should', 'approach', 'from', 'a', 'RvC', 'standpoint']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 5), (7, 3), (8, 3), (9, 2), (10, 2), (11, 2), (12, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  6\n",
      "1 ['the']\n",
      "1 ['current']\n",
      "1 ['approach']\n",
      "1 ['will']\n",
      "9 ['be', 'suboptimal', 'when', 'compared', 'to', 'the', 'ordered', 'statistics', 'approach']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 4), (6, 2), (7, 3), (8, 2), (9, 2), (10, 2), (11, 5), (12, 3), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  5\n",
      "1 ['the']\n",
      "1 ['authors']\n",
      "1 ['using']\n",
      "11 ['algorithms', 'that', 'are', 'much', 'faster', 'such', 'as', 'the', 'sueprlinear', 'secant', 'method']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 6), (7, 2), (8, 5), (9, 7), (10, 4), (11, 8), (12, 6), (13, 4), (14, 3), (15, 2), (16, 1), (17, 2), (18, 4), (19, 7), (20, 6), (21, 6), (22, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['builds']\n",
      "42 ['upon', 'the', 'random', 'smoothing', 'technique', 'for', 'top', '-', '1', 'prediction', 'proposed', 'by', 'Cohen', 'et', 'al', 'for', 'certifying', 'top', '-', 'k', 'predictions', 'with', 'probabilistic', 'guarantees', ',', 'which', 'enjoys', 'good', 'scalability', 'to', 'large', 'neural', 'networks', 'and', 'in', 'principle', 'can', 'be', 'applied', 'to', 'any', 'classifier']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 5), (5, 7), (6, 10), (7, 10), (8, 3), (9, 5), (10, 2), (11, 1), (12, 2), (13, 3), (14, 4), (15, 5), (16, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "2 ['-', 'Contributions']\n",
      "17 [':', '1', '-RRB-', 'The', 'authors', 'aim', 'to', 'provide', '-LRB-', 'probabilistic', '-RRB-', 'certification', 'on', 'top', '-', 'k', 'predictions']\n",
      "1 [',']\n",
      "8 ['which', 'to', 'my', 'knowledge', 'is', 'the', 'first', 'work']\n",
      "4 ['to', 'consider', 'this', 'setup']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 9), (5, 8)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "6 ['Many', 'applications', 'such', 'as', 'recommendation', 'systems']\n",
      "1 ['indeed']\n",
      "9 ['use', 'top', '-', 'k', 'predictions', 'as', 'a', 'performance', 'measure']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 5), (5, 2), (6, 4), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['problem']\n",
      "1 ['setup']\n",
      "1 ['is']\n",
      "3 ['new', 'and', 'important']\n",
      "6 ['in', 'the', 'research', 'of', 'robustness', 'certification']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 10), (4, 6), (5, 9), (6, 5), (7, 7), (8, 6), (9, 11), (10, 5), (11, 4), (12, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "5 ['In', 'terms', 'of', 'technical', 'contributions']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "32 ['identify', 'the', 'difficulty', 'of', 'extending', 'top', '-', '1', 'prediction', 'to', 'top', '-', 'k', 'prediction', ',', 'due', 'to', 'the', 'requirement', 'of', 'simultaneous', 'confidence', 'interval', 'estimation', 'of', 'the', 'bounds', 'on', 'the', 'actual', 'class', 'predictions']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 8), (5, 4), (6, 5), (7, 6), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "5 ['To', 'cope', 'with', 'this', 'difficulty']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "14 ['proposed', 'simultaneous', 'confidence', 'interval', 'estimation', 'based', 'on', 'Clopper', '-', 'Pearson', 'method', 'and', 'Bonferroni', 'correction']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 2), (5, 1), (6, 2), (7, 4), (8, 2), (9, 2), (10, 4), (11, 5), (12, 9), (13, 4), (14, 2), (15, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "24 ['am', 'not', 'sure', 'the', 'difficulty', 'is', 'caused', 'by', 'the', 'necessity', 'of', 'estimating', 'multiple', 'probability', 'bounds', ',', 'or', 'simply', 'the', 'limitation', 'of', 'the', 'proposed', 'algorithm']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 4), (7, 4), (8, 5), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  6\n",
      "1 ['the']\n",
      "1 ['authors']\n",
      "1 ['can']\n",
      "7 ['address', 'my', 'concerns', 'in', 'the', 'Questions', 'below']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 5), (5, 4), (6, 6), (7, 2), (8, 4), (9, 4), (10, 7), (11, 2), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "2 ['3', '-RRB-']\n",
      "8 ['Experimental', 'results', 'on', 'Cifar', '-', '10', 'and', 'ImageNet']\n",
      "13 ['showed', 'improved', 'lower', 'bound', 'on', 'certified', 'L2', '-', 'norm', 'radius', 'when', 'increasing', 'k']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 6), (5, 5), (6, 7), (7, 8), (8, 6), (9, 9), (10, 8), (11, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "2 ['-', 'Questions']\n",
      "1 [':']\n",
      "31 ['1', '-RRB-', 'Intuitively', ',', 'when', 'extending', 'top', '-', '1', 'certification', 'to', 'top', '-', 'k', 'certification', ',', 'one', 'would', 'expect', 'using', 'ordered', 'statistics', 'of', 'the', 'prediction', 'outputs', 'from', 'the', 'randomly', 'perturbed', 'inputs']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 6), (5, 2), (6, 5), (7, 6), (8, 4), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "17 ['As', 'long', 'as', 'the', 'original', 'label', \"'s\", 'prediction', 'probability', 'is', 'in', 'the', 'top', '-', 'k', 'label', 'set']\n",
      "1 [',']\n",
      "3 ['the', 'smoothed', 'classifier']\n",
      "3 ['is', 'directly', 'certified']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 6), (5, 3), (6, 2), (7, 1), (8, 2), (9, 2), (10, 4), (11, 6), (12, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "4 ['Instead', 'of', 'ordered', 'statistics']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "15 ['tackle', 'this', 'problem', 'by', 'considering', 'estimating', 'upper', 'and', 'lower', 'bounds', 'of', 'each', 'class', 'prediction', 'probability']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 4), (5, 6), (6, 7), (7, 2), (8, 2), (9, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Therefore']\n",
      "1 [',']\n",
      "2 ['the', 'problem']\n",
      "18 ['becomes', 'more', 'difficult', 'as', 'k', 'increases', ',', 'since', 'this', 'indirect', 'approach', 'needs', 'to', 'simultaneous', 'estimate', 'those', 'probability', 'bounds']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 1), (6, 2), (7, 3), (8, 5), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  8\n",
      "1 ['the']\n",
      "1 ['authors']\n",
      "1 ['thoughts']\n",
      "1 ['on']\n",
      "2 ['this', 'regard']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 7), (5, 5), (6, 3), (7, 5), (8, 5), (9, 6), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "2 ['That', 'is']\n",
      "1 [',']\n",
      "4 ['is', 'the', 'claimed', 'difficulty']\n",
      "18 ['an', 'outcome', 'when', 'using', 'the', 'proposed', 'indirect', 'bound', 'estimation', 'for', 'certification', ',', 'or', 'it', \"'s\", 'provably', 'more', 'difficult']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 7), (5, 5), (6, 2), (7, 2), (8, 5), (9, 4), (10, 9), (11, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "5 ['The', 'discussion', 'on', 'Fig.', '3']\n",
      "20 ['says', '\"', 'We', 'observe', 'that', '\\\\', 'sigma', 'controls', 'a', 'trade', '-', 'off', 'between', 'normal', 'accuracy', 'under', 'no', 'attacks', 'and', 'robustness']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 16), (5, 15), (6, 12), (7, 7), (8, 3), (9, 3), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "23 ['Specifically', ',', 'when', 'is', 'larger', ',', 'the', 'accuracy', 'under', 'no', 'attacks', '-LRB-', 'i.e.', ',', 'the', 'accuracy', 'when', 'radius', 'is', '0', '-RRB-', 'is', 'larger']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "13 ['the', 'certified', 'top', '-', 'k', 'accuracy', 'drops', 'more', 'quickly', 'as', 'the', 'radius', 'increases']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 2), (5, 3), (6, 6), (7, 7), (8, 3), (9, 4), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "15 ['seems', 'that', 'larger', '\\\\', 'sigma', 'actually', 'gives', 'lower', 'accuracy', 'under', 'no', 'attacks', 'in', 'Figure', '3']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 2)]\n",
      "[0, 1, 2, 3]\n",
      "depth:  3\n",
      "1 ['Please']\n",
      "1 ['clarify']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 11), (5, 3), (6, 9), (7, 4), (8, 2), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "13 ['Overall', ',', 'this', 'paper', 'brings', 'some', 'new', 'insights', 'and', 'results', 'in', 'robustness', 'certification']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "9 ['some', 'claims', 'and', 'statements', 'need', 'to', 'be', 'further', 'justified']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 1), (6, 2), (7, 3), (8, 4), (9, 2), (10, 4), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  8\n",
      "1 ['my']\n",
      "1 ['rating']\n",
      "1 ['if']\n",
      "4 ['my', 'concerns', 'are', 'addressed']\n",
      "[(0, 1), (1, 1), (2, 2)]\n",
      "[0, 1, 2]\n",
      "Depth to split: None\n",
      "depth:  None\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 6), (5, 5), (6, 9), (7, 4), (8, 4), (9, 2), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['proposes']\n",
      "2 ['an', 'extension']\n",
      "7 ['to', 'the', 'work', 'of', 'Cohen', 'et', 'al']\n",
      "11 ['where', 'a', 'certified', 'radius', 'is', 'deduced', 'using', 'a', 'randomized', 'smoothing', 'approach']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 3), (5, 4), (6, 4), (7, 5), (8, 8), (9, 4), (10, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "2 ['In', 'particular']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "20 ['show', 'the', 'radius', 'at', 'which', 'a', 'smoothed', 'classifier', 'g', 'at', 'under', 'Gaussian', 'perturbations', 'is', 'certified', 'for', 'the', 'top', 'k', 'predictions']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 6), (5, 3), (6, 2), (7, 2), (8, 2), (9, 2), (10, 4), (11, 3), (12, 3), (13, 4), (14, 8), (15, 4), (16, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "22 ['That', 'is', 'to', 'say', 'that', 'the', 'prediction', 'will', 'remain', 'within', 'the', 'top', 'k', 'predictions', 'of', 'g.', 'Setting', 'k', '=', '1', ',', 'one']\n",
      "1 ['recovers']\n",
      "4 ['Cohen', 'et', 'al', 'results']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 5), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "1 ['show']\n",
      "6 ['that', 'the', 'derived', 'radius', 'is', 'tight']\n",
      "[(0, 1), (1, 1), (2, 2)]\n",
      "[0, 1, 2]\n",
      "Depth to split: None\n",
      "depth:  None\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 8), (5, 4), (6, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "6 ['I', 'generally', 'find', 'the', 'work', 'interesting']\n",
      "1 ['and']\n",
      "7 ['I', 'do', 'not', 'have', 'any', 'major', 'criticism']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 3), (5, 1), (6, 2), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['is']\n",
      "1 ['also']\n",
      "3 ['easy', 'to', 'read']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 10), (5, 6), (6, 6), (7, 11), (8, 7), (9, 5), (10, 4), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "8 ['I', 'did', 'not', 'go', 'through', 'the', 'tightness', 'proof']\n",
      "1 ['but']\n",
      "8 ['I', 'skimmed', 'through', 'proof', 'of', 'the', 'certified', 'radius']\n",
      "1 ['and']\n",
      "16 ['I', 'find', 'that', 'the', 'argument', 'follows', 'in', 'a', 'similar', 'fashion', 'to', 'previous', 'works', 'using', 'NP', 'lemma']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 4), (5, 3), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['the']\n",
      "1 ['authors']\n",
      "1 ['comment']\n",
      "3 ['on', 'the', 'following']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 3), (5, 5), (6, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "2 ['a', '-RRB-']\n",
      "1 ['How']\n",
      "6 ['does', 'the', 'method', 'scale', 'with', 'k']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 3), (5, 4), (6, 5), (7, 4), (8, 5), (9, 5), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  5\n",
      "3 ['-', 'speedwise', 'particularly']\n",
      "1 ['when']\n",
      "9 ['estimating', 'the', 'lower', 'and', 'upper', 'bounds', 'of', 'the', 'output']\n",
      "1 ['probabilities']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 2), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "2 ['b', '-RRB-']\n",
      "1 ['I']\n",
      "5 ['do', 'not', 'understand', 'Figure', '3']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 4), (5, 2), (6, 4), (7, 6), (8, 9), (9, 12), (10, 7), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['the']\n",
      "1 ['authors']\n",
      "1 ['comment']\n",
      "23 ['why', 'for', 'the', 'radius', '=', '0', 'the', 'certified', 'accuracy', 'of', 'the', 'larger', 'sigma', '-LRB-', '1.0', '-RRB-', 'is', 'actually', 'worse', 'than', 'the', 'smaller', 'sigma']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 8), (4, 4), (5, 8), (6, 5), (7, 5), (8, 6), (9, 3), (10, 4), (11, 6), (12, 6), (13, 5), (14, 4), (15, 6), (16, 5), (17, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "depth:  2\n",
      "2 ['At', 'least']\n",
      "4 ['when', 'k', '=', '1']\n",
      "1 [',']\n",
      "2 ['increasing', 'sigma']\n",
      "33 ['increases', 'the', 'certified', 'radius', 'in', 'which', 'I', 'expect', 'to', 'see', 'that', 'most', 'of', 'the', 'samples', 'to', 'be', 'actually', 'within', 'the', 'radius', 'and', 'it', 'should', 'perform', 'much', 'better', 'than', 'lower', 'sigma', '{', '0.25,0.5', '}']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 6), (5, 12), (6, 11), (7, 15), (8, 12), (9, 7), (10, 7), (11, 5), (12, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "2 ['c', '-RRB-']\n",
      "5 ['Based', 'on', 'the', 'previous', 'comment']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "39 ['find', 'the', 'performance', 'in', 'Figure', '3', 'in', 'not', 'consistent', 'with', 'the', 'authors', \"'\", 'discussion', 'in', 'page', '7', '\"', 'when', 'sigma', 'is', 'larger', ',', 'the', 'accuracy', 'under', 'no', 'attacks', '-LRB-', 'ie', 'the', 'accuracy', 'when', 'radius', 'is', '9', '-RRB-', 'is', 'larger']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 2), (5, 2), (6, 4), (7, 6), (8, 5), (9, 5), (10, 3), (11, 2), (12, 3), (13, 3), (14, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "2 ['d', '-RRB-']\n",
      "1 ['Authors']\n",
      "22 ['use', 'bisection', 'to', 'find', 'the', 'solution', 'to', 'the', 'certified', 'accuracy', 'for', 'every', 't.', 'Bisection', 'is', 'known', 'to', 'only', 'enjoy', 'linear', 'convergence', 'rate']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7)]\n",
      "[0, 1, 2, 3]\n",
      "depth:  2\n",
      "2 ['e', '-RRB-']\n",
      "1 ['Page']\n",
      "1 ['3']\n",
      "3 ['bullet', 'point', '3']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 1), (6, 2), (7, 2), (8, 4), (9, 6), (10, 3), (11, 2), (12, 1), (13, 2), (14, 2), (15, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  8\n",
      "4 ['a', 'L_2', 'radius', '>>']\n",
      "1 ['\"']\n",
      "9 ['it', 'is', 'impossible', 'to', 'certify', 'an', '\\\\', 'ell_2', 'radius']\n",
      "1 ['\"']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 1), (5, 3), (6, 4), (7, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  6\n",
      "1 ['all']\n",
      "1 ['L_2']\n",
      "1 ['to']\n",
      "2 ['\\\\', 'ell_2']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 8), (4, 10), (5, 9), (6, 8), (7, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "1 ['Summary']\n",
      "1 [':']\n",
      "6 ['This', 'paper', 'studies', 'the', 'certifiable', 'bounds']\n",
      "7 ['for', 'adversarial', 'perturbations', 'in', '\\\\', 'ell_2', 'radius']\n",
      "12 ['for', 'top', '-', 'k', 'predictions', 'instead', 'of', 'top', '-', '1', 'predictions', '.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 9), (5, 11), (6, 11), (7, 11), (8, 3), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "20 ['obtains', 'a', 'certifiable', 'radius', 'of', '\\\\', 'ell_2', 'perturbations', 'in', 'the', 'case', 'of', 'top', '-', 'k', 'predictions', '-LRB-', 'Theorem', '1', '-RRB-']\n",
      "1 ['and']\n",
      "6 ['shows', 'that', 'the', 'bounds', 'are', 'tight']\n",
      "4 ['-LRB-', 'Theorem', '2', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 5), (6, 4), (7, 6), (8, 4), (9, 3), (10, 2), (11, 3), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "2 ['The', 'result']\n",
      "1 ['thus']\n",
      "16 ['generalizes', 'the', 'results', 'obtained', 'in', 'Cohen', 'et', 'al', '-LRB-', '2019', '-RRB-', 'by', 'setting', 'k', '=', '1']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 6), (5, 5), (6, 6), (7, 7), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "8 ['Since', 'Theorem', '1', 'requires', 'lower', 'and', 'upper', 'bounds']\n",
      "1 [',']\n",
      "2 ['the', 'paper']\n",
      "10 ['proposes', 'two', 'methods', 'for', 'calculating', 'the', 'bounds', 'on', 'multinomial', 'probabilities']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 3), (6, 5), (7, 6), (8, 2), (9, 5), (10, 6), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['Experimental']\n",
      "1 ['evidence']\n",
      "1 ['suggests']\n",
      "20 ['that', 'one', 'indeed', 'obtains', 'a', 'better', 'certifiable', 'radius', 'for', 'the', 'top', '-', 'k', 'radius', 'vs.', 'the', 'top', '-', '1', 'radius']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 10), (5, 11), (6, 9), (7, 6), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "7 ['My', 'evaluation', 'of', 'the', 'paper', 'is', 'positive']\n",
      "1 [':']\n",
      "19 ['The', 'theoretical', 'results', '-LRB-', 'Theorem', '1', 'and', '2', '-RRB-', 'are', 'new', 'and', 'study', 'practical', 'use', 'cases', 'of', 'these', 'models']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 7), (4, 7), (5, 4), (6, 3), (7, 2), (8, 5), (9, 2), (10, 5), (11, 4), (12, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['experimental']\n",
      "1 ['results']\n",
      "4 ['-LRB-', 'Figure', '1', '-RRB-']\n",
      "1 ['support']\n",
      "2 ['the', 'claim']\n",
      "19 ['that', 'there', 'is', 'a', 'non-trivial', 'difference', 'between', 'the', 'certified', 'radii', 'of', 'top', '-', '1', 'and', 'top', '-', 'k', 'predictions']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 6), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "5 ['the', 'level', 'of', 'technical', 'novelty']\n",
      "3 ['is', 'relatively', 'low']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 8), (5, 13), (6, 12), (7, 6), (8, 5), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "14 ['The', 'proof', 'of', 'Theorem', '1', 'follows', 'the', 'similar', 'procedure', 'for', 'top', '-', '1', 'predictions']\n",
      "1 ['and']\n",
      "14 ['the', 'methods', 'proposed', 'for', 'estimating', 'probabilities', '-LRB-', 'BinoCP', 'and', 'SinuEM', '-RRB-', 'are', 'standard', 'procedures']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 5), (5, 2), (6, 4), (7, 3), (8, 5), (9, 15), (10, 2), (11, 4), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "2 ['Other', 'comments']\n",
      "1 [':']\n",
      "27 ['1', '-RRB-', 'What', 'is', 'the', 'trend', 'of', 'top', '-', 'k', '-', 'clean', '-', 'accuracy', 'and', 'top', '-', 'k', '-', 'adversarial', '-', 'accuracy', 'as', 'a', 'function', 'of', 'k']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['Is']\n",
      "2 ['this', 'trend']\n",
      "4 ['similar', 'across', 'different', 'radii']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 4), (6, 2), (7, 3), (8, 3), (9, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "1 ['What']\n",
      "11 ['is', 'the', 'value', 'of', 'k', 'in', 'Figure', '3', 'and', 'Figure', '4']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 7), (5, 5), (6, 6), (7, 4), (8, 5), (9, 10), (10, 5), (11, 4), (12, 5), (13, 6), (14, 9), (15, 4), (16, 1), (17, 2), (18, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "37 ['The', 'evaluation', 'is', 'limited', ',', 'in', 'that', 'the', 'standard', 'evaluations', '-LRB-', 'eg', ':', 'SimLex', 'would', 'be', 'a', 'good', 'one', 'to', 'add', ',', 'as', 'well', 'as', 'many', 'others', ',', 'please', 'refer', 'to', 'the', 'literature', '-RRB-', 'are', 'not', 'used']\n",
      "1 ['and']\n",
      "7 ['there', 'is', 'no', 'comparison', 'to', 'previous', 'work']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 6), (5, 8), (6, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "7 ['Given', 'the', 'small', 'size', 'of', 'the', 'datasets']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "8 ['is', 'also', 'unclear', 'how', 'generalizable', 'the', 'approach', 'is']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 7), (6, 5), (7, 5), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['idea']\n",
      "1 ['is']\n",
      "14 ['tested', 'on', 'very', 'small', 'data', 'sets', '-LRB-', '80', 'and', '50', 'examples', ',', 'respectively', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 3), (4, 2)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  3\n",
      "1 ['Results']\n",
      "1 ['on']\n",
      "2 ['tiny', 'datasets']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 3), (6, 5), (7, 4), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['is']\n",
      "1 ['unclear']\n",
      "10 ['what', 'exactly', 'helps', ',', 'in', 'which', 'case', ',', 'and', 'why']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 4), (6, 2), (7, 2), (8, 2), (9, 3), (10, 2), (11, 3), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "2 ['3', '-RRB-']\n",
      "1 ['There']\n",
      "13 ['are', 'technical', 'issues', 'with', 'what', 'is', 'presented', ',', 'with', 'some', 'seemingly', 'factual', 'errors']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 8), (3, 8), (4, 9), (5, 6), (6, 6), (7, 6), (8, 3), (9, 2), (10, 2), (11, 4), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "2 ['For', 'example']\n",
      "1 [',']\n",
      "1 ['\"']\n",
      "8 ['In', 'this', 'case', 'we', 'could', 'apply', 'the', 'inversion']\n",
      "1 [',']\n",
      "1 ['however']\n",
      "14 ['it', 'is', 'much', 'more', 'convinient', '[', 'sic', ']', 'to', 'take', 'the', 'negative', 'of', 'distance']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 9), (5, 11), (6, 9), (7, 13), (8, 5), (9, 7), (10, 5), (11, 6), (12, 7), (13, 6), (14, 6), (15, 5), (16, 6), (17, 4), (18, 4), (19, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "depth:  2\n",
      "36 ['Number', '1', 'in', 'the', 'equation', 'stands', 'for', 'the', 'normalizing', ',', 'hence', 'the', 'similarity', 'is', 'defined', 'as', 'follows', '\"', '-', 'the', '1', 'does', 'not', 'stand', 'for', 'normalizing', ',', 'that', 'is', 'the', 'way', 'to', 'invert', 'the', 'cosine', 'distance']\n",
      "1 ['-LRB-']\n",
      "26 ['put', 'differently', ',', 'cosine', 'distance', 'is', '1', '-', 'cosine', 'similarity', ',', 'which', 'is', 'a', 'metric', 'in', 'Euclidean', 'space', 'due', 'to', 'the', 'properties', 'of', 'the', 'dot', 'product']\n",
      "1 ['-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 7), (5, 7), (6, 8), (7, 9), (8, 14), (9, 13), (10, 4), (11, 5), (12, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "15 ['Another', 'example', ',', '\"', 'are', 'obtained', 'using', 'the', 'GloVe', 'vector', ',', 'not', 'using', 'PPMI', '\"']\n",
      "1 ['-']\n",
      "1 ['there']\n",
      "29 ['are', 'close', 'relationships', 'between', 'what', 'GloVe', 'learns', 'and', 'PPMI', ',', 'which', 'the', 'authors', 'seem', 'unaware', 'of', '-LRB-', 'see', 'eg', ':', 'the', 'GloVe', 'paper', 'and', 'Omer', 'Levy', \"'s\", 'work', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 7), (5, 4), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "8 ['4', '-RRB-', 'Then', 'there', 'is', 'the', 'additional', 'question']\n",
      "1 [',']\n",
      "1 ['why']\n",
      "3 ['should', 'we', 'care']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 11), (5, 15), (6, 14), (7, 12), (8, 9), (9, 8), (10, 6), (11, 9), (12, 5), (13, 3), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "16 ['The', 'paper', 'does', 'not', 'really', 'motivate', 'why', 'it', 'is', 'important', 'to', 'score', 'well', 'on', 'these', 'tests']\n",
      "1 [':']\n",
      "37 ['these', 'kinds', 'of', 'tests', 'are', 'often', 'used', 'as', 'ways', 'to', 'measure', 'the', 'quality', 'of', 'word', 'embeddings', ',', 'but', 'in', 'this', 'case', 'the', 'main', 'contribution', 'is', 'the', 'similarity', 'metric', 'used', '*', 'on', 'top', '*', 'of', 'the', 'word', 'embeddings']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 6), (4, 9), (5, 4), (6, 1), (7, 2), (8, 3), (9, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "3 ['In', 'other', 'words']\n",
      "1 [',']\n",
      "9 ['what', 'is', 'supposed', 'to', 'be', 'the', 'take', '-', 'away']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "4 ['why', 'should', 'we', 'care']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 9), (5, 7), (6, 5), (7, 4), (8, 2), (9, 2), (10, 2), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "2 ['As', 'such']\n",
      "1 [',']\n",
      "7 ['I', 'do', 'not', 'recommend', 'it', 'for', 'acceptance']\n",
      "1 ['-']\n",
      "12 ['it', 'needs', 'significant', 'work', 'before', 'it', 'can', 'be', 'accepted', 'at', 'a', 'conference']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 8), (5, 4), (6, 8), (7, 5), (8, 1), (9, 2), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  3\n",
      "1 ['Minor']\n",
      "1 ['points']\n",
      "2 ['-', 'Typo']\n",
      "8 ['in', 'Equation', '10', '-', 'Typo', 'on', 'page', '6']\n",
      "8 ['-LRB-', '/', 'cite', 'instead', 'of', '\\\\', 'cite', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 1), (5, 2), (6, 2), (7, 2), (8, 2), (9, 7), (10, 8), (11, 4), (12, 4), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  9\n",
      "3 ['the', 'current', 'version']\n",
      "3 ['of', 'this', 'paper']\n",
      "1 ['is']\n",
      "1 ['not']\n",
      "1 ['ready']\n",
      "1 [',']\n",
      "5 ['as', 'it', 'is', 'poorly', 'written']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 6), (5, 10), (6, 9), (7, 11), (8, 6), (9, 2), (10, 4), (11, 8), (12, 5), (13, 5), (14, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['authors']\n",
      "30 ['present', 'some', 'observations', 'of', 'the', 'weaknesses', 'of', 'the', 'existing', 'vector', 'space', 'models', 'and', 'list', 'a', '6', '-', 'step', 'approach', 'for', 'refining', 'existing', 'word', 'vectors', '-LRB-', 'GloVe', 'in', 'this', 'work', '-RRB-']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "12 ['test', 'the', 'refined', 'vectors', 'on', '80', 'TOEFL', 'questions', 'and', '50', 'ESL', 'questions']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 4), (5, 3), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "6 ['In', 'addition', 'to', 'the', 'incoherent', 'presentation']\n",
      "1 [',']\n",
      "3 ['the', 'proposed', 'method']\n",
      "3 ['lacks', 'proper', 'justification']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 2)]\n",
      "[0, 1, 2, 3]\n",
      "depth:  2\n",
      "1 ['Pros']\n",
      "1 [':']\n",
      "1 ['1']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 9), (5, 9), (6, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "2 ['Experimental', 'study']\n",
      "2 ['on', 'retrofitting']\n",
      "7 ['existing', 'word', 'vectors', 'for', 'ESL', 'and', 'TOEFL']\n",
      "6 ['lexical', 'similarity', 'datasets', 'Cons', ':', '1']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 11), (5, 4)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "5 ['The', 'paper', 'is', 'poorly', 'written']\n",
      "1 ['and']\n",
      "7 ['the', 'proposed', 'methods', 'are', 'not', 'well', 'justified']\n",
      "1 ['.']\n",
      "2018_HyHmGyZCZ 1425\n",
      "[(0, 1), (1, 1), (2, 3), (3, 9), (4, 7), (5, 4), (6, 8), (7, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "5 ['suggests', 'taking', 'GloVe', 'word', 'vectors']\n",
      "1 [',']\n",
      "2 ['adjust', 'them']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "1 ['then']\n",
      "7 ['use', 'a', 'non-Euclidean', 'similarity', 'function', 'between', 'them']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 10), (5, 3), (6, 6), (7, 5), (8, 5), (9, 11)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "10 ['The', 'proposed', 'techniques', 'are', 'a', 'combination', 'of', 'previously', 'published', 'steps']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "18 ['the', 'new', 'algorithm', 'fails', 'to', 'reach', 'state', '-', 'of', '-', 'the', '-', 'art', 'on', 'the', 'tiny', 'data', 'sets']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 4), (6, 5), (7, 8), (8, 5), (9, 3), (10, 5), (11, 4), (12, 2), (13, 1), (14, 2), (15, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  3\n",
      "1 ['It']\n",
      "1 ['is']\n",
      "1 [\"n't\"]\n",
      "1 ['clear']\n",
      "20 ['what', 'the', 'authors', 'are', 'trying', 'to', 'prove', ',', 'nor', 'whether', 'they', 'have', 'successfully', 'proven', 'what', 'they', 'are', 'trying', 'to', 'prove']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 2), (5, 3), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['Is']\n",
      "2 ['the', 'point']\n",
      "6 ['that', 'GloVe', 'is', 'a', 'bad', 'algorithm']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 4), (5, 1)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['these']\n",
      "1 ['steps']\n",
      "1 ['are']\n",
      "1 ['general']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 8), (4, 3), (5, 6), (6, 2), (7, 3), (8, 3), (9, 2), (10, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "3 ['If', 'the', 'latter']\n",
      "1 [',']\n",
      "1 ['then']\n",
      "3 ['the', 'experimental', 'results']\n",
      "9 ['are', 'far', 'weaker', 'than', 'what', 'I', 'would', 'find', 'convincing']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 5)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  5\n",
      "1 ['on']\n",
      "1 ['multiple']\n",
      "1 ['different']\n",
      "1 ['word']\n",
      "1 ['embeddings']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 2), (5, 2), (6, 2), (7, 3), (8, 2), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "1 ['What']\n",
      "7 ['happens', 'if', 'you', 'start', 'with', 'random', 'vectors']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 2), (5, 2), (6, 3), (7, 3), (8, 3), (9, 5), (10, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  9\n",
      "3 ['a', 'bigger', 'data']\n",
      "1 ['set']\n",
      "1 ['a']\n",
      "2 ['more', 'complex']\n",
      "1 ['problem']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 5), (6, 5), (7, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['proposes']\n",
      "10 ['a', 'ranking', '-', 'based', 'similarity', 'metric', 'for', 'distributional', 'semantic', 'models']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 7), (5, 5), (6, 4), (7, 6), (8, 7), (9, 9), (10, 4), (11, 3), (12, 4), (13, 6), (14, 8), (15, 6), (16, 7), (17, 5), (18, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  2\n",
      "14 ['The', 'main', 'idea', 'is', 'to', 'learn', '\"', 'baseline', '\"', 'word', 'embeddings', ',', 'retrofitting', 'those']\n",
      "1 ['and']\n",
      "33 ['applying', 'localized', 'centering', ',', 'to', 'then', 'calculate', 'similarity', 'using', 'a', 'measure', 'called', '\"', 'Ranking', '-', 'based', 'Exponential', 'Similarity', 'Measure', '\"', '-LRB-', 'RESM', '-RRB-', ',', 'which', 'is', 'based', 'on', 'the', 'recently', 'proposed', 'APSyn', 'measure']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 9), (5, 4), (6, 4), (7, 5), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "8 ['I', 'think', 'the', 'work', 'has', 'several', 'important', 'issues']\n",
      "1 [':']\n",
      "9 ['1', '-RRB-', 'The', 'work', 'is', 'very', 'light', 'on', 'references']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 7), (5, 7), (6, 11), (7, 4), (8, 8), (9, 3), (10, 3), (11, 3), (12, 3), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "13 ['a', 'lot', 'of', 'previous', 'work', 'on', 'evaluating', 'similarity', 'in', 'word', 'embeddings', '-LRB-', 'eg']\n",
      "1 [':']\n",
      "3 ['Hill', 'et', 'al']\n",
      "1 [',']\n",
      "8 ['a', 'lot', 'of', 'the', 'papers', 'in', 'RepEval', 'workshops']\n",
      "1 [',']\n",
      "1 ['etc.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 13), (5, 17), (6, 16), (7, 14), (8, 6), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['-RRB-']\n",
      "1 [';']\n",
      "44 ['specialization', 'for', 'similarity', 'of', 'word', 'embeddings', '-LRB-', 'eg', ':', 'Kiela', 'et', 'al', ',', 'Mrksic', 'et', 'al', ',', 'and', 'many', 'others', '-RRB-', ';', 'multi-sense', 'embeddings', '-LRB-', 'eg', ':', 'from', 'Navigli', \"'s\", 'group', '-RRB-', ';', 'and', 'the', 'hubness', 'problem', '-LRB-', 'eg', ':', 'Dinu', 'et', 'al', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 2), (4, 4), (5, 3), (6, 5), (7, 5), (8, 5), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['For']\n",
      "10 ['the', 'localized', 'centering', 'approach', ',', 'Hara', 'et', 'al', \"'s\", 'introduced']\n",
      "1 ['that']\n",
      "1 ['method']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 6), (5, 4), (6, 3), (7, 3), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['None']\n",
      "3 ['of', 'this', 'work']\n",
      "1 ['is']\n",
      "6 ['cited', ',', 'which', 'I', 'find', 'inexcusable']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 4), (6, 5), (7, 5), (8, 2), (9, 5), (10, 2), (11, 2), (12, 5), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['results']\n",
      "1 ['are']\n",
      "1 ['also']\n",
      "22 ['presented', 'in', 'a', 'confusing', 'way', ',', 'with', 'the', 'current', 'state', 'of', 'the', 'art', 'results', 'separate', 'from', 'the', 'main', 'results', 'of', 'the', 'paper']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 3), (5, 6), (6, 7), (7, 7), (8, 8), (9, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "12 ['While', 'this', 'is', 'an', 'interesting', 'insight', ',', 'and', 'worthy', 'of', 'further', 'discussion']\n",
      "1 [',']\n",
      "3 ['such', 'a', 'claim']\n",
      "12 ['needs', 'backing', 'up', 'with', 'more', 'large', '-', 'scale', 'experiments', 'on', 'real', 'datasets']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 5), (5, 7), (6, 8), (7, 3), (8, 3), (9, 3), (10, 2), (11, 5), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "9 ['While', 'the', 'experiments', 'on', 'toy', 'tasks', 'is', 'clearly', 'useful']\n",
      "1 [',']\n",
      "2 ['the', 'paper']\n",
      "14 ['could', 'be', 'significantly', 'improved', 'by', 'adding', 'experiments', 'on', 'real', 'tasks', 'such', 'as', 'language', 'modelling']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 2), (6, 3), (7, 3), (8, 1), (9, 3), (10, 4), (11, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['paper']\n",
      "1 ['focuses']\n",
      "9 ['on', 'accelerating', 'RNN', 'by', 'applying', 'the', 'method', 'from', 'Blelloch']\n",
      "3 ['-LRB-', '1990', '-RRB-']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 9), (5, 6), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "4 ['The', 'application', 'is', 'straightforward']\n",
      "1 ['and']\n",
      "8 ['thus', 'technical', 'novelty', 'of', 'this', 'paper', 'is', 'limited']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 1)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "1 ['But']\n",
      "2 ['the', 'results']\n",
      "2 ['are', 'impressive']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 6), (6, 3), (7, 5), (8, 6), (9, 6), (10, 3), (11, 4), (12, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  3\n",
      "1 ['One']\n",
      "1 ['concern']\n",
      "1 ['is']\n",
      "18 ['the', 'proposed', 'technique', 'is', 'only', 'applied', 'for', 'few', 'types', 'of', 'RNNs', 'which', 'may', 'limit', 'its', 'applications', 'in', 'practice']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 5), (5, 3)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['Could']\n",
      "3 ['the', 'authors', 'comment']\n",
      "1 ['this']\n",
      "1 ['potential']\n",
      "1 ['limitation']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 9), (5, 5), (6, 1), (7, 2), (8, 1), (9, 4), (10, 5), (11, 6), (12, 9), (13, 8), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['#']\n",
      "3 ['Summary', 'and', 'Assessment']\n",
      "2 ['The', 'paper']\n",
      "1 ['addresses']\n",
      "23 ['an', 'important', 'issue', '–', 'that', 'of', 'making', 'learning', 'of', 'recurrent', 'networks', 'tractable', 'for', 'sequence', 'lengths', 'well', 'beyond', '1', '’', '000s', 'of', 'time', 'steps']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 2), (5, 2), (6, 3), (7, 6), (8, 9), (9, 6), (10, 6), (11, 9), (12, 4), (13, 8), (14, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "3 ['A', 'key', 'problem']\n",
      "1 ['here']\n",
      "33 ['is', 'that', 'processing', 'such', 'sequences', 'with', 'ordinary', 'RNNs', 'requires', 'a', 'reduce', 'operation', ',', 'where', 'the', 'output', 'of', 'the', 'net', 'at', 'time', 'step', 't', 'depends', 'on', 'the', 'outputs', 'of', '*', 'all', '*', 'its', 'predecessor']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 6), (5, 3), (6, 5), (7, 9), (8, 8), (9, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "2 ['The', 'authors']\n",
      "1 ['now']\n",
      "25 ['make', 'a', 'crucial', 'observation', ',', 'namely', 'that', 'a', 'certain', 'class', 'of', 'RNNs', 'allows', 'evaluation', 'in', 'a', 'non-linear', 'fashion', 'through', 'a', 'so', '-', 'called', 'SCAN', 'operator']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 7), (4, 8), (5, 8), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['Here']\n",
      "1 [',']\n",
      "5 ['if', 'certain', 'conditions', 'are', 'satisfied']\n",
      "1 [',']\n",
      "5 ['the', 'calculation', 'of', 'the', 'output']\n",
      "4 ['can', 'be', 'parallelised', 'massively']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 4), (5, 4), (6, 7), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "3 ['In', 'the', 'following']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "9 ['explore', 'the', 'landscape', 'of', 'RNNs', 'satisfying', 'the', 'necessary', 'conditions']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2), (6, 2), (7, 3), (8, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['performance']\n",
      "1 ['is']\n",
      "7 ['investigated', 'in', 'terms', 'of', 'wall', 'clock', 'time']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 5), (5, 2), (6, 3), (7, 3), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['Further']\n",
      "1 [',']\n",
      "9 ['experimental', 'results', 'of', 'problems', 'with', 'previously', 'untacked', 'sequence', 'lengths']\n",
      "2 ['are', 'reported']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 7), (4, 4), (5, 2), (6, 3), (7, 4), (8, 6), (9, 4), (10, 7), (11, 4), (12, 3), (13, 3), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['is']\n",
      "1 ['certainly']\n",
      "1 ['relevant']\n",
      "1 [',']\n",
      "20 ['as', 'it', 'can', 'pave', 'the', 'way', 'towards', 'the', 'application', 'of', 'recurrent', 'architectures', 'to', 'problems', 'that', 'have', 'extremely', 'long', 'term', 'dependencies']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 2)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "2 ['To', 'me']\n",
      "1 [',']\n",
      "2 ['the', 'execution']\n",
      "2 ['seems', 'sound']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['experiments']\n",
      "1 ['back']\n",
      "3 ['up', 'the', 'claim']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 4), (5, 4), (6, 2), (7, 9), (8, 14), (9, 12), (10, 11), (11, 2), (12, 2), (13, 5), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "2 ['##', 'Minor']\n",
      "1 ['-']\n",
      "42 ['I', 'challenge', 'the', 'claim', 'that', 'thousands', 'and', 'millions', 'of', 'time', 'steps', 'are', 'a', 'common', 'issue', 'in', '“', 'robotics', ',', 'remote', 'sensing', ',', 'control', 'systems', ',', 'speech', 'recognition', ',', 'medicine', 'and', 'finance', '”', ',', 'as', 'claimed', 'in', 'the', 'first', 'paragraph', 'of', 'the', 'introduction']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 10), (5, 9), (6, 12), (7, 8), (8, 3), (9, 3), (10, 1), (11, 3), (12, 3), (13, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "15 ['IMHO', ',', 'most', 'problems', 'in', 'these', 'domains', 'get', 'away', 'with', 'a', 'few', 'hundred', 'time', 'steps']\n",
      "1 [';']\n",
      "18 ['nevertheless', ',', 'I', '’d', 'appreciate', 'a', 'few', 'examples', 'where', 'this', 'is', 'a', 'case', 'to', 'better', 'justify', 'the', 'method']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 10), (5, 9), (6, 7), (7, 7), (8, 5), (9, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "3 ['This', 'paper', 'abstracts']\n",
      "11 ['two', 'recently', '-', 'proposed', 'RNN', 'variants', 'into', 'a', 'family', 'of', 'RNNs']\n",
      "1 ['called']\n",
      "3 ['the', 'Linear', 'Surrogate']\n",
      "10 ['RNNs', 'which', 'satisfy', 'Blelloch', \"'s\", 'criteria', 'for', 'parallelizable', 'sequential', 'computation']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 6), (5, 2), (6, 4), (7, 3), (8, 3), (9, 2), (10, 2), (11, 2), (12, 3), (13, 2), (14, 5), (15, 6), (16, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "2 ['The', 'authors']\n",
      "1 ['then']\n",
      "25 ['propose', 'an', 'efficient', 'parallel', 'algorithm', 'for', 'this', 'class', 'of', 'RNNs', ',', 'which', 'produces', 'speedups', 'over', 'the', 'existing', 'implements', 'of', 'Quasi-RNN', ',', 'SRU', ',', 'and', 'LSTM']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 9), (4, 11), (5, 11), (6, 2), (7, 4), (8, 3), (9, 2), (10, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "4 ['Apart', 'from', 'efficiency', 'results']\n",
      "1 [',']\n",
      "2 ['the', 'paper']\n",
      "1 ['also']\n",
      "22 ['contributes', 'a', 'comparison', 'of', 'model', 'convergence', 'on', 'a', 'long', '-', 'term', 'dependency', 'task', 'due', 'to', '-LRB-', 'Hochreiter', 'and', 'Schmidhuber', ',', '1997', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 6), (4, 12), (5, 9), (6, 5), (7, 5), (8, 9), (9, 5)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  3\n",
      "4 ['A', 'novel', 'linearized', 'version']\n",
      "3 ['of', 'the', 'LSTM']\n",
      "10 ['outperforms', 'traditional', 'LSTM', 'on', 'this', 'long', '-', 'term', 'dependency', 'task']\n",
      "1 [',']\n",
      "1 ['and']\n",
      "12 ['raises', 'questions', 'about', 'whether', 'RNNs', 'and', 'LSTMs', 'truly', 'need', 'the', 'nonlinear', 'structure']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 4), (6, 2), (7, 4), (8, 5), (9, 2), (10, 1), (11, 2), (12, 2), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['is']\n",
      "15 ['written', 'very', 'well', ',', 'with', 'explanation', '-LRB-', 'as', 'opposed', 'to', 'obfuscation', '-RRB-', 'as', 'the', 'goal']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 7), (6, 5), (7, 2), (8, 2), (9, 2), (10, 1), (11, 2), (12, 3), (13, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  3\n",
      "1 ['Linear']\n",
      "1 ['Surrogate']\n",
      "1 ['RNNs']\n",
      "1 ['is']\n",
      "18 ['an', 'important', 'concept', 'that', 'is', 'useful', 'to', 'understand', 'RNN', 'variants', 'today', ',', 'and', 'potentially', 'other', 'future', 'novel', 'architectures']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 5), (6, 5), (7, 3), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['paper']\n",
      "1 ['provides']\n",
      "4 ['argument', 'and', 'experimental', 'evidence']\n",
      "7 ['against', 'the', 'rotation', 'used', 'typically', 'in', 'RNNs']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 4)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "2 ['4', '-RRB-']\n",
      "2 ['The', 'author']\n",
      "5 ['missed', 'some', 'important', 'baselines', 'here']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 11), (3, 10), (4, 8), (5, 15), (6, 11), (7, 6), (8, 7), (9, 8), (10, 7), (11, 10)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['However']\n",
      "1 [',']\n",
      "19 ['it', \"'s\", 'not', 'completely', 'fair', 'to', 'compare', 'a', 'label', '-', 'noise', '+', 'semi-supervised', 'method', 'with', 'other', 'label', '-', 'noise']\n",
      "1 ['only']\n",
      "1 ['methods']\n",
      "1 ['...']\n",
      "5 ['As', 'a', 'matter', 'of', 'fact']\n",
      "1 [',']\n",
      "1 ['you']\n",
      "21 ['do', \"n't\", 'need', 'to', 'apply', 'perturbation', 'consistency', '-LRB-', 'or', 'other', 'semi-supervised', '-RRB-', 'regularization', 'after', 'identifying', 'the', 'training', 'data', 'with', 'incorrect', 'labels']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 2), (5, 2), (6, 5), (7, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "1 ['Experiments']\n",
      "12 ['are', 'conducted', 'on', 'various', 'dataset', 'CIFAR10', ',', 'CIFAR', '-', '100', 'and', 'ImageNet']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 10), (4, 6), (5, 4), (6, 3), (7, 4), (8, 5), (9, 2), (10, 2), (11, 2), (12, 1), (13, 2), (14, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "4 ['For', 'the', 'experimental', 'comparisons']\n",
      "1 [',']\n",
      "2 ['the', 'authors']\n",
      "2 ['at', 'least']\n",
      "18 ['should', 'report', 'the', 'acc', 'on', 'clean', 'test', 'set', ',', 'which', 'is', 'useful', 'for', 'understanding', 'the', 'ideal', 'case', 'performance']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 1)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "2 ['3', '-RRB-']\n",
      "2 ['The', 'comparisons']\n",
      "3 ['are', 'not', 'fair']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 7), (5, 5), (6, 9), (7, 1), (8, 3), (9, 4), (10, 5), (11, 5), (12, 5), (13, 8), (14, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "30 ['This', 'paper', 'proposed', '\"', 'self', '-', 'ensemble', 'label', 'filtering', '\"', 'for', 'learning', 'with', 'noisy', 'labels', 'where', 'the', 'label', 'noise', 'is', 'instance', '-', 'independent', '-LRB-', 'in', 'fact', ',', 'the', 'noise', 'model']\n",
      "6 ['is', 'the', 'class', '-', 'conditional', 'noise']\n",
      "1 ['-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 10), (5, 8), (6, 11), (7, 4), (8, 5), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "15 ['Among', 'the', 'existing', 'directions', 'in', 'this', 'area', ',', 'it', 'falls', 'into', 'the', 'sample', 'selection', 'direction']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "13 ['it', 'also', 'takes', 'semi-supervised', 'learning', 'based', 'on', 'the', 'likely', 'noisy', 'data', 'into', 'account']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 2)]\n",
      "[0, 1, 2, 3]\n",
      "depth:  2\n",
      "1 ['Novelty']\n",
      "1 [':']\n",
      "1 ['borderline']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 5), (5, 7), (6, 2), (7, 3), (8, 5), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "8 ['As', 'other', 'sample', 'selection', 'methods', ',', 'the', 'proposed']\n",
      "1 ['one']\n",
      "10 ['would', 'like', 'to', 'identify', 'the', 'training', 'data', 'with', 'correct', 'labels']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 6), (6, 5), (7, 4), (8, 2), (9, 4), (10, 5), (11, 5), (12, 7), (13, 7), (14, 9), (15, 14), (16, 7), (17, 8), (18, 2), (19, 4), (20, 4), (21, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "depth:  3\n",
      "1 ['What']\n",
      "2 [\"'s\", 'new']\n",
      "1 ['is']\n",
      "53 ['that', 'the', 'authors', '\"', 'form', 'running', 'averages', 'of', 'predictions', 'over', 'the', 'entire', 'training', 'dataset', 'using', 'the', 'network', 'output', 'at', 'different', 'training', 'epochs', '\"', 'and', 'show', 'that', '\"', 'these', 'ensemble', 'estimates', 'yield', 'more', 'accurate', 'identification', 'of', 'inconsistent', 'predictions', 'throughout', 'training', 'than', 'the', 'single', 'estimates', 'of', 'the', 'network', 'at', 'the', 'most', 'recent', 'training', 'epoch', '\"']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 5), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  5\n",
      "1 ['the']\n",
      "1 ['major']\n",
      "1 ['contribution']\n",
      "1 ['of']\n",
      "2 ['the', 'paper']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 7), (5, 5), (6, 5), (7, 5), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "1 ['Furthermore']\n",
      "1 [',']\n",
      "7 ['the', 'data', 'likely', 'to', 'have', 'incorrect', 'labels']\n",
      "10 ['are', 'not', 'thrown', 'away', 'but', 'used', 'in', 'a', 'semi-supervised', 'manner']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 5), (5, 2), (6, 5), (7, 3), (8, 4), (9, 7), (10, 6), (11, 4), (12, 4), (13, 7), (14, 4), (15, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  3\n",
      "1 ['This']\n",
      "1 ['is']\n",
      "3 ['a', 'minor', 'contribution']\n",
      "1 [',']\n",
      "25 ['because', 'semi-supervised', 'learning', 'is', 'orthogonal', 'to', 'label', '-', 'noise', 'learning', 'and', 'everybody', 'in', 'this', 'area', 'knows', 'the', 'combination', 'of', 'them', 'can', 'work', 'better', 'in', 'practice']\n",
      "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 2), (5, 2), (6, 5), (7, 6), (8, 10), (9, 4), (10, 2), (11, 1), (12, 2), (13, 2), (14, 2), (15, 4), (16, 2), (17, 2), (18, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  6\n",
      "1 ['this']\n",
      "1 ['is']\n",
      "10 ['an', 'academic', '/', 'scientific', 'paper', ',', 'not', 'an', 'industrial', 'product']\n",
      "1 [',']\n",
      "12 ['so', 'you', 'do', \"n't\", 'need', 'to', 'combine', 'all', 'things', 'that', 'might', 'work']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 2), (4, 1)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "1 ['Significance']\n",
      "1 [':']\n",
      "1 ['high']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 3)]\n",
      "[0, 1, 2, 3, 4]\n",
      "depth:  2\n",
      "3 ['The', 'proposed', 'method']\n",
      "1 ['significantly']\n",
      "4 ['outperformed', 'all', 'baseline', 'methods']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 5), (4, 8), (5, 5)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  3\n",
      "2 ['Semi-supervised', 'regularization']\n",
      "5 ['such', 'as', 'virtual', 'adversarial', 'training']\n",
      "1 ['can']\n",
      "1 ['even']\n",
      "3 ['improve', 'supervised', 'learning']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 4), (4, 8), (5, 7), (6, 3), (7, 4), (8, 5), (9, 6), (10, 5), (11, 10), (12, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "1 ['Issues']\n",
      "1 [':']\n",
      "9 ['It', \"'s\", 'known', 'under', 'class', '-', 'conditional', 'noise', 'model']\n",
      "1 [',']\n",
      "25 ['the', 'backward', 'loss', 'correction', 'is', 'the', 'unique', 'way', 'to', 'estimate', 'the', 'classification', 'risk', '-LRB-', 'or', 'equivalently', ',', 'the', 'classification', 'accuracy', '-RRB-', 'given', 'noisy', 'validation', 'data']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 8), (6, 3), (7, 3), (8, 4), (9, 4), (10, 7), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "1 ['So']\n",
      "1 ['how']\n",
      "24 ['can', 'the', 'validation', '-LRB-', 'i.e.', ',', 'hyperparameter', 'tuning', '-RRB-', 'be', 'performed', 'for', 'the', 'proposed', 'and', 'baseline', 'methods', 'in', 'Table', '1', 'given', 'noisy', 'validation', 'data']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 6), (4, 4), (5, 1), (6, 3), (7, 4), (8, 3), (9, 4), (10, 2), (11, 4), (12, 4), (13, 5), (14, 7), (15, 6), (16, 6), (17, 5), (18, 4), (19, 2), (20, 2), (21, 2), (22, 3), (23, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "depth:  2\n",
      "1 ['---']\n",
      "1 ['Overall']\n",
      "1 ['---']\n",
      "2 ['This', 'paper']\n",
      "37 ['proposes', 'an', 'algorithm', 'for', 'learning', 'from', 'data', 'with', 'noisy', 'labels', 'which', 'alternates', 'between', 'updating', 'the', 'model', 'and', 'removing', 'samples', 'that', 'look', 'like', 'they', 'have', 'noisy', 'labels', ',', 'thereby', 'allowing', 'the', 'training', 'procedure', 'to', 'focus', 'on', 'clean', 'samples']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 6), (5, 7), (6, 10)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['Overall']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "17 ['found', 'the', 'paper', 'very', 'well', '-', 'written', ',', 'the', 'proposed', 'approach', 'reasonable', ',', 'and', 'the', 'experiments', 'convincing']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 6), (4, 8), (5, 4), (6, 6), (7, 5), (8, 6), (9, 3), (10, 3), (11, 3), (12, 5), (13, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "depth:  2\n",
      "15 ['I', 'have', 'some', 'questions', 'about', 'what', 'assumptions', 'are', 'required', 'for', 'such', 'a', 'procedure', 'to', 'work']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "10 ['in', 'general', ',', 'I', 'think', 'this', 'is', 'a', 'strong', 'paper']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 8), (5, 10), (6, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['---']\n",
      "6 ['Major', 'comments', '---', '1', '-RRB-', 'I']\n",
      "10 ['found', 'it', 'somewhat', 'unclear', 'how', 'large', 'the', 'methodological', 'contribution', 'was']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 3), (5, 4), (6, 1), (7, 3), (8, 3), (9, 5), (10, 7), (11, 5), (12, 9), (13, 6), (14, 4), (15, 2), (16, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "depth:  2\n",
      "2 ['In', 'particular']\n",
      "1 [',']\n",
      "28 ['has', 'the', 'approach', 'of', 'filtering', 'out', 'samples', 'based', 'on', 'disagreement', 'with', 'predictions', 'from', 'the', 'model', 'been', 'tried', 'before', '-LRB-', 'ie', 'the', 'primary', 'contribution', 'is', 'self', '-', 'ensembling', '-RRB-']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 5), (5, 3), (6, 5), (7, 6), (8, 5), (9, 4), (10, 6), (11, 9), (12, 9), (13, 4), (14, 5), (15, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Depth to split: 4\n",
      "depth:  4\n",
      "1 ['the']\n",
      "1 ['proposed']\n",
      "1 ['method']\n",
      "1 ['includes']\n",
      "30 ['multiple', 'pieces', '-LRB-', 'Mean', 'teacher', '+', 'iteratively', 'creating', 'a', 'filtered', 'dataset', '+', 'self', '-', 'ensembling', '-RRB-', ',', 'I', 'recommend', 'being', '*', 'very', '*', 'explicit', 'about', 'which', 'parts', 'are', 'new', 'contributions']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 6), (5, 5), (6, 3), (7, 3), (8, 2), (9, 4), (10, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "3 ['With', 'that', 'said']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "1 ['greatly']\n",
      "12 ['appreciated', 'the', 'ablation', 'experiments', 'which', 'really', 'highlight', 'the', 'importance', 'of', 'each', 'piece']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 2), (5, 2), (6, 2), (7, 3), (8, 2), (9, 5), (10, 2), (11, 3), (12, 3), (13, 1), (14, 2), (15, 3), (16, 3), (17, 4), (18, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "1 ['I']\n",
      "22 ['do', \"n't\", 'feel', 'like', 'I', 'have', 'a', 'good', 'sense', 'for', 'what', 'assumptions', 'need', 'to', 'be', 'satisfied', 'for', 'the', 'proposed', 'method', 'to', 'work']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 7), (3, 8), (4, 8), (5, 6), (6, 8), (7, 8), (8, 3), (9, 2), (10, 2), (11, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "2 ['For', 'example']\n",
      "1 [',']\n",
      "3 ['in', 'section', '2.2']\n",
      "2 ['the', 'authors']\n",
      "22 ['say', '\"', 'If', 'the', 'noise', 'is', 'sufficiently', 'random', ',', 'the', 'set', 'of', 'correct', 'labels', 'will', 'be', 'representative', 'to', 'achieve', 'high', 'model', 'performance']\n",
      "1 ['\"']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 3), (5, 5), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  5\n",
      "1 ['by']\n",
      "1 ['\"']\n",
      "2 ['sufficiently', 'random']\n",
      "1 ['\"']\n",
      "1 ['here']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 5), (5, 2)]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "depth:  2\n",
      "1 ['Is']\n",
      "1 ['there']\n",
      "6 ['a', 'formal', 'version', 'of', 'this', 'assumption']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 1), (6, 2), (7, 2), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "3 ['Do', 'any', 'independence']\n",
      "1 ['or']\n",
      "6 ['positivity', 'assumptions', 'need', 'to', 'be', 'satisfied']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 4), (4, 2), (5, 2), (6, 3), (7, 6), (8, 2), (9, 2), (10, 2), (11, 3), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "2 ['Most', 'importantly']\n",
      "1 [',']\n",
      "1 ['what']\n",
      "12 ['happens', 'when', 'the', 'label', 'noise', 'does', 'not', 'look', 'like', 'what', 'you', 'expect']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 3), (4, 2), (5, 1), (6, 2), (7, 3), (8, 3), (9, 2), (10, 2), (11, 5), (12, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  11\n",
      "1 ['the']\n",
      "1 ['failure']\n",
      "1 ['modes']\n",
      "1 ['of']\n",
      "2 ['the', 'algorithm']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 3), (5, 2), (6, 2), (7, 3), (8, 4), (9, 3), (10, 5), (11, 4), (12, 9), (13, 6), (14, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "2 ['For', 'example']\n",
      "1 [',']\n",
      "24 ['what', 'happens', 'when', 'label', 'errors', 'are', 'concentrated', 'in', 'a', 'particular', 'region', 'of', 'the', 'feature', 'space', '-LRB-', 'or', 'just', 'generally', 'depend', 'on', 'the', 'features', '-RRB-']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 6), (5, 8), (6, 11), (7, 2), (8, 5), (9, 2), (10, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "depth:  2\n",
      "11 ['In', 'this', 'case', ',', 'even', 'if', 'the', 'filtering', 'procedure', 'work', 'perfectly']\n",
      "1 [',']\n",
      "3 ['the', 'filtered', 'dataset']\n",
      "16 ['will', 'have', 'a', 'different', 'feature', 'distribution', 'than', 'the', 'data', 'distribution', 'leading', 'to', 'potential', 'covariate', 'shift', 'problems']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 7), (4, 5), (5, 6), (6, 6), (7, 3), (8, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  2\n",
      "6 ['If', 'I', 'understood', 'the', 'experiments', 'correctly']\n",
      "1 [',']\n",
      "2 ['the', 'method']\n",
      "9 ['was', 'only', 'tested', 'on', 'label', '-', 'depended', 'noise', 'models']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 3), (4, 4), (5, 5), (6, 7), (7, 4), (8, 5), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['3']\n",
      "1 ['-RRB-']\n",
      "10 ['Along', 'the', 'same', 'lines', ':', 'what', 'are', 'the', 'necessary', 'conditions']\n",
      "6 ['to', 'guarantee', 'that', 'this', 'procedure', 'converges']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 6), (5, 9), (6, 2), (7, 2), (8, 6), (9, 3), (10, 1), (11, 2), (12, 4), (13, 2), (14, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "depth:  2\n",
      "18 ['While', 'the', 'authors', 'suggest', 'that', 'self', '-', 'ensembling', 'prevents', 'samples', 'from', 'oscillating', 'in', 'and', 'out', 'of', 'training', 'set']\n",
      "1 [',']\n",
      "8 ['is', 'this', 'a', 'guarantee', 'or', 'an', 'empirical', 'observation']\n",
      "1 ['?']\n",
      "[(0, 1), (1, 1), (2, 6), (3, 7), (4, 5), (5, 4), (6, 5), (7, 10), (8, 7), (9, 9), (10, 9), (11, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "2 ['More', 'broadly']\n",
      "1 [',']\n",
      "1 ['it']\n",
      "36 ['is', 'not', 'totally', 'clear', 'what', 'the', 'filtering', 'does', 'to', 'the', 'objective', 'function', 'or', 'whether', 'this', 'procedure', 'is', 'even', 'formally', 'optimizing', 'a', 'well', 'specified', 'objective', 'function', '-LRB-', 'potentially', 'some', 'temperature', 'limit', 'of', 'a', 'soft', '-', 'weighted', 'objective']\n",
      "1 ['?']\n",
      "1 ['-RRB-']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 7), (5, 9), (6, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  2\n",
      "1 ['---']\n",
      "9 ['Minor', 'comments', '---', '1', '-RRB-', 'Figures', '1', 'and', '4']\n",
      "9 ['are', 'not', 'readable', 'in', 'black', 'and', 'grey', '-', 'scale']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 5), (4, 1), (5, 3), (6, 2), (7, 1), (8, 3), (9, 4), (10, 1), (11, 2), (12, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "2 ['2', '-RRB-']\n",
      "1 ['I']\n",
      "11 ['would', 'front', '-', 'load', 'the', 'justification', 'for', 'using', 'self', '-', 'ensembling']\n",
      "1 ['.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 8), (3, 8), (4, 8), (5, 9), (6, 6), (7, 4), (8, 2), (9, 2), (10, 2), (11, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "depth:  2\n",
      "2 ['In', 'particular']\n",
      "1 [',']\n",
      "1 ['I']\n",
      "13 ['think', 'the', 'two', 'sentences', 'starting', 'with', '\"', 'When', 'learning', 'under', 'label', 'noise', ',']\n",
      "1 ['...']\n",
      "1 ['\"']\n",
      "8 ['on', 'page', '2', 'could', 'be', 'moved', 'much', 'earlier']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 5), (4, 9), (5, 5), (6, 4), (7, 2), (8, 4), (9, 3), (10, 2), (11, 3), (12, 4)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "depth:  2\n",
      "13 ['3', '-RRB-', 'I', \"'ll\", 'be', 'interested', 'to', 'see', 'what', 'the', 'other', 'reviewers', 'say']\n",
      "1 [',']\n",
      "1 ['but']\n",
      "7 ['I', 'found', 'Figure', '2', 'hard', 'to', 'follow']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 6), (4, 7), (5, 6), (6, 4), (7, 3), (8, 3), (9, 3), (10, 4), (11, 4), (12, 2), (13, 2), (14, 2), (15, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "depth:  2\n",
      "2 ['4', '-RRB-']\n",
      "5 ['The', 'formatting', 'of', 'Section', '4.2.4']\n",
      "20 ['makes', 'it', 'a', 'bit', 'hard', 'to', 'figure', 'out', 'where', 'the', 'text', 'starts', '-LRB-', 'as', 'opposed', 'to', 'the', 'table', 'captions', '-RRB-']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 3), (4, 4), (5, 2), (6, 5), (7, 8), (8, 5), (9, 6)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Summary']\n",
      "1 [':']\n",
      "21 ['The', 'paper', 'proposed', 'a', 'self', '-', 'ensemble', 'label', 'filtering', '-LRB-', 'SELF', '-RRB-', 'method', 'to', 'deal', 'with', 'the', 'noisy', 'label', 'learning', 'problem']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 5), (3, 9), (4, 10), (5, 11), (6, 8), (7, 8), (8, 4), (9, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "11 ['They', 'progressively', 'filter', 'out', 'the', 'wrong', 'labels', 'during', 'training', ',', 'i.e.']\n",
      "1 [',']\n",
      "2 ['filtered', 'samples']\n",
      "19 ['are', 'removed', 'entirely', 'from', 'the', 'supervised', 'training', 'loss', ',', 'and', 'are', 'leveraged', 'via', 'semi-supervised', 'learning', 'in', 'the', 'unsupervised', 'loss']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 3), (5, 4), (6, 3), (7, 3), (8, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "depth:  3\n",
      "1 ['The']\n",
      "1 ['filtering']\n",
      "1 ['is']\n",
      "8 ['based', 'on', 'identification', 'of', 'inconsistent', 'predictions', 'throughout', 'training']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 2), (6, 3), (7, 1), (8, 2), (9, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Strengths']\n",
      "1 [':']\n",
      "10 ['1', '-RRB-', 'The', 'motivation', 'of', 'the', 'paper', 'is', 'very', 'clear']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 4), (4, 6), (5, 2), (6, 2), (7, 1), (8, 3), (9, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "depth:  2\n",
      "1 ['Weakness']\n",
      "1 [':']\n",
      "9 ['1', '-RRB-', 'The', 'contribution', 'of', 'SELF', 'is', 'not', 'clear']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 2), (5, 3), (6, 2)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['Just']\n",
      "1 ['a']\n",
      "1 ['combining']\n",
      "5 ['of', 'several', 'previously', 'proposed', 'components']\n",
      "[(0, 1), (1, 1), (2, 4), (3, 7), (4, 7), (5, 5), (6, 2), (7, 1)]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "depth:  2\n",
      "2 ['3', '-RRB-']\n",
      "7 ['The', 'organization', 'of', 'the', 'tables', 'and', 'figures']\n",
      "5 ['are', 'somehow', 'hard', 'to', 'read']\n",
      "1 ['.']\n",
      "[(0, 1), (1, 1), (2, 3), (3, 4), (4, 4), (5, 2), (6, 3)]\n",
      "[0, 1, 2, 3, 4, 5, 6]\n",
      "depth:  3\n",
      "1 ['SELF']\n",
      "1 ['incorporate']\n",
      "2 ['semi-supervised', 'techniques']\n",
      "4 ['while', 'baselines', 'are', 'not']\n"
     ]
    }
   ],
   "source": [
    "chunks_from_sent = {\"mcomp\": [], \"nmcomp\": []}\n",
    "\n",
    "for pid in gt_dict:\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = get_chunks(df.loc[mcs][\"Sent\"])\n",
    "            chunks_from_sent[\"mcomp\"].append((df.loc[mcs][\"Sent\"], mcomp_chunks_from_sent))\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs)\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = get_chunks(df.loc[mcs][\"Sent\"])\n",
    "            chunks_from_sent[\"nmcomp\"].append((df.loc[mcs][\"Sent\"], mcomp_chunks_from_sent))\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It would be interesting to explore the practicability of the method on more large-scale experiments on image related tasks.',\n",
       "  ['the practicability',\n",
       "   'of the method',\n",
       "   'on',\n",
       "   'more large - scale experiments',\n",
       "   'on image related tasks']),\n",
       " ('I think a clearer emphasis on the novelty, eg: current algorithm with mixing rate analyses or more thorough empirical comparisons will make the paper stronger for resubmission.',\n",
       "  ['I',\n",
       "   'think a clearer emphasis on the novelty , eg',\n",
       "   ':',\n",
       "   'current algorithm with mixing rate analyses or more thorough empirical comparisons',\n",
       "   'will make the paper stronger for resubmission',\n",
       "   '.']),\n",
       " ('The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).',\n",
       "  ['The',\n",
       "   'authors',\n",
       "   'propose',\n",
       "   'k - DPP',\n",
       "   'as an open loop -LRB- oblivious to the evaluation of configurations -RRB- method for hyperparameter optimization',\n",
       "   'and',\n",
       "   'provide',\n",
       "   'its empirical study and comparison',\n",
       "   'with other methods such as grid search , uniform random search , low - discrepancy Sobol sequences , BO - TPE -LRB- Bayesian optimization using tree - structured Parzen estimator -RRB-',\n",
       "   'by Bergstra et al -LRB- 2011 -RRB-']),\n",
       " ('Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20) The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.',\n",
       "  ['Second',\n",
       "   ',',\n",
       "   'their study',\n",
       "   'only',\n",
       "   'applies to a small number like 3 - 6 hyperparameters with a small k = 20',\n",
       "   '-RRB-',\n",
       "   'The real challenge',\n",
       "   'lies in scaling up to many hyperparameters or even k - DPP sampling for larger k. Third',\n",
       "   ',',\n",
       "   'the authors',\n",
       "   'do not compare against some relevant , recent work , e.g.',\n",
       "   ',',\n",
       "   'Springenberg et al',\n",
       "   '-LRB- http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf -RRB- and Snoek et al -LRB- https://arxiv.org/pdf/1502.05700.pdf -RRB- that is essential for this kind of empirical study',\n",
       "   '.']),\n",
       " ('COMMENTS ON THE CHANGES SINCE THE LAST YEAR\\nI am not convinced by the comparison with Spearmint added by the authors since the previous version.',\n",
       "  ['COMMENTS',\n",
       "   'ON',\n",
       "   'THE CHANGES SINCE THE LAST YEAR I am not convinced by the comparison with Spearmint',\n",
       "   'by',\n",
       "   'added',\n",
       "   'by',\n",
       "   'the authors since the previous version']),\n",
       " ('It would be stronger with more empirical interrogation of why this works and exploration of the nearby conceptual space.',\n",
       "  ['more empirical interrogation',\n",
       "   'of why this works',\n",
       "   'exploration',\n",
       "   'of the nearby conceptual space']),\n",
       " ('The experimental validation is also not extensive since comparison to SOTA is not included.',\n",
       "  ['The',\n",
       "   'experimental',\n",
       "   'validation',\n",
       "   'is',\n",
       "   'also',\n",
       "   'not',\n",
       "   'extensive',\n",
       "   'since',\n",
       "   'comparison to SOTA is not included']),\n",
       " ('If possible, could the author provide results on different architecture choices for the stolen model as well as the surrogate model?',\n",
       "  ['If possible',\n",
       "   ',',\n",
       "   'could',\n",
       "   'the author',\n",
       "   'provide results on different architecture choices for the stolen model as well as the surrogate model',\n",
       "   '?']),\n",
       " ('Including results on a dataset like ImageNet would be nice.',\n",
       "  ['Including results on', 'a dataset like ImageNet', 'would be nice', '.']),\n",
       " ('Authors perform experiments on numerous tasks showing that SRU performs on par with LSTMs, but the baselines for these tasks are a little problematic (see below).',\n",
       "  ['Authors',\n",
       "   'perform experiments on numerous tasks showing that SRU performs on par with LSTMs',\n",
       "   ',',\n",
       "   'but',\n",
       "   'the baselines for these tasks',\n",
       "   'are a little problematic -LRB- see below -RRB-',\n",
       "   '.']),\n",
       " ('On the negative side, the authors present the results without fully referencing and acknowledging state-of-the-art.',\n",
       "  ['On the negative side',\n",
       "   ',',\n",
       "   'the authors',\n",
       "   'present',\n",
       "   'the results',\n",
       "   'without fully referencing and acknowledging state - of - the - art',\n",
       "   '.']),\n",
       " ('As another example: Table 5 that presents results for English-German WMT translation only compares to OpenNMT setups with maximum BLEU about 21) But already a long time ago Wu et al presented LSTMs reaching 25 BLEU and current SOTA is above 28 with training time much faster than those early models (https://arxiv.org/abs/1706.03762).',\n",
       "  ['As another example : Table 5 that presents results for English - German WMT translation only compares to OpenNMT setups with maximum BLEU about 21',\n",
       "   '-RRB-',\n",
       "   'But',\n",
       "   'already a long time ago Wu et al presented LSTMs reaching 25 BLEU',\n",
       "   'and',\n",
       "   'current SOTA',\n",
       "   'is above 28 with training time much faster than those early models',\n",
       "   '-LRB- https://arxiv.org/abs/1706.03762 -RRB-',\n",
       "   '.']),\n",
       " ('While the latest are non-RNN architectures, a table like Table 5 should include them too, for a fair presentation.',\n",
       "  ['While',\n",
       "   'the latest are non-RNN architectures',\n",
       "   ',',\n",
       "   'a table like Table 5',\n",
       "   'should',\n",
       "   'include them too , for a fair presentation',\n",
       "   '.']),\n",
       " ('What is left is the gated incremental pooling operation; but to show that this operation is beneficial when added to autoregressive CNNs, a thorough comparison with an autoregressive CNN baseline is necessary.',\n",
       "  ['What is left',\n",
       "   'is the gated incremental pooling operation',\n",
       "   ';',\n",
       "   'but',\n",
       "   'to show that this operation is beneficial when added to autoregressive CNNs',\n",
       "   ',',\n",
       "   'a thorough comparison with an autoregressive CNN baseline',\n",
       "   'is necessary',\n",
       "   '.']),\n",
       " ('- Slightly unfortunate naming that does not account for autoregressive CNNs\\n- Lack of comparison with autoregressive CNN baselines, which signals a major conceptual error in the paper.',\n",
       "  ['- Slightly unfortunate',\n",
       "   'naming',\n",
       "   'that',\n",
       "   'does not account for autoregressive CNNs - Lack of comparison with autoregressive CNN baselines , which signals a major conceptual error in the paper']),\n",
       " ('This might be another baseline to consider for comparison.',\n",
       "  ['This', 'might', 'be', 'another baseline to consider for comparison']),\n",
       " ('The parts that a bit lacking with the current version of the paper in this are the evaluation tasks are few and a bit simple and I think there needs to be more discussion on the \"coverage\" of the intrinsic reward types.',\n",
       "  ['The parts',\n",
       "   'that',\n",
       "   'a bit lacking with the current version of the paper in this are the evaluation tasks are few and a bit simple and I think there',\n",
       "   'needs',\n",
       "   'to be more discussion on the \" coverage \" of the intrinsic reward types']),\n",
       " ('It would be better to use more standard tasks if they are available.',\n",
       "  ['more', 'standard', 'tasks', 'if', 'they are available']),\n",
       " ('- The fact that applying intrinsic motivation to multi-agent simulations seems like a natural idea would be to convert the problem to a \"single\" agent problem to compare against the \"normal\" application of intrinsic rewards.',\n",
       "  ['-',\n",
       "   'The fact',\n",
       "   'that',\n",
       "   'applying intrinsic motivation to multi-agent simulations seems like a natural idea',\n",
       "   'would',\n",
       "   'be to convert the problem to a \" single \" agent problem to compare against the \" normal \" application of intrinsic rewards',\n",
       "   '.']),\n",
       " ('Tests on another dataset would have been welcomed.',\n",
       "  ['Tests', 'on another dataset', 'would', 'have been welcomed']),\n",
       " ('The paper does not consider the more recent and highly relevant Moosavi-Dezfooli et al “Universal Adversarial Perturbations” CVPR 2017.',\n",
       "  ['The',\n",
       "   'paper',\n",
       "   'does',\n",
       "   'not',\n",
       "   'consider',\n",
       "   'the more recent and highly relevant Moosavi - Dezfooli et al “ Universal Adversarial Perturbations ” CVPR 2017']),\n",
       " ('The distance metrics that are considered are only L_inf and L1, whereas it would be interesting to see more relevant “perceptual losses” such as those used in style transfer and domain adaptation with GANs.',\n",
       "  ['The distance metrics',\n",
       "   'that are considered',\n",
       "   'are',\n",
       "   'only',\n",
       "   'L_inf and L1',\n",
       "   ',',\n",
       "   'whereas',\n",
       "   'it would be interesting to see more relevant “ perceptual losses ” such as those used in style transfer and domain adaptation with GANs']),\n",
       " (\"In summary, while I think the paper is interesting, I suspect that the applicability of this technique is possibly limited at present, and I'm unsure how much we can really read into the findings of the paper when the experiments are based on MNIST alone.\",\n",
       "  ['In summary',\n",
       "   ',',\n",
       "   'while',\n",
       "   'I think the paper is interesting',\n",
       "   'I',\n",
       "   ',',\n",
       "   'I',\n",
       "   'suspect that the applicability of this technique is possibly limited at present',\n",
       "   ',',\n",
       "   'and',\n",
       "   'I',\n",
       "   \"'m unsure how much we can really read into the findings of the paper when the experiments are based on MNIST alone\",\n",
       "   '.']),\n",
       " ('Not only synthetic data but also several popularly-used data and models are being conducted and compared.',\n",
       "  ['Not only synthetic data',\n",
       "   'but also',\n",
       "   'several popularly - used data',\n",
       "   'models',\n",
       "   'are',\n",
       "   'being conducted and compared']),\n",
       " ('To provide better understanding, the paper evaluates the performance on synthesized digit sequence data as well as several sentence-encoding tasks.',\n",
       "  ['To provide better understanding',\n",
       "   ',',\n",
       "   'the paper',\n",
       "   'evaluates',\n",
       "   'the performance',\n",
       "   'on synthesized digit sequence data as well as several sentence - encoding tasks',\n",
       "   '.']),\n",
       " ('I would be willing to increase my score if the authors added a comparison to Wei and Ma ‘19, and more evidence was provided that the bound is tighter for typical convolutional networks found in practice (please see detailed comments below).',\n",
       "  ['I',\n",
       "   'would be willing to increase my score if the authors added a comparison to Wei and Ma ‘ 19',\n",
       "   ',',\n",
       "   'and',\n",
       "   'more evidence',\n",
       "   'was provided that the bound is tighter for typical convolutional networks found in practice -LRB- please see detailed comments below -RRB-',\n",
       "   '.']),\n",
       " ('An analysis of a few networks used in practice would make the comparison more meaningful (included the comparison to Wei and Ma).',\n",
       "  ['An analysis',\n",
       "   'of',\n",
       "   'a few networks used in practice',\n",
       "   'would',\n",
       "   'make',\n",
       "   'the comparison more meaningful -LRB- included the comparison to Wei and Ma -RRB-']),\n",
       " ('How do the bounds presented in the paper compare to Wei and Ma bounds?',\n",
       "  ['How',\n",
       "   'do',\n",
       "   'the bounds',\n",
       "   'presented in the paper',\n",
       "   'compare',\n",
       "   'to Wei and Ma bounds']),\n",
       " ('I appreciate that the authors added a detailed comparison to Bartlett et al ‘17 bound.',\n",
       "  ['the',\n",
       "   'authors',\n",
       "   'added',\n",
       "   'a detailed comparison',\n",
       "   'to',\n",
       "   'Bartlett et al ‘ 17 bound']),\n",
       " ('The algorithm is evaluated and compared to the state-of-the-art on various image classification tasks and on RNN.',\n",
       "  ['The',\n",
       "   'algorithm',\n",
       "   'is',\n",
       "   'evaluated',\n",
       "   'compared',\n",
       "   'to the state - of - the - art on various image classification tasks and on RNN']),\n",
       " ('However, the search space is different between CoNAS and the others methods for some experiments, making it difficult to decide if the search strategy of CoNAS is definitely competitive compared to other methods or not.',\n",
       "  ['However',\n",
       "   ',',\n",
       "   'the search space',\n",
       "   'different between CoNAS and the others methods for some experiments',\n",
       "   ',',\n",
       "   'making it difficult to decide if the search strategy of CoNAS is definitely competitive compared to other methods or not',\n",
       "   '.']),\n",
       " ('Also, in general, a discussion on the efficiency of training the proposed model as compared to TreeNN would be helpful.',\n",
       "  ['Also',\n",
       "   ',',\n",
       "   'in general',\n",
       "   ',',\n",
       "   'a discussion',\n",
       "   'on the efficiency of training the proposed model as compared to TreeNN',\n",
       "   'would be helpful',\n",
       "   '.']),\n",
       " ('Several different encoding benchmarks of the entailment task are designed to compare against the performance of the proposed model, using a newly created dataset.',\n",
       "  ['Several different encoding benchmarks',\n",
       "   'of the entailment task',\n",
       "   'are',\n",
       "   'designed',\n",
       "   'to compare against the performance of the proposed model',\n",
       "   ',',\n",
       "   'using a newly created dataset']),\n",
       " ('One weakness with the paper was that it was only tested on 1 dataset.',\n",
       "  ['One weakness',\n",
       "   'with the paper',\n",
       "   'was',\n",
       "   'that',\n",
       "   'it was only tested on 1 dataset']),\n",
       " ('The paper would have been improved through testing of multiple datasets, and not just on there self generated dataset, but the contribution of their research on their network and older networks is still justification enough for this paper.',\n",
       "  ['The paper would have been improved through testing of multiple datasets',\n",
       "   'not',\n",
       "   'just on there self generated dataset',\n",
       "   ',',\n",
       "   'but',\n",
       "   'the contribution of their research on their network and older networks',\n",
       "   'is still justification enough for this paper',\n",
       "   '.']),\n",
       " ('- There were no comparisons with baseline models or different model architectures.',\n",
       "  ['-',\n",
       "   'There',\n",
       "   'no',\n",
       "   'comparisons',\n",
       "   'with',\n",
       "   'baseline models',\n",
       "   'or',\n",
       "   'different model architectures']),\n",
       " ('I would like to see some results on the same structure, but with an Linear model, MLP or LSTM across the time dimension, or search through different types of convolutional networks.',\n",
       "  ['see',\n",
       "   'some results',\n",
       "   'on the same structure , but with an Linear model',\n",
       "   ',',\n",
       "   'MLP or LSTM',\n",
       "   'across the time dimension',\n",
       "   ',',\n",
       "   'or',\n",
       "   'search',\n",
       "   'through different types of convolutional networks']),\n",
       " ('Would the approach work as well using a more standard encoder-decoder model with determinstic Z?',\n",
       "  ['Would',\n",
       "   'the approach work',\n",
       "   'as well using a more standard encoder - decoder model with determinstic Z',\n",
       "   '?']),\n",
       " ('The data is derived from an online repository of ~1500 Android apps, and from that were extracted ~150k methods, which makes the data very respectable in terms of realisticness and scale.',\n",
       "  ['The data',\n",
       "   'is derived from an online repository of ~ 1500 Android apps',\n",
       "   ',',\n",
       "   'and',\n",
       "   'from that',\n",
       "   'were extracted',\n",
       "   '~ 150k methods , which makes the data very respectable in terms of realisticness and scale',\n",
       "   '.']),\n",
       " ('To demonstrate that this is a broadly applicable family, it would also be good to do experiments on a more standard datasets like MNIST.',\n",
       "  ['To demonstrate that this is a broadly applicable family',\n",
       "   ',',\n",
       "   'it',\n",
       "   'would',\n",
       "   'also',\n",
       "   'be good to do experiments on a more standard datasets like MNIST',\n",
       "   '.']),\n",
       " ('The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM.',\n",
       "  ['The experimental results',\n",
       "   'are very good for document modeling',\n",
       "   ',',\n",
       "   'but',\n",
       "   'without ablation analysis against the baseline',\n",
       "   'it',\n",
       "   'is hard to see why they should be with such a small modification in G - NVDM',\n",
       "   '.']),\n",
       " ('Overall, the very strong improvements on the text modeling task over NVDM seem hard to understand, and I would like to see an ablation analysis of all the differences between that model and the proposed one.',\n",
       "  ['Overall',\n",
       "   'the very strong improvements on the text modeling task over NVDM',\n",
       "   'seem hard to understand',\n",
       "   ',',\n",
       "   'and',\n",
       "   'I',\n",
       "   'would like to see an ablation analysis of all the differences between that model and the proposed one',\n",
       "   '.']),\n",
       " ('It is expected to see whether the proposed method is also effective for image classification.',\n",
       "  ['the',\n",
       "   'proposed',\n",
       "   'method',\n",
       "   'is',\n",
       "   'also',\n",
       "   'effective for image classification']),\n",
       " ('More datasets for evaluation are needed, even only for the object detection application.',\n",
       "  ['More datasets',\n",
       "   'for evaluation',\n",
       "   'are',\n",
       "   'needed',\n",
       "   ',',\n",
       "   'even only for the object detection application']),\n",
       " ('(-) More than two datasets are necessary to show the effectiveness of the methods\\ncomments)\\n- What is the higher level feature map P_m?',\n",
       "  ['-LRB- - -RRB-',\n",
       "   'More than two datasets',\n",
       "   'are',\n",
       "   'necessary to show the effectiveness of the methods comments -RRB- - What is the higher level feature map P_m',\n",
       "   '?']),\n",
       " ('The authors should discuss their difference with self-paced learning.',\n",
       "  ['The',\n",
       "   'authors',\n",
       "   'should',\n",
       "   'discuss',\n",
       "   'their difference',\n",
       "   'with self - paced learning']),\n",
       " ('I do not know the reason why the authors only apply for object detection on a very specific dataset.',\n",
       "  ['I',\n",
       "   'do',\n",
       "   'not',\n",
       "   'know',\n",
       "   'the reason',\n",
       "   'why the authors only apply for object detection on a very specific dataset']),\n",
       " ('Paper Strengths: \\n-- Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting very large datasets in a computationally feasible manner\\n-- The effective batch size for training the MoE drastically increased also\\n-- Interesting experimental results on the effects of increasing the number of MoEs, which is expected.',\n",
       "  ['Paper Strengths',\n",
       "   ':',\n",
       "   'Elegant use',\n",
       "   'of MoE for expanding model capacity and enabling training large models necessary for exploiting very large datasets in a computationally feasible manner -- The effective batch size for training the MoE drastically increased also -- Interesting experimental results on the effects of increasing the number of MoEs , which is expected',\n",
       "   '.']),\n",
       " ('There are also some glitches in the writing, eg: the end of Section 3.1) \\n- The paper is missing some important references in conditional computation (eg: https://arxiv.org/pdf/1308.3432.pdf) which deal with very similar issues in deep learning.',\n",
       "  ['There',\n",
       "   'are also some glitches in the writing , eg',\n",
       "   ':',\n",
       "   'the end of Section 3.1 -RRB- - The paper',\n",
       "   'is missing some important references in conditional computation -LRB- eg : https://arxiv.org/pdf/1308.3432.pdf -RRB- which deal with very similar issues in deep learning',\n",
       "   '.']),\n",
       " ('A much simpler version of a similar trick may have been proposed in the context of memory networks, also for ICLR\\'17 (see match type in \"LEARNING END-TO-END GOAL-ORIENTED DIALOG\" by Bordes et al)\\n\\nThe authors also mention the time and size needed to train the model: is the issue arising for learning, inference or both?',\n",
       "  ['A much simpler version of a similar trick',\n",
       "   'may have been proposed in the context of memory networks , also for ICLR \\'17 -LRB- see match type in \" LEARNING END - TO - END GOAL - ORIENTED DIALOG \" by Bordes et al -RRB-',\n",
       "   'also',\n",
       "   'The authors',\n",
       "   'also',\n",
       "   'mention the time and size needed to train the model',\n",
       "   ':',\n",
       "   'is',\n",
       "   'the issue',\n",
       "   'arising for learning , inference or both',\n",
       "   '?']),\n",
       " ('Even if the assumption does not hold, relatively close values for average activation between the networks would make the comparison more convincing.',\n",
       "  ['Even',\n",
       "   'if',\n",
       "   'the assumption does not hold',\n",
       "   ',',\n",
       "   'relatively close values',\n",
       "   'for average activation between the networks',\n",
       "   'would',\n",
       "   'make the comparison more convincing',\n",
       "   '.']),\n",
       " (\"(Nit: I don't see measurements for the full-precision baseline).\",\n",
       "  ['-LRB-',\n",
       "   'Nit',\n",
       "   ':',\n",
       "   'I',\n",
       "   \"do n't see measurements for the full - precision baseline -RRB-\",\n",
       "   '.']),\n",
       " ('I would have loved to see a SOTA result on ImageNet and a result on a strong LSTM baseline to be fully convinced.',\n",
       "  ['a SOTA result on ImageNet',\n",
       "   'and',\n",
       "   'a result',\n",
       "   'on a strong LSTM baseline',\n",
       "   'to be fully convinced']),\n",
       " ('It goes on to provide an exhaustive analysis of performance (essentially no loss) on real benchmarks (this paper is remarkably MNIST-free).',\n",
       "  ['It',\n",
       "   'goes',\n",
       "   'on',\n",
       "   'to provide an exhaustive analysis of performance -LRB- essentially no loss -RRB- on real benchmarks',\n",
       "   '-LRB-',\n",
       "   '-RRB-',\n",
       "   '-LRB-',\n",
       "   'this paper is remarkably MNIST - free',\n",
       "   '-RRB-']),\n",
       " ('I also agree with Reviewer 2 that there is a lack of comparison against baselines.',\n",
       "  ['I',\n",
       "   'also',\n",
       "   'agree',\n",
       "   'with Reviewer 2',\n",
       "   'that there is a lack of comparison against baselines',\n",
       "   '.']),\n",
       " ('How does the method work on established semantic segmentation datasets with many classes, such as PASCAL?',\n",
       "  ['the',\n",
       "   'method',\n",
       "   'work',\n",
       "   'on',\n",
       "   'established semantic segmentation datasets with many classes',\n",
       "   ',',\n",
       "   'such as PASCAL']),\n",
       " ('Even the ADE20K dataset, from which this paper samples, is substantially larger and has an established benchmarking methodology (see http://placeschallenge.csail.mit.edu/).',\n",
       "  ['Even the ADE20K dataset',\n",
       "   ',',\n",
       "   'from which this paper samples',\n",
       "   ',',\n",
       "   'is substantially larger',\n",
       "   'and',\n",
       "   'has an established benchmarking methodology',\n",
       "   '-LRB- see http://placeschallenge.csail.mit.edu/ -RRB-']),\n",
       " ('An additional problem is that performance is not compared to any external prior work.',\n",
       "  ['An',\n",
       "   'additional',\n",
       "   'problem',\n",
       "   'is',\n",
       "   'that',\n",
       "   'performance is not compared to any external prior work']),\n",
       " ('Only simple baselines (eg: autoencoder, kmeans) implemented by this paper are included.',\n",
       "  ['Only simple baselines',\n",
       "   '-LRB-',\n",
       "   'eg : autoencoder , kmeans',\n",
       "   '-RRB-',\n",
       "   'implemented by this paper',\n",
       "   'are',\n",
       "   'included']),\n",
       " ('How well does the approach compare to supervised CNNs on an established segmentation task?',\n",
       "  ['How',\n",
       "   'well',\n",
       "   'does',\n",
       "   'the approach',\n",
       "   'compare',\n",
       "   'to supervised CNNs',\n",
       "   'on an established segmentation task']),\n",
       " ('Note that the proposed method need not necessarily outperform supervised approaches, but the reader should be provided with some idea of the size of the gap between this unsupervised method and the state-of-the-art supervised approach.',\n",
       "  ['the proposed method',\n",
       "   'need not necessarily outperform supervised approaches',\n",
       "   ',',\n",
       "   'but',\n",
       "   'the reader',\n",
       "   'should be provided with some idea of the size of the gap between this unsupervised method',\n",
       "   'state - of - the - art',\n",
       "   'supervised approach']),\n",
       " ('(3) Regarding the fine-tuning baselines, the comparison is a bit unfair since the proposed method performs pooling over images, while the baseline (average mask) is not translation invariant.',\n",
       "  ['-LRB- 3 -RRB-',\n",
       "   'Regarding',\n",
       "   'the fine - tuning baselines',\n",
       "   ',',\n",
       "   'the comparison',\n",
       "   'a bit unfair since the proposed method performs pooling over images',\n",
       "   ',',\n",
       "   'while the baseline -LRB- average mask -RRB- is not translation invariant',\n",
       "   '.']),\n",
       " ('Even if it does work as advertised, the utilization of implicit labels would make it subject to comparisons with a lot of weakly-supervised learning papers with far better results than shown in this paper.',\n",
       "  ['Even',\n",
       "   'if',\n",
       "   'it does work as advertised',\n",
       "   ',',\n",
       "   'the utilization of implicit labels',\n",
       "   'would',\n",
       "   'make it subject to comparisons with a lot of weakly - supervised learning papers with far better results than shown in this paper',\n",
       "   '.']),\n",
       " ('The authors present a convincing set of results over many translation tasks and compare with very competitive baselines.',\n",
       "  ['The',\n",
       "   'authors',\n",
       "   'present',\n",
       "   'a convincing set of results over many translation tasks',\n",
       "   'and',\n",
       "   'compare with very competitive baselines']),\n",
       " ('I suggest the authors compare their model with these approaches. [1] Spatially Adaptive Computation Time for Residual Networks., Figurnov et al\\u2028\\n[2] Recurrent Models of Visual Attention, Mnih et al\\n\\u2028[3] Action recognition using visual attention, Sharma et al\\n\\u2028[4] End-to-end learning of action detection from frame glimpses in videos, Yeung et al\\n\\u2028[5] Two-Stream Convolutional Networks for Action Recognition in Videos, Simonyan et al\\u2028\\n',\n",
       "  ['the', 'authors', 'compare', 'their model', 'with these approaches']),\n",
       " ('In this case, the approach should be also compared to the state-of-the-art tracking approaches (that are cheaper to acquire) in terms of computational efficiency and performance.',\n",
       "  ['In this case',\n",
       "   ',',\n",
       "   'the approach',\n",
       "   'should',\n",
       "   'be also compared to the state - of - the - art tracking approaches -LRB- that are cheaper to acquire -RRB- in terms of computational efficiency and performance',\n",
       "   '.']),\n",
       " ('The authors should make more comprehensive evaluation on the larger dataset.',\n",
       "  ['The',\n",
       "   'authors',\n",
       "   'should',\n",
       "   'make',\n",
       "   'more comprehensive evaluation',\n",
       "   'on the larger dataset']),\n",
       " ('The authors term this as dynamic convolution and evaluate this method on the SSD architecture across datasets like PETS, AVSS, VIRAT.',\n",
       "  ['The authors term this as dynamic convolution',\n",
       "   'and',\n",
       "   'evaluate this method on the SSD architecture across datasets like PETS',\n",
       "   ',',\n",
       "   'AVSS , VIRAT',\n",
       "   '.']),\n",
       " ('Paper weaknesses\\n- A simple baseline that only processes a frame if \\\\sum_{ij} D_{ij} exceeds a threshold is never mentioned or compared against.',\n",
       "  ['Paper weaknesses',\n",
       "   '-',\n",
       "   'A simple baseline',\n",
       "   'that only processes a frame if \\\\ sum _ { ij } D _ { ij } exceeds a threshold',\n",
       "   'is',\n",
       "   'never',\n",
       "   'mentioned or compared against']),\n",
       " ('In general, the paper does not compare against any other existing work which reduces compute for video analysis, e.g., tracking.',\n",
       "  ['In general',\n",
       "   ',',\n",
       "   'the paper',\n",
       "   'does',\n",
       "   'not',\n",
       "   'compare against any other existing work which reduces compute for video analysis , e.g. , tracking',\n",
       "   '.']),\n",
       " ('- Overall, I think this paper can be substantially improved in terms of providing details on the proposed approach and comparing against baselines to demonstrate that Dynamic-Convolutions are helpful.',\n",
       "  ['- Overall , I',\n",
       "   'think this paper',\n",
       "   'can',\n",
       "   'be',\n",
       "   'substantially',\n",
       "   'improved in terms of providing details on the proposed approach and comparing against baselines to demonstrate that Dynamic - Convolutions are helpful']),\n",
       " ('- State of the art is not well-studied in the paper.',\n",
       "  ['- State', 'of the art', 'is', 'not', 'well - studied', 'in the paper']),\n",
       " ('Minor comments:\\n- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.',\n",
       "  ['Minor comments',\n",
       "   ':',\n",
       "   '-',\n",
       "   'I',\n",
       "   'believe',\n",
       "   'one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models',\n",
       "   '.']),\n",
       " ('Can authors provide empirical comparison between blind-spot attacks and the work by Song et al (2018), e.g., attack success rate & distortion?',\n",
       "  ['Can',\n",
       "   'authors',\n",
       "   'provide',\n",
       "   'empirical comparison between blind - spot attacks and the work by Song et al -LRB- 2018 -RRB- , e.g. , attack success rate & distortion',\n",
       "   '?']),\n",
       " ('(1)  A deep BNN to show that the cumulative error is negligible as the number of the hidden layers increases \\n(2)  Small latent dimension since CLT may not hold\\n(3)  A heavy-tailed variational distribution since the second moment may not be finite \\n(4)  Other nonlinear activations since the Gaussian approximation may not be accurate due to (generalized) Berry-Esseen theorem\\n(5) A BNN with skip connections  since a Bayesian multiplayer perceptron with skip connections is also a feed-forward BNN\\n \\nAmong these cases, I am eager to see some results on a deep thin BNN.',\n",
       "  ['-LRB- 1 -RRB-',\n",
       "   'A deep BNN to show that the cumulative error is negligible as the number of the hidden layers increases -LRB- 2 -RRB- Small latent dimension since CLT may not hold -LRB- 3 -RRB- A heavy - tailed variational distribution since the second moment may not be finite -LRB- 4 -RRB- Other nonlinear activations since the Gaussian approximation may not be accurate due to -LRB- generalized -RRB- Berry - Esseen theorem -LRB- 5 -RRB- A BNN with skip connections since a Bayesian multiplayer perceptron with skip connections',\n",
       "   'is also a feed - forward BNN',\n",
       "   'Among these cases',\n",
       "   ',',\n",
       "   'I',\n",
       "   'am eager to see some results on a deep thin BNN',\n",
       "   '.']),\n",
       " ('Furthermore, I would like to see some empirical comparison on real-world datasets between DVI and MCVI under a *fixed* prior since such comparison demonstrates the approximation accuracy of DVI and rule out the confounding factor introduced by the empirical Bayes approach.',\n",
       "  ['Furthermore',\n",
       "   ',',\n",
       "   'I',\n",
       "   'would',\n",
       "   'like to see some empirical comparison on real - world datasets between DVI and MCVI under a * fixed * prior since such comparison demonstrates the approximation accuracy of DVI and rule out the confounding factor introduced by the empirical Bayes approach',\n",
       "   '.']),\n",
       " ('Why do you not also compare against this, and show it really\\n   does not work?',\n",
       "  ['Why',\n",
       "   'do',\n",
       "   'you',\n",
       "   'also',\n",
       "   'compare against this , and show it really does not work']),\n",
       " ('The experiments are for rather small datasets and for the DVI method if I understand correctly only models with a single hidden layer are considered.',\n",
       "  ['The',\n",
       "   'experiments',\n",
       "   'are',\n",
       "   'for rather small datasets',\n",
       "   'and',\n",
       "   'for the DVI method',\n",
       "   'if',\n",
       "   'I understand correctly only models with a single hidden layer are considered']),\n",
       " ('If so a comparison of DVI with MCVI in a more complex example is of interest.',\n",
       "  ['If',\n",
       "   'so',\n",
       "   'a comparison of DVI with MCVI in a more complex example',\n",
       "   'is of interest',\n",
       "   '.']),\n",
       " ('Why not evaluate at least dDVI with diagonal q(w) on\\n   some much larger models and datasets?',\n",
       "  ['at least',\n",
       "   'dDVI',\n",
       "   'with',\n",
       "   'diagonal q -LRB- w -RRB-',\n",
       "   'on some much larger models and datasets']),\n",
       " ('- Experiments are OK, but on pretty small datasets, and for single hidden\\n   layer NNs.',\n",
       "  ['- Experiments',\n",
       "   'are',\n",
       "   'OK',\n",
       "   ',',\n",
       "   'but',\n",
       "   'on pretty small datasets',\n",
       "   ',',\n",
       "   'and',\n",
       "   'for single hidden layer NNs']),\n",
       " ('On such data and models, the Barber&Bishop 98 method could\\n   be run as well\\n- Was MCVI run with re-parameterization?',\n",
       "  ['On',\n",
       "   'such data and models , the Barber & Bishop 98 method could be run as well',\n",
       "   '-',\n",
       "   'Was MCVI run with re-parameterization',\n",
       "   '?']),\n",
       " ('- Why not show the PBP-1 results, comparing to dDVI, in the main text?',\n",
       "  ['Why',\n",
       "   'not',\n",
       "   'show the PBP - 1',\n",
       "   'results',\n",
       "   'comparing to dDVI , in the main text']),\n",
       " ('However, I was hoping to see a direct comparison between FP16 and INT16.',\n",
       "  ['However',\n",
       "   ',',\n",
       "   'I',\n",
       "   'was',\n",
       "   'hoping to see a direct comparison between FP16 and INT16',\n",
       "   '.']),\n",
       " ('- For reference, please include wallclock time and actual overall memory consumption comparisons of the proposed methods and other methods as well as the baseline (default FP32 training).',\n",
       "  ['-',\n",
       "   'For reference',\n",
       "   ',',\n",
       "   'please',\n",
       "   'include wallclock time',\n",
       "   'and',\n",
       "   'actual overall memory',\n",
       "   'consumption comparisons of the proposed methods and other methods as well as the baseline',\n",
       "   '-LRB- default FP32 training -RRB-',\n",
       "   '.']),\n",
       " ('There have also been a number of papers on “automatic post editing”, including the shared task at WMT2016, and there are not only standard test sets and baselines, but also datasets that could actually be used to train a post editing model with human-generated data.',\n",
       "  ['There',\n",
       "   'have also been a number of papers on “ automatic post editing ” , including the shared task at WMT2016',\n",
       "   ',',\n",
       "   'and',\n",
       "   'there',\n",
       "   'are not only standard test sets and baselines , but also datasets that could actually be used to train a post editing model with human - generated data',\n",
       "   '.']),\n",
       " ('Specific comments:\\n- It would be interesting to see what the improvements are if the baseline model is a neural system.',\n",
       "  ['Specific comments',\n",
       "   ':',\n",
       "   '- It',\n",
       "   'would be interesting to see what the improvements are if the baseline model is a neural system']),\n",
       " ('Indeed, unsurprisingly, the authors note that \"the probability of correctly labelling a word as a mistake remains low (62%)\" - this admittedly beats a random-chance baseline, but is not compared to something more meaningful, such as simply contrasting the existing system with a more powerful convolutional model and labelling all discrepancies as mistakes.',\n",
       "  ['Indeed',\n",
       "   ',',\n",
       "   'unsurprisingly',\n",
       "   'the authors',\n",
       "   'note that \" the probability of correctly labelling a word as a mistake remains low -LRB- 62 % -RRB- \"',\n",
       "   '-',\n",
       "   'this',\n",
       "   'admittedly',\n",
       "   'beats a random - chance baseline , but is not compared to something more meaningful',\n",
       "   ',',\n",
       "   'such',\n",
       "   'as simply contrasting the existing system with a more powerful convolutional model and labelling all discrepancies as mistakes',\n",
       "   '.']),\n",
       " ('Although I do like the paper on the whole, to really convince me that main objective -- ie that **iterative** improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model.',\n",
       "  ['Although',\n",
       "   'I do like the paper on the whole , to really convince me that main objective -- ie that',\n",
       "   '**',\n",
       "   'iterative',\n",
       "   '**',\n",
       "   'improvement is beneficial',\n",
       "   '--',\n",
       "   'has',\n",
       "   'been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular , to show that an iterative refinement scheme can really improve over a system closely matched to the attention - based model , both when used in isolation and when used in system combination with a PBMT system , and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention - based model',\n",
       "   '.']),\n",
       " ('- How does the approach compare to a model that simply re-ranks the k-best output?',\n",
       "  ['How',\n",
       "   'does',\n",
       "   'the approach',\n",
       "   'compare',\n",
       "   'to a model that simply re-ranks the k - best output',\n",
       "   '?']),\n",
       " ('Experimental results on two popular benchmark datasets validate the advantage of DMPN over other state-of-the-art methods.',\n",
       "  ['Experimental results',\n",
       "   'on two popular benchmark datasets',\n",
       "   'validate',\n",
       "   'the advantage of DMPN',\n",
       "   'over',\n",
       "   'other state - of - the - art methods']),\n",
       " ('- I am concerned about whether the proposed method works well with harder datasets such as Office-Home dataset, because each class data are modeled by a simple Gaussian distribution in the proposed method.',\n",
       "  ['-',\n",
       "   'I',\n",
       "   'am',\n",
       "   'concerned about whether the proposed method works well with harder datasets such as Office - Home dataset , because each class data are modeled by a simple Gaussian distribution in the proposed method',\n",
       "   '.']),\n",
       " ('- The paper is a bit hard to follow, and would be improved by giving a more explicit comparison of the methods used here to past work, especially [1] and [3].',\n",
       "  ['-',\n",
       "   'The paper',\n",
       "   'is a bit hard to follow',\n",
       "   'would be improved by giving a more explicit comparison of the methods used here to past work , especially [ 1 ] and',\n",
       "   '[ 3 ]',\n",
       "   '.']),\n",
       " ('- The effectiveness of the contributions is validated on multiple UDA tasks, and the ablative analysis supports the claims (that prototype-level alignment and within-class compactness helps).',\n",
       "  ['-',\n",
       "   'The effectiveness of the contributions',\n",
       "   'is validated on multiple UDA tasks',\n",
       "   ',',\n",
       "   'and',\n",
       "   'the ablative analysis',\n",
       "   'supports the claims -LRB- that prototype - level alignment and within - class compactness helps -RRB-',\n",
       "   '.']),\n",
       " ('Recently there is a paper [2] about model selection for UDA, maybe the authors should try it.',\n",
       "  ['Recently',\n",
       "   'there',\n",
       "   'is a paper [ 2 ] about model selection for UDA',\n",
       "   ',',\n",
       "   'maybe',\n",
       "   'the authors',\n",
       "   'should try it',\n",
       "   '.']),\n",
       " ('Significant work could also be done to explore the effect of using different neural network structures for the NRT - in this paper only a fairly simple 3 layer architecture is used.',\n",
       "  ['Significant',\n",
       "   'work',\n",
       "   'could',\n",
       "   'also',\n",
       "   'be',\n",
       "   'done to explore the effect of using different neural network structures for the NRT - in this paper only a fairly simple 3 layer architecture is used']),\n",
       " ('The proposed approach is an elaborate extension of this approach, but if we want prediction performance for regression, we would use some ensembles of regression trees such as Random forest, GBDT, ExtraTrees, ... instead of a single CART.',\n",
       "  ['The proposed approach',\n",
       "   'is an elaborate extension of this approach',\n",
       "   ',',\n",
       "   'but',\n",
       "   'if we want prediction performance for regression',\n",
       "   'we',\n",
       "   'we',\n",
       "   'would use some ensembles of regression trees such as Random forest , GBDT , ExtraTrees , ... instead of a single CART',\n",
       "   '.']),\n",
       " ('Over all the paper is well written and easy to follow but is limited by its lack of well detailed motivation and insufficient baselines and applied tasks.',\n",
       "  ['Over',\n",
       "   'all',\n",
       "   'the',\n",
       "   'paper',\n",
       "   'is',\n",
       "   'well written and easy to follow',\n",
       "   'but',\n",
       "   'is',\n",
       "   'limited by its lack of well detailed motivation and insufficient baselines',\n",
       "   'and',\n",
       "   'applied tasks']),\n",
       " ('The experiments against CART and SVR would be too naive in the current context of supervised learning.',\n",
       "  ['The experiments',\n",
       "   'against CART and SVR',\n",
       "   'would',\n",
       "   'be',\n",
       "   'too naive in the current context of supervised learning']),\n",
       " ('The presented experiments are also not thorough, there are stronger and simpler baselines for regression like random forests, gradient boosted trees  or kernel ridge regression which are not evaluated and compared.',\n",
       "  ['The presented experiments',\n",
       "   'are also not thorough',\n",
       "   ',',\n",
       "   'there',\n",
       "   'are stronger and simpler baselines for regression like random forests',\n",
       "   ',',\n",
       "   'gradient',\n",
       "   'boosted trees or kernel ridge regression which are not evaluated and compared',\n",
       "   '.']),\n",
       " ('- The experiments on single datasets of a very specific speaker profiling problem would be somewhat misleading.',\n",
       "  ['-',\n",
       "   'The experiments',\n",
       "   'on single datasets of a very specific speaker profiling problem',\n",
       "   'would be somewhat misleading',\n",
       "   '.']),\n",
       " ('On the other hand, if this is for benchmarking purpose, a regression by neural nets and tree ensemble (random forest or something) can be included as other baselines, and also other types of regression problems can be tested.',\n",
       "  ['On the other hand',\n",
       "   ',',\n",
       "   'if',\n",
       "   'this is for benchmarking purpose',\n",
       "   ',',\n",
       "   'a regression by neural nets and tree ensemble -LRB- random forest or something -RRB-',\n",
       "   'can be included as other baselines',\n",
       "   ',',\n",
       "   'and',\n",
       "   'also',\n",
       "   'other types of regression problems',\n",
       "   'can be tested',\n",
       "   '.']),\n",
       " ('The paper would also be significantly more compelling if the strategy was applied to more varied tasks.',\n",
       "  ['The',\n",
       "   'paper',\n",
       "   'would',\n",
       "   'also',\n",
       "   'be',\n",
       "   'significantly more compelling if the strategy was applied to more varied tasks']),\n",
       " ('I wonder the current approach will be suboptimal when compared to the ordered statistics approach.',\n",
       "  ['the',\n",
       "   'current',\n",
       "   'approach',\n",
       "   'will',\n",
       "   'be',\n",
       "   'suboptimal',\n",
       "   'when compared to the ordered statistics approach']),\n",
       " ('Have the authors considered using algorithms that are much faster such as the sueprlinear secant method?',\n",
       "  ['the',\n",
       "   'authors',\n",
       "   'using',\n",
       "   'algorithms',\n",
       "   'that are much faster such as the sueprlinear secant method']),\n",
       " ('2)The evaluation is limited, in that the standard evaluations (eg: SimLex would be a good one to add, as well as many others, please refer to the literature) are not used and there is no comparison to previous work.',\n",
       "  ['2 -RRB-',\n",
       "   'The evaluation',\n",
       "   'is limited , in that the standard evaluations -LRB- eg : SimLex would be a good one to add , as well as many others , please refer to the literature -RRB- are not used',\n",
       "   'and',\n",
       "   'there',\n",
       "   'is no comparison to previous work',\n",
       "   '.']),\n",
       " ('Given the small size of the datasets, it is also unclear how generalizable the approach is.',\n",
       "  ['Given',\n",
       "   'the small size of the datasets',\n",
       "   ',',\n",
       "   'it',\n",
       "   'also',\n",
       "   'unclear',\n",
       "   'how generalizable the approach is',\n",
       "   '.']),\n",
       " ('The idea is tested on very small data sets (80 and 50 examples, respectively).',\n",
       "  ['The',\n",
       "   'idea',\n",
       "   'is',\n",
       "   'tested',\n",
       "   'on very small data sets',\n",
       "   '-LRB- 80 and 50 examples , respectively -RRB-']),\n",
       " ('Results on tiny datasets', ['Results', 'on', 'tiny datasets']),\n",
       " ('While this is an interesting insight, and worthy of further discussion, such a claim needs backing up with more large-scale experiments on real datasets.',\n",
       "  ['While',\n",
       "   'this is an interesting insight , and worthy of further discussion',\n",
       "   ',',\n",
       "   'such a claim',\n",
       "   'needs',\n",
       "   'backing up with more large - scale experiments on real datasets',\n",
       "   '.']),\n",
       " ('While the experiments on toy tasks is clearly useful, the paper could be significantly improved by adding experiments on real tasks such as language modelling.',\n",
       "  ['While',\n",
       "   'the experiments on toy tasks is clearly useful',\n",
       "   'the',\n",
       "   ',',\n",
       "   'the paper',\n",
       "   'could',\n",
       "   'be significantly improved by adding experiments on real tasks such as language modelling',\n",
       "   '.']),\n",
       " ('4)The author missed some important baselines here.\\n\"1) Symmetric cross entropy for robust learning with noisy labels, ICCV2019 \\n     2) Joint Optimization Framework for Learning with Noisy Labels, CVPR2018 \\n     3) Dimensionality-driven learning with noisy labels, ICML2018\"',\n",
       "  ['4 -RRB-', 'The author', 'missed some important baselines here', '.']),\n",
       " (\"However, it's not completely fair to compare a label-noise + semi-supervised method with other label-noise only methods... As a matter of fact, you don't need to apply perturbation consistency (or other semi-supervised) regularization after identifying the training data with incorrect labels.\",\n",
       "  ['However',\n",
       "   ',',\n",
       "   \"it 's not completely fair to compare a label - noise + semi-supervised method with other label - noise\",\n",
       "   'only',\n",
       "   'methods',\n",
       "   '...',\n",
       "   'As a matter of fact',\n",
       "   ',',\n",
       "   'you',\n",
       "   'do',\n",
       "   \"n't\",\n",
       "   'need to apply perturbation consistency -LRB- or other semi-supervised -RRB- regularization after identifying the training data with incorrect labels',\n",
       "   '.']),\n",
       " ('2)Experiments are conducted on various dataset CIFAR10, CIFAR-100 and ImageNet.',\n",
       "  ['2 -RRB-',\n",
       "   'Experiments',\n",
       "   'are',\n",
       "   'conducted on various dataset CIFAR10 , CIFAR - 100 and ImageNet',\n",
       "   '.']),\n",
       " ('2)For the experimental comparisons, the authors at least should report the acc on clean test set, which is useful for understanding the ideal case performance.',\n",
       "  ['2 -RRB-',\n",
       "   'For the experimental comparisons',\n",
       "   ',',\n",
       "   'the authors',\n",
       "   'at least',\n",
       "   'should',\n",
       "   'report the acc on clean test set , which is useful for understanding the ideal case performance',\n",
       "   '.']),\n",
       " ('3)The comparisons are not fair.',\n",
       "  ['3 -RRB-', 'The comparisons', 'are not fair', '.'])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_from_sent[\"mcomp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(cd.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Guangdong University of Foreign Studies is located in Guangzhou.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP\n",
      "      (NP (NNP Guangdong) (NNP University))\n",
      "      (PP (IN of)\n",
      "        (NP (NNP Foreign) (NNPS Studies))))\n",
      "    (VP (VBZ is)\n",
      "      (VP (VBN located)\n",
      "        (PP (IN in)\n",
      "          (NP (NNP Guangzhou)))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "parse_str = corenlp.parse(sentence)\n",
    "print(parse_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tree = Tree.fromstring(parse_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, (0, 0, 0, 1, 0))"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_tree.leaves()), nltk_tree.leaf_treeposition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtrees_list = list(nltk_tree.subtrees())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtrees_tpos = nltk_tree.treepositions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtrees_tpos[_]\n",
    "for i in range(0, len(nltk_tree.leaves())):\n",
    "    tp_leaf = nltk_tree.leaf_treeposition(i)\n",
    "    subtrees_tpos.remove(tp_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           ROOT                                       \n",
      "                                            |                                          \n",
      "                                            S                                         \n",
      "                           _________________|_______________________________________   \n",
      "                          |                                   VP                    | \n",
      "                          |                        ___________|___                  |  \n",
      "                          NP                      |               VP                | \n",
      "            ______________|_____                  |      _________|___              |  \n",
      "           |                    PP                |     |             PP            | \n",
      "           |               _____|_____            |     |          ___|______       |  \n",
      "           NP             |           NP          |     |         |          NP     | \n",
      "     ______|______        |      _____|_____      |     |         |          |      |  \n",
      "   NNP           NNP      IN   NNP         NNPS  VBZ   VBN        IN        NNP     . \n",
      "    |             |       |     |           |     |     |         |          |      |  \n",
      "Guangdong     University  of Foreign     Studies  is located      in     Guangzhou  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nltk_tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 ROOT ()\n",
      "7 S (0,)\n",
      "5 NP (0, 0)\n",
      "3 NP (0, 0, 0)\n",
      "2 NNP (0, 0, 0, 0)\n",
      "2 NNP (0, 0, 0, 1)\n",
      "4 PP (0, 0, 1)\n",
      "2 IN (0, 0, 1, 0)\n",
      "3 NP (0, 0, 1, 1)\n",
      "2 NNP (0, 0, 1, 1, 0)\n",
      "2 NNPS (0, 0, 1, 1, 1)\n",
      "6 VP (0, 1)\n",
      "2 VBZ (0, 1, 0)\n",
      "5 VP (0, 1, 1)\n",
      "2 VBN (0, 1, 1, 0)\n",
      "4 PP (0, 1, 1, 1)\n",
      "2 IN (0, 1, 1, 1, 0)\n",
      "3 NP (0, 1, 1, 1, 1)\n",
      "2 NNP (0, 1, 1, 1, 1, 0)\n",
      "2 . (0, 2)\n"
     ]
    }
   ],
   "source": [
    "depth_of_subtree = []\n",
    "for _, i in enumerate(subtrees_list):\n",
    "    depth_of_subtree.append((i, len(subtrees_tpos[_])))\n",
    "    print(i.height(), i.label(), subtrees_tpos[_])\n",
    "#     break\n",
    "#     if type(i) != Tree:\n",
    "#         print(i)\n",
    "#           subtrees_tpos[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdepths = []\n",
    "for d in depth_of_subtree:\n",
    "    cdepths.append(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1, 1: 1, 2: 3, 3: 4, 4: 6, 5: 4, 6: 1})"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(cdepths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.treeposition_spanning_leaves(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, 1, 0)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.leaf_treeposition(1)\n",
    "# for x in :\n",
    "#     if not isinstance(x, Tree):\n",
    "#         print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Guangdong',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Foreign',\n",
       " 'Studies',\n",
       " 'is',\n",
       " 'located',\n",
       " 'in',\n",
       " 'Guangzhou',\n",
       " '.']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using chunks from the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This DT swallow\n",
      "an DT swallow\n",
      "unladen JJ swallow\n",
      "swallow NN swallow\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This an unladen swallow.\")\n",
    "for sent in doc.sents:\n",
    "    for tok in sent:\n",
    "        if tok.is_alpha:\n",
    "            print(tok.orth_, tok.tag_, tok.head.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_extension',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'string',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree: (DT This)\n",
      "tree: (DT an)\n",
      "tree: (NNS unladen)\n",
      "tree: (VBP swallow)\n",
      "tree: (, .)\n",
      "tree: (DT This)\n",
      "tree: (VBZ is)\n",
      "tree: (DT a)\n",
      "tree: (NN test)\n",
      "tree: (. .)\n"
     ]
    }
   ],
   "source": [
    "def traverse_tree(tree):\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == nltk.tree.Tree:\n",
    "            traverse_tree(subtree)\n",
    "        else:\n",
    "            print(\"tree:\", tree)\n",
    "traverse_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import breadth_first\n",
    "from nltk.tree import ParentedTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in breadth_first(tree[0]):\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import CoreNLPParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.parse??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = list(parser.parse('This an unladen swallow.'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NP', [Tree('DT', ['This'])]), Tree('NP', [Tree('DT', ['an']), Tree('NNS', ['unladen'])])]), Tree('VP', [Tree('VBP', ['swallow'])]), Tree('.', ['.'])])])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NP', [Tree('DT', ['This'])]), Tree('NP', [Tree('DT', ['an']), Tree('NNS', ['unladen'])])]), Tree('S|<VP-.>', [Tree('VP', [Tree('VBP', ['swallow'])]), Tree('.', ['.'])])])])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dir(tree[0])\n",
    "tree[0].chomsky_normal_form()\n",
    "tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (S\n",
      "  (NP (NP (DT This)) (NP (DT an) (NNS unladen)))\n",
      "  (VP (VBP swallow))\n",
      "  (. .)) (S This an unladen swallow .)\n"
     ]
    }
   ],
   "source": [
    "for (i,child) in enumerate(tree[0]): \n",
    "    print(i,child, child.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ROOT -> S,\n",
       " S -> NP VP .,\n",
       " NP -> NP NP,\n",
       " NP -> DT,\n",
       " DT -> 'This',\n",
       " NP -> DT NNS,\n",
       " DT -> 'an',\n",
       " NNS -> 'unladen',\n",
       " VP -> VBP,\n",
       " VBP -> 'swallow',\n",
       " . -> '.']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[0].productions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[(governor, dep, dependent) for governor, dep, dependent in parse.triples()] for parse in tree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (NP (DT This)) (NP (DT an) (NNS unladen)))\n",
      "  (VP (VBP swallow))\n",
      "  (. .)) [(), (0,), (0, 0), (0, 0, 0), (0, 0, 0, 0), (0, 1), (0, 1, 0), (0, 1, 0, 0), (0, 1, 1), (0, 1, 1, 0), (1,), (1, 0), (1, 0, 0), (2,), (2, 0)]\n"
     ]
    }
   ],
   "source": [
    "for t in tree[0]:\n",
    "    print(t, t.treepositions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'an', 'unladen', 'swallow', '.']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[0].leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "strees_list = list(tree[0].subtrees())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 11)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(strees_list), len(strees_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(nltk.tree.Tree, ['an', 'unladen'])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(strees_list[0]), strees_list[5].leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__radd__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_frozen_class',\n",
       " '_get_node',\n",
       " '_label',\n",
       " '_parse_error',\n",
       " '_pformat_flat',\n",
       " '_repr_png_',\n",
       " '_set_node',\n",
       " 'append',\n",
       " 'chomsky_normal_form',\n",
       " 'clear',\n",
       " 'collapse_unary',\n",
       " 'convert',\n",
       " 'copy',\n",
       " 'count',\n",
       " 'draw',\n",
       " 'extend',\n",
       " 'flatten',\n",
       " 'freeze',\n",
       " 'fromstring',\n",
       " 'height',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'label',\n",
       " 'leaf_treeposition',\n",
       " 'leaves',\n",
       " 'node',\n",
       " 'pformat',\n",
       " 'pformat_latex_qtree',\n",
       " 'pop',\n",
       " 'pos',\n",
       " 'pprint',\n",
       " 'pretty_print',\n",
       " 'productions',\n",
       " 'remove',\n",
       " 'reverse',\n",
       " 'set_label',\n",
       " 'sort',\n",
       " 'subtrees',\n",
       " 'treeposition_spanning_leaves',\n",
       " 'treepositions',\n",
       " 'un_chomsky_normal_form']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NP', [Tree('DT', ['This'])]), Tree('NP', [Tree('DT', ['an']), Tree('NNS', ['unladen'])])]), Tree('VP', [Tree('VBP', ['swallow'])]), Tree('.', ['.'])])])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ROOT ['This', 'an', 'unladen', 'swallow', '.']\n",
      "6 ROOT ['This', 'an', 'unladen', 'swallow', '.']\n",
      "5 S ['This', 'an', 'unladen', 'swallow', '.']\n",
      "4 NP ['This', 'an', 'unladen']\n",
      "3 NP ['This']\n",
      "2 DT ['This']\n",
      "3 NP ['an', 'unladen']\n",
      "2 DT ['an']\n",
      "2 NNS ['unladen']\n",
      "3 VP ['swallow']\n",
      "2 VBP ['swallow']\n",
      "2 . ['.']\n"
     ]
    }
   ],
   "source": [
    "st_depth_list = []\n",
    "\n",
    "for _, st in enumerate(strees_list):\n",
    "    st_depth_list.append((len(st.treepositions()), st))\n",
    "    print(st.height(), st.label(), st.leaves(), )\n",
    "    ss = list(st.subtrees())\n",
    "    for i in ss:\n",
    "        print(i.height(), i.label(), i.leaves())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.leaf_treeposition??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__radd__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_frozen_class',\n",
       " '_get_node',\n",
       " '_label',\n",
       " '_parse_error',\n",
       " '_pformat_flat',\n",
       " '_repr_png_',\n",
       " '_set_node',\n",
       " 'append',\n",
       " 'chomsky_normal_form',\n",
       " 'clear',\n",
       " 'collapse_unary',\n",
       " 'convert',\n",
       " 'copy',\n",
       " 'count',\n",
       " 'draw',\n",
       " 'extend',\n",
       " 'flatten',\n",
       " 'freeze',\n",
       " 'fromstring',\n",
       " 'height',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'label',\n",
       " 'leaf_treeposition',\n",
       " 'leaves',\n",
       " 'node',\n",
       " 'pformat',\n",
       " 'pformat_latex_qtree',\n",
       " 'pop',\n",
       " 'pos',\n",
       " 'pprint',\n",
       " 'pretty_print',\n",
       " 'productions',\n",
       " 'remove',\n",
       " 'reverse',\n",
       " 'set_label',\n",
       " 'sort',\n",
       " 'subtrees',\n",
       " 'treeposition_spanning_leaves',\n",
       " 'treepositions',\n",
       " 'un_chomsky_normal_form']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(), (0,)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.treepositions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 ROOT ['What', 'is', 'the', 'airspeed', 'of', 'an', 'unladen', 'swallow', '?']\n",
      "26 SBARQ ['What', 'is', 'the', 'airspeed', 'of', 'an', 'unladen', 'swallow', '?']\n",
      "3 WHNP ['What']\n",
      "2 WP ['What']\n",
      "20 SQ ['is', 'the', 'airspeed', 'of', 'an', 'unladen', 'swallow']\n",
      "2 VBZ ['is']\n",
      "14 NP ['the', 'airspeed', 'of', 'an', 'unladen']\n",
      "5 NP ['the', 'airspeed']\n",
      "2 DT ['the']\n",
      "2 NN ['airspeed']\n",
      "8 PP ['of', 'an', 'unladen']\n",
      "2 IN ['of']\n",
      "5 NP ['an', 'unladen']\n",
      "2 DT ['an']\n",
      "2 JJ ['unladen']\n",
      "3 S+VP ['swallow']\n",
      "3 VP ['swallow']\n",
      "2 VB ['swallow']\n",
      "2 . ['?']\n"
     ]
    }
   ],
   "source": [
    "for st in st_depth_list:\n",
    "    print(st[0], st[1].label(), st[1].leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(nltk.tree.Tree,\n",
       " ['__add__',\n",
       "  '__class__',\n",
       "  '__contains__',\n",
       "  '__copy__',\n",
       "  '__deepcopy__',\n",
       "  '__delattr__',\n",
       "  '__delitem__',\n",
       "  '__dict__',\n",
       "  '__dir__',\n",
       "  '__doc__',\n",
       "  '__eq__',\n",
       "  '__format__',\n",
       "  '__ge__',\n",
       "  '__getattribute__',\n",
       "  '__getitem__',\n",
       "  '__gt__',\n",
       "  '__hash__',\n",
       "  '__iadd__',\n",
       "  '__imul__',\n",
       "  '__init__',\n",
       "  '__init_subclass__',\n",
       "  '__iter__',\n",
       "  '__le__',\n",
       "  '__len__',\n",
       "  '__lt__',\n",
       "  '__module__',\n",
       "  '__mul__',\n",
       "  '__ne__',\n",
       "  '__new__',\n",
       "  '__radd__',\n",
       "  '__reduce__',\n",
       "  '__reduce_ex__',\n",
       "  '__repr__',\n",
       "  '__reversed__',\n",
       "  '__rmul__',\n",
       "  '__setattr__',\n",
       "  '__setitem__',\n",
       "  '__sizeof__',\n",
       "  '__str__',\n",
       "  '__subclasshook__',\n",
       "  '__weakref__',\n",
       "  '_frozen_class',\n",
       "  '_get_node',\n",
       "  '_label',\n",
       "  '_parse_error',\n",
       "  '_pformat_flat',\n",
       "  '_repr_png_',\n",
       "  '_set_node',\n",
       "  'append',\n",
       "  'chomsky_normal_form',\n",
       "  'clear',\n",
       "  'collapse_unary',\n",
       "  'convert',\n",
       "  'copy',\n",
       "  'count',\n",
       "  'draw',\n",
       "  'extend',\n",
       "  'flatten',\n",
       "  'freeze',\n",
       "  'fromstring',\n",
       "  'height',\n",
       "  'index',\n",
       "  'insert',\n",
       "  'label',\n",
       "  'leaf_treeposition',\n",
       "  'leaves',\n",
       "  'node',\n",
       "  'pformat',\n",
       "  'pformat_latex_qtree',\n",
       "  'pop',\n",
       "  'pos',\n",
       "  'pprint',\n",
       "  'pretty_print',\n",
       "  'productions',\n",
       "  'remove',\n",
       "  'reverse',\n",
       "  'set_label',\n",
       "  'sort',\n",
       "  'subtrees',\n",
       "  'treeposition_spanning_leaves',\n",
       "  'treepositions',\n",
       "  'un_chomsky_normal_form'])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tree[0]), dir(tree[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (SBARQ\n",
      "    (WHNP (WP What))\n",
      "    (SQ\n",
      "      (VBZ is)\n",
      "      (NP\n",
      "        (NP (DT the) (NN airspeed))\n",
      "        (PP (IN of) (NP (DT an) (JJ unladen))))\n",
      "      (S (VP (VB swallow))))\n",
      "    (. ?)))\n"
     ]
    }
   ],
   "source": [
    "for st in tree:\n",
    "    print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " ['__add__',\n",
       "  '__class__',\n",
       "  '__contains__',\n",
       "  '__delattr__',\n",
       "  '__delitem__',\n",
       "  '__dir__',\n",
       "  '__doc__',\n",
       "  '__eq__',\n",
       "  '__format__',\n",
       "  '__ge__',\n",
       "  '__getattribute__',\n",
       "  '__getitem__',\n",
       "  '__gt__',\n",
       "  '__hash__',\n",
       "  '__iadd__',\n",
       "  '__imul__',\n",
       "  '__init__',\n",
       "  '__init_subclass__',\n",
       "  '__iter__',\n",
       "  '__le__',\n",
       "  '__len__',\n",
       "  '__lt__',\n",
       "  '__mul__',\n",
       "  '__ne__',\n",
       "  '__new__',\n",
       "  '__reduce__',\n",
       "  '__reduce_ex__',\n",
       "  '__repr__',\n",
       "  '__reversed__',\n",
       "  '__rmul__',\n",
       "  '__setattr__',\n",
       "  '__setitem__',\n",
       "  '__sizeof__',\n",
       "  '__str__',\n",
       "  '__subclasshook__',\n",
       "  'append',\n",
       "  'clear',\n",
       "  'copy',\n",
       "  'count',\n",
       "  'extend',\n",
       "  'index',\n",
       "  'insert',\n",
       "  'pop',\n",
       "  'remove',\n",
       "  'reverse',\n",
       "  'sort'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tree), dir(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(sent):\n",
    "    \n",
    "    words = nltk.word_tokenize(sent)\n",
    "    grammar = \"NP:{<DT>?<JJ>*<NN>}\"\n",
    "    Reg_parser = nltk.RegexpParser(grammar)\n",
    "    tree = Reg_parser.parse(nltk.pos_tag(words))\n",
    "    \n",
    "    strees = list(tree.subtrees())\n",
    "    for i in strees:\n",
    "        print(i.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('only', 'RB'), ('a', 'DT'), ('test', 'NN'), ('.', '.')]\n",
      "[('a', 'DT'), ('test', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "get_chunks(\"This is only a test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunks_using_parse_tree(conssentence, replace_with_dataset=True):\n",
    "    \n",
    "    conssentence = mask_entities(conssentence, replace_with_dataset)\n",
    "    doc = nlp(conssentence)\n",
    "    verb_subtree = []\n",
    "\n",
    "    for s in doc.sents:\n",
    "#         find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"technique\": [], \"task\": [], \"dataset\": [], \"material\": [], \"metric\": []}\n",
    "        find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"baseline\": [], \"task\": [], \n",
    "                               \"dataset\": [],  \"metric\": [], \"unfair\": [], \"fair\": []}\n",
    "\n",
    "        for tok in s:\n",
    "\n",
    "            if tok.text.lower().startswith(\"compar\"):\n",
    "                find_special_tokens[\"compar\"].append(tok)\n",
    "            else:\n",
    "                for k in sp_toks:\n",
    "                    if tok.text.lower().startswith(k):\n",
    "                        find_special_tokens[k].append(tok)\n",
    "                        break\n",
    "\n",
    "        verb_tokens = []\n",
    "        if find_special_tokens[\"compar\"]:\n",
    "            for t in find_special_tokens[\"compar\"]:\n",
    "#                     verb_subtree.append(t.subtree)\n",
    "                if t == s.root:\n",
    "                    simplified_sent = \"\"\n",
    "                    for chh in t.lefts:\n",
    "                        simplified_sent = simplified_sent + \" \" + chh.text\n",
    "                    simplified_sent = simplified_sent + \" \" + t.text\n",
    "                    for chh in t.rights:\n",
    "                        simplified_sent = simplified_sent + \" \" + chh.text\n",
    "#                         print(\"SIMP: \", simplified_sent)\n",
    "                    verb_subtree.append(simplified_sent)\n",
    "                else:\n",
    "                    verb_subtree.append(t.subtree)\n",
    "        else:\n",
    "            for k in sp_toks:\n",
    "                for i in find_special_tokens[k]:\n",
    "                    local_vt = []\n",
    "                    for j in i.ancestors:\n",
    "                        if j.pos_ == \"NOUN\":\n",
    "                            local_vt.append(j)\n",
    "                    if not local_vt:\n",
    "                        for j in i.ancestors:\n",
    "                            if j.pos_ == \"VERB\":\n",
    "                                local_vt.append(j)\n",
    "                    verb_tokens = verb_tokens + local_vt\n",
    "\n",
    "\n",
    "            for i in verb_tokens:\n",
    "                verb_subtree.append(i.subtree)\n",
    "\n",
    "    eecc = []\n",
    "    for i in verb_subtree:\n",
    "        if type(i) == str:\n",
    "            eecc.append(i)\n",
    "        else:\n",
    "            local_chunk = \"\"\n",
    "            for lcaltok in i:\n",
    "                local_chunk = local_chunk + \" \" + lcaltok.text\n",
    "            eecc.append(local_chunk)\n",
    "#     if not eecc:\n",
    "#         print(conssentence)\n",
    "    return list(set(eecc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' more large - scale experiments on image related tasks',\n",
       " ' the practicability of the method']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"It would be interesting to explore the practicability of the method on more large-scale experiments on image related tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' comparison to SOTA']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"The experimental validation is also not extensive since comparison to SOTA is not included.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_HyVxPsC9tm 938 2\n",
      "2019_HyVxPsC9tm 940 3\n",
      "2020_Byg79h4tvB 1272 [1] Conditional adversarial domain adaptation, Long et.al, in NeurIPS 2018\n",
      "[2] Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation, You et.al, in ICML 2019\n",
      "2018_HyHmGyZCZ 1425 2\n"
     ]
    }
   ],
   "source": [
    "roberta_vectors = defaultdict(dict)\n",
    "skip_uids = []\n",
    "\n",
    "for pid in gt_dict:\n",
    "    roberta_vectors[pid] = {}\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunk = \". \".join(mcomp_chunks_from_sent)\n",
    "            else:\n",
    "                final_chunk = df.loc[mcs][\"Sent\"]\n",
    "            \n",
    "            vec = embed_text_using_roberta(final_chunk.strip()).mean(1).detach().numpy()\n",
    "            roberta_vectors[pid][mcs] = vec / norm(vec)\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunk = \". \".join(mcomp_chunks_from_sent)\n",
    "            else:\n",
    "                final_chunk = df.loc[mcs][\"Sent\"]\n",
    "            \n",
    "            vec = embed_text_using_roberta(final_chunk.strip()).mean(1).detach().numpy()\n",
    "            roberta_vectors[pid][mcs] = vec / norm(vec)\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 1384\n"
     ]
    }
   ],
   "source": [
    "mcomp_sentences = {}\n",
    "not_mcomp_sentences = {}\n",
    "\n",
    "for pid in gt_dict:\n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            mcomp_sentences[mcs] = pid\n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            not_mcomp_sentences[mcs] = pid\n",
    "print(len(mcomp_sentences), len(not_mcomp_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_with_mcomp = defaultdict(dict)\n",
    "sim_with_not_mcomp = defaultdict(dict)\n",
    "sim_with_notmcomp_paper_sents = defaultdict(dict)\n",
    "\n",
    "mean_at_k = [\"1\", \"3\", \"5\", \"7\", \"10\", \"20\", \"30\", \"50\", \"100\", \"500\", \"1000\", \"1380\"]\n",
    "\n",
    "for sid in mcomp_sentences:\n",
    "    \n",
    "    # 1. With other mcomp sentences\n",
    "    temp_list = []    \n",
    "    for osid in mcomp_sentences:\n",
    "        if osid != sid:\n",
    "            temp_list.append(np.inner(roberta_vectors[mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "\n",
    "    \n",
    "    # 2. With other not_mcomp_sentences\n",
    "    temp_list = []\n",
    "    for osid in not_mcomp_sentences:\n",
    "        temp_list.append(np.inner(roberta_vectors[not_mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_not_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_not_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "\n",
    "    \n",
    "    # 3. With not_mcomp_sentences of the same paper\n",
    "    temp_list = []    \n",
    "    for osid in not_mcomp_sentences:\n",
    "        if not_mcomp_sentences[osid] == mcomp_sentences[sid]:\n",
    "            temp_list.append(np.inner(roberta_vectors[not_mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0])\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    sim_with_notmcomp_paper_sents[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        sim_with_notmcomp_paper_sents[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_sim_plot\n",
    "diff12 = {\"all\": []}\n",
    "for vv in mean_at_k:\n",
    "    diff12[str(vv)] = []\n",
    "\n",
    "diff13 = {\"all\": []}\n",
    "for vv in mean_at_k:\n",
    "    diff13[str(vv)] = []\n",
    "\n",
    "for sid in sim_with_mcomp:\n",
    "    diff12[\"all\"].append(sim_with_mcomp[sid][\"mean\"] - sim_with_not_mcomp[sid][\"mean\"])\n",
    "    diff13[\"all\"].append(sim_with_mcomp[sid][\"mean\"] - sim_with_notmcomp_paper_sents[sid][\"mean\"])\n",
    "    \n",
    "    for vv in mean_at_k:\n",
    "        diff12[str(vv)].append(sim_with_mcomp[sid][\"mean_{}\".format(vv)] - sim_with_not_mcomp[sid][\"mean_{}\".format(vv)])\n",
    "        diff13[str(vv)].append(sim_with_mcomp[sid][\"mean_{}\".format(vv)] - sim_with_notmcomp_paper_sents[sid][\"mean_{}\".format(vv)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analyse chunks after masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_chunks = {\"mcs\": [], \"nmcs\": []}\n",
    "\n",
    "for pid in gt_dict:\n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunk = \". \".join(mcomp_chunks_from_sent)\n",
    "            else:\n",
    "                final_chunk = df.loc[mcs][\"Sent\"]\n",
    "            \n",
    "            masked_chunks[\"mcs\"].append((df.loc[mcs][\"Sent\"], final_chunk))\n",
    "        except Exception as ex:\n",
    "            continue\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunk = \". \".join(mcomp_chunks_from_sent)\n",
    "            else:\n",
    "                final_chunk = df.loc[mcs][\"Sent\"]\n",
    "            masked_chunks[\"nmcs\"].append((df.loc[mcs][\"Sent\"], final_chunk))\n",
    "        except Exception as ex:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' comparison']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The authors propose k-DPP as an open loop oblivious to the evaluation of configurations method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search , uniform random search , low-discrepancy Sobol sequences , BO-TPE Bayesian optimization using tree-structured Parzen estimator by Bergstra et al 2011 .'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' comparison']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' comparison']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).',\n",
       "  ' comparison'),\n",
       " ('Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20) The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.',\n",
       "  ' a small number like 3 - 6 metric with a small k 20.  lies , authors do not compare against'),\n",
       " ('COMMENTS ON THE CHANGES SINCE THE LAST YEAR\\nI am not convinced by the comparison with Spearmint added by the authors since the previous version.',\n",
       "  ' the comparison with Spearmint added by the authors since the previous version')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_chunks[\"mcs\"][2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The authors propose k-DPP as an open loop oblivious to the evaluation of configurations method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search , uniform random search , low-discrepancy Sobol sequences , BO-TPE Bayesian optimization using tree-structured Parzen estimator by Bergstra et al 2011 .'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Third , the authors do not compare against some relevant , recent work , e.g. , Springenberg et al http : aad.informatik.uni-freiburg.de papers 16-NIPS-BOHamiANN.pdf and Snoek et al https : arxiv.org pdf 1502.05700.pdf that is essential for this kind of empirical study .'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mask_entities(sentence, replace_with_dataset=False):\n",
    "# #     cleaned_sent = re.sub('[^0-9a-zA-Z ]+', ' ', sentence)\n",
    "#     cleaned_sent = sentence\n",
    "#     while cleaned_sent.find(\"  \") > -1:\n",
    "#         cleaned_sent = cleaned_sent.replace(\"  \", \" \")\n",
    "    \n",
    "#     entities_found = []\n",
    "#     for i in entity_key_map:\n",
    "#         if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "#             entities_found.append(i)\n",
    "    \n",
    "#     entities_found.sort(key=lambda s: len(s))\n",
    "#     len_sorted_entities = entities_found.copy()\n",
    "    \n",
    "#     subset_entities = []\n",
    "#     # Remove subset entities (eg: Among cnn and 3-layer-cnn, prefer the latter)\n",
    "#     for fe in len_sorted_entities:\n",
    "#         for other_ent in len_sorted_entities:\n",
    "#             if fe != other_ent and other_ent.find(fe) > -1:\n",
    "#                 subset_entities.append(fe)\n",
    "#                 break\n",
    "#     for se in subset_entities:\n",
    "#         len_sorted_entities.remove(se)\n",
    "#     for maxents in len_sorted_entities:\n",
    "#         mask_name = entity_dict[entity_key_map[i]].lower()\n",
    "#         if replace_with_dataset:\n",
    "#             if mask_name == \"material\":\n",
    "#                 mask_name = \"dataset\"\n",
    "#         cleaned_sent = cleaned_sent.replace(maxents, mask_name)\n",
    "#     words_cleaned = nltk.word_tokenize(cleaned_sent)\n",
    "#     dups_removed = [v for i, v in enumerate(words_cleaned) if i == 0 or v != words_cleaned[i-1]]\n",
    "#     new_dup_removed_sent = \" \".join(dups_removed)\n",
    "#     return new_dup_removed_sent.strip()\n",
    "\n",
    "# #     #print(cleaned_sent)\n",
    "# #     for i in entity_key_map:\n",
    "# #         if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "# #             #print(\"Substituting ent: {} with mask: {}\".format(i, entity_dict[entity_key_map[i]].lower()))\n",
    "# #             cleaned_sent = cleaned_sent.replace(i, entity_dict[entity_key_map[i]].lower())\n",
    "# #     return cleaned_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse meaningful sentences that are more similar to NMCS in comparison to MCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_sim_with_mcomp = defaultdict(list)\n",
    "ana_sim_with_not_mcomp = defaultdict(list)\n",
    "\n",
    "\n",
    "mean_at_k = [\"1\", \"3\", \"5\", \"7\", \"10\", \"20\", \"30\", \"50\", \"100\", \"500\", \"1000\", \"1380\"]\n",
    "\n",
    "for sid in mcomp_sentences:\n",
    "    \n",
    "    # 1. With other mcomp sentences\n",
    "    temp_list = []    \n",
    "    for osid in mcomp_sentences:\n",
    "        if osid != sid:\n",
    "            temp_list.append((osid, np.inner(roberta_vectors[mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0]))\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, key=lambda x: x[1], reverse=True)\n",
    "    ana_sim_with_mcomp[sid] = sorted_temp_list\n",
    "\n",
    "    \n",
    "    # 2. With other not_mcomp_sentences\n",
    "    temp_list = []\n",
    "    for osid in not_mcomp_sentences:\n",
    "        temp_list.append((osid, np.inner(roberta_vectors[not_mcomp_sentences[osid]][osid], roberta_vectors[mcomp_sentences[sid]][sid])[0][0]))\n",
    "    \n",
    "    sorted_temp_list = sorted(temp_list, key=lambda x: x[1], reverse=True)\n",
    "    ana_sim_with_not_mcomp[sid] = sorted_temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_sentences_at_k = defaultdict(list)\n",
    "unproblematic_sentences_at_k = defaultdict(list)\n",
    "vv = 1\n",
    "\n",
    "for sid in sim_with_mcomp:\n",
    "    sim_diff = (sim_with_mcomp[sid][\"mean_{}\".format(vv)] - sim_with_not_mcomp[sid][\"mean_{}\".format(vv)])\n",
    "    if sim_diff < 0:\n",
    "        problematic_sentences_at_k[vv].append((sid,-1.0* sim_diff))\n",
    "    else:\n",
    "        unproblematic_sentences_at_k[vv].append((sid, sim_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(113, 0.1638484001159668),\n",
       " (1464, 0.12271469831466675),\n",
       " (931, 0.1226879358291626),\n",
       " (950, 0.12097209692001343),\n",
       " (1318, 0.10917872190475464)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 1\n",
    "sorted_problematic_sentences_at_1 = sorted(problematic_sentences_at_k[k], key=lambda x: x[1], reverse=True)\n",
    "sorted(problematic_sentences_at_k[k], key=lambda x: x[1], reverse=True)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 1\n",
    "# sorted_problematic_sentences_at_3 = sorted(problematic_sentences_at_k[3], key=lambda x: x[1], reverse=True)\n",
    "# sorted(problematic_sentences_at_k[3], key=lambda x: x[1], reverse=True)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_problematic_sentences_at_3[0:3], sorted_problematic_sentences_at_3[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_unproblematic_sentences_at_3 = sorted(unproblematic_sentences_at_k[3], key=lambda x: x[1], reverse=True)\n",
    "# sorted_unproblematic_sentences_at_3[0:4], sorted_unproblematic_sentences_at_3[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sent:  The experimental results are very good for document modeling, but without ablation analysis against the baseline it is hard to see why they should be with such a small modification in G-NVDM.\n",
      "\n",
      "Meaningful comparison sentences: \n",
      "[(124, 0.74430156)]\n",
      "What is left is the gated incremental pooling operation; but to show that this operation is beneficial when added to autoregressive CNNs, a thorough comparison with an autoregressive CNN baseline is necessary.\n",
      "\n",
      "Non Meaningful comparison sentence: \n",
      "[(842, 0.8440055)]\n",
      "Paper Weaknesses:\n",
      "- The evaluation of the model is not great: (1) It would be interesting to combine bedroom and kitchen images and train jointly to see what it learns.\n",
      "\n",
      "\n",
      "\n",
      "Test sent:  The paper does not consider the more recent and highly relevant Moosavi-Dezfooli et al “Universal Adversarial Perturbations” CVPR 2017.\n",
      "\n",
      "Meaningful comparison sentences: \n",
      "[(1202, 0.5903547)]\n",
      "- I am concerned about whether the proposed method works well with harder datasets such as Office-Home dataset, because each class data are modeled by a simple Gaussian distribution in the proposed method.\n",
      "\n",
      "Non Meaningful comparison sentence: \n",
      "[(989, 0.6897952)]\n",
      "This paper is interesting since most of the existing works focus on Monte Carlo variational inference.\n",
      "\n",
      "\n",
      "\n",
      "Test sent:  Indeed, unsurprisingly, the authors note that \"the probability of correctly labelling a word as a mistake remains low (62%)\" - this admittedly beats a random-chance baseline, but is not compared to something more meaningful, such as simply contrasting the existing system with a more powerful convolutional model and labelling all discrepancies as mistakes.\n",
      "\n",
      "Meaningful comparison sentences: \n",
      "[(869, 0.60373676)]\n",
      "An additional problem is that performance is not compared to any external prior work.\n",
      "\n",
      "Non Meaningful comparison sentence: \n",
      "[(732, 0.69845736)]\n",
      "The preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches.\n",
      "\n",
      "\n",
      "\n",
      "Test sent:  - State of the art is not well-studied in the paper.\n",
      "\n",
      "Meaningful comparison sentences: \n",
      "[(110, 0.63996166)]\n",
      "On the negative side, the authors present the results without fully referencing and acknowledging state-of-the-art.\n",
      "\n",
      "Non Meaningful comparison sentence: \n",
      "[(230, 0.7335436)]\n",
      "The network analysed here does not reach the state-of-the-art on MNIST from almost two decades ago.\n",
      "\n",
      "\n",
      "\n",
      "Test sent:  Minor comments:\n",
      "- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.\n",
      "\n",
      "Meaningful comparison sentences: \n",
      "[(615, 0.64030004)]\n",
      "Overall, the very strong improvements on the text modeling task over NVDM seem hard to understand, and I would like to see an ablation analysis of all the differences between that model and the proposed one.\n",
      "\n",
      "Non Meaningful comparison sentence: \n",
      "[(1388, 0.729555)]\n",
      "2)What is the value of k in Figure 3 and Figure 4?\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in sorted_problematic_sentences_at_1[5:10]:\n",
    "    print(\"Test sent: \", df.loc[s[0]][\"Sent\"])\n",
    "    \n",
    "    print(\"\\nMeaningful comparison sentences: \")\n",
    "    print(ana_sim_with_mcomp[s[0]][0:1])\n",
    "    for i in ana_sim_with_mcomp[s[0]][0:1]:\n",
    "        print(df.loc[i[0]][\"Sent\"])\n",
    "    \n",
    "    print(\"\\nNon Meaningful comparison sentence: \")\n",
    "    print(ana_sim_with_not_mcomp[s[0]][0:1])\n",
    "    for i in ana_sim_with_not_mcomp[s[0]][0:1]:\n",
    "        print(df.loc[i[0]][\"Sent\"])\n",
    "    \n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Minor comments : - I believe one should not compare the metric shown between the left and right columns of Figure 3 as they are obtained from two different models .'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"Minor comments:- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' one should not compare the metric shown between the left and right columns of Figure 3 as they are obtained from two different models']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"Minor comments:- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Although I do like the paper on the whole , to really convince me that main objective -- ie that iterative improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular , to show that an metric scheme can really improve over a system closely matched to the attention-based model , both when used in isolation and when used in system combination with a PBMT system , and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model .'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"Although I do like the paper on the whole, to really convince me that main objective -- ie that **iterative** improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' an metric scheme']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"Although I do like the paper on the whole, to really convince me that main objective -- ie that **iterative** improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In summary , while I think the paper is interesting , I suspect that the applicability of this technique is possibly limited at present , and I m unsure how much we can really read into the findings of the paper when the experiments are based on MNIST alone .'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_entities(\"In summary, while I think the paper is interesting, I suspect that the applicability of this technique is possibly limited at present, and I'm unsure how much we can really read into the findings of the paper when the experiments are based on MNIST alone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' to compare a label - noise semi - supervised method with other label - noise only methods',\n",
       " ' perturbation consistency or other semi - supervised metric']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_chunks_using_spacy_dp(\"However, it's not completely fair to compare a label-noise + semi-supervised method with other label-noise only methods... As a matter of fact, you don't need to apply perturbation consistency (or other semi-supervised) regularization after identifying the training data with incorrect labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
