{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Feature Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"InputTestSet-Reviews48_Ann_NEW.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Sent</th>\n",
       "      <th>MComp</th>\n",
       "      <th>Cat</th>\n",
       "      <th>SubCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The authors propose to use k-DPP to select a s...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>This paper covers the related work nicely, wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The rest of the paper are also clearly written.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>However, I have some concerns about the propos...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>- It is not clear how to define the kernel, th...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID              PID     Dec  \\\n",
       "0    0  2019_SJf_XhCqKm  Reject   \n",
       "1    1  2019_SJf_XhCqKm  Reject   \n",
       "2    2  2019_SJf_XhCqKm  Reject   \n",
       "3    3  2019_SJf_XhCqKm  Reject   \n",
       "4    4  2019_SJf_XhCqKm  Reject   \n",
       "\n",
       "                                                Sent  MComp  Cat SubCat  \n",
       "0  The authors propose to use k-DPP to select a s...      0  NaN    NaN  \n",
       "1  This paper covers the related work nicely, wit...      0  NaN    NaN  \n",
       "2    The rest of the paper are also clearly written.      0  NaN    NaN  \n",
       "3  However, I have some concerns about the propos...      0  NaN    NaN  \n",
       "4  - It is not clear how to define the kernel, th...      0  NaN    NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dict = {}\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    if not pid in gt_dict:\n",
    "        gt_dict[pid] = {\"dec\": df.loc[i][\"Dec\"], \"mcomp\": set(), \"not_mcomp\": set()}\n",
    "    if df.loc[i][\"MComp\"] == 1:\n",
    "        gt_dict[pid][\"mcomp\"].add(df.loc[i][\"UID\"])\n",
    "    else:\n",
    "        gt_dict[pid][\"not_mcomp\"].add(df.loc[i][\"UID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accept': [49, 643], 'Reject': [68, 745]}\n"
     ]
    }
   ],
   "source": [
    "stats_dict = {\"Accept\": [0, 0], \"Reject\": [0, 0]}\n",
    "\n",
    "for k, v in gt_dict.items():\n",
    "    #print(len(v[\"mcomp\"]), len(v[\"not_mcomp\"]), v[\"dec\"])\n",
    "    stats_dict[v[\"dec\"]][0] += len(v[\"mcomp\"])\n",
    "    stats_dict[v[\"dec\"]][1] += len(v[\"not_mcomp\"])\n",
    "    \n",
    "print(stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestSet length: 32\n",
      " ['2019_SJf_XhCqKm', '2017_Bk0MRI5lg', '2020_SyevYxHtDB', '2018_rJBiunlAW', '2020_rkltE0VKwH', '2018_Hki-ZlbA-', '2019_BJx0sjC5FX', '2020_r1e_FpNFDr', '2020_B1lsXREYvr', '2018_SkZxCk-0Z', '2019_rJzoujRct7', '2018_HkfXMz-Ab', '2017_BJ9fZNqle', '2019_SyxZJn05YX', '2017_B1ckMDqlg', '2017_HJ0NvFzxl', '2017_S1_pAu9xl', '2018_SyYYPdg0-', '2017_BJAA4wKxg', '2019_HyVxPsC9tm', '2019_HylTBhA5tQ', '2019_B1l08oAct7', '2018_H135uzZ0-', '2017_H1oyRlYgg', '2017_r1y1aawlg', '2020_r1eX1yrKwB', '2020_Byg79h4tvB', '2019_H1lFZnR5YX', '2020_BkeWw6VFwr', '2018_HyHmGyZCZ', '2018_HyUNwulC-', '2020_HkgsPhNYPS']\n"
     ]
    }
   ],
   "source": [
    "test_set = list(gt_dict.keys())\n",
    "print(\"TestSet length: %d\\n\"%len(test_set), test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_SJf_XhCqKm     {39, 17, 27, 28, 30}\n",
      "2017_Bk0MRI5lg      {57}\n",
      "2020_SyevYxHtDB     {76, 87}\n",
      "2018_rJBiunlAW      {108, 110, 112, 113, 124, 126}\n",
      "2020_rkltE0VKwH     {160, 184, 155, 157, 159}\n",
      "2018_Hki-ZlbA-      {267, 235, 236, 271}\n",
      "2019_BJx0sjC5FX     {292, 287}\n",
      "2020_r1e_FpNFDr     {312, 322, 315, 308}\n",
      "2020_B1lsXREYvr     {376, 401}\n",
      "2018_SkZxCk-0Z      {449, 443, 445, 486}\n",
      "2019_rJzoujRct7     {518, 519}\n",
      "2018_HkfXMz-Ab      {573, 566}\n",
      "2017_BJ9fZNqle      {627, 623, 615}\n",
      "2019_SyxZJn05YX     {672, 673, 657, 669, 671}\n",
      "2017_B1ckMDqlg      {714, 707}\n",
      "2017_HJ0NvFzxl      {739}\n",
      "2017_S1_pAu9xl      {806, 808, 809, 810, 792}\n",
      "2018_SyYYPdg0-      {834, 867, 868, 869, 870, 872, 873, 844, 830}\n",
      "2017_BJAA4wKxg      {884}\n",
      "2019_HyVxPsC9tm     {931, 933, 905, 909, 912, 913, 919, 926}\n",
      "2019_HylTBhA5tQ     {972, 950}\n",
      "2019_B1l08oAct7     {994, 996, 1064, 1004, 1007, 1044, 1047, 1048, 1055}\n",
      "2018_H135uzZ0-      {1072, 1079}\n",
      "2017_H1oyRlYgg      set()\n",
      "2017_r1y1aawlg      {1125, 1162, 1100, 1102, 1168}\n",
      "2020_r1eX1yrKwB     {1177, 1202, 1212}\n",
      "2020_Byg79h4tvB     {1243, 1268}\n",
      "2019_H1lFZnR5YX     {1281, 1316, 1284, 1318, 1289, 1331, 1333, 1279}\n",
      "2020_BkeWw6VFwr     {1347, 1373}\n",
      "2018_HyHmGyZCZ      {1406, 1421, 1390, 1426}\n",
      "2018_HyUNwulC-      {1451, 1452}\n",
      "2020_HkgsPhNYPS     {1504, 1464, 1497, 1500, 1502}\n"
     ]
    }
   ],
   "source": [
    "for k in test_set:\n",
    "    print('{:20}{}'.format(k, gt_dict[k][\"mcomp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_for_test = defaultdict(list)\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    test_sent_raw = str(df.loc[i][\"Sent\"])\n",
    "    \n",
    "    # Replace URLs with [URL]\n",
    "    test_sent_raw = re.sub(r'http[s]?://[a-zA-z\\.\\-/0-9~]*', \"[URL]\", test_sent_raw)\n",
    "    test_sent_raw = re.sub(r'papers.nips.cc/paper/[a-zA-z\\.\\-/0-9~]*', \"[URL]\", test_sent_raw)\n",
    "    test_sent_raw = re.sub(r'arxiv.org/[a-zA-z\\.\\-/0-9~]*', \"[URL]\", test_sent_raw)\n",
    "    \n",
    "    sents_for_test[pid].append((df.loc[i][\"UID\"], test_sent_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Sent</th>\n",
       "      <th>MComp</th>\n",
       "      <th>Cat</th>\n",
       "      <th>SubCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>243</td>\n",
       "      <td>2020_ryen_CEFwr</td>\n",
       "      <td>Reject</td>\n",
       "      <td>It extends this approach by introducing an add...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179</td>\n",
       "      <td>2018_H1LAqMbRW</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Experimentally, the results are rather weak co...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157</td>\n",
       "      <td>2017_HyTqHL5xg</td>\n",
       "      <td>Accept</td>\n",
       "      <td>The experiments are interesting but I'm still ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>146</td>\n",
       "      <td>2017_HyTqHL5xg</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Section 2.2 says they do the latter in the int...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>2017_ByToKu9ll</td>\n",
       "      <td>Reject</td>\n",
       "      <td>4)This paper proposed an improved version of t...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID              PID     Dec  \\\n",
       "0  243  2020_ryen_CEFwr  Reject   \n",
       "1  179   2018_H1LAqMbRW  Reject   \n",
       "2  157   2017_HyTqHL5xg  Accept   \n",
       "3  146   2017_HyTqHL5xg  Accept   \n",
       "4   90   2017_ByToKu9ll  Reject   \n",
       "\n",
       "                                                Sent  MComp   Cat SubCat  \n",
       "0  It extends this approach by introducing an add...      0  None   None  \n",
       "1  Experimentally, the results are rather weak co...      0  None   None  \n",
       "2  The experiments are interesting but I'm still ...      0  None   None  \n",
       "3  Section 2.2 says they do the latter in the int...      0  None   None  \n",
       "4  4)This paper proposed an improved version of t...      0  None   None  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_excel(\"InputTrainSet-Reviews7_Ann.xlsx\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sets = {\"mcomp\": [], \"non_mcomp\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, df_train.shape[0]):\n",
    "    pid = df_train.loc[i][\"PID\"]\n",
    "    train_sent_raw = str(df_train.loc[i][\"Sent\"])\n",
    "    \n",
    "    type_comp = df_train.loc[i][\"MComp\"]\n",
    "    \n",
    "    if type_comp == 1:\n",
    "        train_sets[\"mcomp\"].append(train_sent_raw)\n",
    "    else:\n",
    "        train_sets[\"non_mcomp\"].append(train_sent_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 270)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sets[\"mcomp\"]), len(train_sets[\"non_mcomp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"entities_dict_smaller\", \"r\") as f:\n",
    "    entity_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Material', 'Method', 'Metric', 'Task'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(entity_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('convolutional neural networks', 'Method'),\n",
       " ('convnets', 'Method'),\n",
       " ('recognition', 'Task'),\n",
       " ('visual recognition tasks', 'Task'),\n",
       " ('age estimation', 'Task'),\n",
       " ('head pose estimation', 'Task'),\n",
       " ('multi - label classification', 'Task'),\n",
       " ('semantic segmentation', 'Task'),\n",
       " ('classification', 'Task'),\n",
       " ('deep convnets', 'Method'),\n",
       " ('dldl', 'Method'),\n",
       " ('feature learning', 'Task'),\n",
       " ('deep learning', 'Method'),\n",
       " ('image classification', 'Task'),\n",
       " ('deep learning methods', 'Method'),\n",
       " ('image classification tasks', 'Task'),\n",
       " ('human pose estimation', 'Task'),\n",
       " ('convnet', 'Method'),\n",
       " ('recognition tasks', 'Task'),\n",
       " ('ensemble', 'Method')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_dict.items())[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1784\n"
     ]
    }
   ],
   "source": [
    "entity_key_map = {}\n",
    "for i in entity_dict:\n",
    "    s = re.sub('[^0-9a-zA-Z,:;.?!\\- ]+', '', i)\n",
    "    while s.find(\"  \") > -1:\n",
    "        s = s.replace(\"  \", \" \")\n",
    "    if len(s) > 2:\n",
    "        cl = re.sub('[^0-9a-zA-Z ]+', '', i)\n",
    "        while cl.find(\"  \") > -1:\n",
    "            cl = cl.replace(\"  \", \" \")\n",
    "        entity_key_map[cl.strip()] = i\n",
    "print(len(entity_key_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    }
   ],
   "source": [
    "coun = 0\n",
    "for i in entity_dict:\n",
    "    if len(i) < 5:\n",
    "        coun +=1\n",
    "#         print(i)\n",
    "print(coun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('convolutional neural networks', 'convolutional neural networks'),\n",
       " ('convnets', 'convnets'),\n",
       " ('recognition', 'recognition'),\n",
       " ('visual recognition tasks', 'visual recognition tasks'),\n",
       " ('age estimation', 'age estimation')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_key_map.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Material': 165, 'Method': 1191, 'Metric': 158, 'Task': 289})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(entity_dict.values())\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(c)\n",
    "reverse_map = defaultdict(list)\n",
    "\n",
    "for k, v in entity_dict.items():\n",
    "    reverse_map[v].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in reverse_map[\"Task\"]:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"MNIST\" in entity_key_map, \"mnist\" in entity_key_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. USE4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: pip3.7: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!pip3.7 list | grep -E 'transformers|tokenizers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 180.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 360.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 540.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 720.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 900.00MB\n",
      "INFO:absl:Downloaded https://tfhub.dev/google/universal-sentence-encoder/4, Total size: 987.47MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n"
     ]
    }
   ],
   "source": [
    "embed_text_using_use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_using_roberta(text):\n",
    "    vec = embed_text_using_use([text])\n",
    "    return vec/norm(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def embed_text_using_roberta(text):\n",
    "#     input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "#     outputs = model(input_ids)\n",
    "#     last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "#     return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mask_entities(sentence, replace_with_dataset=True):\n",
    "#     cleaned_sent = re.sub('[^0-9a-zA-Z,:;.?!\\- ]+', ' ', sentence)\n",
    "#     while cleaned_sent.find(\"  \") > -1:\n",
    "#         cleaned_sent = cleaned_sent.replace(\"  \", \" \")\n",
    "    \n",
    "#     entity_key_map_keys = list(entity_key_map.keys()) # As we will be dunamically adding entries to this dict an dthat will throw an error.\n",
    "#     entities_found = []\n",
    "#     for i in entity_key_map_keys:\n",
    "#         if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "#             entities_found.append(i)\n",
    "#         elif cleaned_sent.lower().find(\" \" + i + \" \") > -1:\n",
    "#             found_idx = cleaned_sent.lower().find(\" \" + i + \" \")\n",
    "#             entity_dict[cleaned_sent[found_idx:found_idx+len(\" \" + i + \" \")]] = entity_dict[i]\n",
    "#             entity_key_map[cleaned_sent[found_idx:found_idx+len(\" \" + i + \" \")]] = entity_key_map[i]\n",
    "    \n",
    "#     entities_found.sort(key=lambda s: len(s))\n",
    "#     len_sorted_entities = entities_found.copy()\n",
    "    \n",
    "#     subset_entities = []\n",
    "#     # Remove subset entities (eg: Among cnn and 3-layer-cnn, prefer the latter)\n",
    "#     for fe in len_sorted_entities:\n",
    "#         for other_ent in len_sorted_entities:\n",
    "#             if fe != other_ent and other_ent.find(fe) > -1:\n",
    "#                 subset_entities.append(fe)\n",
    "#                 break\n",
    "#     for se in subset_entities:\n",
    "#         len_sorted_entities.remove(se)\n",
    "#     for maxents in len_sorted_entities:\n",
    "#         mask_name = \" \" + entity_dict[entity_key_map[i]].lower() + \" \"\n",
    "#         if replace_with_dataset:\n",
    "#             if mask_name == \" material \":\n",
    "#                 mask_name = \" dataset \"\n",
    "#         cleaned_sent = cleaned_sent.replace(\" \" + maxents + \" \", mask_name)\n",
    "#     words_cleaned = nltk.word_tokenize(cleaned_sent)\n",
    "#     dups_removed = [v for i, v in enumerate(words_cleaned) if i == 0 or v != words_cleaned[i-1]]\n",
    "#     new_dup_removed_sent = \" \".join(dups_removed)\n",
    "#     return new_dup_removed_sent.strip()\n",
    "\n",
    "# #     #print(cleaned_sent)\n",
    "# #     for i in entity_key_map:\n",
    "# #         if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "# #             #print(\"Substituting ent: {} with mask: {}\".format(i, entity_dict[entity_key_map[i]].lower()))\n",
    "# #             cleaned_sent = cleaned_sent.replace(i, entity_dict[entity_key_map[i]].lower())\n",
    "# #     return cleaned_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "sp_toks = [\"result\", \"method\", \"task\", \"dataset\", \"metric\", \"baseline\", \"fair\", \"unfair\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunks_using_spacy_dp(conssentence, replace_with_dataset=True):\n",
    "    return []\n",
    "# #     conssentence = mask_entities(conssentence, replace_with_dataset)\n",
    "# #     print(conssentence)\n",
    "#     doc = nlp(conssentence)\n",
    "#     verb_subtree = []\n",
    "\n",
    "#     for s in doc.sents:\n",
    "# #         find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"technique\": [], \"task\": [], \"dataset\": [], \"material\": [], \"metric\": []}\n",
    "#         find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"baseline\": [], \"task\": [], \n",
    "#                                \"dataset\": [],  \"metric\": [], \"unfair\": [], \"fair\": []}\n",
    "\n",
    "#         for tok in s:\n",
    "\n",
    "#             if tok.text.lower().startswith(\"compar\"):\n",
    "#                 find_special_tokens[\"compar\"].append(tok)\n",
    "#             else:\n",
    "#                 for k in sp_toks:\n",
    "#                     if tok.text.lower().startswith(k):\n",
    "#                         find_special_tokens[k].append(tok)\n",
    "#                         break\n",
    "\n",
    "#         verb_tokens = []\n",
    "#         if find_special_tokens[\"compar\"]:\n",
    "#             for t in find_special_tokens[\"compar\"]:\n",
    "# #                     verb_subtree.append(t.subtree)\n",
    "#                 if t == s.root:\n",
    "#                     simplified_sent = \"\"\n",
    "#                     for chh in t.lefts:\n",
    "#                         simplified_sent = simplified_sent + \" \" + chh.text\n",
    "#                     simplified_sent = simplified_sent + \" \" + t.text\n",
    "#                     for chh in t.rights:\n",
    "#                         simplified_sent = simplified_sent + \" \" + chh.text\n",
    "# #                         print(\"SIMP: \", simplified_sent)\n",
    "#                     verb_subtree.append(simplified_sent)\n",
    "#                 else:\n",
    "#                     verb_subtree.append(t.subtree)\n",
    "#         else:\n",
    "#             for k in sp_toks:\n",
    "#                 for i in find_special_tokens[k]:\n",
    "#                     local_vt = []\n",
    "#                     for j in i.ancestors:\n",
    "#                         if j.pos_ == \"NOUN\":\n",
    "#                             local_vt.append(j)\n",
    "#                     if not local_vt:\n",
    "#                         for j in i.ancestors:\n",
    "#                             if j.pos_ == \"VERB\":\n",
    "#                                 local_vt.append(j)\n",
    "#                     verb_tokens = verb_tokens + local_vt\n",
    "\n",
    "\n",
    "#             for i in verb_tokens:\n",
    "#                 verb_subtree.append(i.subtree)\n",
    "\n",
    "#     eecc = []\n",
    "#     for i in verb_subtree:\n",
    "#         if type(i) == str:\n",
    "#             eecc.append(i)\n",
    "#         else:\n",
    "#             local_chunk = \"\"\n",
    "#             for lcaltok in i:\n",
    "#                 local_chunk = local_chunk + \" \" + lcaltok.text\n",
    "#             eecc.append(local_chunk)\n",
    "# #     if not eecc:\n",
    "# #         print(conssentence)\n",
    "#     return list(set(eecc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing vectors of the initial training pool of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool_roberta_vecs = {\"mcomp\": [], \"non_mcomp\": []}\n",
    "single_train_pool_roberta_vecs = {\"mcomp\": [], \"non_mcomp\": []}\n",
    "train_pool_uid_vecs = defaultdict(list)\n",
    "mc_nmc_fake = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_idx = 0\n",
    "\n",
    "for i in train_sets[\"mcomp\"]:\n",
    "    mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(i)\n",
    "    if mcomp_chunks_from_sent:\n",
    "        final_chunks = mcomp_chunks_from_sent\n",
    "    else:\n",
    "        final_chunks = [i]\n",
    "    \n",
    "    mc_nmc_fake[fake_idx] = 1\n",
    "    for single_chunk in final_chunks:\n",
    "        vec = embed_text_using_roberta(single_chunk.strip())\n",
    "        train_pool_uid_vecs[fake_idx].append(vec / norm(vec))\n",
    "        train_pool_roberta_vecs[\"mcomp\"].append(vec/norm(vec))\n",
    "    \n",
    "    collated_chunk = \" \".join(final_chunks)\n",
    "    vec = embed_text_using_roberta(collated_chunk.strip())\n",
    "    single_train_pool_roberta_vecs[\"mcomp\"].append(vec/norm(vec))\n",
    "    fake_idx += 1\n",
    "\n",
    "\n",
    "for i in train_sets[\"non_mcomp\"]:\n",
    "    mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(i)\n",
    "    if mcomp_chunks_from_sent:\n",
    "        final_chunks = mcomp_chunks_from_sent\n",
    "    else:\n",
    "        final_chunks = [i]\n",
    "    \n",
    "    mc_nmc_fake[fake_idx] = 0\n",
    "    for single_chunk in final_chunks:\n",
    "        vec = embed_text_using_roberta(single_chunk.strip())\n",
    "        train_pool_uid_vecs[fake_idx].append(vec / norm(vec))\n",
    "        train_pool_roberta_vecs[\"non_mcomp\"].append(vec/norm(vec))\n",
    "    \n",
    "    collated_chunk = \" \".join(final_chunks)\n",
    "    vec = embed_text_using_roberta(collated_chunk.strip())\n",
    "    single_train_pool_roberta_vecs[\"non_mcomp\"].append(vec/norm(vec))\n",
    "    fake_idx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_HyVxPsC9tm 938 2\n",
      "2019_HyVxPsC9tm 940 3\n",
      "2018_HyHmGyZCZ 1425 2\n"
     ]
    }
   ],
   "source": [
    "roberta_vectors = defaultdict(dict)\n",
    "\n",
    "skip_uids = []\n",
    "\n",
    "for pid in gt_dict:\n",
    "    roberta_vectors[pid] = {}\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunks = mcomp_chunks_from_sent\n",
    "            else:\n",
    "                final_chunks = [df.loc[mcs][\"Sent\"]]\n",
    "            \n",
    "            roberta_vectors[pid][mcs] = []\n",
    "            for single_chunk in final_chunks:\n",
    "                vec = embed_text_using_roberta(single_chunk.strip())\n",
    "                roberta_vectors[pid][mcs].append(vec / norm(vec))\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunks = mcomp_chunks_from_sent\n",
    "            else:\n",
    "                final_chunks = [df.loc[mcs][\"Sent\"]]\n",
    "            \n",
    "            roberta_vectors[pid][mcs] = []\n",
    "            for single_chunk in final_chunks:\n",
    "                vec = embed_text_using_roberta(single_chunk.strip())\n",
    "                roberta_vectors[pid][mcs].append(vec / norm(vec))\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 1385\n"
     ]
    }
   ],
   "source": [
    "mcomp_sentences = {}\n",
    "not_mcomp_sentences = {}\n",
    "\n",
    "for pid in gt_dict:\n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            mcomp_sentences[mcs] = pid\n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            not_mcomp_sentences[mcs] = pid\n",
    "print(len(mcomp_sentences), len(not_mcomp_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=10172, shape=(1, 512), dtype=float32, numpy=\n",
       " array([[ 7.05207139e-02,  1.61148086e-02,  5.43363951e-02,\n",
       "         -7.93926343e-02, -3.68419737e-02, -3.21301669e-02,\n",
       "         -1.34001672e-02,  3.91253680e-02,  7.19493330e-02,\n",
       "          3.89517508e-02,  7.39617720e-02,  4.03119735e-02,\n",
       "          6.88205957e-02, -2.46249381e-02,  2.50111483e-02,\n",
       "         -5.90326712e-02, -8.85217078e-03, -2.10552961e-02,\n",
       "         -6.06053136e-02,  1.33090438e-02,  3.69791463e-02,\n",
       "         -3.33877541e-02, -5.45941368e-02,  4.16288599e-02,\n",
       "          5.80235012e-02,  6.57391250e-02, -6.42068461e-02,\n",
       "          4.42074761e-02, -3.19292247e-02, -4.47518984e-03,\n",
       "         -7.72632509e-02, -6.16794676e-02, -1.43654365e-02,\n",
       "         -4.77261953e-02, -4.64392230e-02,  1.25910733e-02,\n",
       "         -1.04360163e-01, -6.84794262e-02,  9.82781593e-03,\n",
       "          4.96249199e-02,  8.56826268e-03,  4.63088118e-02,\n",
       "         -2.49133911e-03,  2.43370309e-02, -4.92904801e-03,\n",
       "          1.05162496e-02,  4.37574685e-02,  4.68532629e-02,\n",
       "          7.32601136e-02, -4.40260656e-02, -2.83016730e-02,\n",
       "         -7.81648383e-02, -3.45726199e-02,  1.04597807e-02,\n",
       "         -3.44674066e-02,  8.10972694e-03,  1.10779488e-02,\n",
       "          4.76924591e-02, -6.49306700e-02,  7.05704167e-02,\n",
       "         -3.94304842e-02,  5.18652387e-02, -1.42346304e-02,\n",
       "         -7.93348625e-02,  1.45993577e-02,  3.40605415e-02,\n",
       "         -1.00408690e-02, -5.06209843e-02,  1.05324376e-03,\n",
       "         -9.74883884e-03, -2.21953485e-02,  8.54476675e-05,\n",
       "         -7.51831606e-02, -4.69711050e-02, -3.76358144e-02,\n",
       "         -2.47213785e-02,  3.04537658e-02,  4.77920137e-02,\n",
       "         -7.31817633e-02,  7.09680617e-02, -4.87638488e-02,\n",
       "         -2.69344784e-02,  2.99652126e-02, -2.00060476e-02,\n",
       "         -7.98877236e-03,  3.14475447e-02,  6.16113748e-03,\n",
       "          6.11200035e-02, -5.95492423e-02,  3.37165482e-02,\n",
       "         -1.60428658e-02, -3.87892872e-02,  1.96479913e-02,\n",
       "         -5.55759072e-02, -2.46347450e-02, -1.40451221e-02,\n",
       "          6.53385818e-02,  1.11828670e-02,  7.09921718e-02,\n",
       "         -9.57573950e-02, -2.68991180e-02, -5.44016026e-02,\n",
       "         -6.93451166e-02,  1.57828797e-02,  4.18826379e-02,\n",
       "         -1.84629187e-02, -4.31537032e-02, -6.90103024e-02,\n",
       "         -5.03747985e-02, -2.32913122e-02, -3.26822735e-02,\n",
       "          4.86450642e-02, -5.26805446e-02,  5.26515804e-02,\n",
       "          2.54762955e-02,  4.51614559e-02,  1.94037221e-02,\n",
       "         -6.64263368e-02,  1.60287954e-02, -2.46215574e-02,\n",
       "          4.81336676e-02,  3.17616835e-02,  1.81547143e-02,\n",
       "          5.74353375e-02, -7.95020908e-02, -7.11033121e-02,\n",
       "         -1.29935499e-02,  1.44288773e-02,  6.59199804e-02,\n",
       "         -4.18530963e-02, -2.99408864e-02,  1.48595888e-02,\n",
       "          3.23622301e-02, -6.10118061e-02,  3.91885303e-02,\n",
       "         -7.40551278e-02, -4.42597270e-02,  1.97888687e-02,\n",
       "         -5.28957099e-02, -5.38017489e-02,  7.18219904e-03,\n",
       "         -3.42748091e-02, -4.04225141e-02,  2.32581384e-02,\n",
       "          9.19405892e-02,  1.51222721e-02,  8.27041566e-02,\n",
       "         -1.99961403e-04,  5.49509451e-02,  3.51796374e-02,\n",
       "         -1.23172300e-02, -5.40739559e-02,  9.90852248e-03,\n",
       "         -3.81939113e-02,  7.84445647e-03,  8.02321360e-03,\n",
       "          5.39884269e-02,  8.12136680e-02,  6.86560571e-02,\n",
       "          6.26251623e-02,  2.44667158e-02, -1.31930504e-02,\n",
       "          6.22503497e-02,  3.50104570e-02, -4.56960276e-02,\n",
       "         -7.60600844e-04,  2.65570655e-02, -3.73957902e-02,\n",
       "         -5.61624542e-02,  6.59224438e-03, -2.78558135e-02,\n",
       "          7.67124817e-02,  3.51772131e-03, -3.24149989e-02,\n",
       "         -7.89183453e-02, -8.14673863e-03, -4.22159508e-02,\n",
       "         -6.87882751e-02,  1.71572454e-02,  7.48154297e-02,\n",
       "          1.36122145e-02,  1.93872377e-02, -4.83024269e-02,\n",
       "         -1.43468427e-03,  6.09183535e-02,  5.54705448e-02,\n",
       "          1.57255456e-02, -3.17235664e-02,  7.74749890e-02,\n",
       "          5.07229147e-03,  6.99084401e-02,  1.20897172e-02,\n",
       "         -6.30325526e-02,  6.15447536e-02,  1.13306288e-02,\n",
       "         -3.16621251e-02, -3.74910161e-02,  4.86358106e-02,\n",
       "          2.91194525e-02, -4.59117070e-02,  4.16475832e-02,\n",
       "         -4.65100668e-02,  1.06456419e-02,  4.17220891e-02,\n",
       "          1.33785224e-02,  4.27507237e-03, -3.84386890e-02,\n",
       "         -5.14540763e-04, -6.16452731e-02,  8.95817671e-03,\n",
       "          1.65221356e-02, -7.55389454e-04,  3.17501836e-02,\n",
       "         -5.39804213e-02,  3.83766107e-02, -5.87203167e-02,\n",
       "         -6.46546558e-02,  5.22300974e-02,  7.70104676e-02,\n",
       "         -5.31154759e-02, -2.61133630e-02,  3.25072780e-02,\n",
       "          4.75307275e-03, -1.53988516e-02, -3.99411544e-02,\n",
       "          1.62126832e-02,  1.43949213e-02, -3.96889895e-02,\n",
       "         -3.61374244e-02,  5.43085933e-02,  5.95419034e-02,\n",
       "         -5.61993346e-02,  3.91133223e-03,  2.00903565e-02,\n",
       "          6.31963089e-02, -2.65644360e-02, -3.31407110e-03,\n",
       "         -2.56319321e-03, -3.27002369e-02,  6.24530055e-02,\n",
       "          1.49902813e-02, -3.81900533e-03, -6.00933060e-02,\n",
       "         -4.70172279e-02,  9.04359668e-02,  8.34382512e-03,\n",
       "          2.34211702e-02, -2.04934478e-02,  7.23482817e-02,\n",
       "          1.27515998e-02, -5.32216728e-02, -2.98952702e-02,\n",
       "          9.20953508e-03, -3.59797999e-02, -4.84488293e-04,\n",
       "          9.26438048e-02, -4.10445109e-02,  7.94079751e-02,\n",
       "          3.22116446e-03, -3.62923630e-02, -1.21334409e-02,\n",
       "         -5.07994927e-02,  1.84090342e-02, -2.77372506e-02,\n",
       "          2.64109820e-02, -1.90687440e-02,  1.04861781e-02,\n",
       "         -6.37285337e-02,  4.44112457e-02,  5.88247888e-02,\n",
       "         -9.37640900e-04,  1.75154302e-02, -7.14911986e-03,\n",
       "         -8.15602660e-04,  8.36564880e-03,  3.21100578e-02,\n",
       "         -6.41284138e-02, -7.19523877e-02, -4.52244356e-02,\n",
       "         -5.95369637e-02, -8.49639326e-02, -4.33308780e-02,\n",
       "         -3.49047258e-02,  7.08363801e-02, -2.63808258e-02,\n",
       "         -1.38554145e-02, -4.54627648e-02,  8.68350863e-02,\n",
       "         -2.96097789e-02, -6.06305599e-02,  3.53947580e-02,\n",
       "          4.57528792e-02, -3.01398914e-02, -1.18371509e-02,\n",
       "          9.18489322e-02, -7.16836154e-02, -6.20504692e-02,\n",
       "          2.77867187e-02,  1.20278904e-02, -3.92639777e-03,\n",
       "         -4.90124561e-02,  1.76519342e-02,  6.74759150e-02,\n",
       "         -2.78331004e-02, -6.36050254e-02,  4.05246876e-02,\n",
       "          4.45488468e-02,  3.37198116e-02, -7.91852421e-04,\n",
       "         -7.92938098e-02, -5.55064203e-03,  1.57540068e-02,\n",
       "          8.87256488e-02, -2.59258188e-02,  3.78517024e-02,\n",
       "          1.80004276e-02, -9.97232646e-03,  6.60357699e-02,\n",
       "         -3.02115809e-02,  2.89292913e-02,  4.09135595e-02,\n",
       "         -4.85084280e-02, -4.92890775e-02, -7.28593394e-03,\n",
       "          1.22855930e-02, -5.06341867e-02,  6.04524501e-02,\n",
       "          5.97299077e-02, -4.00464907e-02,  6.82881474e-02,\n",
       "         -5.44508919e-02, -3.21882442e-02, -2.97725685e-02,\n",
       "         -2.25900877e-02, -2.53082402e-02,  6.03932980e-03,\n",
       "          6.60501933e-03,  4.33083400e-02, -8.31740908e-03,\n",
       "         -2.83236317e-02,  5.30309975e-03, -3.97934169e-02,\n",
       "         -5.94779551e-02, -4.95130681e-02, -9.29261819e-02,\n",
       "         -3.45436931e-02,  1.89649668e-02, -3.34418990e-04,\n",
       "         -4.43235785e-02,  7.92006701e-02, -2.54199840e-02,\n",
       "          2.82250363e-02, -4.36892398e-02,  5.05162179e-02,\n",
       "          7.51257986e-02, -2.40767654e-02, -8.90307054e-02,\n",
       "          3.86950299e-02, -4.25681248e-02, -6.81317598e-02,\n",
       "         -5.44954129e-02, -7.67685175e-02, -3.87231074e-02,\n",
       "         -7.04111904e-02, -5.01268357e-02, -9.92116332e-02,\n",
       "         -3.44799794e-02,  7.51651824e-02, -1.08087910e-02,\n",
       "         -6.21522479e-02, -7.12433457e-03, -3.03164572e-02,\n",
       "         -8.72278493e-03, -1.44423209e-02, -4.25855592e-02,\n",
       "          1.22563997e-02,  1.06972745e-02, -1.87229775e-02,\n",
       "          4.15570065e-02,  3.51068052e-03, -2.25222372e-02,\n",
       "          3.09074521e-02,  1.01182736e-01,  3.72678749e-02,\n",
       "         -1.14434138e-02,  5.72136184e-03,  6.93538338e-02,\n",
       "          1.62777095e-03,  4.26810123e-02, -2.64808573e-02,\n",
       "         -1.06210679e-01, -2.37685852e-02,  4.22469527e-02,\n",
       "          1.54751940e-02, -1.38308173e-02, -5.57321459e-02,\n",
       "          4.01938669e-02, -2.93057971e-02,  4.63546030e-02,\n",
       "         -3.36675695e-03, -3.75178680e-02,  4.48327921e-02,\n",
       "         -3.66731808e-02,  4.03367877e-02,  2.64073368e-02,\n",
       "          2.17720345e-02,  1.68998260e-02,  5.14513403e-02,\n",
       "          5.34079596e-02,  2.07696296e-02, -1.69093218e-02,\n",
       "          2.43318882e-02,  3.10570151e-02, -5.99562526e-02,\n",
       "          2.27460600e-02,  4.88572977e-02,  1.61183495e-02,\n",
       "         -7.37801716e-02,  7.68527314e-02, -2.18537375e-02,\n",
       "          5.45434169e-02, -1.98877398e-02,  5.25342748e-02,\n",
       "          4.91367094e-02,  2.36500949e-02, -2.74079908e-02,\n",
       "         -3.67319249e-02, -6.04397170e-02,  2.68184654e-02,\n",
       "         -6.50279745e-02, -3.10579352e-02, -3.59245576e-02,\n",
       "         -7.73632079e-02,  4.38024551e-02, -8.71259570e-02,\n",
       "          3.68348137e-03,  9.30423010e-03, -2.67858710e-02,\n",
       "         -7.65673677e-03, -1.37599390e-02, -4.79766279e-02,\n",
       "          2.77837534e-02,  1.86462384e-02,  7.89552554e-03,\n",
       "          5.63425720e-02,  2.19165925e-02,  2.49222852e-02,\n",
       "         -5.74246645e-02,  6.39789701e-02, -1.09777902e-03,\n",
       "         -1.13654891e-02,  7.80590996e-02,  4.95437812e-03,\n",
       "          2.17316374e-02, -1.00964708e-02,  4.09866087e-02,\n",
       "         -2.83412319e-02,  3.70686948e-02, -3.01603973e-02,\n",
       "          7.41942525e-02, -1.23418225e-02,  5.62682934e-02,\n",
       "          1.36889736e-04,  1.00925863e-02, -4.35733348e-02,\n",
       "         -2.59843990e-02, -2.07222905e-02, -3.41914743e-02,\n",
       "          5.18413447e-02, -1.70187559e-02, -2.29660235e-02,\n",
       "         -3.23116481e-02,  1.88749544e-02,  2.24862676e-02,\n",
       "         -3.58110741e-02, -6.67492524e-02,  6.23269705e-03,\n",
       "         -2.15890296e-02,  7.03686401e-02, -1.86183006e-02,\n",
       "          3.19478638e-03, -2.21953094e-02, -7.93885216e-02,\n",
       "         -3.25904824e-02,  1.65090729e-02, -6.01064377e-02,\n",
       "          4.84269857e-02,  1.51889613e-02,  2.75832415e-02,\n",
       "          3.18646133e-02, -3.88708785e-02,  2.31709927e-02,\n",
       "          2.93965377e-02,  5.41172773e-02,  3.76354940e-02,\n",
       "          3.38517725e-02, -6.05301149e-02,  3.67324124e-03,\n",
       "         -1.08773876e-02,  2.49375254e-02,  1.05351433e-02,\n",
       "          6.27449751e-02, -3.00462525e-02,  1.23516740e-02,\n",
       "         -8.47477838e-02, -2.66624857e-02,  3.16642486e-02,\n",
       "          2.93688532e-02, -2.25035697e-02, -5.31470589e-02,\n",
       "          3.27989459e-03, -1.41815729e-02]], dtype=float32)>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check dimensions to set below for initialization\n",
    "roberta_vectors[pid][mcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2020_HkgsPhNYPS', 1503)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid, mcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1505, 513)\n"
     ]
    }
   ],
   "source": [
    "testdf = df.copy()\n",
    "\n",
    "xtest = testdf.drop(columns=[\"PID\", \"Dec\", \"MComp\", \"Sent\", \"Cat\", \"SubCat\"])\n",
    "ytest = testdf.drop(columns=[\"PID\", \"Dec\", \"Sent\", \"Cat\", \"SubCat\"])\n",
    "# print(xtest.head())\n",
    "# print(ytest.head())\n",
    "\n",
    "for i in range(1, 513):\n",
    "    xtest[i] = np.nan\n",
    "\n",
    "for pid in gt_dict:\n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            xtest.iloc[mcs] = [mcs] + list(np.mean(roberta_vectors[pid][mcs], axis=0)[0])\n",
    "        else:\n",
    "            xtest.iloc[mcs] = [mcs] + list(np.zeros(512))\n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            xtest.iloc[mcs] = [mcs] + list(np.mean(roberta_vectors[pid][mcs], axis=0)[0])\n",
    "        else:\n",
    "            xtest.iloc[mcs] = [mcs] + list(np.zeros(512))\n",
    "print(xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.031231</td>\n",
       "      <td>-0.043420</td>\n",
       "      <td>0.016040</td>\n",
       "      <td>-0.013123</td>\n",
       "      <td>0.033781</td>\n",
       "      <td>-0.068314</td>\n",
       "      <td>0.032943</td>\n",
       "      <td>0.039433</td>\n",
       "      <td>-0.095107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045744</td>\n",
       "      <td>-0.045012</td>\n",
       "      <td>0.025624</td>\n",
       "      <td>-0.018927</td>\n",
       "      <td>0.082092</td>\n",
       "      <td>0.04946</td>\n",
       "      <td>0.023653</td>\n",
       "      <td>0.051591</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>0.017926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.031806</td>\n",
       "      <td>-0.008774</td>\n",
       "      <td>0.046461</td>\n",
       "      <td>-0.029627</td>\n",
       "      <td>-0.061475</td>\n",
       "      <td>-0.047746</td>\n",
       "      <td>0.014361</td>\n",
       "      <td>0.039624</td>\n",
       "      <td>0.075027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004503</td>\n",
       "      <td>-0.019470</td>\n",
       "      <td>-0.029604</td>\n",
       "      <td>-0.023130</td>\n",
       "      <td>0.044001</td>\n",
       "      <td>0.06457</td>\n",
       "      <td>-0.023551</td>\n",
       "      <td>0.094495</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.021212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID         1         2         3         4         5         6         7  \\\n",
       "0    0 -0.031231 -0.043420  0.016040 -0.013123  0.033781 -0.068314  0.032943   \n",
       "1    1 -0.031806 -0.008774  0.046461 -0.029627 -0.061475 -0.047746  0.014361   \n",
       "\n",
       "          8         9  ...       503       504       505       506       507  \\\n",
       "0  0.039433 -0.095107  ...  0.045744 -0.045012  0.025624 -0.018927  0.082092   \n",
       "1  0.039624  0.075027  ... -0.004503 -0.019470 -0.029604 -0.023130  0.044001   \n",
       "\n",
       "       508       509       510       511       512  \n",
       "0  0.04946  0.023653  0.051591  0.060051  0.017926  \n",
       "1  0.06457 -0.023551  0.094495  0.001949  0.021212  \n",
       "\n",
       "[2 rows x 513 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296, 513)\n",
      "(296, 2)\n"
     ]
    }
   ],
   "source": [
    "# xtrain = pd.DataFrame(columns=[\"UID\"]+[str(x) for x in range(1,769)])\n",
    "# ytrain = pd.DataFrame(columns=[\"UID\", \"MComp\"])\n",
    "xtlist = []\n",
    "ytlist = []\n",
    "\n",
    "for i in train_pool_uid_vecs:\n",
    "    xtlist.append([i] + list(np.mean(train_pool_uid_vecs[i], axis=0)[0]))\n",
    "    ytlist.append([i, mc_nmc_fake[i]])\n",
    "\n",
    "xtrain = pd.DataFrame(xtlist)\n",
    "ytrain = pd.DataFrame(ytlist)\n",
    "\n",
    "xtrain = xtrain.rename(columns={0: 'UID'})\n",
    "ytrain = ytrain.rename(columns={0: 'UID', 1: 'MComp'})\n",
    "\n",
    "print(xtrain.shape)\n",
    "print(ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.042910</td>\n",
       "      <td>-0.013350</td>\n",
       "      <td>0.065731</td>\n",
       "      <td>-0.056224</td>\n",
       "      <td>0.027743</td>\n",
       "      <td>-0.077422</td>\n",
       "      <td>-0.060043</td>\n",
       "      <td>0.020768</td>\n",
       "      <td>-0.001715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>0.037075</td>\n",
       "      <td>-0.068602</td>\n",
       "      <td>-0.067326</td>\n",
       "      <td>-0.057479</td>\n",
       "      <td>-0.010620</td>\n",
       "      <td>0.062733</td>\n",
       "      <td>0.022102</td>\n",
       "      <td>0.029480</td>\n",
       "      <td>-0.011256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.005359</td>\n",
       "      <td>-0.015001</td>\n",
       "      <td>-0.026401</td>\n",
       "      <td>-0.065217</td>\n",
       "      <td>0.002959</td>\n",
       "      <td>-0.048467</td>\n",
       "      <td>0.074967</td>\n",
       "      <td>-0.005379</td>\n",
       "      <td>0.031549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085651</td>\n",
       "      <td>0.066054</td>\n",
       "      <td>-0.032924</td>\n",
       "      <td>-0.030025</td>\n",
       "      <td>-0.046595</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.073984</td>\n",
       "      <td>0.050455</td>\n",
       "      <td>0.044768</td>\n",
       "      <td>-0.003457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID         1         2         3         4         5         6         7  \\\n",
       "0    0  0.042910 -0.013350  0.065731 -0.056224  0.027743 -0.077422 -0.060043   \n",
       "1    1 -0.005359 -0.015001 -0.026401 -0.065217  0.002959 -0.048467  0.074967   \n",
       "\n",
       "          8         9  ...       503       504       505       506       507  \\\n",
       "0  0.020768 -0.001715  ...  0.011550  0.037075 -0.068602 -0.067326 -0.057479   \n",
       "1 -0.005379  0.031549  ... -0.085651  0.066054 -0.032924 -0.030025 -0.046595   \n",
       "\n",
       "        508       509       510       511       512  \n",
       "0 -0.010620  0.062733  0.022102  0.029480 -0.011256  \n",
       "1  0.016992  0.073984  0.050455  0.044768 -0.003457  \n",
       "\n",
       "[2 rows x 513 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>MComp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID  MComp\n",
       "0    0      1\n",
       "1    1      1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing & results----------------\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# nlp preprocessing\n",
    "import spacy\n",
    "\n",
    "# Models-------------------------\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "import sklearn.gaussian_process.kernels as kls\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "\n",
    "# for visualizing ---------------\n",
    "from sklearn import tree\n",
    "from six import StringIO \n",
    "from IPython.display import Image, display\n",
    "import seaborn as sns\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# General purpose\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --user graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dict = {\n",
    "    'DecisionTree': {\"model\": DecisionTreeClassifier(random_state=42), \"params\": {'max_depth': list(range(10, 250, 20))}},\n",
    "    'RandomForest': {\"model\": RandomForestClassifier(random_state=42),\n",
    "                     \"params\": {'n_estimators': list(range(5, 100, 5)), 'max_depth': list(range(10, 250, 20))}},\n",
    "    'LogisticR_L1': {\"model\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "                     \"params\": {'penalty': ['l1'], 'solver': ['liblinear', 'saga']}},\n",
    "    'LogisticR_L2': {\"model\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "                     \"params\": {'penalty': ['l2'], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}},\n",
    "    'LogisticR': {\"model\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "                  \"params\": {'penalty': ['none'], 'solver': ['newton-cg', 'lbfgs', 'sag', 'saga']}},\n",
    "    'RidgeClf': {\"model\": RidgeClassifier(max_iter=1000), \"params\": {}},\n",
    "    'SVC_linear': {\"model\": SVC(random_state=42), \"params\": {'kernel': ['linear'], \n",
    "                                                             'C': [0.5, 1.0, 1.5, 2.0, 2.5]}},\n",
    "    'SVC_poly': {\"model\": SVC(random_state=42),\n",
    "                 \"params\": {'kernel': ['poly'], 'degree': [3, 4, 5], 'gamma': ['scale', 'auto'], \n",
    "                            'C': [0.5, 1.0, 1.5, 2.0, 2.5]}},\n",
    "    'SVC_others': {\"model\": SVC(random_state=42), \"params\": {'kernel': ['rbf', 'sigmoid'], \n",
    "                                                             'gamma': ['scale', 'auto'], \n",
    "                                                             'C': [0.5, 1.0, 1.5, 2.0, 2.5]}},\n",
    "    'GussianNB': {\"model\": GaussianNB(), \"params\": {}},\n",
    "    'KNN': {\"model\": KNeighborsClassifier(), \"params\": {'n_neighbors': list(range(1, 20))}},\n",
    "    'GaussianProcessClf': {\"model\": GaussianProcessClassifier(random_state=42, kernel=kls.RBF()), \"params\": {}},\n",
    "    'Bagging_SVC': {\"model\": BaggingClassifier(random_state=42), \"params\": {'n_estimators': list(range(5, 100, 5)),\n",
    "                                                                            'base_estimator': [SVC(kernel='linear'),\n",
    "                                                                                               SVC(kernel='poly',\n",
    "                                                                                                   degree=3,\n",
    "                                                                                                   gamma='scale')]}},\n",
    "    'BaggingDT': {\"model\": BaggingClassifier(random_state=42), \"params\": {'n_estimators': list(range(5, 100, 5)),\n",
    "                                                                          'base_estimator': [\n",
    "                                                                              DecisionTreeClassifier(random_state=42,\n",
    "                                                                                                     max_depth=10),\n",
    "                                                                              DecisionTreeClassifier(random_state=42,\n",
    "                                                                                                     max_depth=50),\n",
    "                                                                              DecisionTreeClassifier(random_state=42,\n",
    "                                                                                                     max_depth=100)]}},\n",
    "    'AdaBoost': {\"model\": AdaBoostClassifier(random_state=42), \"params\": {'n_estimators': list(range(5, 100, 5)),\n",
    "                                                                          'base_estimator': [DecisionTreeClassifier(\n",
    "                                                                                                 random_state=42,\n",
    "                                                                                                 max_depth=10),\n",
    "                                                                                             DecisionTreeClassifier(\n",
    "                                                                                                 random_state=42,\n",
    "                                                                                                 max_depth=50),\n",
    "                                                                                             DecisionTreeClassifier(\n",
    "                                                                                                 random_state=42,\n",
    "                                                                                                 max_depth=100)]}},\n",
    "    'ExtraTrees': {\"model\": ExtraTreesClassifier(random_state=42), \"params\": {'n_estimators': list(range(5, 105, 5)), \n",
    "                                                                              'max_depth': [10, 50, 100, 250, 400]}},\n",
    "    'MLP_l1': {\"model\": MLPClassifier(random_state=42), \"params\": {'hidden_layer_sizes': [(x,) for x in \n",
    "                                                                                          range(50, 600, 100)], \n",
    "                                                                  'activation': ['logistic', 'tanh', 'relu'],\n",
    "                                                                  'solver': ['adam', 'sgd'], 'early_stopping': \n",
    "                                                                   [True]}},\n",
    "    'MLP_l2': {\"model\": MLPClassifier(random_state=42), \"params\": {'hidden_layer_sizes': [(x, y) for x in \n",
    "                                                                                          range(50, 600, 100) \n",
    "                                                                                          for y in range(50, 360, 100)], \n",
    "                                                                  'activation': ['logistic', 'tanh', 'relu'],\n",
    "                                                                  'solver': ['adam', 'sgd'], 'early_stopping': \n",
    "                                                                                               [True]}},\n",
    "#     'MLP_l3': {\"model\": MLPClassifier(random_state=42), \"params\": {'hidden_layer_sizes': [(x, y, z) for x in \n",
    "#                                                                                           range(50, 600, 100) \n",
    "#                                                                                           for y in range(50, 600, 100)\n",
    "#                                                                                           for z in range(50, 360, 100)], \n",
    "#                                                                   'activation': ['logistic', 'tanh', 'relu'],\n",
    "#                                                                   'solver': ['adam', 'sgd'], 'early_stopping': \n",
    "#                                                                                                [True]}},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree 0.8515819209039549 {'max_depth': 10}\n",
      "RandomForest 0.9155932203389832 {'max_depth': 30, 'n_estimators': 5}\n",
      "LogisticR_L1 0.9122033898305085 {'penalty': 'l1', 'solver': 'liblinear'}\n",
      "LogisticR_L2 0.9122033898305085 {'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "LogisticR 0.9257062146892656 {'penalty': 'none', 'solver': 'newton-cg'}\n",
      "RidgeClf 0.9020903954802261 {}\n",
      "SVC_linear 0.9122033898305085 {'C': 0.5, 'kernel': 'linear'}\n",
      "SVC_poly 0.9122033898305085 {'C': 0.5, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "SVC_others 0.9122033898305085 {'C': 0.5, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "GussianNB 0.9021468926553672 {}\n",
      "KNN 0.9155932203389832 {'n_neighbors': 2}\n",
      "GaussianProcessClf 0.9122033898305085 {}\n",
      "Bagging_SVC 0.9122033898305085 {'base_estimator': SVC(kernel='poly'), 'n_estimators': 15}\n",
      "BaggingDT 0.9156497175141244 {'base_estimator': DecisionTreeClassifier(max_depth=10, random_state=42), 'n_estimators': 30}\n",
      "AdaBoost 0.8546892655367232 {'base_estimator': DecisionTreeClassifier(max_depth=10, random_state=42), 'n_estimators': 5}\n",
      "ExtraTrees 0.9155932203389832 {'max_depth': 10, 'n_estimators': 20}\n",
      "MLP_l1 0.9122033898305085 {'activation': 'logistic', 'early_stopping': True, 'hidden_layer_sizes': (50,), 'solver': 'sgd'}\n",
      "MLP_l2 0.9223728813559322 {'activation': 'tanh', 'early_stopping': True, 'hidden_layer_sizes': (350, 50), 'solver': 'adam'}\n",
      "================================================================================\n",
      "RidgeClassifier(max_iter=1000)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1388\n",
      "           1       0.89      0.14      0.24       117\n",
      "\n",
      "    accuracy                           0.93      1505\n",
      "   macro avg       0.91      0.57      0.60      1505\n",
      "weighted avg       0.93      0.93      0.91      1505\n",
      "\n",
      "Test acc: 0.9315614617940199\n",
      "Weighted F1 score:  0.9076456642110838\n"
     ]
    }
   ],
   "source": [
    "# model_results = pd.DataFrame()\n",
    "# model_results['Train_Accuracy'] = None\n",
    "# model_results['Test_Accuracy'] = None\n",
    "# model_results['best_params'] = None\n",
    "\n",
    "# # X_train_final = X_train_normalized.drop(columns=[\"ref_latest\"])\n",
    "# # X_test_normalized_remgsdata = X_test_normalized.drop(columns=[\"ref_latest\"])\n",
    "# # X_train_normalized_remgsdata = X_train_normalized.copy()\n",
    "# # X_test_normalized_remgsdata = X_test_normalized.copy()\n",
    "\n",
    "# xtrain_final = xtrain.drop(columns=[\"UID\"])\n",
    "# ytrain_final = ytrain.drop(columns=[\"UID\"])\n",
    "\n",
    "# xtest_final = xtest.drop(columns=[\"UID\"])\n",
    "# ytest_final = ytest.drop(columns=[\"UID\"])\n",
    "\n",
    "\n",
    "# best_clf_ours = None\n",
    "# best_clf_val = 0\n",
    "\n",
    "# for clf_name, clf in clf_dict.items():\n",
    "#     classifier = GridSearchCV(clf['model'], clf['params'], n_jobs=5)\n",
    "#     classifier.fit(xtrain_final, ytrain_final)\n",
    "#     best_model = classifier.best_estimator_\n",
    "#     print(clf_name, classifier.best_score_, classifier.best_params_)\n",
    "    \n",
    "#     y_predicted = best_model.predict(xtest_final)\n",
    "#     test_acc = accuracy_score(ytest_final, y_predicted)\n",
    "    \n",
    "#     if test_acc > best_clf_val:\n",
    "#         best_clf_val = test_acc\n",
    "#         best_clf_ours = best_model\n",
    "    \n",
    "#     model_results.loc[clf_name, ['Train_Accuracy', 'Test_Accuracy', 'best_params']] = [classifier.best_score_, test_acc, classifier.best_params_]\n",
    "#     clsr = classification_report(ytest_final, y_predicted)\n",
    "\n",
    "# print(\"================================================================================\")\n",
    "# print(best_clf_ours)\n",
    "# best_y_hat = best_clf_ours.predict(xtest_final)\n",
    "# clsr = classification_report(ytest_final, best_y_hat)\n",
    "# print(clsr)\n",
    "# test_acc = accuracy_score(ytest_final, best_y_hat)\n",
    "# print(\"Test acc:\", test_acc )\n",
    "# print(\"Weighted F1 score: \", f1_score(ytest_final, best_y_hat, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision_recall_fscore_support(ytest_final, best_y_hat, average='macro')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree 0.8445945945945946 {'max_depth': 10}\n",
      "RandomForest 0.9155405405405406 {'max_depth': 10, 'n_estimators': 10}\n",
      "LogisticR_L1 0.9121621621621622 {'penalty': 'l1', 'solver': 'liblinear'}\n",
      "LogisticR_L2 0.9121621621621622 {'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "LogisticR 0.9459459459459459 {'penalty': 'none', 'solver': 'lbfgs'}\n",
      "RidgeClf 0.918918918918919 {}\n",
      "SVC_linear 0.9459459459459459 {'C': 2.5, 'kernel': 'linear'}\n",
      "SVC_poly 0.9121621621621622 {'C': 0.5, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "SVC_others 0.9358108108108109 {'C': 2.5, 'gamma': 'scale', 'kernel': 'sigmoid'}\n",
      "GussianNB 0.9358108108108109 {}\n",
      "KNN 0.9222972972972973 {'n_neighbors': 3}\n",
      "GaussianProcessClf 0.9121621621621622 {}\n",
      "Bagging_SVC 0.918918918918919 {'base_estimator': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
      "    shrinking=True, tol=0.001, verbose=False), 'n_estimators': 5}\n",
      "BaggingDT 0.9121621621621622 {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=42, splitter='best'), 'n_estimators': 10}\n",
      "AdaBoost 0.8513513513513513 {'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=42, splitter='best'), 'n_estimators': 5}\n",
      "ExtraTrees 0.9121621621621622 {'max_depth': 10, 'n_estimators': 10}\n",
      "MLP_l1 0.9121621621621622 {'activation': 'logistic', 'early_stopping': True, 'hidden_layer_sizes': (150,), 'solver': 'adam'}\n",
      "MLP_l2 0.9121621621621622 {'activation': 'logistic', 'early_stopping': True, 'hidden_layer_sizes': (50, 50), 'solver': 'adam'}\n",
      "================================================================================\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95      1388\n",
      "           1       0.42      0.50      0.45       117\n",
      "\n",
      "    accuracy                           0.91      1505\n",
      "   macro avg       0.69      0.72      0.70      1505\n",
      "weighted avg       0.92      0.91      0.91      1505\n",
      "\n",
      "Test acc: 0.9076411960132891\n",
      "Weighted F1 score:  0.9110922028879928\n"
     ]
    }
   ],
   "source": [
    "model_results = pd.DataFrame()\n",
    "model_results['Train_Accuracy'] = None\n",
    "model_results['Test_Accuracy'] = None\n",
    "model_results['best_params'] = None\n",
    "\n",
    "# X_train_final = X_train_normalized.drop(columns=[\"ref_latest\"])\n",
    "# X_test_normalized_remgsdata = X_test_normalized.drop(columns=[\"ref_latest\"])\n",
    "# X_train_normalized_remgsdata = X_train_normalized.copy()\n",
    "# X_test_normalized_remgsdata = X_test_normalized.copy()\n",
    "\n",
    "xtrain_final = xtrain.drop(columns=[\"UID\"])\n",
    "ytrain_final = ytrain.drop(columns=[\"UID\"])\n",
    "\n",
    "xtest_final = xtest.drop(columns=[\"UID\"])\n",
    "ytest_final = ytest.drop(columns=[\"UID\"])\n",
    "\n",
    "\n",
    "best_clf_ours = None\n",
    "best_clf_val = 0\n",
    "\n",
    "for clf_name, clf in clf_dict.items():\n",
    "    classifier = GridSearchCV(clf['model'], clf['params'], n_jobs=5)\n",
    "    classifier.fit(xtrain_final, ytrain_final)\n",
    "    best_model = classifier.best_estimator_\n",
    "    print(clf_name, classifier.best_score_, classifier.best_params_)\n",
    "    \n",
    "    y_predicted = best_model.predict(xtest_final)\n",
    "    test_acc_macro = precision_recall_fscore_support(ytest_final, y_predicted, average='macro')[2]#accuracy_score(ytest_final, y_predicted)\n",
    "    \n",
    "    if test_acc_macro > best_clf_val:\n",
    "        best_clf_val = test_acc_macro\n",
    "        best_clf_ours = best_model\n",
    "    \n",
    "    model_results.loc[clf_name, ['Train_Accuracy', 'Test_Accuracy', 'best_params']] = [classifier.best_score_, test_acc_macro, classifier.best_params_]\n",
    "    clsr = classification_report(ytest_final, y_predicted)\n",
    "\n",
    "print(\"================================================================================\")\n",
    "print(best_clf_ours)\n",
    "best_y_hat = best_clf_ours.predict(xtest_final)\n",
    "clsr = classification_report(ytest_final, best_y_hat)\n",
    "print(clsr)\n",
    "test_acc = accuracy_score(ytest_final, best_y_hat)\n",
    "print(\"Test acc:\", test_acc )\n",
    "print(\"Weighted F1 score: \", f1_score(ytest_final, best_y_hat, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sim_with_mcomp = defaultdict(dict)\n",
    "mean_sim_with_not_mcomp = defaultdict(dict)\n",
    "max_sim_with_mcomp = defaultdict(dict)\n",
    "max_sim_with_not_mcomp = defaultdict(dict)\n",
    "\n",
    "mean_at_k = [\"1\", \"3\", \"5\", \"7\", \"10\", \"26\"]\n",
    "\n",
    "\n",
    "for sid in mcomp_sentences:\n",
    "    \n",
    "    # 1. With other training set mcomp sentences\n",
    "    temp_list = []    \n",
    "    for init_train_vec in train_pool_roberta_vecs[\"mcomp\"]:\n",
    "        for cvec2 in roberta_vectors[mcomp_sentences[sid]][sid]:\n",
    "            temp_list.append(np.inner(init_train_vec, cvec2)[0][0])\n",
    "        \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    mean_sim_with_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    max_sim_with_mcomp[sid][\"max\"] = max(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        mean_sim_with_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "#         max_sim_with_mcomp[sid][\"max_{}\".format(vv)] = max(sorted_temp_list[0:int(vv)])\n",
    "    \n",
    "    \n",
    "    # 2. With other training set non_mcomp sentences\n",
    "    temp_list = []    \n",
    "    for init_train_vec in train_pool_roberta_vecs[\"non_mcomp\"]:\n",
    "        for cvec2 in roberta_vectors[mcomp_sentences[sid]][sid]:\n",
    "            temp_list.append(np.inner(init_train_vec, cvec2)[0][0])\n",
    "        \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    mean_sim_with_not_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    max_sim_with_not_mcomp[sid][\"max\"] = max(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        mean_sim_with_not_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "#         max_sim_with_not_mcomp[sid][\"max_{}\".format(vv)] = max(sorted_temp_list[0:int(vv)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for sid in not_mcomp_sentences:\n",
    "    \n",
    "    # 1. With other training set mcomp sentences\n",
    "    temp_list = []    \n",
    "    for init_train_vec in train_pool_roberta_vecs[\"mcomp\"]:\n",
    "        for cvec2 in roberta_vectors[not_mcomp_sentences[sid]][sid]:\n",
    "            temp_list.append(np.inner(init_train_vec, cvec2)[0][0])\n",
    "        \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    mean_sim_with_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    max_sim_with_mcomp[sid][\"max\"] = max(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        mean_sim_with_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "#         max_sim_with_mcomp[sid][\"max_{}\".format(vv)] = max(sorted_temp_list[0:int(vv)])\n",
    "    \n",
    "    \n",
    "    # 2. With other training set non_mcomp sentences\n",
    "    temp_list = []    \n",
    "    for init_train_vec in train_pool_roberta_vecs[\"non_mcomp\"]:\n",
    "        for cvec2 in roberta_vectors[not_mcomp_sentences[sid]][sid]:\n",
    "            temp_list.append(np.inner(init_train_vec, cvec2)[0][0])\n",
    "        \n",
    "    sorted_temp_list = sorted(temp_list, reverse=True)\n",
    "    mean_sim_with_not_mcomp[sid][\"mean\"] = np.mean(sorted_temp_list)\n",
    "    max_sim_with_not_mcomp[sid][\"max\"] = max(sorted_temp_list)\n",
    "    for vv in mean_at_k:\n",
    "        mean_sim_with_not_mcomp[sid][\"mean_{}\".format(vv)] = np.mean(sorted_temp_list[0:int(vv)])\n",
    "#         max_sim_with_not_mcomp[sid][\"max_{}\".format(vv)] = max(sorted_temp_list[0:int(vv)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(sent_sim_dict, k=10, sim_type=\"max\", mean_at=1):\n",
    "    \"\"\"mean_at can take values: 1, 3, 5, 7, 10, 26\"\"\"\n",
    "    \n",
    "    if sim_type == \"max\":\n",
    "        sorted_sims = sorted(sent_sim_dict.items(), key=lambda x: x[1][\"max\"], reverse=True)\n",
    "        top_k_sorted_sims = sorted_sims[0:k]\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        for i in top_k_sorted_sims:\n",
    "            if df.loc[i[0]][\"MComp\"] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        return tp/(tp+fp)\n",
    "            \n",
    "    elif sim_type == \"mean\":\n",
    "        sorted_sims = sorted(sent_sim_dict.items(), key=lambda x: x[1][\"mean_\"+str(mean_at)], reverse=True)\n",
    "        top_k_sorted_sims = sorted_sims[0:k]\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        for i in top_k_sorted_sims:\n",
    "            if df.loc[i[0]][\"MComp\"] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        return tp/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(sent_sim_dict, k=10, sim_type=\"max\", mean_at=1):\n",
    "    if sim_type == \"max\":\n",
    "        sorted_sims = sorted(sent_sim_dict.items(), key=lambda x: x[1][\"max\"], reverse=True)\n",
    "        top_k_sorted_sims = sorted_sims[0:k]\n",
    "        tp = 0\n",
    "        for i in top_k_sorted_sims:\n",
    "            if df.loc[i[0]][\"MComp\"] == 1:\n",
    "                tp += 1\n",
    "        return tp/min(k, 117)\n",
    "            \n",
    "    elif sim_type == \"mean\":\n",
    "        sorted_sims = sorted(sent_sim_dict.items(), key=lambda x: x[1][\"mean_\"+str(mean_at)], reverse=True)\n",
    "        top_k_sorted_sims = sorted_sims[0:k]\n",
    "        tp = 0\n",
    "        for i in top_k_sorted_sims:\n",
    "            if df.loc[i[0]][\"MComp\"] == 1:\n",
    "                tp += 1\n",
    "        return tp/min(k, 117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on max similarity with meaningful group and removing entries more similar to non-meaningful group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "17\n",
      "28\n",
      "76\n",
      "108\n",
      "112\n",
      "124\n",
      "184\n",
      "155\n",
      "157\n",
      "159\n",
      "235\n",
      "236\n",
      "271\n",
      "287\n",
      "312\n",
      "308\n",
      "401\n",
      "449\n",
      "519\n",
      "573\n",
      "627\n",
      "615\n",
      "672\n",
      "673\n",
      "657\n",
      "669\n",
      "671\n",
      "714\n",
      "707\n",
      "739\n",
      "806\n",
      "808\n",
      "809\n",
      "810\n",
      "870\n",
      "830\n",
      "931\n",
      "933\n",
      "909\n",
      "926\n",
      "972\n",
      "950\n",
      "994\n",
      "1004\n",
      "1047\n",
      "1125\n",
      "1162\n",
      "1100\n",
      "1102\n",
      "1212\n",
      "1243\n",
      "1268\n",
      "1281\n",
      "1316\n",
      "1318\n",
      "1331\n",
      "1333\n",
      "1279\n",
      "1347\n",
      "1406\n",
      "1390\n",
      "1452\n",
      "1504\n",
      "1464\n",
      "1497\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "29\n",
      "31\n",
      "32\n",
      "33\n",
      "37\n",
      "38\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "48\n",
      "49\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "85\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "128\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "109\n",
      "111\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "125\n",
      "127\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "156\n",
      "158\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "183\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "268\n",
      "269\n",
      "222\n",
      "223\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "309\n",
      "310\n",
      "311\n",
      "314\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "333\n",
      "334\n",
      "335\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "351\n",
      "352\n",
      "353\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "414\n",
      "416\n",
      "417\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "377\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "439\n",
      "440\n",
      "442\n",
      "444\n",
      "446\n",
      "447\n",
      "451\n",
      "452\n",
      "453\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "506\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "544\n",
      "545\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "579\n",
      "580\n",
      "640\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "624\n",
      "625\n",
      "626\n",
      "628\n",
      "629\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "654\n",
      "655\n",
      "656\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "670\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "692\n",
      "693\n",
      "694\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "730\n",
      "731\n",
      "732\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "804\n",
      "805\n",
      "807\n",
      "811\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "831\n",
      "832\n",
      "833\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "842\n",
      "843\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "861\n",
      "862\n",
      "863\n",
      "865\n",
      "866\n",
      "871\n",
      "874\n",
      "896\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "903\n",
      "904\n",
      "906\n",
      "907\n",
      "908\n",
      "911\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "920\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "932\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "939\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "973\n",
      "974\n",
      "975\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1045\n",
      "1046\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "995\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1005\n",
      "1006\n",
      "1008\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1077\n",
      "1078\n",
      "1080\n",
      "1081\n",
      "1083\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1098\n",
      "1099\n",
      "1101\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1151\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1213\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1255\n",
      "1256\n",
      "1258\n",
      "1259\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1270\n",
      "1271\n",
      "1280\n",
      "1282\n",
      "1283\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1317\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1332\n",
      "1273\n",
      "1274\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1389\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1407\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1475\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1498\n",
      "1499\n",
      "1501\n",
      "1503\n"
     ]
    }
   ],
   "source": [
    "# DIff of max similarity of meaningful and non-meaningful sentences\n",
    "\n",
    "diff_max = {}\n",
    "preserve_non_negative_max = {}\n",
    "\n",
    "for x in max_sim_with_mcomp:\n",
    "    diff_max_sim_for_x = max_sim_with_mcomp[x][\"max\"] - max_sim_with_not_mcomp[x][\"max\"]\n",
    "    diff_max[x] = {\"max\": diff_max_sim_for_x}\n",
    "    \n",
    "    if diff_max_sim_for_x > 0:\n",
    "        preserve_non_negative_max[x] = {\"max\": max_sim_with_mcomp[x][\"max\"]}\n",
    "    else:\n",
    "        print(x)\n",
    "        preserve_non_negative_max[x] = {\"max\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P and R based on preserving non-neg max sim with initial pool of \n",
      " meaningful & non meaningful sents:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Precision at:</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">20  </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">117   </td><td style=\"text-align: right;\">150   </td><td style=\"text-align: right;\">200   </td><td style=\"text-align: right;\">300   </td><td style=\"text-align: right;\">400   </td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">600  </td></tr>\n",
       "<tr><td>Val          </td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 1</td><td style=\"text-align: right;\"> 0.6</td><td style=\"text-align: right;\"> 0.52</td><td style=\"text-align: right;\">  0.38</td><td style=\"text-align: right;\">  0.36</td><td style=\"text-align: right;\">  0.43</td><td style=\"text-align: right;\">  0.57</td><td style=\"text-align: right;\">  0.39</td><td style=\"text-align: right;\">  0.29</td><td style=\"text-align: right;\">  0.23</td><td style=\"text-align: right;\">  0.2</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Recall at:</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">10</td><td style=\"text-align: right;\">20  </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">117   </td><td style=\"text-align: right;\">150   </td><td style=\"text-align: right;\">200   </td><td style=\"text-align: right;\">300</td><td style=\"text-align: right;\">400</td><td style=\"text-align: right;\">500</td><td style=\"text-align: right;\">600</td><td style=\"text-align: right;\">700</td><td style=\"text-align: right;\">800</td><td style=\"text-align: right;\">900</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">1200</td><td style=\"text-align: right;\">1388</td></tr>\n",
       "<tr><td>Val       </td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 1</td><td style=\"text-align: right;\"> 0.6</td><td style=\"text-align: right;\"> 0.52</td><td style=\"text-align: right;\">  0.38</td><td style=\"text-align: right;\">  0.36</td><td style=\"text-align: right;\">  0.56</td><td style=\"text-align: right;\">  0.98</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">  1</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">   1</td><td style=\"text-align: right;\">   1</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"P and R based on preserving non-neg max sim with initial pool of \\n meaningful & non meaningful sents:\")\n",
    "\n",
    "patk = [1, 5, 10, 20, 50, 100, 117, 150, 200, 300, 400, 500, 600]\n",
    "res_table = [[\"Precision at:\"] + patk, [\"Val\"]]\n",
    "\n",
    "for k in patk:\n",
    "    v1 = precision_at_k(preserve_non_negative_max, k) #round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(round(v1, 2))\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))\n",
    "\n",
    "\n",
    "\n",
    "ratk = [1, 5, 10, 20, 50, 100, 117, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1200, 1388]\n",
    "res_table = [[\"Recall at:\"] + ratk, [\"Val\"]]\n",
    "\n",
    "for k in ratk:\n",
    "    v1 = recall_at_k(preserve_non_negative_max, k) #round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(round(v1,2))\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on diff of max similarity with initial pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P and R based on diff of max sim with initial pool of \n",
      " meaningful & non meaningful sents:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Precision at:</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">10  </td><td style=\"text-align: right;\">20  </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">117   </td><td style=\"text-align: right;\">150   </td><td style=\"text-align: right;\">200   </td><td style=\"text-align: right;\">300   </td><td style=\"text-align: right;\">400   </td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">600   </td></tr>\n",
       "<tr><td>Val          </td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 0.6</td><td style=\"text-align: right;\"> 0.5</td><td style=\"text-align: right;\"> 0.44</td><td style=\"text-align: right;\">  0.41</td><td style=\"text-align: right;\">  0.37</td><td style=\"text-align: right;\">  0.37</td><td style=\"text-align: right;\">  0.32</td><td style=\"text-align: right;\">  0.25</td><td style=\"text-align: right;\">  0.21</td><td style=\"text-align: right;\">  0.18</td><td style=\"text-align: right;\">  0.15</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Recall at:</td><td style=\"text-align: right;\">117   </td><td style=\"text-align: right;\">150   </td><td style=\"text-align: right;\">200   </td><td style=\"text-align: right;\">300   </td><td style=\"text-align: right;\">400   </td><td style=\"text-align: right;\">500   </td><td style=\"text-align: right;\">600   </td><td style=\"text-align: right;\">700   </td><td style=\"text-align: right;\">800   </td><td style=\"text-align: right;\">900   </td><td style=\"text-align: right;\">1000   </td><td style=\"text-align: right;\">1200   </td><td style=\"text-align: right;\">1388   </td></tr>\n",
       "<tr><td>Val       </td><td style=\"text-align: right;\">  0.37</td><td style=\"text-align: right;\">  0.47</td><td style=\"text-align: right;\">  0.54</td><td style=\"text-align: right;\">  0.65</td><td style=\"text-align: right;\">  0.72</td><td style=\"text-align: right;\">  0.77</td><td style=\"text-align: right;\">  0.79</td><td style=\"text-align: right;\">  0.85</td><td style=\"text-align: right;\">  0.85</td><td style=\"text-align: right;\">  0.89</td><td style=\"text-align: right;\">   0.92</td><td style=\"text-align: right;\">   0.96</td><td style=\"text-align: right;\">   0.99</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"P and R based on diff of max sim with initial pool of \\n meaningful & non meaningful sents:\")\n",
    "\n",
    "patk = [1, 10, 20, 50, 100, 117, 150, 200, 300, 400, 500, 600]\n",
    "res_table = [[\"Precision at:\"] + patk, [\"Val\"]]\n",
    "\n",
    "for k in patk:\n",
    "    v1 = precision_at_k(diff_max, k) #round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(round(v1, 2))\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))\n",
    "\n",
    "\n",
    "\n",
    "ratk = [117, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1200, 1388]\n",
    "res_table = [[\"Recall at:\"] + ratk, [\"Val\"]]\n",
    "\n",
    "for k in ratk:\n",
    "    v1 = recall_at_k(diff_max, k) #round(sum(i > 0 for i in diff12[val])/len(diff12[val]), 2)\n",
    "    res_table[1].append(round(v1,2))\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
