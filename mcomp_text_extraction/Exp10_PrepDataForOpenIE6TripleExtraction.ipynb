{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appropriate files present on lingo-phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_excel(\"InputTestSet-Reviews48_Ann_NEW.xlsx\")\n",
    "df_train = pd.read_excel(\"InputTrainSet-Reviews7_Ann.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   UID              PID     Dec  \\\n",
       " 0  243  2020_ryen_CEFwr  Reject   \n",
       " 1  179   2018_H1LAqMbRW  Reject   \n",
       " \n",
       "                                                 Sent  MComp   Cat SubCat  \n",
       " 0  It extends this approach by introducing an add...      0  None   None  \n",
       " 1  Experimentally, the results are rather weak co...      0  None   None  ,\n",
       "    UID              PID     Dec  \\\n",
       " 0    0  2019_SJf_XhCqKm  Reject   \n",
       " 1    1  2019_SJf_XhCqKm  Reject   \n",
       " \n",
       "                                                 Sent  MComp  Cat SubCat  \n",
       " 0  The authors propose to use k-DPP to select a s...      0  NaN    NaN  \n",
       " 1  This paper covers the related work nicely, wit...      0  NaN    NaN  )"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2), df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1505, 7), (296, 7))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape, df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data to feed to OpenIE6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of train set: 296\n",
      "Len of test set: 1505\n"
     ]
    }
   ],
   "source": [
    "test_sentences = []\n",
    "train_sentences = []\n",
    "\n",
    "\n",
    "for i in range(0, df_train.shape[0]):\n",
    "    uid = df_train.loc[i][\"UID\"]\n",
    "    train_sentences.append(str(df_train.loc[i][\"Sent\"]).replace(\"\\n\", \" \"))\n",
    "print(\"Len of train set: {}\".format(len(train_sentences)))\n",
    "\n",
    "\n",
    "for i in range(0, df_test.shape[0]):\n",
    "    uid = df_test.loc[i][\"UID\"]\n",
    "    test_sentences.append(str(df_test.loc[i][\"Sent\"]).replace(\"\\n\", \" \"))\n",
    "print(\"Len of test set: {}\".format(len(test_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input_train_sentences.txt\", \"w\") as f:\n",
    "    for i in train_sentences:\n",
    "        f.write(i+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 input_train_sentences.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l input_train_sentences.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input_test_sentences.txt\", \"w\") as f:\n",
    "    for i in test_sentences:\n",
    "        f.write(i+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1505 input_test_sentences.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l input_test_sentences.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It extends this approach by introducing an additional separation of foreground and background in the image.\r\n",
      "Experimentally, the results are rather weak compared to pure model-free agents.\r\n",
      "The experiments are interesting but I'm still not completely convinced by the regression results in Figure 3, namely that one could obtain the angle and velocity from the state but using a function more powerful than a linear function.\r\n",
      "Section 2.2 says they do the latter in the interest of solving control-related tasks, but I’m not clear why this follows.\r\n",
      "4)This paper proposed an improved version of the AEC algorithm.\r\n",
      "Network sizes, learning rates, decay schedules, initialisations etc.\r\n",
      "4)evaluates different f within MCTS for MiniRTS.\r\n",
      "Because f is not good?\r\n",
      "The methods proposed here are not all too original, RAD was proposed by Li et al, distillation was proposed in Goodfellow et al's \"Explaining and harnessing adversarial examples\", stacked autoencoders were proposed by Szegedy et al's \"Intriguing Properties of Neural Networks\".\r\n",
      "Then, one has to carefully balance the approximation quality from one layer to the next and essentially union bound the results over all layers and all sampled points.\r\n",
      "Major Comments: I find the paper to not be lacking in exposition and clarity.\r\n",
      "Is there a reason SVAEs don’t meet all the desiderata mentioned at the end of the Introduction?\r\n",
      "5 sub-network are used to model pose, appearance, foreground, background, and decoders.\r\n",
      "Only the state-reconstruction error is shown now.\r\n",
      "The main contribution of the paper are the analytical derivative of the solution to the DARE.\r\n",
      "Their methods let the network focus more on the foreground to regress the landmark and improve state-of-the-art performance on landmark regression (unsupervised.\r\n",
      "2)why the video prediction only on KTH.\r\n",
      "After rereading I'm not sure I understand why the coordinates should be combined in a 3x3 checkerboard as said in Figure 5a.\r\n",
      "The paper: 1)describes how to train strong agents that might have learned an informative latent representation of the observed state-space.\r\n",
      "The paper proposes to use a pretrained model-free RL agent to extract the developed state representation and further re-use it for learning forward model of the environment and planning.\r\n",
      "It is unclear if the proposed techniques will perform provide improvements over a well-tuned baseline for some realistic tasks, or are they suitable only for toy problems.\r\n",
      "2)Section 3.3 (the 'additional' attack) is a interesting investigation.\r\n",
      "However, there are still lots of details missing for the training of the whole network even with the supplementary.\r\n",
      "- The analysis seems to be sound (apart from the issues discussed below) - The experimental results look promising, at least in the limited setup.\r\n",
      "Approximate inference is performed over these innovation variables, rather the states.\r\n",
      "Which forward model is trained from which model-free agent?\r\n",
      "The paper documents a series of experiments on making models robust against adversarial examples.\r\n",
      "5)are the learned landmark all unimodal?\r\n",
      "Doesn’t this support the argument about need in stochastic prediction?\r\n",
      "In the vein of recent work on learning “ticking” behaviour for LSTMs such as Phased LSTM, this paper proposes to add additional data independent gates to LSTM units that are defined as Gaussian functions of time indices.\r\n",
      "- The paper devotes a lot of space (sect 4.1) on details of learning and behavior of the model-free agents X.\r\n",
      "Although the final result about the defense methods is negative, its results are still inspiring.\r\n",
      "The idea of learning a model based on the features from a model-free agent seems novel but lacks significance in that the results are not very compelling (see below).\r\n",
      "This seems rather trivial to achieve.\r\n",
      "The idea of pre-stabilization is interesting, and seems related to this paper: [URL] **** After Author Response **** Thanks for the response, I am raising my score to weak accept.\r\n",
      "In all experiments, g-LSTM converges faster.\r\n",
      "Additionally, it is proposed that one can reduce the amount of computations performed by the network by adding a computation budget term to the optimized loss that encouraged the cells to update less often.\r\n",
      "I assume it's similar to what AlphaGo does, but right now it's not clear at all how everything is put together.\r\n",
      "A major advantage of the newly introduced Gaussian-gated LSTM (g-LSTM; I suggest using a capital G for Gauss, e.g., GgLSTM).\r\n",
      "Are the author exploring the phenomenon of retraining off one algorithm and then evaluating adversarial images derived on another?\r\n",
      "Too much components in the design make this work hard to follow.\r\n",
      "It would be great if you could provide some more details about the steps you took to verify that your internal baseline is indeed comparable to previous work (eg: Lorenz 2019).\r\n",
      "Overall, RAD and distillation have the best performances, but none of the methods can really resist the 'additional' attack from cg or adam.\r\n",
      "The paper establishes experimental evidence that the RAD framework provides the best defense mechanism against adversarial attacks which makes the introduction of the improved autoencoder mechanism less appealing.\r\n",
      "The testbed used is MiniRTS, a simulation environemnt for 1v1 RTS.\r\n",
      "H36M is also a video-based dataset.\r\n",
      "Since the main contribution of this paper seems to be evaluating the efficacy of RAD, AEC and IAEC, I would suggest that the authors provide more discussion and exposition.\r\n",
      "They use the latent state representation to learn a model for planning, which performs slightly better than a random baseline (win rate ~25%).\r\n",
      "The parametrization introduced in Phased LSTMs allows the memory cells and outputs of LSTMs to be updated periodically.\r\n",
      "Minor comments: Page 3: Equation (3) is also non-convex.\r\n",
      "As a step in this non-convex direction, this paper provides a nice investigation in the convex LTI case.\r\n",
      "1)what are the details of the color jitter process?\r\n",
      "- Interesting evaluation of which input features are important for the model-free algorithm, such as base HP ratio and the amount of resources available.\r\n",
      "Forecasting the future suffers from buildup / propagation of prediction errors, hence the paper uses multi-step errors to stabilize learning.\r\n",
      "cons: 1)The work was framed as an easier-to-optimize alternative to the time-based gating mechanism introduced in phased LSTMs, which takes a parametrization form that is much harder to learn, the gating mechanism covered by the new model gLSTM however, is much more limited.\r\n",
      "The experiments show interesting results on illustrative toy examples.\r\n",
      "The pre-stabilising controller reformulation is a neat trick.\r\n",
      "The paper shows how to use the Discrete-time Algebraic Riccati Equation (DARE) to provide infinite horizon stability & optimality to differentiable MPC learning.\r\n",
      "I also could find the details on how figure 1 was produced.\r\n",
      "all appear to be fixed, so one can not be sure of the “real” performance or convergence behavior of the models.\r\n",
      "So the non-convexity of Equation (2) should not be the motivation of Equation (3).\r\n",
      "It would be interesting to see if this can improve the results.\r\n",
      "3)Without further explanations and analyses about the experimental results, the contribution of the paper seems limited.\r\n",
      "The paper provides a theoretical characterization of the problem setting, which shows that prior work on differentiable MPC learning may lead to unstable controllers without the proposed augmentations using DARE.\r\n",
      "Can the authors elaborate?\r\n",
      "The performance of the modified g-LSTM is compared to LSTM on the Addition, sequential MNIST and sequential CIFAR-10 tasks.\r\n",
      "2)How is the forward model / value function used in MCTS?\r\n",
      "The implication in the abstract and introduction (at least as I interpreted it) is that the learned model would outperform a model-free method, but upon reading the rest of the paper I was disappointed to learn that in reality it drastically underperforms.\r\n",
      "The second concern is about the results on CelebA presented in Figure 6 in the Appendix: It looks like the background net reconstructs almost the entire image.\r\n",
      "I consider this to be a well-executed paper which brings together the main ideas from the coreset literature and shows one avenue of establishing provable results.\r\n",
      "It seems a bit odd that the foreground is so focused on the central part of the face.\r\n",
      "Their main result is stated as Theorem 4.\r\n",
      "The authors argue that g-LSTM results in better performance and has faster convergence on these tasks.\r\n",
      "- In Figure 3b, it is not clear to me what the difference between the red and blue curves is.\r\n",
      "It would be interesting to see the histogram/distribution of the weights per layer and at an aggregate level for the datasets used.\r\n",
      "Pro: - I commend the authors for a clean and polished writeup.\r\n",
      "The experiments are on toy examples, but show promise.\r\n",
      "For instance, on CelebA your baseline seems to fall short of Lorenz 2019, which may suggest that your substantial looking improvement on BBC Pose is indeed due to more favorable cropping.\r\n",
      "Is mini-RTS a deterministic environment?\r\n",
      "With this additional gate, the network can skip updating the states by closing the time-gate, as a result enabling longer memory persistence, and better gradient flow.\r\n",
      "This requires a much more thorough evaluation.\r\n",
      "Strengths: + Nicely motivates the approach of separating foreground and background + Fewer landmarks are needed than in previous work + Approach seems beneficial for video prediction + Clear and well written + Detailed description of architecture and training\r\n",
      "Weaknesses: - The changes and improvements feel somewhat incremental - Some uncertainty about the solidity of the evaluation/comparability with baselines Results on CelebA somewhat weak Overall the paper is well written, easy to follow, presents a straightforward extension of previous work and appears to show an improvement.\r\n",
      "gLSTMs on the other hand only allows updates within a single window over the entire sequence; As a result, one would expect phased LSTM to outperform gLSTM on tasks with periodical temporal dependencies;  2) The empirical results are not convincing enough.\r\n",
      "Detailed: - What are the right prediction tasks that ensure the latent space captures enough of the forward model?\r\n"
     ]
    }
   ],
   "source": [
    "!head -85 input_train_sentences.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Sent</th>\n",
       "      <th>MComp</th>\n",
       "      <th>Cat</th>\n",
       "      <th>SubCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>243</td>\n",
       "      <td>2020_ryen_CEFwr</td>\n",
       "      <td>Reject</td>\n",
       "      <td>It extends this approach by introducing an add...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179</td>\n",
       "      <td>2018_H1LAqMbRW</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Experimentally, the results are rather weak co...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157</td>\n",
       "      <td>2017_HyTqHL5xg</td>\n",
       "      <td>Accept</td>\n",
       "      <td>The experiments are interesting but I'm still ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>146</td>\n",
       "      <td>2017_HyTqHL5xg</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Section 2.2 says they do the latter in the int...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>2017_ByToKu9ll</td>\n",
       "      <td>Reject</td>\n",
       "      <td>4)This paper proposed an improved version of t...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID              PID     Dec  \\\n",
       "0  243  2020_ryen_CEFwr  Reject   \n",
       "1  179   2018_H1LAqMbRW  Reject   \n",
       "2  157   2017_HyTqHL5xg  Accept   \n",
       "3  146   2017_HyTqHL5xg  Accept   \n",
       "4   90   2017_ByToKu9ll  Reject   \n",
       "\n",
       "                                                Sent  MComp   Cat SubCat  \n",
       "0  It extends this approach by introducing an add...      0  None   None  \n",
       "1  Experimentally, the results are rather weak co...      0  None   None  \n",
       "2  The experiments are interesting but I'm still ...      0  None   None  \n",
       "3  Section 2.2 says they do the latter in the int...      0  None   None  \n",
       "4  4)This paper proposed an improved version of t...      0  None   None  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read back the OpenIE6 triples data from txt files into dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/singh_shruti/workspace/meaningful_comparison/meaningful-comparison/mcomp_text_extraction'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy back files\n",
    "! cp ~/workspace/meaningful_comparison/openie6_from_lexico/openie6/mc_test_svo_predictions.txt ./\n",
    "! cp ~/workspace/meaningful_comparison/openie6_from_lexico/openie6/mc_train_svo_predictions.txt ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4418 mc_test_svo_predictions.txt\n",
      "878 mc_train_svo_predictions.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l mc_test_svo_predictions.txt\n",
    "!wc -l mc_train_svo_predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors propose to use k-DPP to select a set of diverse parameters and use them to search for a good a hyperparameter setting.\r\n",
      "0.90: (The authors; propose to use k-DPP to select; a set of diverse parameters)\r\n",
      "\r\n",
      "This paper covers the related work nicely, with details on both closed loop and open loop methods.\r\n",
      "0.51: (This paper; covers nicely,; the related work)\r\n",
      "\r\n",
      "The rest of the paper are also clearly written.\r\n",
      "0.58: (The rest of the paper; are also clearly written.; )\r\n",
      "\r\n",
      "However, I have some concerns about the proposed method.\r\n",
      "0.56: (I; have; some concerns about the proposed method.)\r\n",
      "\r\n",
      "- It is not clear how to define the kernel, the feature function and the quality function for the proposed method.\r\n",
      "0.28: (It; is not; clear how to define the)\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -15 mc_test_svo_predictions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mc_train_svo_predictions.txt\", \"r\") as f:\n",
    "    train_triple_sents = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It extends this approach by introducing an additional separation of foreground and background in the image.\\n',\n",
       " '0.93: (It; extends; this approach)\\n',\n",
       " '\\n',\n",
       " 'Experimentally, the results are rather weak compared to pure model-free agents.\\n',\n",
       " '0.08: (the results; are; rather weak to pure agents.)\\n',\n",
       " '\\n',\n",
       " \"The experiments are interesting but I'm still not completely convinced by the regression results in Figure 3, namely that one could obtain the angle and velocity from the state but using a function more powerful than a linear function.\\n\",\n",
       " '0.89: (The experiments; are; interesting)\\n',\n",
       " '\\n',\n",
       " \"Section 2.2 says they do the latter in the interest of solving control-related tasks, but I'm not clear why this follows.\\n\",\n",
       " '0.80: (Section 2.2; says; they do the latter in the interest of solving control-related tasks,)\\n',\n",
       " '\\n',\n",
       " '4)This paper proposed an improved version of the AEC algorithm.\\n',\n",
       " '0.34: (paper; proposed; an improved version of the AEC algorithm.)\\n',\n",
       " '\\n',\n",
       " 'Network sizes, learning rates, decay schedules, initialisations etc.\\n',\n",
       " '0.77: (Network sizes,; learning; rates, decay schedules, initialisations etc.)\\n',\n",
       " '\\n',\n",
       " '4)evaluates different f within MCTS for MiniRTS.\\n',\n",
       " '\\n',\n",
       " 'Because f is not good?\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_triple_sents[0:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It extends this approach by introducing an additional separation of foreground and background in the image.',\n",
       " 'Experimentally, the results are rather weak compared to pure model-free agents.',\n",
       " \"The experiments are interesting but I'm still not completely convinced by the regression results in Figure 3, namely that one could obtain the angle and velocity from the state but using a function more powerful than a linear function.\",\n",
       " 'Section 2.2 says they do the latter in the interest of solving control-related tasks, but I’m not clear why this follows.',\n",
       " '4)This paper proposed an improved version of the AEC algorithm.',\n",
       " 'Network sizes, learning rates, decay schedules, initialisations etc.',\n",
       " '4)evaluates different f within MCTS for MiniRTS.',\n",
       " 'Because f is not good?',\n",
       " 'The methods proposed here are not all too original, RAD was proposed by Li et al, distillation was proposed in Goodfellow et al\\'s \"Explaining and harnessing adversarial examples\", stacked autoencoders were proposed by Szegedy et al\\'s \"Intriguing Properties of Neural Networks\".',\n",
       " 'Then, one has to carefully balance the approximation quality from one layer to the next and essentially union bound the results over all layers and all sampled points.',\n",
       " 'Major Comments: I find the paper to not be lacking in exposition and clarity.',\n",
       " 'Is there a reason SVAEs don’t meet all the desiderata mentioned at the end of the Introduction?',\n",
       " '5 sub-network are used to model pose, appearance, foreground, background, and decoders.',\n",
       " 'Only the state-reconstruction error is shown now.',\n",
       " 'The main contribution of the paper are the analytical derivative of the solution to the DARE.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This requires a much more thorough evaluation.',\n",
       " 'Strengths: + Nicely motivates the approach of separating foreground and background + Fewer landmarks are needed than in previous work + Approach seems beneficial for video prediction + Clear and well written + Detailed description of architecture and training',\n",
       " 'Weaknesses: - The changes and improvements feel somewhat incremental - Some uncertainty about the solidity of the evaluation/comparability with baselines Results on CelebA somewhat weak Overall the paper is well written, easy to follow, presents a straightforward extension of previous work and appears to show an improvement.',\n",
       " 'gLSTMs on the other hand only allows updates within a single window over the entire sequence; As a result, one would expect phased LSTM to outperform gLSTM on tasks with periodical temporal dependencies;  2) The empirical results are not convincing enough.',\n",
       " 'Detailed: - What are the right prediction tasks that ensure the latent space captures enough of the forward model?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[80:85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Strengths: + Nicely motivates the approach of separating foreground and background + Fewer landmarks are needed than in previous work + Approach seems beneficial for video prediction + Clear and well written + Detailed description of architecture and training\\n',\n",
       " '0.67: (Fewer landmarks; are needed; than in previous work)\\n',\n",
       " '\\n',\n",
       " 'Strengths: + Nicely motivates the approach of separating foreground and background + Fewer landmarks are needed than in previous work + Approach seems beneficial for video prediction + Clear and well written + Detailed description of architecture and training\\n',\n",
       " '0.67: (Fewer landmarks; are needed; than in previous work)\\n',\n",
       " '\\n',\n",
       " 'Weaknesses: - The changes and improvements feel somewhat incremental - Some uncertainty about the solidity of the evaluation/comparability with baselines Results on CelebA somewhat weak Overall the paper is well written, easy to follow, presents a straightforward extension of previous work and appears to show an improvement.\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_triple_sents[238:245]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing dups from the train_triple_sents list preserving order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triples_wo_dups = []\n",
    "prev_sent = train_triple_sents[0]\n",
    "skip_ids = []\n",
    "\n",
    "for _, i in enumerate(train_triple_sents):\n",
    "    if _ in skip_ids:\n",
    "        continue\n",
    "    if i != \"\\n\" and i in train_triples_wo_dups:\n",
    "        if train_triple_sents[_+1].startswith(\"0.\") or train_triple_sents[_+1] == \"\\n\":\n",
    "            skip_ids.append(_+1)\n",
    "        if train_triple_sents[_+2] == \"\\n\":\n",
    "            skip_ids.append(_+2)\n",
    "    if i != \"\\n\" and not i in train_triples_wo_dups:\n",
    "        train_triples_wo_dups.append(i)\n",
    "        prev_sent = i\n",
    "    elif i == \"\\n\" and prev_sent != \"\\n\":\n",
    "        train_triples_wo_dups.append(i)\n",
    "        prev_sent = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(869, 878)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_triples_wo_dups), len(train_triple_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['It extends this approach by introducing an additional separation of foreground and background in the image.\\n',\n",
       "  '0.93: (It; extends; this approach)\\n',\n",
       "  '\\n',\n",
       "  'Experimentally, the results are rather weak compared to pure model-free agents.\\n',\n",
       "  '0.08: (the results; are; rather weak to pure agents.)\\n',\n",
       "  '\\n',\n",
       "  \"The experiments are interesting but I'm still not completely convinced by the regression results in Figure 3, namely that one could obtain the angle and velocity from the state but using a function more powerful than a linear function.\\n\",\n",
       "  '0.89: (The experiments; are; interesting)\\n',\n",
       "  '\\n',\n",
       "  \"Section 2.2 says they do the latter in the interest of solving control-related tasks, but I'm not clear why this follows.\\n\"],\n",
       " ['It extends this approach by introducing an additional separation of foreground and background in the image.\\n',\n",
       "  '0.93: (It; extends; this approach)\\n',\n",
       "  '\\n',\n",
       "  'Experimentally, the results are rather weak compared to pure model-free agents.\\n',\n",
       "  '0.08: (the results; are; rather weak to pure agents.)\\n',\n",
       "  '\\n',\n",
       "  \"The experiments are interesting but I'm still not completely convinced by the regression results in Figure 3, namely that one could obtain the angle and velocity from the state but using a function more powerful than a linear function.\\n\",\n",
       "  '0.89: (The experiments; are; interesting)\\n',\n",
       "  '\\n',\n",
       "  \"Section 2.2 says they do the latter in the interest of solving control-related tasks, but I'm not clear why this follows.\\n\"])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_triples_wo_dups[0:10], train_triple_sents[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The authors achieve lower error rates using their RAD and IAEC methods perform better then previously introduced distillation methods for retraining networks to be robust to adversarial examples.\\n',\n",
       " '0.21: (lower error rates retraining networks; to be; robust to adversarial examples.)\\n',\n",
       " '\\n',\n",
       " '0.27: (their IAEC methods; better introduced; distillation methods for retraining networks then previously)\\n',\n",
       " '\\n',\n",
       " 'This paper presents a variational inference based method for learning nonlinear dynamical systems.\\n']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_triples_wo_dups[524:530]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'On the other hand, the usefulness of the learned representation for planning is unclear.\\n',\n",
       " '0.63: (the usefulness of the learned representation for planning; is; unclear.)\\n',\n",
       " '\\n',\n",
       " \"I'm thus generally supportive of the paper.\\n\",\n",
       " '\\n',\n",
       " 'The authors achieve lower error rates using their RAD and IAEC methods perform better then previously introduced distillation methods for retraining networks to be robust to adversarial examples.\\n',\n",
       " '0.21: (lower error rates retraining networks; to be; robust to adversarial examples.)\\n',\n",
       " '\\n',\n",
       " 'The authors achieve lower error rates using their RAD and IAEC methods perform better then previously introduced distillation methods for retraining networks to be robust to adversarial examples.\\n',\n",
       " '0.27: (their IAEC methods; better introduced; distillation methods for retraining networks then previously)\\n']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_triple_sents[524:535]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I’m thus generally supportive of the paper.',\n",
       " 'The authors achieve lower error rates using their RAD and IAEC methods perform better then previously introduced distillation methods for retraining networks to be robust to adversarial examples.',\n",
       " 'This paper presents a variational inference based method for learning nonlinear dynamical systems.',\n",
       " \"Unless I'm mistaken, it seems there isn't a thorough empirical study of the theoretical claims, especially as it relates to previous work.\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[177:181]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "3 1\n",
      "6 2\n",
      "9 3\n",
      "12 4\n",
      "15 5\n",
      "18 6\n",
      "20 6\n",
      "20 7\n",
      "23 8\n",
      "26 9\n",
      "29 10\n",
      "32 11\n",
      "35 12\n",
      "38 13\n",
      "41 14\n",
      "44 15\n",
      "47 16\n",
      "49 16\n",
      "49 17\n",
      "52 18\n",
      "55 19\n",
      "58 20\n",
      "61 21\n",
      "64 22\n",
      "67 23\n",
      "70 24\n",
      "73 25\n",
      "76 26\n",
      "79 27\n",
      "81 27\n",
      "81 28\n",
      "84 29\n",
      "87 30\n",
      "90 31\n",
      "93 32\n",
      "96 33\n",
      "99 34\n",
      "102 35\n",
      "105 36\n",
      "108 37\n",
      "111 38\n",
      "114 39\n",
      "117 40\n",
      "120 41\n",
      "123 42\n",
      "126 43\n",
      "129 44\n",
      "132 45\n",
      "135 46\n",
      "138 47\n",
      "141 48\n",
      "144 49\n",
      "147 50\n",
      "150 51\n",
      "153 52\n",
      "156 53\n",
      "159 54\n",
      "162 55\n",
      "165 56\n",
      "168 57\n",
      "171 58\n",
      "174 59\n",
      "177 60\n",
      "180 61\n",
      "182 61\n",
      "182 62\n",
      "185 63\n",
      "188 64\n",
      "190 64\n",
      "190 65\n",
      "193 66\n",
      "196 67\n",
      "199 68\n",
      "202 69\n",
      "205 70\n",
      "208 71\n",
      "211 72\n",
      "214 73\n",
      "217 74\n",
      "220 75\n",
      "223 76\n",
      "226 77\n",
      "229 78\n",
      "232 79\n",
      "235 80\n",
      "238 81\n",
      "241 82\n",
      "244 83\n",
      "247 84\n",
      "250 85\n",
      "253 86\n",
      "256 87\n",
      "259 88\n",
      "262 89\n",
      "265 90\n",
      "268 91\n",
      "271 92\n",
      "274 93\n",
      "277 94\n",
      "279 94\n",
      "279 95\n",
      "282 96\n",
      "285 97\n",
      "288 98\n",
      "291 99\n",
      "294 100\n",
      "297 101\n",
      "300 102\n",
      "303 103\n",
      "305 103\n",
      "305 104\n",
      "308 105\n",
      "311 106\n",
      "314 107\n",
      "317 108\n",
      "320 109\n",
      "323 110\n",
      "326 111\n",
      "329 112\n",
      "332 113\n",
      "335 114\n",
      "338 115\n",
      "341 116\n",
      "344 117\n",
      "347 118\n",
      "350 119\n",
      "352 119\n",
      "352 120\n",
      "355 121\n",
      "358 122\n",
      "361 123\n",
      "364 124\n",
      "367 125\n",
      "370 126\n",
      "372 126\n",
      "372 127\n",
      "375 128\n",
      "378 129\n",
      "381 130\n",
      "384 131\n",
      "387 132\n",
      "390 133\n",
      "393 134\n",
      "396 135\n",
      "399 136\n",
      "402 137\n",
      "405 138\n",
      "408 139\n",
      "411 140\n",
      "414 141\n",
      "417 142\n",
      "420 143\n",
      "423 144\n",
      "426 145\n",
      "429 146\n",
      "432 147\n",
      "435 148\n",
      "438 149\n",
      "441 150\n",
      "444 151\n",
      "447 152\n",
      "450 153\n",
      "453 154\n",
      "456 155\n",
      "459 156\n",
      "462 157\n",
      "465 158\n",
      "468 159\n",
      "471 160\n",
      "474 161\n",
      "477 162\n",
      "480 163\n",
      "483 164\n",
      "486 165\n",
      "489 166\n",
      "492 167\n",
      "495 168\n",
      "498 169\n",
      "501 170\n",
      "504 171\n",
      "507 172\n",
      "510 173\n",
      "513 174\n",
      "516 175\n",
      "519 176\n",
      "522 177\n",
      "524 177\n",
      "524 178\n",
      "527 179\n",
      "530 180\n",
      "533 181\n",
      "536 182\n",
      "538 182\n",
      "538 183\n",
      "541 184\n",
      "544 185\n",
      "546 185\n",
      "546 186\n",
      "549 187\n",
      "552 188\n",
      "555 189\n",
      "558 190\n",
      "561 191\n",
      "564 192\n",
      "567 193\n",
      "570 194\n",
      "572 194\n",
      "572 195\n",
      "575 196\n",
      "578 197\n",
      "581 198\n",
      "584 199\n",
      "587 200\n",
      "589 200\n",
      "589 201\n",
      "592 202\n",
      "595 203\n",
      "598 204\n",
      "601 205\n",
      "604 206\n",
      "607 207\n",
      "610 208\n",
      "613 209\n",
      "616 210\n",
      "619 211\n",
      "622 212\n",
      "625 213\n",
      "628 214\n",
      "631 215\n",
      "634 216\n",
      "637 217\n",
      "640 218\n",
      "643 219\n",
      "646 220\n",
      "649 221\n",
      "651 221\n",
      "651 222\n",
      "654 223\n",
      "657 224\n",
      "660 225\n",
      "663 226\n",
      "666 227\n",
      "669 228\n",
      "672 229\n",
      "675 230\n",
      "678 231\n",
      "681 232\n",
      "684 233\n",
      "687 234\n",
      "690 235\n",
      "693 236\n",
      "696 237\n",
      "699 238\n",
      "702 239\n",
      "705 240\n",
      "708 241\n",
      "711 242\n",
      "714 243\n",
      "717 244\n",
      "720 245\n",
      "723 246\n",
      "726 247\n",
      "729 248\n",
      "732 249\n",
      "735 250\n",
      "737 250\n",
      "737 251\n",
      "740 252\n",
      "742 252\n",
      "742 253\n",
      "745 254\n",
      "747 254\n",
      "747 255\n",
      "750 256\n",
      "753 257\n",
      "756 258\n",
      "759 259\n",
      "762 260\n",
      "765 261\n",
      "768 262\n",
      "771 263\n",
      "774 264\n",
      "777 265\n",
      "780 266\n",
      "783 267\n",
      "786 268\n",
      "789 269\n",
      "792 270\n",
      "795 271\n",
      "798 272\n",
      "801 273\n",
      "804 274\n",
      "807 275\n",
      "810 276\n",
      "813 277\n",
      "816 278\n",
      "819 279\n",
      "822 280\n",
      "825 281\n",
      "828 282\n",
      "831 283\n",
      "834 284\n",
      "837 285\n",
      "840 286\n",
      "843 287\n",
      "846 288\n",
      "849 289\n",
      "852 290\n",
      "855 291\n",
      "857 291\n",
      "857 292\n",
      "860 293\n",
      "863 294\n",
      "866 295\n"
     ]
    }
   ],
   "source": [
    "triple_file_counter = 0\n",
    "train_sent_counter = 0\n",
    "\n",
    "train_sentid_triple_dict = {}\n",
    "\n",
    "while True:\n",
    "    print(triple_file_counter, train_sent_counter)\n",
    "    triple_sent = ''.join(ch for ch in train_triples_wo_dups[triple_file_counter].strip() if ch.isalnum())\n",
    "    org_sent = ''.join(ch for ch in train_sentences[train_sent_counter].strip() if ch.isalnum())\n",
    "    if  triple_sent == org_sent:\n",
    "        if train_triples_wo_dups[triple_file_counter+1] != \"\\n\":\n",
    "            train_sentid_triple_dict[train_sent_counter] = train_triples_wo_dups[triple_file_counter+1][7:-2].split(\";\")\n",
    "            train_sent_counter += 1\n",
    "            triple_file_counter += 3\n",
    "        else:\n",
    "            triple_file_counter += 2\n",
    "        if triple_file_counter >= len(train_triples_wo_dups):\n",
    "                break\n",
    "    else:\n",
    "        train_sent_counter += 1\n",
    "        if train_sent_counter >= len(train_sentences):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentid_triple_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(294,\n",
       "  [\"why isn't the model from (Watter et al, 2015)\", ' not included', ' ']),\n",
       " (295, ['it', ' is to evaluate', ' it on more datasets.'])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_sentid_triple_dict.items())[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296,\n",
       " ['Can the authors compare & contrast with this paper?',\n",
       "  'One broad reason for my doubts is that the comparisons don’t seem to utilise proper hyperparameter tuning for the baseline LSTM.',\n",
       "  'Since the SVAE code is publicly available, one could probably compare against it in the experiments.',\n",
       "  \"Also, why isn't the model from (Watter et al, 2015) not included ?\",\n",
       "  'Finally, except CIFAR-10, it is better to evaluate it on more datasets.'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), train_sentences[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save triples of C and NC class in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 in train_sentid_triple_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_c_triples = []\n",
    "train_nc_triples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in train_sentid_triple_dict.items():\n",
    "    seqid = k # earlier was doing uid=k\n",
    "    triple = v\n",
    "    #print(k, df_train.iloc[[seqid]])\n",
    "    comp_label = df_train.iloc[[seqid]][\"MComp\"].values[0]\n",
    "    if comp_label == 0:\n",
    "        train_nc_triples.append((seqid, triple))\n",
    "    elif comp_label == 1:\n",
    "        train_c_triples.append((seqid, triple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 252)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_c_triples), len(train_nc_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read triples of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mc_test_svo_predictions.txt\", \"r\") as f:\n",
    "    test_triple_sents = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_triples_wo_dups = []\n",
    "prev_sent = test_triple_sents[0]\n",
    "skip_ids = []\n",
    "\n",
    "for _, i in enumerate(test_triple_sents):\n",
    "    if _ in skip_ids:\n",
    "        continue\n",
    "    if i != \"\\n\" and i in test_triples_wo_dups:\n",
    "        if test_triple_sents[_+1].startswith(\"0.\") or test_triple_sents[_+1] == \"\\n\":\n",
    "            skip_ids.append(_+1)\n",
    "        if test_triple_sents[_+2] == \"\\n\":\n",
    "            skip_ids.append(_+2)\n",
    "    if i != \"\\n\" and not i in test_triples_wo_dups:\n",
    "        test_triples_wo_dups.append(i)\n",
    "        prev_sent = i\n",
    "    elif i == \"\\n\" and prev_sent != \"\\n\":\n",
    "        test_triples_wo_dups.append(i)\n",
    "        prev_sent = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4398, 4418)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_triples_wo_dups), len(test_triple_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '3)The comparisons are not fair.\\n',\n",
       " '0.19: (comparisons; are not; fair.)\\n',\n",
       " '\\n',\n",
       " 'SELF incorporate semi-supervised techniques while baselines are not.\\n',\n",
       " '0.85: (SELF; incorporate; semi-supervised techniques)\\n',\n",
       " '\\n',\n",
       " '4)The author missed some important baselines here. \"1) Symmetric cross entropy for robust learning with noisy labels, ICCV2019 2) Joint Optimization Framework for Learning with Noisy Labels, CVPR2018 3) Dimensionality-driven learning with noisy labels, ICML2018\"\\n',\n",
       " '0.46: (author; missed; some important baselines here.)\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_triple_sents[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1505"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['- it would be good to mention Figure 2 in the text first before showing it.\\n',\n",
       "  '\\n',\n",
       "  '[Post rebuttal] I would like to thank the authors for their clarifications.\\n',\n",
       "  '0.64: (I; would like; to thank the authors for their clarifications.)\\n',\n",
       "  '\\n',\n",
       "  'However, I am still concerned with the novelty.\\n'],\n",
       " ['- it would be good to mention Figure 2 in the text first before showing it.',\n",
       "  '[Post rebuttal] I would like to thank the authors for their clarifications.',\n",
       "  'However, I am still concerned with the novelty.'])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_triples_wo_dups[39:45], test_sentences[13:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 39 \n",
      "\n",
      "46 137 \n",
      "\n",
      "47 139 \n",
      "\n",
      "61 180 \n",
      "\n",
      "91 269 \n",
      "\n",
      "158 469 \n",
      "\n",
      "179 531 \n",
      "\n",
      "183 542 \n",
      "\n",
      "190 562 \n",
      "\n",
      "196 579 \n",
      "\n",
      "197 581 \n",
      "\n",
      "249 736 \n",
      "\n",
      "283 837 \n",
      "\n",
      "297 878 \n",
      "\n",
      "320 946 \n",
      "\n",
      "325 960 \n",
      "\n",
      "338 998 1.00: (The main theorems; are based; on previous results)\n",
      "\n",
      "425 1259 \n",
      "\n",
      "426 1261 \n",
      "\n",
      "427 1263 \n",
      "\n",
      "428 1265 \n",
      "\n",
      "429 1267 \n",
      "\n",
      "430 1263 \n",
      "\n",
      "431 1269 \n",
      "\n",
      "432 1271 \n",
      "\n",
      "434 1276 \n",
      "\n",
      "435 1278 \n",
      "\n",
      "436 1280 \n",
      "\n",
      "437 1282 \n",
      "\n",
      "438 1284 \n",
      "\n",
      "448 1313 \n",
      "\n",
      "466 1366 \n",
      "\n",
      "477 1398 \n",
      "\n",
      "478 1400 1.00: (The dataset; was also created; by the researchers for the express purpose of testing neural network capacity)\n",
      "\n",
      "493 1445 \n",
      "\n",
      "522 1531 \n",
      "\n",
      "597 1752 \n",
      "\n",
      "600 1760 1.00: (They; introduce; a gating mechanism between prior and posterior.)\n",
      "\n",
      "639 1877 \n",
      "\n",
      "690 2029 1.00: (you; drop; performance for couch)\n",
      "\n",
      "759 2236 1.00: (It; can be seen; as a succesfull attempt)\n",
      "\n",
      "770 2269 \n",
      "\n",
      "779 2295 \n",
      "\n",
      "788 2321 \n",
      "\n",
      "799 2353 \n",
      "\n",
      "803 2364 \n",
      "\n",
      "823 2423 1.00: (The mask; being; contiguous.)\n",
      "\n",
      "846 2492 \n",
      "\n",
      "867 2554 \n",
      "\n",
      "878 2586 1.00: (They; describe; system's sensitivity to some of these choices)\n",
      "\n",
      "935 2757 \n",
      "\n",
      "938 2765 \n",
      "\n",
      "940 2770 \n",
      "\n",
      "947 2790 \n",
      "\n",
      "967 2849 \n",
      "\n",
      "975 2872 \n",
      "\n",
      "1006 2964 \n",
      "\n",
      "1022 3011 1.00: (The idea of matching Gaussian moments along the network graph; is done; in PBP previously)\n",
      "\n",
      "1036 3053 \n",
      "\n",
      "1037 3055 \n",
      "\n",
      "1042 3069 \n",
      "\n",
      "1043 3071 \n",
      "\n",
      "1044 3073 \n",
      "\n",
      "1045 3075 \n",
      "\n",
      "1054 3101 \n",
      "\n",
      "1138 3349 \n",
      "\n",
      "1155 3399 \n",
      "\n",
      "1170 3443 1.00: (you; can attend; to both x and y_g. When decoding)\n",
      "\n",
      "1186 3491 \n",
      "\n",
      "1198 3526 \n",
      "\n",
      "1199 3528 \n",
      "\n",
      "1204 3542 \n",
      "\n",
      "1213 3568 \n",
      "\n",
      "1214 3570 \n",
      "\n",
      "1216 3575 \n",
      "\n",
      "1217 3577 \n",
      "\n",
      "1218 3579 \n",
      "\n",
      "1220 3581 \n",
      "\n",
      "1221 3583 \n",
      "\n",
      "1222 3585 \n",
      "\n",
      "1223 3587 \n",
      "\n",
      "1224 3589 \n",
      "\n",
      "1239 3633 \n",
      "\n",
      "1241 3638 \n",
      "\n",
      "1245 3649 \n",
      "\n",
      "1250 3663 \n",
      "\n",
      "1270 3722 \n",
      "\n",
      "1291 3784 \n",
      "\n",
      "1295 3795 1.00: (The authors; could have provided; on details on why this choice)\n",
      "\n",
      "1296 3798 1.00: (it; affects; MAE)\n",
      "\n",
      "1298 3804 1.00: (all the node-specific neural networks; need; the same parameter size then)\n",
      "\n",
      "1302 3816 \n",
      "\n",
      "1327 3890 \n",
      "\n",
      "1332 3904 \n",
      "\n",
      "1349 3954 \n",
      "\n",
      "1356 3974 \n",
      "\n",
      "1361 3988 \n",
      "\n",
      "1366 4002 \n",
      "\n",
      "1367 4004 \n",
      "\n",
      "1374 4024 \n",
      "\n",
      "1376 4029 \n",
      "\n",
      "1386 4058 \n",
      "\n",
      "1387 4060 \n",
      "\n",
      "1403 4107 \n",
      "\n",
      "1409 4124 \n",
      "\n",
      "1413 4135 \n",
      "\n",
      "1417 4146 \n",
      "\n",
      "1422 4160 \n",
      "\n",
      "1423 4162 \n",
      "\n",
      "1425 2765 \n",
      "\n",
      "1426 4167 \n",
      "\n",
      "1437 4199 1.00: (The performance; is investigated; )\n",
      "\n",
      "1455 4253 \n",
      "\n",
      "1462 4273 \n",
      "\n",
      "1478 4320 \n",
      "\n",
      "1489 4352 \n",
      "\n",
      "1499 4381 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentid_triple_dict = {}\n",
    "\n",
    "for _, i in enumerate(test_sentences):\n",
    "    for tripleidx, j in enumerate(test_triples_wo_dups):\n",
    "        sent_in_triple_file = ''.join(ch for ch in j.strip().lower() if ch.isalnum())\n",
    "        test_sent = ''.join(ch for ch in i.strip().lower() if ch.isalnum())\n",
    "        if sent_in_triple_file == test_sent and test_triples_wo_dups[tripleidx+1].startswith(\"0.\"):\n",
    "            test_sentid_triple_dict[_] =  test_triples_wo_dups[tripleidx+1][7:-2].split(\";\")\n",
    "        elif sent_in_triple_file == test_sent:\n",
    "            print(_, tripleidx, test_triples_wo_dups[tripleidx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1386, 1505)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_sentid_triple_dict), len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The choices of those', ' seem', ' '],\n",
       " 'The choices of those seem to have a huge impact on the performance.')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentid_triple_dict[5], test_sentences[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triple_file_counter = 0\n",
    "# test_sent_counter = 0\n",
    "\n",
    "# test_sentid_triple_dict = {}\n",
    "\n",
    "# while True:\n",
    "#     print(triple_file_counter, test_sent_counter)\n",
    "#     triple_sent = ''.join(ch for ch in test_triples_wo_dups[triple_file_counter].strip() if ch.isalnum())\n",
    "#     org_sent = ''.join(ch for ch in test_sentences[test_sent_counter].strip() if ch.isalnum())\n",
    "#     if  triple_sent == org_sent:\n",
    "#         if test_triples_wo_dups[triple_file_counter+1] != \"\\n\":\n",
    "#             test_sentid_triple_dict[train_sent_counter] = test_triples_wo_dups[triple_file_counter+1][7:-2].split(\";\")\n",
    "#             test_sent_counter += 1\n",
    "#             triple_file_counter += 3\n",
    "#         else:\n",
    "#             triple_file_counter += 2\n",
    "#         if triple_file_counter >= len(test_triples_wo_dups):\n",
    "#                 break\n",
    "#     else:\n",
    "#         test_sent_counter += 1\n",
    "#         if test_sent_counter >= len(test_sentences):\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_c_triples = []\n",
    "test_nc_triples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in test_sentid_triple_dict.items():\n",
    "    seqid = k # earlier was doing uid=k\n",
    "    triple = v\n",
    "    #print(k, df_train.iloc[[seqid]])\n",
    "    comp_label = df_test.iloc[[seqid]][\"MComp\"].values[0]\n",
    "    if comp_label == 0:\n",
    "        test_nc_triples.append((seqid, triple))\n",
    "    elif comp_label == 1:\n",
    "        test_c_triples.append((seqid, triple))\n",
    "    else:\n",
    "        print(\"Inspect mcomp label: \", comp_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 1272)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_c_triples), len(test_nc_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_c_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  ['The authors',\n",
       "   ' propose to use k-DPP to select',\n",
       "   ' a set of diverse parameters']),\n",
       " (1, ['This paper', ' covers nicely,', ' the related work']),\n",
       " (2, ['The rest of the paper', ' are also clearly written.', ' ']),\n",
       " (3, ['I', ' have', ' some concerns about the proposed method.']),\n",
       " (4, ['It', ' is not', ' clear how to define the']),\n",
       " (5, ['The choices of those', ' seem', ' ']),\n",
       " (6,\n",
       "  ['those functions how sensitive',\n",
       "   ' is',\n",
       "   ' the result to hyperparameters of those functions?']),\n",
       " (7, ['the search space', ' is', ' continuous,']),\n",
       " (8, ['how \"mixed\"', ' is', ' decided?']),\n",
       " (9, ['What exactly', ' is', ' the time complexity?']),\n",
       " (10, ['L', ' is', ' ']),\n",
       " (11,\n",
       "  ['I',\n",
       "   ' think',\n",
       "   ' it would have more novelty if some theoretical analyses can be shown on the mixing rate and how good this optimization algorithm is.']),\n",
       " (12, ['Section 4.1) It', ' should be', ' \\\\citep instead of \\\\cite.']),\n",
       " (14, ['I', ' would like', ' to thank the authors for their clarifications.']),\n",
       " (15, ['I', ' am', ' still concerned with the novelty.']),\n",
       " (16,\n",
       "  ['The absence of provable mixing rate',\n",
       "   ' is also',\n",
       "   ' a potential weakness.']),\n",
       " (18, ['I', ' reviewed', ' the same paper last year.']),\n",
       " (19, ['I', ' am appending', ' a few lines based on the changes']),\n",
       " (20, ['The authors', ' propose', ' k-DPP as an open loop']),\n",
       " (21, ['the concept of k-DPP-RBF over hyperparameters', ' are not', ' new,']),\n",
       " (22,\n",
       "  ['k-DPP-RBF',\n",
       "   ' gives',\n",
       "   ' better star discrepancy than uniform random search while being comparable to low-discrepancy Sobol sequences in other metrics such as distance from the center']),\n",
       " (23,\n",
       "  ['The second experiment',\n",
       "   ' shows surprisingly',\n",
       "   ' that for the hard learning rate range, k-DPP-RBF performs better than uniform random search,']),\n",
       " (24,\n",
       "  ['The third experiment',\n",
       "   ' shows',\n",
       "   ' that on stable ranges, k-DPP-RBF slightly outperform uniform random search respectively.']),\n",
       " (25, ['I', ' have', ' a few reservations.']),\n",
       " (26,\n",
       "  ['I',\n",
       "   ' do not find',\n",
       "   ' these outcomes very surprising except for the second experiment First,']),\n",
       " (29, ['It', ' is', ' unclear to me']),\n",
       " (31,\n",
       "  ['This paper',\n",
       "   ' proposes',\n",
       "   ' an approach to get samples with high dispersion for hyperparameter optimisation.']),\n",
       " (32,\n",
       "  ['It',\n",
       "   ' theoretically motivates',\n",
       "   ' the use of Determinantal Point Processes in yielding such samples.']),\n",
       " (33, ['an iterative mixing algorithm', ' is proposed', ' ']),\n",
       " (34,\n",
       "  ['Experiments on finding hyperparameter for sentence classification',\n",
       "   ' are presented.',\n",
       "   ' ']),\n",
       " (35,\n",
       "  ['it', ' performs better', ' than other open-loop methods. of accuracy,']),\n",
       " (36,\n",
       "  ['it',\n",
       "   ' yields',\n",
       "   ' parameter settings In comparison to closed-loop methods,']),\n",
       " (37,\n",
       "  ['The distinction from close-loop approaches',\n",
       "   ' makes',\n",
       "   ' it easy to parallelise.']),\n",
       " (38,\n",
       "  ['This paper',\n",
       "   ' is',\n",
       "   ' novel in its modelling of hyperparameter optimisation with DPP and the theoretical justification']),\n",
       " (40, ['The proposed regularizer', ' seems', ' ']),\n",
       " (41,\n",
       "  ['the performance',\n",
       "   ' achieved',\n",
       "   ' by similar methods by a large enough margin']),\n",
       " (42,\n",
       "  ['The method',\n",
       "   ' proposed essential trains',\n",
       "   ' neural networks multiplicative gating by the CDF of a Gaussian']),\n",
       " (43, ['Experiments', ' are performed', ' with both.']),\n",
       " (44, ['The work', ' is', ' somewhat novel and interesting.']),\n",
       " (45,\n",
       "  ['this',\n",
       "   ' is',\n",
       "   ' preferable to other similar parameterizations of the same (sigmoidal?']),\n",
       " (48, ['this', ' works', ' ']),\n",
       " (49, ['The CIFAR results', ' look okay', ' ']),\n",
       " (50,\n",
       "  ['results on frame classification', \" also aren't\", ' that interesting']),\n",
       " (51,\n",
       "  ['SOI map networks without additional nonlinearities',\n",
       "   ' are',\n",
       "   ' comparable to linear functions']),\n",
       " (52,\n",
       "  ['Varying an input example by adding a constant will',\n",
       "   ' not be linearly reflected',\n",
       "   ' in the expected output of the network.']),\n",
       " (53, ['they', ' are', ' more nonlinear than ReLU networks In this sense']),\n",
       " (54, ['The plots', ' are', ' very difficult to read in grayscale,']),\n",
       " (55,\n",
       "  ['Approaches like adaptive dropout',\n",
       "   ' have',\n",
       "   ' the binary mask as a function of input to a neuron very similar to the proposed approach.']),\n",
       " (56, ['the proposed approach', ' differs', ' to Adaptive dropout']),\n",
       " (58,\n",
       "  ['This paper', ' proposed', ' an effective defense against model stealing']),\n",
       " (59, ['this paper', ' is well written', ' and easy to follow.']),\n",
       " (60,\n",
       "  ['The approach',\n",
       "   ' is',\n",
       "   ' a significant supplement to existing defense against model stealing attacks.']),\n",
       " (62, ['I', ' have', ' concerns about the current version. still']),\n",
       " (63,\n",
       "  ['I', ' will possibly adjust', \" my score based on the authors' response.\"]),\n",
       " (64, ['the model', ' stealing', ' setting,']),\n",
       " (65,\n",
       "  ['thus problem (4)',\n",
       "   ' should is',\n",
       "   ' in Section 3) a black-box optimization problem for defense.']),\n",
       " (66, ['a table', ' to summarize', ' have the notations.']),\n",
       " (67, ['Problem', ' only relies', ' on the transfer set,']),\n",
       " (68, ['they', ' have', ' the same D^{test}?']),\n",
       " (69, ['How', ' to determine', ' them, in particularly for F_A?']),\n",
       " (70, ['One utility constraint', ' is missing', ' in problem (4).']),\n",
       " (71, ['I', ' suggest to add', ' it to the formulation (4).']),\n",
       " (72, ['2) The details of heuristic solver', ' are', ' unclear.']),\n",
       " (73, ['the authors', ' pointed out', ' the pseudocode in the appendix,']),\n",
       " (74, ['3) G,', ' to select', ' the surrogate model? In Estimating']),\n",
       " (75,\n",
       "  ['the authors',\n",
       "   ' mentioned',\n",
       "   ' that defense performances are unaffected by choice of architectures, and hence use the victim architecture for the stolen model. in the experiment,']),\n",
       " (77, ['I', ' am', \" satisfied with the authors' response.\"]),\n",
       " (78, ['I', ' would like', ' to keep my positive comments on this paper.']),\n",
       " (79,\n",
       "  ['I',\n",
       "   ' decide',\n",
       "   ' to increase my score to 8 due to its novelty in formulation and extensive experiments. finally']),\n",
       " (80,\n",
       "  ['The paper',\n",
       "   ' proposes',\n",
       "   ' a new method for defending against stealing attacks.']),\n",
       " (81, ['The paper', ' was', ' very readable and clear.']),\n",
       " (82, ['The proposed method', ' is', ' straightforward and well motivated.']),\n",
       " (83, ['The authors', ' included', ' a good amount of experimental results.']),\n",
       " (84,\n",
       "  ['You',\n",
       "   ' note',\n",
       "   ' that the random perturbation to the outputs performs poorly compared to your method, but this performance gap seems decrease']),\n",
       " (85,\n",
       "  ['this',\n",
       "   ' may indicate',\n",
       "   ' that the attackers are generally weak and this threat model may not be very serious.']),\n",
       " (86, ['the attackers', ' require', ' a very large number of queries,']),\n",
       " (88, ['this optimization procedure', ' take?', ' How long does']),\n",
       " (89, ['the victim', ' to implement', ' this defense']),\n",
       " (90, ['it', ' would be', ' nice']),\n",
       " (92, ['You', ' use', ' the validation set,']),\n",
       " (93,\n",
       "  ['the attacker', ' trained', ' with some synthetic data/other dataset,']),\n",
       " (94,\n",
       "  ['It this',\n",
       "   ' seems is discussed',\n",
       "   \" in the context of the victim in the ''Attack Models'' subsection,\"]),\n",
       " (95, ['the perturbed labels', ' were not crafted', ' plot for a model']),\n",
       " (96, ['This', ' would motivate more.', ' the defense']),\n",
       " (97,\n",
       "  ['This paper',\n",
       "   ' aims stealing',\n",
       "   ' at defending against model by perturbing the posterior prediction of a protected DNN with a balanced goal of maintaining accuracy and maximizing misleading gradient deviation.']),\n",
       " (98,\n",
       "  ['The maximizing angular deviation formulation',\n",
       "   ' makes',\n",
       "   ' sense and seemingly correct.']),\n",
       " (99, ['The heuristic solver toward this objective', ' is shown', ' ']),\n",
       " (100, ['the theoretical novelty of the method', ' is', ' limited,']),\n",
       " (101, ['This work', ' presents', ' the Simple Recurrent Unit architecture']),\n",
       " (102, ['it', ' can be paralleled', ' ']),\n",
       " (103, ['The idea', ' is well explained', ' ']),\n",
       " (104,\n",
       "  [\"The experiment's tables\",\n",
       "   ' alternate',\n",
       "   \" between ''time'' and ''speed'', good to just have one of them.\"]),\n",
       " (105, ['Table 4', ' has', ' time/epoch only time']),\n",
       " (106,\n",
       "  ['the Simple Recurrent Unit',\n",
       "   ' can be used',\n",
       "   ' as a substitute for GRU cells in RNNs.']),\n",
       " (107, ['it', ' trains much faster: almost as fast', ' ']),\n",
       " (109,\n",
       "  ['the SRU',\n",
       "   ' is',\n",
       "   ' a superbly elegant architecture with a fair bit of originality in its structure,']),\n",
       " (111,\n",
       "  ['Some of this',\n",
       "   ' has been pointed out',\n",
       "   ' in the comments below already.']),\n",
       " (114, ['current non-RNN architectures', ' could be', ' both faster']),\n",
       " (115, ['this reviewer', ' to revise', ' the score).']),\n",
       " (116,\n",
       "  ['this',\n",
       "   ' is',\n",
       "   ' a significant contribution to deep learning deserves in all cases,']),\n",
       " (117, ['the comments', ' show', ' new evidence of potential applications,']),\n",
       " (118,\n",
       "  ['The authors',\n",
       "   ' propose',\n",
       "   ' to drop the recurrent state-to-gates connections from RNNs']),\n",
       " (119, ['The recurrent connections', ' are', ' core to an RNN.']),\n",
       " (120,\n",
       "  ['the RNN',\n",
       "   ' defaults simply',\n",
       "   ' to a CNN with gated incremental pooling.']),\n",
       " (121,\n",
       "  ['This',\n",
       "   ' most importantly makes',\n",
       "   ' a comparison with autoregressive sequence CNNs']),\n",
       " (122,\n",
       "  ['autoregressive CNNs with gated incremental pooling',\n",
       "   ' perform comparably',\n",
       "   ' to RNNs on a number of tasks']),\n",
       " (123,\n",
       "  ['the *CNN* part of the paper',\n",
       "   ' cannot be counted',\n",
       "   ' as a novel contribution.']),\n",
       " (125, ['Quasi-RNNs', ' are', ' almost identical']),\n",
       " (127,\n",
       "  ['I',\n",
       "   ' would suggest',\n",
       "   ' to focus on a small set of tasks instead of focussing on many tasks with just relative improvements over the RNN baseline.']),\n",
       " (128,\n",
       "  ['I',\n",
       "   ' recommend',\n",
       "   ' showing exhaustively that gated incremental pooling can be helpful for autoregressive CNNs on sequence tasks']),\n",
       " (129, ['the experiments', ' are presented.', ' ']),\n",
       " (130,\n",
       "  ['The paper',\n",
       "   ' proposes',\n",
       "   ' to use a set of handcrafted intrinsic rewards']),\n",
       " (131, ['a meta-policy', ' choses', ' at the beginning of each episode then']),\n",
       " (132, ['the \"coordination\"', ' occurs', ' here']),\n",
       " (133,\n",
       "  ['The agents',\n",
       "   ' learn',\n",
       "   ' whatever trajectory they need to perform, and then proceed to do so without any interaction with the other agents. eventually']),\n",
       " (134, ['they', ' to explore collaboratively.', ' ']),\n",
       " (135, ['agents', ' trained', ' on task 1 in a given maze']),\n",
       " (136, ['each agent', ' learns', ' its own policy']),\n",
       " (137,\n",
       "  ['the scope of the work',\n",
       "   ' basically reduces',\n",
       "   ' to the exploration of a fixed environment']),\n",
       " (138, ['This \"multi-agent\" formulation', ' is presumably meant', ' ']),\n",
       " (139,\n",
       "  ['the experiments',\n",
       "   ' are conducted',\n",
       "   ' only with a very limited number of agents']),\n",
       " (140,\n",
       "  ['the',\n",
       "   ' applied considers',\n",
       "   ' to the cartesian product of all the agents action spaces only 3 actions,']),\n",
       " (141,\n",
       "  ['they',\n",
       "   ' can be distilled individually',\n",
       "   ' Once the trajectories of both agents are found, to each of them so that they only depend on the local observation.']),\n",
       " (142, ['the experimental setup', ' presented', ' visual in this paper.']),\n",
       " (143,\n",
       "  ['this',\n",
       "   ' weakens',\n",
       "   ' the claim that the method \"scales to more complex environments\" since providing the position essentially makes the environment similar to a grid-world isn\\'t even needed In my opinion,']),\n",
       " (144,\n",
       "  ['The use of a dynamic policy selection', ' is', ' somewhat interesting,']),\n",
       " (145, ['all the selection of the policy', ' to use', ' during training']),\n",
       " (146, ['the \"reward\"', ' is', ' the return obtained by the policy.']),\n",
       " (147,\n",
       "  ['you',\n",
       "   ' share',\n",
       "   ' the reason behind the choice of an actor-critic algorithm over classical bandit algorithms?']),\n",
       " (148,\n",
       "  ['One obvious advantage of the latter', ' are', ' provable regret bounds.']),\n",
       " (149, ['the selection policy', ' seems', ' In all,']),\n",
       " (150,\n",
       "  ['some form of curriculum over the rewards',\n",
       "   ' occurring',\n",
       "   ' during training,']),\n",
       " (151,\n",
       "  ['This', ' could potentially solve', ' the issues observed in task 2.']),\n",
       " (152,\n",
       "  ['Relational Deep Reinforcement Generalization Cooperative Multi-Agent Reinforcement Learning, Carion',\n",
       "   ' Learning,',\n",
       "   ' ']),\n",
       " (153, ['Overall I', ' like', ' the approach in the paper.']),\n",
       " (154,\n",
       "  ['It',\n",
       "   ' proposes',\n",
       "   ' a nice 2 pronged method for exploiting exploration via intrinsic rewards for multi-agent systems.']),\n",
       " (156, ['the ones', ' proposed motivated', ' by the tasks in the paper']),\n",
       " (161, ['It', ' says', ' that all agents share the same replay buffer.']),\n",
       " (162,\n",
       "  ['this every agent',\n",
       "   ' is performing',\n",
       "   ' the same task there are just many agents?']),\n",
       " (163,\n",
       "  ['This',\n",
       "   ' does not make',\n",
       "   ' the problem very multi-agent with different goals.']),\n",
       " (164, ['the agents', ' have', ' various types of goals? an environment']),\n",
       " (165,\n",
       "  ['this method', ' to work well', ' in the centralized training scheme']),\n",
       " (166, ['It', ' makes', ' me wonder if there is a way']),\n",
       " (167,\n",
       "  ['The ability to ask other agents in the world about there preferences',\n",
       "   ' to be',\n",
       "   ' a strong assumption, especially in a multi-agent robotics problem.']),\n",
       " (168,\n",
       "  ['the authors',\n",
       "   ' note',\n",
       "   ' that the intrinsic rewards used in this work are not comprehensive']),\n",
       " (169, ['a few', ' were left out', ' on purpose.']),\n",
       " (170, ['this set', ' is', ' sufficient.']),\n",
       " (171, ['the authors', ' found', ' one that worked.']),\n",
       " (172, ['It', ' be', ' good to expand on this discussion more.']),\n",
       " (173,\n",
       "  ['More detail for Figure 1',\n",
       "   ' would be',\n",
       "   ' helpful to understand the overall network design.']),\n",
       " (174, ['that', ' figure', ' it helpful']),\n",
       " (175, ['a more compressed n agent version', ' can also be shown.', ' Then']),\n",
       " (176,\n",
       "  ['The paper',\n",
       "   ' describes',\n",
       "   ' a policy selector that is a type of high-level policy for HRL.']),\n",
       " (177, ['This design', ' seems', ' ']),\n",
       " (178, ['I', ' like', ' it.']),\n",
       " (180, ['this', ' be analyzed', ' an empirical way?']),\n",
       " (181, ['this', ' Is', ' true for most environments/tasks?']),\n",
       " (182, ['Task 2', ' seems', ' a bit contrived.']),\n",
       " (185,\n",
       "  ['section 6.1 the paper', ' is discussing', ' rewards the are received.']),\n",
       " (186,\n",
       "  ['It',\n",
       "   ' would be',\n",
       "   ' good to more explicit about where these rewards are coming from.']),\n",
       " (187,\n",
       "  ['I',\n",
       "   ' think',\n",
       "   ' it is meant that these rewards are the extrinsic rewards but it does not say.']),\n",
       " (188, ['it', ' seems', ' for the collection of tasks']),\n",
       " (189, ['this decision', ' is', ' less obvious.']),\n",
       " (191, ['an steps into a black hole they', ' never to be seen', ' again.']),\n",
       " (192, ['one end', ' is', ' non-stationary... a wormhole']),\n",
       " (193, ['It the novel metric', ' appears is based.', ' count']),\n",
       " (194, ['this', ' can work', ' in practice']),\n",
       " (195, ['something more like ICM', ' was referenced', ' in the paper?']),\n",
       " (198,\n",
       "  ['It',\n",
       "   ' be',\n",
       "   ' good to include this information in the caption for the table.']),\n",
       " (199,\n",
       "  ['I',\n",
       "   ' am not',\n",
       "   ' sure if the discussion on the behaviours the intrinsic reward functions result in are very surprising.']),\n",
       " (200,\n",
       "  ['a more interesting behaviour',\n",
       "   ' results',\n",
       "   ' from the combination of two intrinsic rewards?']),\n",
       " (201,\n",
       "  ['The paper',\n",
       "   ' proposes',\n",
       "   ' a method for coordinating the exploration efforts of agents in a multi-agent reinforcement learning setting.']),\n",
       " (202,\n",
       "  ['The approach',\n",
       "   ' has',\n",
       "   ' two main components: (i) learning different exploration policies using different \"joint\" intrinsic rewards',\n",
       "   '']),\n",
       " (203,\n",
       "  ['Each agent',\n",
       "   ' has',\n",
       "   ' its own novelty function which quantifies the novelty of observation']),\n",
       " (204,\n",
       "  ['these novelty functions',\n",
       "   ' are combined',\n",
       "   ' using aggregation functions produce']),\n",
       " (205,\n",
       "  ['Each such aggregating function',\n",
       "   ' yields',\n",
       "   ' a different intrinsic reward.']),\n",
       " (206,\n",
       "  ['The authors',\n",
       "   ' propose',\n",
       "   ' several such aggregating functions as examples,']),\n",
       " (207,\n",
       "  ['the exploration policies',\n",
       "   ' is executed',\n",
       "   ' for the entire episode. then']),\n",
       " (208, ['The episode data', ' is used', ' ']),\n",
       " (209,\n",
       "  ['Experiments done on VizDoom environment for three different tasks',\n",
       "   ' demonstrate',\n",
       "   ' that, on most tasks, the proposed approach performs at least as well as separately trained individual intrinsic rewards.']),\n",
       " (210,\n",
       "  ['Further ablation studies',\n",
       "   ' confirm',\n",
       "   ' that both the \"joint\" intrinsic rewards are useful.']),\n",
       " (211,\n",
       "  ['Questions to the Authors: 1) The second sentence in section 5',\n",
       "   ' is not',\n",
       "   ' clear']),\n",
       " (212,\n",
       "  ['The high-level policy',\n",
       "   ' selects',\n",
       "   ' an exploration strategy at the beginning of each episode']),\n",
       " (213,\n",
       "  ['Changing the exploration strategy over the course of training',\n",
       "   ' might be',\n",
       "   ' useful in cases']),\n",
       " (214,\n",
       "  ['this',\n",
       "   ' would require',\n",
       "   ' the exploration strategy to be changed in the middle of an episode']),\n",
       " (215,\n",
       "  ['the exploration strategy', ' must be changed', ' over time an example']),\n",
       " (216,\n",
       "  ['why',\n",
       "   ' not select',\n",
       "   ' the exploration strategy after every fixed number of time steps within each episode (by making high-level policy a function of the current state)?']),\n",
       " (217,\n",
       "  ['2) Analyzing the role of high-level policy',\n",
       "   ' would be',\n",
       "   ' a very nice addition to the paper.']),\n",
       " (218,\n",
       "  ['Qualitative experiments',\n",
       "   ' demonstrating',\n",
       "   ' that it provides a curriculum']),\n",
       " (219, ['Should \\\\Pi in', ' depend', ' on i?']),\n",
       " (220, ['paper', ' is reasonably well written', ' ']),\n",
       " (221, ['authors', ' can bring out', ' the impact of the contributions']),\n",
       " (222,\n",
       "  ['The paper',\n",
       "   ' proposes',\n",
       "   ' a method to compute adversarial examples with minimum distance to the original inputs, and the']),\n",
       " (223, ['I', ' like', ' the idea and the proposed applications.']),\n",
       " (224, ['It', ' is certainly', ' highly relevant,']),\n",
       " (225,\n",
       "  ['Some of the suggested insights in the analysis of defense techniques',\n",
       "   ' are',\n",
       "   ' interesting.']),\n",
       " (226, ['Cons: The', ' is not', ' much technical novelty.']),\n",
       " (227,\n",
       "  ['The method',\n",
       "   ' boils down',\n",
       "   ' to applying Reluplex (Katz et al 2017b) in a binary search']),\n",
       " (228, ['the search', ' is', ' very slow']),\n",
       " (229,\n",
       "  ['accuracy rates',\n",
       "   ' make',\n",
       "   ' them interesting for deployment in potentially safety critical applications']),\n",
       " (230, ['The network', ' analysed', ' here']),\n",
       " (231, ['The analysis', ' to be done', ' for each sample.']),\n",
       " (232,\n",
       "  ['The long runtime',\n",
       "   ' does not permit',\n",
       "   ' to analyse large amounts of input samples,']),\n",
       " (233,\n",
       "  ['The statement',\n",
       "   ' can only be made',\n",
       "   ' for the very limited set of tested samples.']),\n",
       " (234, ['more sophisticated attacks', ' fool', ' network']),\n",
       " (237,\n",
       "  ['I',\n",
       "   ' would consider',\n",
       "   \" calling them ''minimal adversarial samples'' instead of ''ground-truth''.\"]),\n",
       " (238, ['the paragraph', ' describing', ' Carlini & Wagner']),\n",
       " (239,\n",
       "  ['Algorithm 1', ' is essentially', ' only a description of binary search,']),\n",
       " (240,\n",
       "  ['is the timeout for the computation,', ' mentioned', ' in Section 4?']),\n",
       " (241,\n",
       "  ['I',\n",
       "   \" wouldn't say\",\n",
       "   ' the observation is in line with Carlini & Wagner,']),\n",
       " (242, [\"That's\", ' also', ' the conclusion two paragraphs below, no?']),\n",
       " (243,\n",
       "  ['I',\n",
       "   \" don't fully agree\",\n",
       "   ' with the conclusion that the defense of Madry does not overfit to the specific method of creating adversarial examples.']),\n",
       " (244,\n",
       "  ['Those', ' are related', ' because CW was used to initialize the search.']),\n",
       " (245,\n",
       "  ['The authors',\n",
       "   ' propose',\n",
       "   ' to employ provably minimal-distance examples as a tool']),\n",
       " (246, ['This', ' is demonstrated', ' on a small-scale network']),\n",
       " (247,\n",
       "  ['I',\n",
       "   ' striking',\n",
       "   ' it that a trained network with 97% accuracy (as claimed by the authors) seems extremely brittle -- considering the fact that all the adversarial examples in Figure 1 are hardly borderline examples at all, at least to my eyes. First of all,']),\n",
       " (248, ['This', ' does reinforce', ' the (well-known?)']),\n",
       " (250,\n",
       "  ['I',\n",
       "   ' therefore find',\n",
       "   ' the authors\\' statement on page 3 disturbing: \"... they perform well, in general, on previously-unseen inputs\" -- which seems false (with high probability over all possible worlds).']),\n",
       " (251,\n",
       "  ['the term \"ground truth\" example', ' seems', ' very misleading to me.']),\n",
       " (252, ['\"closest misclassified examples\"?', ' Perhaps', ' ']),\n",
       " (253, ['I', ' am not', ' convinced that they are the right way Finally,']),\n",
       " (254, ['All such examples', ' shown', ' in the paper']),\n",
       " (255,\n",
       "  ['we',\n",
       "   ' could equally consider',\n",
       "   ' another extreme, where the trained network is \"over-regularized\" in the sense that the closest misclassified examples are indeed from another class,']),\n",
       " (256, ['adversarial examples', ' could seriously degrade', ' the accuracy.']),\n",
       " (257,\n",
       "  ['adding misclassified examples',\n",
       "   ' are',\n",
       "   ' closest a much more efficient training approach,']),\n",
       " (258,\n",
       "  ['adversaries',\n",
       "   ' have',\n",
       "   ' minimal (L1 or L_inf) distance to the training example']),\n",
       " (259,\n",
       "  ['The technique',\n",
       "   ' uses',\n",
       "   ' the recently developed reluplex, which can be used to verify certian properties of deep neural networks']),\n",
       " (260,\n",
       "  ['The authors',\n",
       "   ' show',\n",
       "   ' how the L1 distance can be formulated using a ReLU and therefore extend the reluplex also work with L1 distances.']),\n",
       " (261,\n",
       "  ['The experiments on MNIST',\n",
       "   ' suggest',\n",
       "   ' that the C&W attack produces close to optimal adversarial examples,']),\n",
       " (262,\n",
       "  ['training with iterative adversarial examples', ' does not overfit', ' ']),\n",
       " (263, ['this', ' is', ' a nice idea,']),\n",
       " (264, ['it', ' would have been', ' useful']),\n",
       " (265,\n",
       "  ['The experiments',\n",
       "   ' are',\n",
       "   ' quite small scale, which I expect is due to the computational cost of generating the adversarial examples.']),\n",
       " (266,\n",
       "  ['the findings',\n",
       "   ' to say can be generalized',\n",
       "   ' how far from MNIST to more realistic situations.']),\n",
       " (268,\n",
       "  ['adversarial examples',\n",
       "   ' have',\n",
       "   ' minimal L_p distance from training examples really that useful in practice?']),\n",
       " (269,\n",
       "  ['it',\n",
       "   ' could be argued',\n",
       "   ' that L_p norms are not a good way of judging the similarity of an adversarial example to a true example.']),\n",
       " (270, ['these', ' to be', ' a concern for real world systems.']),\n",
       " (272, ['This paper', ' is not', ' standalone.']),\n",
       " (273,\n",
       "  ['A section on the basics of document analysis',\n",
       "   ' would have been',\n",
       "   ' nice.']),\n",
       " (274, ['The work', ' proposes', ' Tensor Product Decomposition Networks']),\n",
       " (275, ['the encoding', ' produced', ' by RNNs.']),\n",
       " (276, ['TRDN as a result', ' shed', ' light into inspecting representation']),\n",
       " (277,\n",
       "  ['the structures captured in RNNs', ' can be well captured', ' by TPRs']),\n",
       " (278, ['1) The paper', ' is mostly clearly written', ' and easy to follow.']),\n",
       " (279, ['The diagrams', ' shown', ' in Figure 2']),\n",
       " (280,\n",
       "  ['the training task influence the kinds of structural representation',\n",
       "   ' learned.',\n",
       "   ' ']),\n",
       " (281,\n",
       "  ['RNNs',\n",
       "   ' learn',\n",
       "   ' more complex structural dependencies which TPRs cannot capture',\n",
       "   '']),\n",
       " (282, ['these numbers', ' be interpreted?', ' should']),\n",
       " (284,\n",
       "  ['RNN representations and TPDN approximations', ' shown', ' in Table 2']),\n",
       " (285,\n",
       "  ['This paper',\n",
       "   ' presents',\n",
       "   ' an analysis of popularly-use RNN model for structure modeling abilities']),\n",
       " (286,\n",
       "  ['The results',\n",
       "   ' show',\n",
       "   ' that the representations exhibit interpretable compositional structure.']),\n",
       " (288, ['Pros: 1) The paper', ' is', ' well-written and easy to follow.']),\n",
       " (289, ['The design of the TPDN', ' are', ' understandable.']),\n",
       " (290, ['It', ' makes', ' good point at the end of the paper']),\n",
       " (291, ['The experiments', ' to support', ' their claims.']),\n",
       " (293,\n",
       "  ['An addition of analogy dataset',\n",
       "   ' demonstrate',\n",
       "   ' the effect of TPDN on modeling structural regularities.']),\n",
       " (294,\n",
       "  ['readers',\n",
       "   ' understand',\n",
       "   \" what's the point of investigating TPDN on RNN models.\"]),\n",
       " (295,\n",
       "  ['Some details', ' are missing', ' to better understand the construction.']),\n",
       " (296, ['TPDN encoder', ' is trained, specifically,', ' ']),\n",
       " (298,\n",
       "  ['It',\n",
       "   ' is also',\n",
       "   ' unclear of whether the models in Figure 3(c) use bidirectional or unidirectional or tree decoder?']),\n",
       " (299,\n",
       "  ['it',\n",
       "   ' could be',\n",
       "   ' better to roughly introduce each of the existing 4 models. In Section 3,']),\n",
       " (300,\n",
       "  ['TPDN these 4 sentence encoding models',\n",
       "   ' to be illustrated.',\n",
       "   ' further']),\n",
       " (301,\n",
       "  ['More reasons', ' should be discussed', ' for the results in Table 2']),\n",
       " (302,\n",
       "  ['It',\n",
       "   ' be',\n",
       "   ' better to provide the actual performance (accuracy) given by TPDN on the 4 existing tasks.']),\n",
       " (303,\n",
       "  ['you',\n",
       "   ' considered',\n",
       "   ' applying these analysis on other models besides RNN?']),\n",
       " (304,\n",
       "  ['The paper',\n",
       "   ' describes',\n",
       "   ' new norm-based generalization bounds that were specifically adapted to convolutional neural networks.']),\n",
       " (305, ['these bounds', ' share', ' the same property.']),\n",
       " (306,\n",
       "  ['Further additional improvement over Bartlett et al ‘17 bound,',\n",
       "   ' is',\n",
       "   ' that this new bound depends on the sum of the operator norms of the parameter matrices, rather than the product.']),\n",
       " (307, ['The paper', ' is clearly written', ' and self-contained.']),\n",
       " (309, ['the main result', ' seems', ' ']),\n",
       " (310,\n",
       "  ['The experiments', ' are also', ' very limited and not too convincing.']),\n",
       " (311,\n",
       "  ['Further empirical evaluation', ' is needed', ' to demonstrate progress.']),\n",
       " (313, ['Ma ‘19', ' cited', ' in the beginning only,']),\n",
       " (314, ['They', ' proved', ' bounds with similar dependencies.']),\n",
       " (316, ['the bounds', ' presented', ' in Theorem 2.1?']),\n",
       " (317, ['\\\\eta', ' only appears', ' next to the empirical risk term.']),\n",
       " (318,\n",
       "  ['a concrete example there exists setting which this new (up',\n",
       "   ' is',\n",
       "   ' tighter Bartlett et al bound.']),\n",
       " (319, ['Three things', ' remain', ' unclear to me:']),\n",
       " (321,\n",
       "  ['the norms in the bounded',\n",
       "   ' are measured',\n",
       "   ' on typical trained neural network weights?']),\n",
       " (323, ['a VC', ' bound', ' in any (reasonable) setting?']),\n",
       " (324,\n",
       "  ['the bound under typical settings',\n",
       "   ' training',\n",
       "   ' when standard vision networks?']),\n",
       " (326, ['their bounds', ' are', ' size-free, which refers to the bounds']),\n",
       " (327,\n",
       "  ['this',\n",
       "   ' comes',\n",
       "   \" almost free'' when using convolutional neural network.\"]),\n",
       " (328,\n",
       "  ['I',\n",
       "   ' think',\n",
       "   ' that size-free in the title is misleading, and should be replaced with input size-free.']),\n",
       " (329,\n",
       "  ['most recent bounds',\n",
       "   ' depend',\n",
       "   ' on the distance from the initialization instead of the size of the weights.']),\n",
       " (330, ['This idea', ' was presented', ' in Roy ‘17, first']),\n",
       " (331, [\"I've\", ' reread', ' the rebuttals']),\n",
       " (332, ['I', ' increased', ' my score to weak accept.']),\n",
       " (333,\n",
       "  ['the generalization',\n",
       "   ' bound',\n",
       "   ' for deep neural networks, specifically, convolutional neural networks,']),\n",
       " (334,\n",
       "  ['The paper',\n",
       "   ' presents',\n",
       "   ' a generalization bound based on the distance of the final weights from the initialization, without dependence on the dimension of the input.']),\n",
       " (335,\n",
       "  ['the size convolutional kernel',\n",
       "   ' is',\n",
       "   ' much less than the width of the network, assumption.']),\n",
       " (336,\n",
       "  ['The paper',\n",
       "   ' gives',\n",
       "   ' another bound which works for fully connected layers with an additional term']),\n",
       " (337, ['the contribution of the paper', ' is not', ' clear for me.']),\n",
       " (339,\n",
       "  ['the remaining work of the paper', ' is mainly deriving', ' the Lipchitz']),\n",
       " (340, ['I', ' think', ' this should be clearly stated in the paper.']),\n",
       " (341, ['The experiment part', ' is not', ' quite convincing.']),\n",
       " (342, ['It', ' is not', ' clear from the figures']),\n",
       " (343, ['The writing of the paper', ' also can be improved.', ' ']),\n",
       " (344,\n",
       "  ['The paper',\n",
       "   ' presents',\n",
       "   ' math, which is nice, but without much intuition explained.']),\n",
       " (345, ['Overall I', ' would not recommend', ' this paper for admission.']),\n",
       " (346, ['This paper', ' studied', ' the generalization power of CNNs']),\n",
       " (347, ['Their results', ' have', ' two characteristics.']),\n",
       " (348,\n",
       "  ['the quantity', ' is', ' independent of the input dimension (size-free).']),\n",
       " (349,\n",
       "  ['the upper bounds',\n",
       "   ' involve',\n",
       "   ' the distance between initial and learned parameters.']),\n",
       " (350,\n",
       "  ['These results',\n",
       "   ' improved',\n",
       "   ' the upper bounds that we can derive by naively applying the results of Bartlett et al (2017)']),\n",
       " (351,\n",
       "  ['The authors',\n",
       "   ' empirically showed',\n",
       "   ' that there is a correlation between the generalization error of learned CNNs and the dominant term of the upper bound (i.e., the product of the parameter size and the distance from the set of initial parameters).']),\n",
       " (352,\n",
       "  ['this', ' is', ' the first work that proved the size-free generalization']),\n",
       " (353,\n",
       "  ['I',\n",
       "   ' think',\n",
       "   ' the assumption on the hypothesis class is very restrictive']),\n",
       " (354,\n",
       "  ['I',\n",
       "   ' judge',\n",
       "   ' the technical contribution of the paper is moderate and recommend to reject the paper weakly.']),\n",
       " (355, ['we $B$', ' is', ' the infimum of Lipschitz constant of hypotheses,']),\n",
       " (356, ['the size-free generalization', ' bound', ' ']),\n",
       " (357, ['the hypothesis class', ' is defined', ' ']),\n",
       " (358, ['The size-free generalization', ' bound', ' ']),\n",
       " (359, ['They', ' imposed', ' a restricted eigenvalue assumption.']),\n",
       " (360, ['we', ' need', ' more sophisticated analysis']),\n",
       " (361,\n",
       "  ['Comments The authors',\n",
       "   ' claimed',\n",
       "   ' that Figure 3 is consistent with theorems because, according to the upper bound of theorems, the distance from the initialization point decreases when the generalization error the same and the parameter size increases.']),\n",
       " (362,\n",
       "  ['I',\n",
       "   ' think',\n",
       "   ' it is too aggressive to conclude it from Figure 3 because the decreasing trend in the value of $\\\\|K-K_0\\\\|_\\\\sigma$ is found only around $2\\\\times 10^6\\\\leq W \\\\leq 3\\\\times 10^6$.']),\n",
       " (363,\n",
       "  ['the value of 10^5$',\n",
       "   ' is',\n",
       "   ' approximately the same as the value for $W\\\\approx 5\\\\times 3\\\\times 10^6$.']),\n",
       " (364, ['the conclusion section', ' summarizes', ' the paper']),\n",
       " (365,\n",
       "  ['Minor Comments - page 1, section 1, paragraph 1 - ... with roots in (Bartett,',\n",
       "   ' is',\n",
       "   ' that ... → Use \\\\citet - page 2, section 2.1, paragraph 2 - Write the definition of \"expansive\" activations.']),\n",
       " (366,\n",
       "  ['I',\n",
       "   ' think',\n",
       "   ' we should replace $\\\\log(\\\\lambda)$ in equations with $\\\\log(\\\\beta \\\\lambda respectively.']),\n",
       " (367,\n",
       "  ['- page 3, section 2.2, definition', ' →', ' 2.2 - $N$ $\\\\mathbb{N}$']),\n",
       " (368,\n",
       "  ['Neural Architecture Search,',\n",
       "   ' aims',\n",
       "   ' to automate design of neural network models.']),\n",
       " (369, ['Their approach', ' consists', ' in using a two stage algorithm:']),\n",
       " (370,\n",
       "  ['binary sub-graphs coding the sub-architectures',\n",
       "   ' are uniformly sampled',\n",
       "   ' Then']),\n",
       " (371, ['The graph sampling matrix', ' sampled', ' architectures']),\n",
       " (372, ['the optimization problem', ' is solved', ' Then using Lasso.']),\n",
       " (373,\n",
       "  ['The largest Fourier coefficients',\n",
       "   ' are chosen',\n",
       "   ' to build an estimate g of f.']),\n",
       " (374, ['the optimization problem', ' is', ' ill-posed. Since $m << n^d$,']),\n",
       " (375,\n",
       "  ['Theorem 3.2',\n",
       "   ' shows',\n",
       "   ' that if A satisfies the restricted isometry of order s, then the sparse coefficients x can be recovered.']),\n",
       " (377,\n",
       "  ['1/ I', ' did not find', ' the proof of Theorem 3.2 in the main paper']),\n",
       " (378,\n",
       "  ['2/ The authors',\n",
       "   ' claim',\n",
       "   ' that their algorithm performs better than the state-of-the-art,']),\n",
       " (379, ['3/ The key idea of the algorithm', ' is not well explained.', ' ']),\n",
       " (380, ['A one-shot NAS f', ' is', ' pre-trained.']),\n",
       " (381, ['f is', ' assumed to be', ' well-trained.']),\n",
       " (382,\n",
       "  ['it', ' is approximated', ' with a Fourier-sparse Boolean function. Then']),\n",
       " (383, ['using an approximation f', ' is', ' perfect?']),\n",
       " (384,\n",
       "  ['Do you expect',\n",
       "   ' to reduce',\n",
       "   ' the needed number of sampled architectures?']),\n",
       " (385, ['Equation 3.1', ' is not', ' clear.']),\n",
       " (386, ['So all rows', ' seem', ' ']),\n",
       " (387,\n",
       "  ['This paper',\n",
       "   ' tackles',\n",
       "   ' the problem of One-shot Neural architecture search']),\n",
       " (388,\n",
       "  ['The method',\n",
       "   ' consists mainly',\n",
       "   ' of new search strategy of the optimal architecture']),\n",
       " (389,\n",
       "  ['this work',\n",
       "   ' is',\n",
       "   ' an application of recent progress in the field of compressive sensing to One-shot neural architecture search.']),\n",
       " (390,\n",
       "  ['the authors',\n",
       "   ' have also provides',\n",
       "   ' guarantee for the optimality of their method,']),\n",
       " (391, ['the paper', ' is', ' well motivated']),\n",
       " (392,\n",
       "  ['That',\n",
       "   ' said',\n",
       "   ' the structure could be enormously improved to ease the reading and the overall understanding.']),\n",
       " (393, ['better caption for Figure 2', ' explaining', ' what is shown', '']),\n",
       " (394,\n",
       "  ['Novelty The main novelty in my opinion',\n",
       "   ' is',\n",
       "   ' the application of compressive sensing methods to One-shot NAS.']),\n",
       " (395,\n",
       "  ['This approach',\n",
       "   ' is',\n",
       "   ' significantly different from other One-shot NAS method']),\n",
       " (396,\n",
       "  ['the only novelty',\n",
       "   ' was',\n",
       "   ' the framing of One-shot NAS as a recovery of boolean functions from their sparse Fourier expansions']),\n",
       " (397, ['I', ' am', ' open to be proven wrong on this point!']),\n",
       " (398, ['Results The experiment section', ' is not', ' self-content,']),\n",
       " (399,\n",
       "  ['the search strategy of CoNAS',\n",
       "   ' seems',\n",
       "   ' Overall, parameter efficient, fast and competitive.']),\n",
       " (400,\n",
       "  ['small ablation studies',\n",
       "   ' showing',\n",
       "   ' the effect of the different parameters of CoNAS']),\n",
       " (402, ['your method', ' is', ' competitive. the same search space']),\n",
       " (403, ['4 m=1000 in your experiments', ' satisfies', ' theorem 3.2?']),\n",
       " (404, ['What', ' is', ' the value of d in your experiments?']),\n",
       " (405, ['supporting experiments', ' answer', ' those questions?']),\n",
       " (406, ['I', ' will say', ' *weak reject* For now,']),\n",
       " (407,\n",
       "  ['This paper',\n",
       "   ' proposes',\n",
       "   ' a new algorithm for one-shot neural architecture search (NAS) via compressive sensing.']),\n",
       " (408,\n",
       "  ['The authors',\n",
       "   ' propose',\n",
       "   ' a new search strategy, as well as a slightly different search space']),\n",
       " (409, ['They', ' use', ' architecture samples from the one-shot model']),\n",
       " (410,\n",
       "  ['Fourier coefficients',\n",
       "   ' are used',\n",
       "   ' to optimize the vector of binary parameters eventually']),\n",
       " (411,\n",
       "  ['I',\n",
       "   ' think',\n",
       "   ' the proposed algorithm is interesting and of practical usefulness.']),\n",
       " (412,\n",
       "  ['this work',\n",
       "   ' to be',\n",
       "   ' an application of Harmonica [6] to the NAS problem (with small modifications in order']),\n",
       " (413,\n",
       "  ['you',\n",
       "   ' state',\n",
       "   ' some of the differences of your method with Harmonica. In page 10']),\n",
       " (414,\n",
       "  ['the number of function evaluations you (coming',\n",
       "   ' is',\n",
       "   ' from the one-shot model) larger to obtain, however this does not guarantee that these are a good surrogate of the true objective that NAS aims to minimize, ie the validation/test accuracy of final (stand-alone) architectures']),\n",
       " (415,\n",
       "  ['The empirical evaluations of their algorithm the other results',\n",
       "   ' are taken',\n",
       "   ' from the corresponding papers.']),\n",
       " (416,\n",
       "  ['The paper',\n",
       "   ' is',\n",
       "   ' well-written with the caveat of being more than the recommended 8 pages.']),\n",
       " (417, ['I', ' will adjust', ' the authors responses']),\n",
       " (418, ['the stand-alone architectures', ' trained', ' from scratch']),\n",
       " (419,\n",
       "  ['you',\n",
       "   ' tune',\n",
       "   ' the p in the Bernoulli distribution during the one-shot weight updates.']),\n",
       " (420,\n",
       "  ['the ScheduledDropPath probability',\n",
       "   ' is',\n",
       "   ' an important hyperparameter affecting the aforementioned correlation.']),\n",
       " (421,\n",
       "  ['the main motivation for using 5 operations in the operation',\n",
       "   ' set',\n",
       "   ' not in']),\n",
       " (422,\n",
       "  ['the main contribution in the competitive results',\n",
       "   ' come',\n",
       "   ' from the different search space or the search method?']),\n",
       " (423,\n",
       "  ['3)', ' Is', ' there any reference for the correctness of Theorem 3.2?']),\n",
       " (424, ['some parts', ' can be moved', ' in the Supplementary,']),\n",
       " (433, ['Y. Zoph, Quoc V. Le,', ' is Dean. of', ' Jeff']),\n",
       " (439, ['the proposed model', ' is', ' quite intuitive.']),\n",
       " (440,\n",
       "  ['the idea',\n",
       "   ' is',\n",
       "   ' to represent entailment as a product of continuous functions over possible worlds.']),\n",
       " (441,\n",
       "  ['the idea',\n",
       "   ' is',\n",
       "   ' to generate possible worlds, and compute the functions']),\n",
       " (442,\n",
       "  ['The functions',\n",
       "   ' are designed',\n",
       "   ' as tree neural networks to take advantage of logical structure.']),\n",
       " (444,\n",
       "  ['The results',\n",
       "   ' seem',\n",
       "   ' very impressive with > 99% accuracy on tests sets.']),\n",
       " (446,\n",
       "  ['some form of cross-validation',\n",
       "   ' be applied smooth out',\n",
       "   ' to variance in the evaluation results.']),\n",
       " (447,\n",
       "  ['I',\n",
       "   ' am not',\n",
       "   ' sure if there are standard \"shared\" datasets for this task,']),\n",
       " (450, ['this', ' was chosen', ' ']),\n",
       " (451, ['I', ' think,', ' is quite related to model counting.']),\n",
       " (452, ['a lot of work model', ' on', ' counting.']),\n",
       " (453, ['this', ' relates', ' to those lines of work']),\n",
       " (454,\n",
       "  ['I',\n",
       "   ' think',\n",
       "   ' the authors have improved the experiments substantially. After revision']),\n",
       " (455, ['The paper', ' is', ' fairly broad in what it is trying to achieve,']),\n",
       " (456,\n",
       "  ['The purpose of the paper',\n",
       "   ' is',\n",
       "   ' to investigate the effectiveness of prior machine learning methods with predicting logical entailment and then provide a new model']),\n",
       " (457, ['neural networks', ' to detect', ' entailment?']),\n",
       " (458,\n",
       "  ['architectures',\n",
       "   ' are',\n",
       "   ' best at relating features in a purely structural sequence-based problem?\".']),\n",
       " (459, ['they', ' are tasked', ' with learning logical entailment.']),\n",
       " (460,\n",
       "  ['The proposed network architecture,',\n",
       "   ' is viewed',\n",
       "   ' as an improvement on an earlier architecture TreeNet. then']),\n",
       " (461, ['POSITIVES The structure of this paper', ' was', ' very well done.']),\n",
       " (462, ['The paper', ' attempts', ' to do a lot,']),\n",
       " (463, ['The generated dataset', ' used', ' for testing logical entailment']),\n",
       " (464, ['The baseline benchmark networks', ' are covered', ' in depth']),\n",
       " (465, ['the equations', ' provided', ' ']),\n",
       " (467,\n",
       "  ['they',\n",
       "   ' created',\n",
       "   ' a dataset can be used for logical entailment learning, and then explains clearly all the previous network models which will be used in testing as well their model.']),\n",
       " (468,\n",
       "  ['The background information regarding each model',\n",
       "   ' was',\n",
       "   ' exceptionally thorough.']),\n",
       " (469, ['The paper', ' went', ' into great depth']),\n",
       " (470, ['The section', ' describing', ' the creation of a dataset']),\n",
       " (471,\n",
       "  ['They', ' describe', ' the creation of the data, as well as the means']),\n",
       " (472,\n",
       "  ['The paper',\n",
       "   ' provides',\n",
       "   ' an in depth description of their PossibleWorldNet model,']),\n",
       " (473,\n",
       "  ['NEGATIVES One issue I had with the paper',\n",
       "   ' is',\n",
       "   ' regarding the creation of the logical entailment dataset.']),\n",
       " (474, ['they', ' explained', ' the process of creating the dataset,']),\n",
       " (475,\n",
       "  ['I',\n",
       "   ' wonder',\n",
       "   ' if it would be better to find non-generated datasets data entailment']),\n",
       " (476, ['It', ' is', ' questionable']),\n",
       " (479,\n",
       "  ['their proposed network',\n",
       "   ' is',\n",
       "   ' an incredible achievement since PossibleWorldNet effectively beat out other methods on a dataset']),\n",
       " (480,\n",
       "  ['RELATED WORK The paper',\n",
       "   ' has',\n",
       "   ' an extensive section dedicated to covering related work.']),\n",
       " (481, ['the research', ' involved', ' ']),\n",
       " (482,\n",
       "  ['I', ' would accept', ' this paper as a valid scientific contribution.']),\n",
       " (483, ['The paper', ' performs', ' a thorough analysis on the limitations']),\n",
       " (484, ['The paper', ' covers', ' results of the experiments']),\n",
       " (485, ['The structure of the PossibleWorldNet', ' was explained well,', ' ']),\n",
       " (487, ['This', ' is', ' a self-contained paper.']),\n",
       " (488, ['it', ' introduces', ' a very important problem']),\n",
       " (489,\n",
       "  ['The major point of the paper',\n",
       "   ' is demonstrating',\n",
       "   ' that it is possible to model logical entailment in neural networks.']),\n",
       " (490, ['a NN model', ' are introduced.', ' ']),\n",
       " (491,\n",
       "  ['The corpus',\n",
       "   ' is used',\n",
       "   ' to demonstrate that the model, named PossibleWorld, is nearly perfect for the task.']),\n",
       " (492,\n",
       "  ['A comparative analysis',\n",
       "   ' is done',\n",
       "   ' with respect to state of the art recurrent NN.']),\n",
       " (494, ['what the', ' is', ' take home message?']),\n",
       " (495,\n",
       "  ['the message specific neural networks that model the task',\n",
       "   ' is',\n",
       "   ' that generic NN should not be used for specific formal tasks whereas are desirable.']),\n",
       " (496, ['This', ' seems', ' ']),\n",
       " (497, ['the paper', ' leaves', ' unexplained']),\n",
       " (498, ['The description of the network', ' is', ' very criptic.']),\n",
       " (499, ['No examples', ' are given', ' ']),\n",
       " (500, ['this', ' is', ' THE needed model?']),\n",
       " (501, ['research', ' has been done', ' in the past by Plate.']),\n",
       " (502,\n",
       "  ['Plate',\n",
       "   ' has investigated',\n",
       "   ' how symbolic predicates can be described in distributed representations.']),\n",
       " (503, ['This', ' is strictly related', ' to the problem']),\n",
       " (504,\n",
       "  ['2017, the link between symbolic and distributed representations',\n",
       "   ' to be',\n",
       "   ' better investigated in order']),\n",
       " (505, ['Your paper', ' can be', ' one of the first NN model']),\n",
       " (506, ['This paper', ' provides', ' a system to play CCP']),\n",
       " (507,\n",
       "  ['The system',\n",
       "   ' consists',\n",
       "   ' of three modules - the bid module, which based, and the kicker networks,']),\n",
       " (508,\n",
       "  ['8 million game records',\n",
       "   ' consisting',\n",
       "   ' of 80 million state action pairs,']),\n",
       " (509, ['The resulting model', ' is able', ' beat MicroWe,']),\n",
       " (510,\n",
       "  ['a linear', ' can do fairly well compared', ' to a rule based module.']),\n",
       " (511,\n",
       "  ['separating the kicker networks',\n",
       "   ' would be',\n",
       "   ' more advantageous than combining them.']),\n",
       " (512,\n",
       "  ['Thousands of actions',\n",
       "   ' is not',\n",
       "   ' a too large number - language modeling work routinely deals with outputting many more classes than that.']),\n",
       " (513, ['the convolutions', ' Were', ' chosen 1D, 2D, or 3D?']),\n",
       " (514, ['The figure', ' seems', ' ']),\n",
       " (515, ['this', \" doesn't make\", ' too much sense to be,']),\n",
       " (516,\n",
       "  ['There',\n",
       "   \" shouldn't be\",\n",
       "   ' a lot of translational invariance in the Z dimension.']),\n",
       " (517,\n",
       "  [\"I'm\",\n",
       "   ' also not',\n",
       "   ' convinced that translational invariance is helpful in the X dimension.']),\n",
       " (520,\n",
       "  ['hyperparameters', ' were searched', ' through in the learning process?']),\n",
       " (521, ['Missing citations MicroWe', ' being', ' the best CCP AI,']),\n",
       " (523, ['this system', ' is', ' how far from solving CCP.']),\n",
       " (524,\n",
       "  ['Figure 3, 4',\n",
       "   ' should just say',\n",
       "   ' #of games instead of \"iteration\" This paper shows that one choice for a supervised learning system on a CCP game database can achieve amateur level human play.']),\n",
       " (525,\n",
       "  ['It',\n",
       "   ' does not give',\n",
       "   ' insight to why the system was designed this way, why the model choices were made, and how good simpler baselines might be able to achieve.']),\n",
       " (526,\n",
       "  ['The paper',\n",
       "   ' does not provide',\n",
       "   ' enough scientific value to be accepted to the conference.']),\n",
       " (527,\n",
       "  ['The authors',\n",
       "   ' propose',\n",
       "   ' a model that learns to play the China Competitive Poker game.']),\n",
       " (528, ['The model', ' uses', ' CNN to predict the actions,']),\n",
       " (529, ['The model', ' is shown', ' ']),\n",
       " (530, ['The performance', ' is certainly', ' strong (if it were true).']),\n",
       " (531, ['the paper', ' is not', ' currently reproducible at all.']),\n",
       " (532,\n",
       "  ['the following comments',\n",
       "   ' are based',\n",
       "   ' on the trust-worthiness of the paper.']),\n",
       " (533, ['The writing', ' lacks', ' formality']),\n",
       " (534, ['the super-short Section 2', ' is', ' rather unprofessional---it']),\n",
       " (535,\n",
       "  ['I',\n",
       "   ' find',\n",
       "   ' it very hard to follow Section 3) There is a big room for improving the English writing.']),\n",
       " (536, ['the model', ' should be', ' superior than other modeling choices.']),\n",
       " (537, ['what the neighboring connections of CNNs', ' does play?', ' role']),\n",
       " (538, ['What the cons', ' are of', ' choosing CNNs?']),\n",
       " (539, ['strong motivations', ' to design', ' there the model this way?']),\n",
       " (540, ['the model', ' trained', ' with human records readily super-human?']),\n",
       " (541,\n",
       "  ['the typical performance',\n",
       "   ' is bound',\n",
       "   ' by the human-level performance. common imitation learning']),\n",
       " (542, ['how many', ' is', ' many?']),\n",
       " (543, ['the authors', ' separate', ' the professional ones?']),\n",
       " (544,\n",
       "  ['The authors',\n",
       "   ' introduce',\n",
       "   ' an algorithm in the subfield of conditional program generation']),\n",
       " (545, ['an algorithm', ' based', ' on sketches- abstractions of programs']),\n",
       " (546, ['they', ' generate', \" the method's body from the trained sketches.\"]),\n",
       " (547, ['It', ' sets', ' a sketch of the algorithm']),\n",
       " (548, ['Motivation', ' given', ' in form of relevant applications']),\n",
       " (549,\n",
       "  ['It',\n",
       "   ' is introduced',\n",
       "   \" with 'We ask' followed by two well formulated lines\"]),\n",
       " (550, ['It', ' is repeated', ' multiple times throughout the paper.']),\n",
       " (551, ['this', ' is', ' necessary']),\n",
       " (552, ['terms', ' might not be', ' familiar to the reader']),\n",
       " (553, ['This', ' is', ' true for mathematical aspects as well as program']),\n",
       " (555,\n",
       "  ['The authors',\n",
       "   ' present',\n",
       "   ' a very intriguing novel approach that in a coherent way.']),\n",
       " (556, ['The approach', ' is thoroughly explained', ' for a large audience.']),\n",
       " (557, ['The task', ' is', ' interesting and novel.']),\n",
       " (558,\n",
       "  ['The large evaluation section',\n",
       "   ' discusses',\n",
       "   ' many different properties']),\n",
       " (559, ['the paper', ' is not missing', ' any information.']),\n",
       " (560,\n",
       "  ['This paper',\n",
       "   ' aims',\n",
       "   ' to synthesize programs in a Java-like language from a task description']),\n",
       " (561,\n",
       "  ['The paper',\n",
       "   ' argues',\n",
       "   ' that it is too difficult to map directly from the description to a full program,']),\n",
       " (562,\n",
       "  ['a \"sketch\"',\n",
       "   ' containing',\n",
       "   ' high level program structure but no concrete details about, e.g., variable names.']),\n",
       " (563,\n",
       "  ['the sketch',\n",
       "   ' by stochastically filling',\n",
       "   ' in the abstract parts of the sketch with concrete instantiations.']),\n",
       " (564,\n",
       "  ['The paper',\n",
       "   ' presents',\n",
       "   ' an abstraction method for converting a program into a sketch, a stochastic encoder-decoder model for converting descriptions to trees, and rejection sampling-like approach for converting sketches to programs.']),\n",
       " (565,\n",
       "  ['it using sketches as an intermediate abstraction',\n",
       "   ' outperforms',\n",
       "   ' directly mapping to the program AST.']),\n",
       " (567, ['This', ' is', ' one of the strongest points of the paper.']),\n",
       " (568,\n",
       "  ['One point I found confusing',\n",
       "   ' is',\n",
       "   ' how exactly the Combinatorial Concretization step works.']),\n",
       " (569,\n",
       "  ['this step Prog',\n",
       "   ' depends is',\n",
       "   ' Y, given Y, conditionally independent of X?']),\n",
       " (570, ['this', ' is', ' correct,']),\n",
       " (571, ['no learning', ' is required', ' for the P(Prog | Y) step']),\n",
       " (572, ['using a stochastic latent variable', ' is', ' necessary.']),\n",
       " (574,\n",
       "  ['Some discussion of Grammar Variational Autoencoder',\n",
       "   ' would probably be',\n",
       "   ' appropriate.']),\n",
       " (575, ['programs', ' are', ' more like those found \"in the wild\".']),\n",
       " (576,\n",
       "  ['this paper',\n",
       "   ' adds',\n",
       "   ' an interesting new take on the pattern has a very different abstraction than say, DeepCoder),']),\n",
       " (577,\n",
       "  ['a very well-written paper',\n",
       "   ' tackles',\n",
       "   ' the problem of generating/inferring code']),\n",
       " (578,\n",
       "  ['This',\n",
       "   ' is',\n",
       "   ' a novel contribution to existing machine learning approaches to automated programming']),\n",
       " (579,\n",
       "  ['The combination of leveraging of real data',\n",
       "   ' are',\n",
       "   ' a substantial strength of the work']),\n",
       " (580, ['This paper', ' has', ' many strengths:']),\n",
       " (581,\n",
       "  ['I',\n",
       "   ' have read',\n",
       "   \" the authors' rebuttal and also the other comments in this paper's thread.\"]),\n",
       " (582, ['My thoughts', ' have not changed.', ' ']),\n",
       " (583,\n",
       "  ['The authors',\n",
       "   ' propose',\n",
       "   ' using a mixture prior rather than a uni-modal prior for variational auto-encoders.']),\n",
       " (584,\n",
       "  ['They',\n",
       "   ' argue',\n",
       "   ' that the simple uni-modal prior \"hinders the overall expressivity of the learned model']),\n",
       " (585,\n",
       "  ['I',\n",
       "   ' find',\n",
       "   ' the motivation of the paper suspicious because while the prior may be uni-modal, the posterior distribution is certainly not.']),\n",
       " (586,\n",
       "  ['a uni-modal distribution on the latent variable space',\n",
       "   ' can certainly lead',\n",
       "   ' to the capturing of complex, multi-modal data distributions. still']),\n",
       " (587,\n",
       "  ['the likelihood mass',\n",
       "   ' to be',\n",
       "   \" a point by applying the true data distribution's inverse CDF to the uniform.\"]),\n",
       " (588, ['Such a model', ' can capture', ' any distribution.)']),\n",
       " (589,\n",
       "  ['multi-modality',\n",
       "   ' is arguably',\n",
       "   ' an overfocused concept in the literature,']),\n",
       " (590, ['It', ' is', ' unclear from the experiments']),\n",
       " (591,\n",
       "  ['I',\n",
       "   ' recommend',\n",
       "   ' that this paper be rejected, and encourage the authors to more extensively study the effect of different priors.']),\n",
       " (592,\n",
       "  ['the 14 page document',\n",
       "   ' can be significantly condensed',\n",
       "   ' no length restriction at ICLR,']),\n",
       " (593, ['I', ' recommend', ' the authors do so.']),\n",
       " (594,\n",
       "  ['I',\n",
       "   ' think',\n",
       "   \" it's important to note the controversy in this paper. Finally,\"]),\n",
       " (595,\n",
       "  ['It', ' was submitted', ' with many significant incomplete details no']),\n",
       " (596, ['These details', ' were not completed', ' until roughly a week(?)']),\n",
       " (598,\n",
       "  ['I',\n",
       "   ' recommend',\n",
       "   ' the chairs discuss this in light of what should be allowed next year.']),\n",
       " (599,\n",
       "  ['variational autoencoders,',\n",
       "   ' are',\n",
       "   ' compatible with the reparameterization trick,']),\n",
       " (601,\n",
       "  ['They',\n",
       "   ' show',\n",
       "   ' improvements on bag of words document modeling, and dialogue response generation.']),\n",
       " (602,\n",
       "  ['The original abstract',\n",
       "   ' is',\n",
       "   ' overly strong in its assertion that a unimodal latent prior p(z) cannot fit a multimodal marginal int_z p(x|z)p(x)dz DNN response model']),\n",
       " (603,\n",
       "  ['While the assertion that a unimodal latent prior is necessary to model multimodal observations',\n",
       "   ' is',\n",
       "   ' false,']),\n",
       " (604, ['we', ' think', ' of a VAE as a sort of regularized autoencoder']),\n",
       " (605,\n",
       "  ['a hypercube-based tiling of latent code space',\n",
       "   ' is',\n",
       "   ' a sensible idea.']),\n",
       " (606,\n",
       "  ['I',\n",
       "   ' found',\n",
       "   ' the message of the paper to be quite sloppy with respect to the concept of \"multi-modality.\"']),\n",
       " (607,\n",
       "  ['any deep latent Gaussian model, multimodality in the prior p(z),',\n",
       "   ' makes',\n",
       "   ' sense in some situations']),\n",
       " (608, ['The final type of multimodality', ' is', ' harder to argue for,']),\n",
       " (609, ['I', \" don't think\", ' these need to have multiple strong modes.']),\n",
       " (610,\n",
       "  ['I',\n",
       "   ' to see',\n",
       "   ' experiments demonstrating otherwise for real world data.']),\n",
       " (611,\n",
       "  ['I',\n",
       "   ' think',\n",
       "   ' this paper should be more clear about the different types of multi-modality and which parts of their analysis demonstrate which ones.']),\n",
       " (612,\n",
       "  ['I',\n",
       "   ' found',\n",
       "   ' it unsatisfactory that the piecewise variable analysis did not show different components of the multi-modal prior']),\n",
       " (613, ['the powerful networks', ' transforming', ' to latent space']),\n",
       " (614,\n",
       "  ['the learned prior parameters',\n",
       "   ' would strengthen',\n",
       "   ' these experiments. Explicitly separating out the contributions of a reimplemented base model,']),\n",
       " (616, ['the prior modes', ' represent.', ' ']),\n",
       " (617, ['I', ' would be', ' surprised']),\n",
       " (618,\n",
       "  ['The experiments on dialog modeling',\n",
       "   ' are quantitatively.',\n",
       "   ' mostly negative results,']),\n",
       " (619,\n",
       "  ['the the piecewise constant variables', ' encode', ' time-related words']),\n",
       " (620,\n",
       "  ['I',\n",
       "   ' would be',\n",
       "   ' interested in seeing analysis of why this is the case.']),\n",
       " (621, ['I', ' would like', ' to see an analysis of the sorts of words']),\n",
       " (622, ['the piecewise constant variational family', ' is', ' a good idea,']),\n",
       " (624, ['H-NVDM', ' performs better', ' ']),\n",
       " (625,\n",
       "  ['This paper',\n",
       "   ' demonstrate',\n",
       "   ' that those sorts of things are actually being captured by the model.']),\n",
       " (626, ['the paper', ' introduces', ' an interesting variational family']),\n",
       " (628, ['the method', ' yielded', ' interpretable multiple modes']),\n",
       " (629,\n",
       "  ['it', ' could explore', ' the multi-modality of the latent variables']),\n",
       " (630,\n",
       "  ['the introduction of the piecewise constant distribution',\n",
       "   ' helps',\n",
       "   ' achieve better perplexity on modelling documents and seemly better performance on modelling dialogues.']),\n",
       " (631,\n",
       "  ['The idea of having a piecewise constant prior for latent variables',\n",
       "   ' is',\n",
       "   ' interesting,']),\n",
       " (632,\n",
       "  ['a multimodal prior',\n",
       "   ' would help',\n",
       "   ' the VAEs overcome the issues of optimisation.']),\n",
       " (633, ['evidence', ' showing', ' the multimodality of the prior']),\n",
       " (634,\n",
       "  ['the author',\n",
       "   ' claimed',\n",
       "   ' the decoder parameter matrix is directly affected by the latent variables. the last paragraph of 6.1,']),\n",
       " (635,\n",
       "  ['what the connects the decoder',\n",
       "   ' is',\n",
       "   ' a combination of a piecewise constant and Gaussian latent variables.']),\n",
       " (636, ['z=<z_gaussian,', ' is', ' multimodal.']),\n",
       " (637, ['z=<z_gaussian1, z_gaussian2>', ' can be as', ' multimodal well.']),\n",
       " (638, ['None of the claims in this paragraph', ' stands.', ' ']),\n",
       " (640, ['the prior', ' is learned together', ' ']),\n",
       " (641,\n",
       "  ['a fair comparison',\n",
       "   ' would at least be',\n",
       "   ' z=<z_gaussian, z_piecewise> which equals to a double sized z_gaussian.']),\n",
       " (642, ['results', ' shown', ' in Table 3']),\n",
       " (643,\n",
       "  ['I',\n",
       "   ' cannot believe',\n",
       "   ' the author used gradients to evaluate the model.']),\n",
       " (644, ['5', ' is', ' confusing, adding a multiplication sign']),\n",
       " (645, ['people', ' attending', ' ICLR']),\n",
       " (646,\n",
       "  ['Typos:',\n",
       "   ' as well as',\n",
       "   ' as the well as the generated prior-> the generated prior']),\n",
       " (647,\n",
       "  ['This paper',\n",
       "   ' proposes',\n",
       "   ' a novel approach with the hypothesis that the reliable features can guide the less reliable ones.']),\n",
       " (648, ['This approach', ' is applied', ' to the object detection task']),\n",
       " (649, ['This paper', ' is', ' well-written and easy to follow.']),\n",
       " (650, ['the less reliable one', ' is', ' very interesting']),\n",
       " (651,\n",
       "  ['the hypothesis, which',\n",
       "   ' is also',\n",
       "   ' that reliable features can guide the features in the less reliable set interesting.']),\n",
       " (652, ['The performance improvements', ' are', ' quite large.']),\n",
       " (653,\n",
       "  ['Extensive ablative studies',\n",
       "   ' are provided',\n",
       "   ' to support the proposed method well.']),\n",
       " (654,\n",
       "  ['The method of obtaining the representative in buffer B',\n",
       "   ' is not clearly presented.',\n",
       "   ' ']),\n",
       " (655, ['The overall training procedure', ' are not clearly presented.', ' ']),\n",
       " (656, ['Some notations', ' are', ' vague and confusing.']),\n",
       " (658,\n",
       "  ['you',\n",
       "   ' choose',\n",
       "   ' the higher level feature map at the m-th level in option']),\n",
       " (659, ['What', ' is', ' the meaning of the \"past\" features in Section 3.2?']),\n",
       " (660,\n",
       "  ['It',\n",
       "   ' is better',\n",
       "   ' to show the exact architecture of the make-up module and the critic module.']),\n",
       " (661,\n",
       "  ['this method',\n",
       "   ' apply',\n",
       "   ' to the other backbones such as ResNets without FPN?']),\n",
       " (662,\n",
       "  ['The sentences at the bottom of p.4',\n",
       "   ' starting',\n",
       "   ' with \"Note that only~\" looks ambiguous.']),\n",
       " (663, ['- f_critic^j', ' may be', ' the j-th element of F_critic,']),\n",
       " (664, ['I', ' think', ' this paper is above the standard of ICLR']),\n",
       " (665, ['the experimental studies', ' are properly designed', ' ']),\n",
       " (666, ['I', ' would like', \" to see the other reviewers' comments.\"]),\n",
       " (667,\n",
       "  ['This paper', ' aims to facilitate', ' feature learning in NN models']),\n",
       " (668, ['This', ' is', ' very similar to self-paced learning']),\n",
       " (670,\n",
       "  ['The method', ' is positioned', ' as a general one for feature learning.']),\n",
       " (674,\n",
       "  ['The authors',\n",
       "   ' tackle',\n",
       "   ' the problem of detecting small/low resolution objects in an image.']),\n",
       " (675, ['detecting bigger objects', ' is', ' an easier task']),\n",
       " (676, ['This', ' is done', ' ']),\n",
       " (677,\n",
       "  ['The second branch',\n",
       "   ' contains',\n",
       "   ' a make-up layer learned during training (which acts as the guidance from the more reliable set)']),\n",
       " (678,\n",
       "  ['The authors',\n",
       "   ' define',\n",
       "   ' a class buffer that contains representative elements of object features from the reliable set for every category & scale and an intertwiner loss']),\n",
       " (679,\n",
       "  ['They',\n",
       "   ' also use',\n",
       "   ' an Optimal Transport procedure with a Sinkhorn divergence loss between object features from both sets.']),\n",
       " (680,\n",
       "  ['The overall loss of the system',\n",
       "   ' is',\n",
       "   ' now a sum of the detection loss, the intertwiner loss and the optimal transport loss.']),\n",
       " (681,\n",
       "  ['They',\n",
       "   ' evaluate',\n",
       "   ' their model on the COCO Object detection challenge']),\n",
       " (682,\n",
       "  ['They',\n",
       "   ' provide',\n",
       "   ' thorough ablation analysis of various design choices.']),\n",
       " (683,\n",
       "  ['The qualitative result in Fig.1',\n",
       "   ' showing',\n",
       "   ' well clustered features for both high & low resolution objects via t-SNE']),\n",
       " (684, ['Clarity The paper', ' is well written', ' and easy to follow.']),\n",
       " (685, ['The paper', ' tackles', ' an important problem']),\n",
       " (686,\n",
       "  ['it', ' demonstrates', ' the improvement achieved using their approach.']),\n",
       " (687,\n",
       "  ['a single element per object category per scale',\n",
       "   ' to represent',\n",
       "   ' all features.']),\n",
       " (688,\n",
       "  ['The advantage of forcing such a representation',\n",
       "   ' is',\n",
       "   ' tight clustering in the feature space.']),\n",
       " (689,\n",
       "  [\"wouldn't a dictionary approach with multiple elements\",\n",
       "   ' learn',\n",
       "   ' a richer feature representation at the cost of not-so-good clustering']),\n",
       " (691,\n",
       "  ['I',\n",
       "   ' find',\n",
       "   ' it interesting that Mask RCNN, updated results has a might higher AP_S (43.5) compared to you (27.2) and everyone else. In Table 4 of Appendix']),\n",
       " (692,\n",
       "  ['I',\n",
       "   ' was expecting',\n",
       "   ' you to be the best under that metric due to the explicit design for small objects.']),\n",
       " (693,\n",
       "  ['They (MaskRCNN, updated results)',\n",
       "   ' are also worse',\n",
       "   ' significantly better than the rest under AP_M but under AP_L.']),\n",
       " (694, ['you', ' explain', ' this behavior']),\n",
       " (695, ['the ResNeXt', ' Is', ' backbone that much better for small objects']),\n",
       " (696,\n",
       "  ['This paper',\n",
       "   ' proposes',\n",
       "   ' a method for significantly increasing the number of parameters in a single layer while keeping computation in par with (or even less than) current SOTA models.']),\n",
       " (697, ['only a few of them', ' are adaptively activated', ' ']),\n",
       " (698,\n",
       "  ['the gating network',\n",
       "   ' is encouraged',\n",
       "   ' to achieve two objectives: utilizing all available experts (aka importance), and distributing computation fairly across them (aka load).']),\n",
       " (699, ['the batch-size', ' passed', ' to each expert,']),\n",
       " (700,\n",
       "  ['Experiments',\n",
       "   ' applying',\n",
       "   ' the proposed approach on RNNs in language modelling task']),\n",
       " (701,\n",
       "  ['Results on machine translation',\n",
       "   ' show',\n",
       "   ' that a model with more than 30x number of parameters can beat SOTA while incurring half of the effective computation.']),\n",
       " (702, ['I', ' have', ' the several comments on the paper:']),\n",
       " (703,\n",
       "  ['I',\n",
       "   ' find',\n",
       "   ' that Section 3.2 (the crux of the paper) needs better motivation and intuitive explanation.']),\n",
       " (704,\n",
       "  ['equation 8',\n",
       "   ' deserves',\n",
       "   ' more description than currently devoted to it.']),\n",
       " (705, ['Additional space', ' can be easily regained', ' ']),\n",
       " (706,\n",
       "  ['Experiment section',\n",
       "   ' by finishing',\n",
       "   ' on experiment before moving to the other one.']),\n",
       " (708,\n",
       "  ['One very important lesson from the conditional computation literature',\n",
       "   ' is',\n",
       "   ' that while we can in theory incur much less computation, in practice (especially with the current GPU architectures) the actual time does not match the theory.']),\n",
       " (709, ['This', ' can be', ' due to inefficient branching in GPUs.']),\n",
       " (710, ['It', ' would be', ' nice']),\n",
       " (711,\n",
       "  ['I',\n",
       "   ' think',\n",
       "   ' they should be combined in one (maybe moving Table 3 to appendix).']),\n",
       " (712, ['One thing', ' do not understand', ' I']),\n",
       " (713, ['This', ' also related', ' to the pervious comment.']),\n",
       " (715,\n",
       "  ['capacity it',\n",
       "   ' be to discuss',\n",
       "   ' very nice the use of MoE in terms of computational efficiency and other factors.']),\n",
       " (716,\n",
       "  ['This paper',\n",
       "   ' describes',\n",
       "   ' a method for greatly expanding network model size (in terms of number of stored parameters) in the context of a recurrent net, by applying a Mixture of Experts between recurrent net layers']),\n",
       " (717,\n",
       "  ['the effective batch size to the MoE',\n",
       "   ' is increased',\n",
       "   ' by a factor of the number of steps in the model',\n",
       "   ' By process all at the same time,']),\n",
       " (718,\n",
       "  ['Another second technique',\n",
       "   ' redistributes',\n",
       "   ' elements within a distributed model']),\n",
       " (719, ['Experiments', ' showing', ' significant gains']),\n",
       " (720, ['An area', ' falls', ' a bit short']),\n",
       " (721,\n",
       "  ['two loss terms', ' were employed', ' to balance the use of experts,']),\n",
       " (722,\n",
       "  ['It',\n",
       "   ' to see',\n",
       "   ' the effects of these more, along with the effects of increasing effective batch sizes,']),\n",
       " (723, ['a well-described system', ' achieves', ' good results,']),\n",
       " (724, ['I', ' like', ' Figure 3,']),\n",
       " (725, ['The H-H line', ' has', ' 3 points on left but 5 on the right?']),\n",
       " (726, ['the colors', ' matched', ' between corresponding lines.']),\n",
       " (727,\n",
       "  ['This paper',\n",
       "   ' proposes learning',\n",
       "   ' on the fly to represent a dialog as a graph']),\n",
       " (728,\n",
       "  ['long term representation',\n",
       "   ' learning',\n",
       "   ' to learn graph transformation parameters and the encoding of sentences as input to the graph.']),\n",
       " (729,\n",
       "  ['it',\n",
       "   ' is',\n",
       "   ' much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks,']),\n",
       " (730, ['an initial version', ' better read', ' by a computer than a human,']),\n",
       " (731, ['I', ' understood)', ' ']),\n",
       " (732, ['The preliminary results', ' do not tell', ' us yet']),\n",
       " (733,\n",
       "  ['The performance on the bAbI task',\n",
       "   ' is',\n",
       "   ' comparable to the best memory networks,']),\n",
       " (734, ['This', ' is', ' still clearly promising.']),\n",
       " (735, ['the authors', ' do not discuss', ' any other operation ordering.']),\n",
       " (736, ['you', ' need', ' the node state update step T_h']),\n",
       " (737, ['the only trick', ' is', ' essential for proper performance']),\n",
       " (738, [\"the question ''how useful\", ' are', \" all these graph operations''\"]),\n",
       " (740,\n",
       "  ['A description of the actual implementation',\n",
       "   ' would help',\n",
       "   ' pointer to open source code is provide).']),\n",
       " (741, ['the transformations', ' compiled', ' in advance as units?']),\n",
       " (742, ['this one', ' is only described', ' at runtime?']),\n",
       " (743, ['the equation', ' applies', ' the update gate']),\n",
       " (744,\n",
       "  ['the author',\n",
       "   ' could mention',\n",
       "   ' the pioneering work of Lee Giles on representing graphs with RNNs. In the references,']),\n",
       " (745, ['I', ' have improved', ' my rating for the following reasons:']),\n",
       " (746, ['I', ' am', ' confident this will be an impactful paper.']),\n",
       " (747,\n",
       "  ['Much simpler alternatives approaches such as Memory Networks',\n",
       "   ' seem',\n",
       "   ' ']),\n",
       " (748,\n",
       "  ['we',\n",
       "   ' find',\n",
       "   ' simplifications that actually improve performance with DNNs, and']),\n",
       " (749,\n",
       "  ['The main contribution of this paper',\n",
       "   ' to be',\n",
       "   ' an introduction of a set of differential graph transformations']),\n",
       " (750,\n",
       "  ['This', ' maps naturally', ' to a task of learning a cellular automaton']),\n",
       " (751, ['nodes', ' pointing', ' to neighbors and special nodes']),\n",
       " (752,\n",
       "  ['Proposed architecture',\n",
       "   ' allows',\n",
       "   ' one to learn this sequence of graphs,']),\n",
       " (753, ['This idea', ' is combined', ' with ideas from previous papers']),\n",
       " (754,\n",
       "  ['The paper',\n",
       "   ' proposes',\n",
       "   ' an extension of the Gated Graph Sequence Neural Network']),\n",
       " (755, ['The underlying idea', ' is', ' to propose a method']),\n",
       " (756,\n",
       "  ['The author',\n",
       "   ' proposes',\n",
       "   ' 5 different possible differentiable transformations that will be learned on a training set, typically in a supervised fashion']),\n",
       " (757, ['A particular occurence of the model', ' is presented', ' ']),\n",
       " (758,\n",
       "  ['the proposed model',\n",
       "   ' to maintain',\n",
       "   ' a representation of its current state as a complex graph,']),\n",
       " (760,\n",
       "  ['the recent attempts',\n",
       "   ' made to add',\n",
       "   \" some 'symbolic' stuffs in differentiable models\"]),\n",
       " (761, ['My main concerns', ' is', ' about the way']),\n",
       " (762, ['My other concern', ' is', ' about the whole content of the paper']),\n",
       " (763,\n",
       "  ['The paper',\n",
       "   ' shows',\n",
       "   ' a different approach to a ternary quantization of weights.']),\n",
       " (764,\n",
       "  ['paper', ' shows', ' performance improvements over existing solutions']),\n",
       " (765, ['paper', ' is', ' very incremental.']),\n",
       " (766, ['paper', ' is addressed', ' to a very narrow audience.']),\n",
       " (767,\n",
       "  ['The paper',\n",
       "   ' very clearly assumes',\n",
       "   ' that the reader is familiar with the previous work on the ternary quantization.']),\n",
       " (768,\n",
       "  ['It',\n",
       "   ' is',\n",
       "   ' \"what is new in the topic\" update, not really a standalone paper.']),\n",
       " (769,\n",
       "  ['The description of the main algorithm',\n",
       "   ' is',\n",
       "   ' very concise, to say the least,']),\n",
       " (771, ['it', ' really is used', ' in production,']),\n",
       " (772, ['any practical applications', ' require', ' this refinement?']),\n",
       " (773,\n",
       "  ['I',\n",
       "   ' do not find',\n",
       "   ' the motivation \"it is related to mobile, therefore it is cool\" sufficient.']),\n",
       " (774,\n",
       "  ['the authors',\n",
       "   ' do not provide',\n",
       "   ' a sufficient practical motivation for pursuing this particular topic with the next step on a long list of small refinements,']),\n",
       " (775, ['the code', ' was not released', ' ']),\n",
       " (776, ['This paper', ' presents', ' new way for compressing CNN weights.']),\n",
       " (777,\n",
       "  ['this paper',\n",
       "   ' uses',\n",
       "   ' a new neural network quantization method that compresses network weights to ternary values. particular']),\n",
       " (778, ['this one', ' offers', ' possibly the lowest returns I have seen.']),\n",
       " (780,\n",
       "  ['the group',\n",
       "   ' showed',\n",
       "   ' this kind of older style-network can be compressed by large amounts. already']),\n",
       " (781,\n",
       "  ['I',\n",
       "   ' would have liked',\n",
       "   ' to see this group release code for the compression, and also report data on the amount of effort']),\n",
       " (782,\n",
       "  ['This data',\n",
       "   ' is',\n",
       "   ' important to decide if a compression is worth the effort.']),\n",
       " (783,\n",
       "  ['This work',\n",
       "   ' presents',\n",
       "   ' a novel ternary weight quantization approach which quantizes weights to either 0 or one of two layer specific learned values.']),\n",
       " (784,\n",
       "  ['these quantized values',\n",
       "   ' are learned stochastically',\n",
       "   ' alongside all other network parameters.']),\n",
       " (785,\n",
       "  ['This approach',\n",
       "   ' achieves',\n",
       "   ' impressive quantization results while surpassing corresponding full-precision networks on CIFAR10 and ImageNet.']),\n",
       " (786, ['Strengths: - Overall well written', ' is presented clearly.', ' ']),\n",
       " (787, ['Approach', ' appears', ' ']),\n",
       " (789,\n",
       "  ['I',\n",
       "   ' enjoyed',\n",
       "   ' the analysis of sparsity (and how it changes) over the course of training,']),\n",
       " (790,\n",
       "  ['The energy analysis in Table 3',\n",
       "   ' assumes',\n",
       "   ' dense activations due to the unpredictability of sparse activations.']),\n",
       " (791, ['each network', ' to help', ' verify this assumption.']),\n",
       " (793,\n",
       "  ['the authors',\n",
       "   ' suggest',\n",
       "   ' having a fixed t (threshold parameter set at 0.05) for all layers allows for varying sparsity In section 5.1.1,']),\n",
       " (794, ['additional sparsity', ' can be achieved', ' ']),\n",
       " (795,\n",
       "  ['this multiple threshold style network',\n",
       "   ' appear',\n",
       "   ' in any of the tables or figures?']),\n",
       " (796, ['it', ' be', ' Can added?']),\n",
       " (797,\n",
       "  ['The authors',\n",
       "   ' claim',\n",
       "   ' Quantized weights play the role of \"learning rate multipliers\" during back propagation.\"']),\n",
       " (798, ['using', ' of', ' trained quantization factors.']),\n",
       " (800, ['table captions', ' are not', ' very descriptive.']),\n",
       " (801,\n",
       "  ['this',\n",
       "   ' is',\n",
       "   ' an interesting paper with convincing results but is somewhat lacking in novelty.']),\n",
       " (802,\n",
       "  ['Table 3 lists',\n",
       "   ' FLOPS',\n",
       "   ' rather than Energy for the full precision model.']),\n",
       " (804, ['Section 5', \" up'\", ' - 5.1.1 figure reference error last line']),\n",
       " (805,\n",
       "  ['This paper',\n",
       "   ' studies',\n",
       "   ' in depth the idea of quantizing down convolutional layers to 3 bits, with a different positive per-layer scale.']),\n",
       " (807,\n",
       "  ['The relevance of this paper',\n",
       "   ' is',\n",
       "   \" that it likely provides a lower bound on quantization approaches that don't sacrifice any performance, and hence can plausibly become the approach of choice for resource-constrained inference, and might suggest new hardware designs to take\"]),\n",
       " (811,\n",
       "  ['I',\n",
       "   ' would have also liked',\n",
       "   ' to see discussion of the wall time to result using this training procedure.']),\n",
       " (812, ['This paper', ' creates', ' a layered representation in order']),\n",
       " (813,\n",
       "  ['Figure 1',\n",
       "   ' clearly shows',\n",
       "   ' the idea that if the segmentation was removed properly, the result would still be a natural image.']),\n",
       " (814,\n",
       "  ['the method itself as described in the paper',\n",
       "   ' leaves',\n",
       "   ' many questions about whether they can achieve the proposed goal.']),\n",
       " (815,\n",
       "  ['I it',\n",
       "   ' cannot see is advertised.',\n",
       "   ' from the formulation why would this model work']),\n",
       " (816, ['The formulation', ' looks', ' like a standard GAN,']),\n",
       " (817, ['1) Each layer', ' is', ' a natural image.']),\n",
       " (818, ['the loss function', ' is', ' only on the final product G_K.']),\n",
       " (819, ['it', ' is written', ' in the paper,']),\n",
       " (820, ['Nothing', ' seems', ' ']),\n",
       " (821, ['2) None of the layers', ' is', ' degenerate.']),\n",
       " (822, ['or preventing any layer', ' to be', ' non-degenerate.']),\n",
       " (824, ['I', \" don't see\", ' any term ensuring the mask being contiguous,']),\n",
       " (825, ['this paper', ' is', ' for unsupervised semantic segmentation']),\n",
       " (826,\n",
       "  ['A major problem',\n",
       "   ' is',\n",
       "   ' that when conducting experiments, all the images seem to be taken from a single category, this implicitly uses the label information of the category.']),\n",
       " (827,\n",
       "  ['this',\n",
       "   ' cannot be viewed',\n",
       "   ' as an unsupervised algorithm. In that regard,']),\n",
       " (828,\n",
       "  ['the results',\n",
       "   ' definitely looked',\n",
       "   ' too good to be true. Even with that,']),\n",
       " (829,\n",
       "  ['such a standard GAN optimization',\n",
       "   ' would not generate',\n",
       "   ' any of the aforementioned artifacts']),\n",
       " (831,\n",
       "  ['I', ' am', ' pretty sure that this is not up to the standards of ICLR.']),\n",
       " (832, ['I', ' have read', ' the rebuttal']),\n",
       " (833,\n",
       "  ['I',\n",
       "   \" don't think\",\n",
       "   ' the authors managed to convince me that this method would work the']),\n",
       " (835,\n",
       "  ['The paper',\n",
       "   ' proposes',\n",
       "   ' a generative model that decomposes images into multiple layers.']),\n",
       " (836,\n",
       "  ['the objective of the GAN',\n",
       "   ' is',\n",
       "   ' to distinguish real images from images GAN-based,']),\n",
       " (837, ['Some of the layers', ' correspond', ' to objects']),\n",
       " (838, ['The method', ' has been tested', ' on bedroom scenes.']),\n",
       " (839, ['The idea of the paper', ' is', ' interesting.']),\n",
       " (840, ['the learned masks for objects', ' are', ' neat.']),\n",
       " (841,\n",
       "  ['the proposed method', ' outperforms', ' a number of simple baselines.']),\n",
       " (842, ['it', ' learns.', ' what']),\n",
       " (843,\n",
       "  ['It',\n",
       "   ' would be',\n",
       "   ' good to see how the performance changes for different number of layers.']),\n",
       " (845, ['It', ' is', ' unclear why \"contiguous\"']),\n",
       " (847, ['This', ' should be explained', ' in the rebuttal.']),\n",
       " (848, ['it', ' knows', ' the label for the scene category.']),\n",
       " (849, ['no semantics', ' associated', ' to the object.']),\n",
       " (850, ['It', ' is', ' just a binary foreground/background mask.']),\n",
       " (851, ['The plots in Figure 5', ' are', ' a bit strange.']),\n",
       " (852,\n",
       "  ['The precision', ' increases uniformly', ' as the recall goes up, which']),\n",
       " (853, ['It', ' should be explained', ' in the rebuttal']),\n",
       " (854,\n",
       "  ['the generated images',\n",
       "   ' are not',\n",
       "   ' that appealing. Similar to most GAN-based models,']),\n",
       " (855, ['The claim about object removal', ' should be toned down.', ' ']),\n",
       " (856, ['The method', ' is not', ' able to remove any object from a scene.']),\n",
       " (857, ['the learned layers', ' can be removed.', ' Only,']),\n",
       " (858,\n",
       "  ['This paper',\n",
       "   ' proposes',\n",
       "   ' a neural network architecture around the idea of layered scene composition.']),\n",
       " (859, ['a subnetwork', ' to compose', ' ']),\n",
       " (860,\n",
       "  ['An encoder',\n",
       "   ' is trained',\n",
       "   ' to map real images into the space of latent codes for the generator, later']),\n",
       " (861,\n",
       "  ['The idea',\n",
       "   ' is',\n",
       "   ' interesting from established approaches to segmentation.']),\n",
       " (862,\n",
       "  ['Visualization of learned layers for several scene types',\n",
       "   ' shows',\n",
       "   ' that the network does learn a reasonable compositional scene model.']),\n",
       " (863,\n",
       "  ['Experiments',\n",
       "   ' evaluate',\n",
       "   ' the ability to port the model learned in an unsupervised manner to semantic segmentation tasks,']),\n",
       " (864,\n",
       "  ['the included experiments',\n",
       "   ' are not',\n",
       "   ' nearly sufficient to establish the effectiveness of the proposed method.']),\n",
       " (865, ['Only two scene types', ' are used', ' for evaluation.']),\n",
       " (866,\n",
       "  ['This',\n",
       "   ' is',\n",
       "   ' far below the norm for semantic segmentation work in computer vision.']),\n",
       " (871,\n",
       "  ['The range of prior work on semantic segmentation', ' is', ' extensive.']),\n",
       " (874, ['far more experiments', ' are needed.', ' ']),\n",
       " (875,\n",
       "  ['The system',\n",
       "   ' described works comparably',\n",
       "   ' to bi-directional LSTM baseline for']),\n",
       " (876,\n",
       "  ['Key ideas',\n",
       "   ' include',\n",
       "   \" the use of two stacked CNN's (one for each of encoding for translation, with res connections and position embeddings.\"]),\n",
       " (877,\n",
       "  [\"The use of CNN's for translation\", ' has been attempted', ' previously']),\n",
       " (879, ['The experimental results', ' are well reported', ' in detail.']),\n",
       " (880,\n",
       "  ['two figures',\n",
       "   ' would definitely be required',\n",
       "   ' to help clarify the architecture.']),\n",
       " (881,\n",
       "  ['This paper',\n",
       "   ' is',\n",
       "   ' less about new ways of learning representations than about the combination of choices']),\n",
       " (882,\n",
       "  ['I',\n",
       "   ' am',\n",
       "   ' fairly confident that the paper represents good work in machine learning,']),\n",
       " (883,\n",
       "  ['the first (I',\n",
       "   ' to establish',\n",
       "   ' a simple yet important result that Convnets for NMT encoders can be competitive to RNNs.']),\n",
       " (885,\n",
       "  ['I',\n",
       "   ' appreciate',\n",
       "   ' the detailed report on training and generation speed.']),\n",
       " (886,\n",
       "  ['I',\n",
       "   ' find',\n",
       "   \" it's very interesting position embeddings turn out to be hugely important (beside residual connections)\",\n",
       "   '']),\n",
       " (887,\n",
       "  ['The only concern I have (similar to the other reviewer)',\n",
       "   ' is',\n",
       "   ' that this paper perhaps fits better in an NLP conference.']),\n",
       " (888,\n",
       "  ['this well-executed paper',\n",
       "   \" doesn't have\",\n",
       "   ' a single figure on the proposed architecture']),\n",
       " (889,\n",
       "  ['a convolutional network',\n",
       "   ' can be used instead',\n",
       "   ' of the recurrent encoder for neural machine translation.']),\n",
       " (890,\n",
       "  ['the paper',\n",
       "   ' features',\n",
       "   ' one unexpected architectural twist: two stacks of convolutions, one for computing alignment and another for computing the representations.']),\n",
       " (891, ['this', ' was', ' necessary']),\n",
       " (892, ['The experimental evaluation', ' is', ' very extensive']),\n",
       " (893, ['The convnet-based model', ' was', ' faster at evaluation,']),\n",
       " (894, ['the speed advantage of convnets', ' is', ' likely to increase']),\n",
       " (895,\n",
       "  ['the contribution',\n",
       "   ' is',\n",
       "   ' quite incremental and rather application-specific.']),\n",
       " (896, ['ACL, EMNLP conferences', ' would be', ' a better venue,']),\n",
       " (897,\n",
       "  ['the authors',\n",
       "   ' propose a dynamic convolution model by exploiting',\n",
       "   ' the inter-scene similarity.']),\n",
       " (898, ['The computation cost', ' is reduced', ' significantly']),\n",
       " (899, ['the technical contribution', ' is', ' rather incremental.']),\n",
       " (900,\n",
       "  ['1)The authors',\n",
       "   ' should clarify',\n",
       "   ' their advantages over the popular framework of CNN+LSTM.']),\n",
       " (901, ['I', ' did not see', ' it.']),\n",
       " (902,\n",
       "  ['2) What',\n",
       "   ' is',\n",
       "   ' the difference between the proposed method and applying incremental learning on CNN?']),\n",
       " (903,\n",
       "  ['proposed method',\n",
       "   ' reduced',\n",
       "   ' the computation in which phase, training or tesing?']),\n",
       " (904, ['experimental section', ' is', ' rather weak.']),\n",
       " (906, ['short videos,', ' makes', ' the acceleration unnecessary.']),\n",
       " (907, ['This paper', ' proposes', ' a technique to reduce the compute cost']),\n",
       " (908, ['the pixels', ' changed', ' across frames']),\n",
       " (910,\n",
       "  ['Paper strengths of reducing computational requirements when using CNNs for video analysis',\n",
       "   ' is well motivated.',\n",
       "   ' ']),\n",
       " (911, ['The authors', ' analyze', ' a standard model on benchmark datasets']),\n",
       " (914,\n",
       "  ['This',\n",
       "   ' makes',\n",
       "   ' it harder to appreciate the contribution of using this method.']),\n",
       " (915,\n",
       "  ['The paper',\n",
       "   ' has',\n",
       "   ' many spelling mistakes - \"siliarlity\", \"critiria\" etc.']),\n",
       " (916,\n",
       "  ['Continuous convolutions It',\n",
       "   ' is not',\n",
       "   ' clear to me what is meant by this term.']),\n",
       " (917, ['It', ' is used', ' many times']),\n",
       " (918,\n",
       "  ['Section 5.2 - what criteria',\n",
       "   ' is used',\n",
       "   ' to compute scene similarity?']),\n",
       " (920,\n",
       "  ['Design decisions such as cell-based convolution',\n",
       "   ' are never evaluated empirically.',\n",
       "   ' ']),\n",
       " (921,\n",
       "  ['The paper',\n",
       "   ' addresses',\n",
       "   ' the problem of computational inefficiency in video surveillance understanding approaches.']),\n",
       " (922, ['an approach', ' called', ' Dynamic Convolution']),\n",
       " (923,\n",
       "  ['The idea',\n",
       "   ' is',\n",
       "   ' to reuse some of the convolutional feature maps, particularly when there is a significant similarity among the frames.']),\n",
       " (924, ['The paper', ' evaluates', ' the results on 4 public datasets.']),\n",
       " (925, ['it', ' just compares', ' the approach to a baseline,']),\n",
       " (927,\n",
       "  ['Video understanding approaches',\n",
       "   ' are not just applying',\n",
       "   ' convnet on all frames. usually']),\n",
       " (928,\n",
       "  ['Many of the approaches on video analysis,',\n",
       "   ' select',\n",
       "   ' a random set of frames (or just a single frame)']),\n",
       " (929,\n",
       "  ['another set of work on attention,',\n",
       "   ' try',\n",
       "   ' to extracts the most important spatio-temporal [1-4] information']),\n",
       " (930,\n",
       "  ['These approaches',\n",
       "   ' are',\n",
       "   ' usually computationally less expensive than applying convnet on all video frames.']),\n",
       " (932,\n",
       "  ['pedestrian detection performance',\n",
       "   ' is',\n",
       "   ' part of the evaluation process.']),\n",
       " (934,\n",
       "  ['The writing of the paper',\n",
       "   ' should also improve',\n",
       "   ' to make the paper more understandable and easier to follow.']),\n",
       " (936, ['Unnecessary information', ' can be summarized.', ' ']),\n",
       " (937,\n",
       "  ['many details on the computational costs in the introduction',\n",
       "   ' can just simply be replaced',\n",
       "   \" by stating that ''these approaches are computationally costly''.\"]),\n",
       " (939,\n",
       "  ['Using present tense for the SoTA approaches',\n",
       "   ' is',\n",
       "   \" more common.''ShuffleNet\"]),\n",
       " (941, ['Long sentences', ' are', ' difficult to follow:']),\n",
       " (942,\n",
       "  ['the authors',\n",
       "   ' associated',\n",
       "   ' with the generalization gap of robust adversarial training with the distance between the test point and the manifold of training data. In this paper,']),\n",
       " (943,\n",
       "  [\"A so-called 'blind-spot attack'\",\n",
       "   ' is proposed',\n",
       "   ' to show the weakness of robust adversarial training.']),\n",
       " (944, ['I', ' have', ' several concerns about the current version.']),\n",
       " (945,\n",
       "  ['we',\n",
       "   ' found',\n",
       "   ' that the results are not sensitive to the selection of k\".']),\n",
       " (946,\n",
       "  ['authors',\n",
       "   ' provide',\n",
       "   ' more details, e.g., empirical results, about it?']),\n",
       " (948, ['these blind-spots', ' can be easily found', ' ']),\n",
       " (949,\n",
       "  ['we',\n",
       "   ' propose',\n",
       "   ' a simple transformation to find the blind-spots in this model.\" For the MNIST dataset']),\n",
       " (951,\n",
       "  ['The linear transformation',\n",
       "   ' yields',\n",
       "   ' a blind-spot attack which can defeat robust adversarial training.']),\n",
       " (952, ['one', ' can further modify', ' the inner maximization']),\n",
       " (953,\n",
       "  ['the $\\\\ell_infty$ attack',\n",
       "   ' satisfies',\n",
       "   ' max_{\\\\alpha, \\\\beta} f(\\\\alpha + \\\\beta) subject to \\\\|']),\n",
       " (954,\n",
       "  ['robust training framework',\n",
       "   ' can defend',\n",
       "   ' blind-spot attacks, In this case,']),\n",
       " (955,\n",
       "  ['I',\n",
       "   ' agree',\n",
       "   ' with the authors that the generalization error is due to the mismatch between training data and test data distribution,']),\n",
       " (956, ['we', ' scale', ' the image by a factor of \\\\alpha,']),\n",
       " (957, ['I', ' did not get', ' the point.']),\n",
       " (958,\n",
       "  ['the universal perturbation rule', ' should be', ' x - x^\\\\prime still']),\n",
       " (959, ['The metric', ' used', ' the authors']),\n",
       " (960,\n",
       "  ['This paper',\n",
       "   ' provides',\n",
       "   ' some insights on influence of data distribution on robustness of adversarial training.']),\n",
       " (961, ['the training', ' sets', ' an test data']),\n",
       " (962,\n",
       "  ['the paper',\n",
       "   ' proposes',\n",
       "   ' an approach to measure the distance between the two data sets']),\n",
       " (963,\n",
       "  ['The paper',\n",
       "   ' shows',\n",
       "   ' that under simple transformation to the test dataset (eg: scaling), performance of adversarial training reduces significantly due to the large gap between training and test data set.']),\n",
       " (964,\n",
       "  ['This',\n",
       "   ' to impact',\n",
       "   ' high dimensional data sets more than low dimensional data sets']),\n",
       " (965, ['adversarial training', ' is', ' less effective on some datasets.']),\n",
       " (966,\n",
       "  ['a metric',\n",
       "   ' seems',\n",
       "   ' to strongly correlate with the effectiveness of adversarial training.']),\n",
       " (968, ['It', ' could have been', ' nice']),\n",
       " (969,\n",
       "  ['The marketing phrase',\n",
       "   ' falls',\n",
       "   ' short in delivering what one may expect from the paper after reading it.']),\n",
       " (970, ['The paper', ' would read much better', ' ']),\n",
       " (971, ['it', ' could actually be', ' huge portion of the input space!']),\n",
       " (973, ['it', ' would help', ' to clarify it in the paper.']),\n",
       " (974, ['it', ' would help', ' ']),\n",
       " (976,\n",
       "  ['it',\n",
       "   ' could be also',\n",
       "   ' interesting to see the average K-L divergence between an adversarially and a naturally trained network on the same dataset.']),\n",
       " (977, ['those', ' shown', ' in Figure 4.']),\n",
       " (978, ['The paper', ' is well written', ' ']),\n",
       " (979, ['The empirical results', ' presented', ' in Figure 1']),\n",
       " (980,\n",
       "  ['The gain of using a sufficiently more complicated approach',\n",
       "   ' is not',\n",
       "   ' clear,']),\n",
       " (981,\n",
       "  ['a simple score',\n",
       "   ' based',\n",
       "   ' on the histogram, or even the mean distance?']),\n",
       " (982,\n",
       "  ['Of course providing a single measure',\n",
       "   ' would allow',\n",
       "   ' to leverage that information during training.']),\n",
       " (983,\n",
       "  ['this',\n",
       "   ' seems',\n",
       "   ' rather complicated and computationally expensive (KL-based). in its current form']),\n",
       " (984,\n",
       "  ['the histograms',\n",
       "   ' are not',\n",
       "   ' informative enough to detect such blind-spot transformation.']),\n",
       " (985, ['the distance', ' is based', ' on the network embedding']),\n",
       " (986,\n",
       "  ['the overall KL-based data similarity measure',\n",
       "   ' would help',\n",
       "   ' in this case since it seems likely that it would also exhibit the same issue.']),\n",
       " (987,\n",
       "  ['The authors',\n",
       "   ' propose',\n",
       "   ' a new approach to perform deterministic variational inference for feed-forward BNN with specific nonlinear activation functions by approximating layerwise moments.']),\n",
       " (988,\n",
       "  ['the proposed method',\n",
       "   ' achieves',\n",
       "   ' better performance than existing Monte Carlo variational inference.']),\n",
       " (989,\n",
       "  ['most of the existing works',\n",
       "   ' focus',\n",
       "   ' on Monte Carlo variational inference.']),\n",
       " (990,\n",
       "  ['The main contribution of this paper',\n",
       "   ' is',\n",
       "   ' to perform Gaussian approximation.']),\n",
       " (991,\n",
       "  ['The authors',\n",
       "   ' show',\n",
       "   ' that for specific activation functions, the Gaussian approximation is reasonable.']),\n",
       " (992,\n",
       "  ['The main concern',\n",
       "   ' is',\n",
       "   ' the cumulative error due to the Gaussian approximation.']),\n",
       " (993,\n",
       "  ['the authors',\n",
       "   ' argue',\n",
       "   ' that the proposed method fixes the issues of stochastic VI for BNN,']),\n",
       " (995,\n",
       "  ['the latent dimension at each layer',\n",
       "   ' is',\n",
       "   ' less than 32. a BNN with 5 hidden layers,']),\n",
       " (997,\n",
       "  ['This paper',\n",
       "   ' considers',\n",
       "   ' a purely deterministic approach to learning variational posterior approximations for Bayesian neural networks.']),\n",
       " (998, ['Variational lower bound gradients', ' are obtained', ' ']),\n",
       " (999, ['This', ' is', ' an interesting paper.']),\n",
       " (1000, ['the derivation', ' is', ' rather heuristic.']),\n",
       " (1001, ['the approximations', ' work well.', ' ']),\n",
       " (1002,\n",
       "  ['The paper',\n",
       "   ' is generally well written',\n",
       "   ' in the context of the existing literature.']),\n",
       " (1003, ['The approximations', ' work well', ' for the examples']),\n",
       " (1005,\n",
       "  ['the moment propagation approximations',\n",
       "   ' cause',\n",
       "   ' difficulty when applied repeatedly in deeper networks.']),\n",
       " (1008,\n",
       "  ['similar approximations',\n",
       "   ' been used',\n",
       "   ' in the literature before, in addition to the work']),\n",
       " (1009,\n",
       "  ['I',\n",
       "   \" don't feel\",\n",
       "   ' there is much to compare the proposed EB approximations to, although a comparison with manual tuning is given in Section 6.']),\n",
       " (1010, ['This work', ' is tackling', ' two difficulties in current VB']),\n",
       " (1011,\n",
       "  ['MC approximations of intractable expectations',\n",
       "   ' are replaced',\n",
       "   ' by deterministic approximations. First,']),\n",
       " (1012, ['this', ' has been done', ' before,']),\n",
       " (1013,\n",
       "  ['a Gaussian prior with length scales',\n",
       "   ' is learned',\n",
       "   ' by VB empirical Bayes alongside the normal training,']),\n",
       " (1014, ['much older work like Barber&Bishop', ' would apply', ' ']),\n",
       " (1015, ['the writing', ' is', ' excellent,']),\n",
       " (1016, ['I', ' learned', ' a lot from it.']),\n",
       " (1017, ['Approximations', ' are tested,', ' ']),\n",
       " (1018,\n",
       "  ['the major technical novelty, the expression for <h_j h_l>,',\n",
       "   ' is',\n",
       "   ' really interesting and useful.']),\n",
       " (1019, ['Clarity: it', ' comes', ' writing to the experiments.']),\n",
       " (1020, ['example what q(w)', ' is', ' (fully factorized Gaussian?).']),\n",
       " (1021, ['Very nice literature review,', ' also', ' historical.']),\n",
       " (1023, ['Porting this from ADF to VB', ' gives', ' dDVI.']),\n",
       " (1024,\n",
       "  ['PBP', ' has', ' the property that a DL system gives you the gradients.']),\n",
       " (1025, ['I', ' think', ' dDVI may be more useful than PBP.']),\n",
       " (1026, ['they', ' miss', ' the expression for <h_j h_l> in there.']),\n",
       " (1027, ['what is done here,', ' does not need', ' 1D quadrature. Now,']),\n",
       " (1028, ['one', ' only looks', ' at test log likelihood*.']),\n",
       " (1029, [\"I'd\", ' give', ' this the benefit of the doubt, still']),\n",
       " (1030, ['the authors', ' may tone down', ' their language a bit.']),\n",
       " (1031,\n",
       "  ['I', ' recommend', ' to comment beyond just test log likelihood scores.']),\n",
       " (1032, ['less tuning', ' required,', ' more automatic?']),\n",
       " (1033, ['you', ' make', ' a big point out of reducing variance?']),\n",
       " (1034, ['it', ' Does converge', ' faster?']),\n",
       " (1035, ['your posterior', ' cannot', ' that normal DNN methods do?']),\n",
       " (1038, ['who', ' really cares', ' about test log likelihood? In the end,']),\n",
       " (1039, ['the q(w) family', ' being used', ' here?']),\n",
       " (1040, ['Fully', ' factorized', ' Gaussian?']),\n",
       " (1041, ['I', ' suppose', ' so for dDVI.']),\n",
       " (1046, ['Show you', ' really gain', ' what by reducing the variance.']),\n",
       " (1049, ['This', ' is', ' really important.']),\n",
       " (1050, ['this', ' would be', ' an important missing comparison.']),\n",
       " (1051,\n",
       "  ['Please',\n",
       "   ' be not',\n",
       "   ' clear very large. in the main text - Advantages over MCVI']),\n",
       " (1052, ['dDVI', ' should be', ' faster to converge than MCVI.']),\n",
       " (1053, ['Can you', ' say', ' something about robustness of training?']),\n",
       " (1056, ['they', ' obtained', ' the same model?']),\n",
       " (1057, ['dDVI', ' is doing', ' better.']),\n",
       " (1058,\n",
       "  ['Other points:',\n",
       "   ' Please acknowledge',\n",
       "   ' the <h_j h_l> expression in Barber&Bishop 98.']),\n",
       " (1059, ['Yours', ' is', ' more elegant (does not need 1D quadrature)']),\n",
       " (1060, ['I', ' need', ' to compute gradients for every datapoint.']),\n",
       " (1061, ['I', ' can do', ' mini-batch updates. In dDVI,']),\n",
       " (1062, ['I', ' just', ' the header \"Wild approximations\".']),\n",
       " (1063, ['I', ' tend', ' to refer to this kind of work as \"weak analogies\".']),\n",
       " (1065,\n",
       "  ['This paper',\n",
       "   ' describes',\n",
       "   ' an implementation of reduced precision deep learning']),\n",
       " (1066, ['This field', ' has seen', ' a lot of publications recently']),\n",
       " (1067,\n",
       "  ['These schemes',\n",
       "   ' have generally achieved',\n",
       "   ' close-to-SOTA accuracy for small networks on datasets such as MNIST and CIFAR-10.']),\n",
       " (1068,\n",
       "  ['a significant accuracy drop',\n",
       "   ' are reported.',\n",
       "   ' for larger networks (ResNET, Vgg, etc) on large dataset such as ImageNET,']),\n",
       " (1069,\n",
       "  ['a careful implementation of mixed-precision dynamic fixed point computation',\n",
       "   ' can achieve',\n",
       "   ' SOTA on 4 large networks on the ImageNET-1K datasets.']),\n",
       " (1070,\n",
       "  ['Using a INT16 (as opposed to FP16)',\n",
       "   ' has',\n",
       "   ' the advantage of enabling the use of new SIMD mul-acc instructions such as QVNNI16.']),\n",
       " (1071,\n",
       "  ['The reported accuracy numbers',\n",
       "   ' show convincingly',\n",
       "   ' that INT16 weights can be used without loss of accuracy in large CNNs.']),\n",
       " (1073, ['The paper', ' is written clearly', ' ']),\n",
       " (1074, ['This paper', ' is', ' about low-precision training for ConvNets.']),\n",
       " (1075,\n",
       "  ['It',\n",
       "   ' proposed',\n",
       "   ' a \"dynamic fixed point\" scheme that shares the exponent part for a tensor, and developed procedures']),\n",
       " (1076, ['The proposed method', ' is shown', ' ']),\n",
       " (1077,\n",
       "  ['this',\n",
       "   ' is',\n",
       "   ' the first time such kind of performance are demonstrated for limited precision training.']),\n",
       " (1078,\n",
       "  ['Potential improvements:',\n",
       "   ' Please define',\n",
       "   ' the terms like WTGRAD at the first occurance.']),\n",
       " (1080, ['This work', ' presents', ' a CNN training setup']),\n",
       " (1081, ['The work', ' is clearly presented', ' ']),\n",
       " (1082,\n",
       "  ['The presented implementations',\n",
       "   ' are',\n",
       "   ' competitive in terms of accuracy, when compared to the FP32 representation.']),\n",
       " (1083, ['the contribution', ' seems', ' relevant to me,']),\n",
       " (1084, ['large batch gradient descent', ' does not work too well', ' ']),\n",
       " (1085, ['2. flatter minima', ' have', ' better generalization ability.']),\n",
       " (1086,\n",
       "  ['I',\n",
       "   ' think',\n",
       "   ' the work is of great value in shedding light into some interesting questions around generalization of deep networks.']),\n",
       " (1087,\n",
       "  ['I',\n",
       "   ' think',\n",
       "   ' such results may have impact on both practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes.']),\n",
       " (1088,\n",
       "  ['I', ' had', ' some concern about the correctness of a claim Earlier']),\n",
       " (1089,\n",
       "  ['They',\n",
       "   ' had claimed',\n",
       "   ' their proposed sharpness criterion is scale invariance.']),\n",
       " (1090, ['They', ' took', ' care of it']),\n",
       " (1091, ['I', ' think', ' that the paper is quite interesting and useful.']),\n",
       " (1092, ['It', ' might benefit e.g.,', ' from additional investigations,']),\n",
       " (1093,\n",
       "  ['This paper',\n",
       "   ' proposes',\n",
       "   ' a method for iteratively improving the output of an existing machine translation by proposing a substitution, in this case']),\n",
       " (1094, ['It', ' is motivated', ' by the method']),\n",
       " (1095, ['The paper', ' is', ' interesting and imaginative.']),\n",
       " (1096, ['I', ' am', ' somewhat sceptical of this kind of approach']),\n",
       " (1097, ['the method', ' has', ' no new information compared to previously,']),\n",
       " (1098, ['an iterative approach', ' can be shown', ' ']),\n",
       " (1099, ['This paper', ' does not convince', ' me on these points.']),\n",
       " (1101,\n",
       "  ['they',\n",
       "   ' just serve',\n",
       "   ' to confirm that improving a translation is very easy']),\n",
       " (1103, ['I', ' find', ' the notation excessively fiddly at times - eg:']),\n",
       " (1104, ['anything', ' could be done', ' about this?']),\n",
       " (1105,\n",
       "  ['This paper',\n",
       "   ' proposes',\n",
       "   ' a model for iteratively refining translation hypotheses.']),\n",
       " (1106,\n",
       "  ['This',\n",
       "   ' has',\n",
       "   \" several benefits, including enabling the translation model to condition not only on ''left context'', but also on ''right context'', and potentially enabling accurate decoding.\"]),\n",
       " (1107,\n",
       "  ['The motivation given',\n",
       "   ' is',\n",
       "   ' that often translators (and text generators generally) use a process of refinement in generating outputs.']),\n",
       " (1108,\n",
       "  ['This',\n",
       "   ' is',\n",
       "   ' an important idea that is not currently playing much of a role in neural net models,']),\n",
       " (1109,\n",
       "  ['I',\n",
       "   ' do feel',\n",
       "   ' that the lack of in depth analysis suggests this paper is not quite ready for a final publication version.']),\n",
       " (1110,\n",
       "  ['many possible connections to prior work in NLP,',\n",
       "   ' could better',\n",
       "   ' contextualize this work (see specifics below).']),\n",
       " (1111,\n",
       "  ['undirected (~CRF) translation model',\n",
       "   ' trained',\n",
       "   ' using a pseudo-likelihood objective.']),\n",
       " (1112,\n",
       "  ['the decoding algorithm',\n",
       "   ' looks more',\n",
       "   ' like a standard greedy hill-climbing algorithm (albeit with an extra heuristic model for selecting which variable to update), which is also then']),\n",
       " (1113, ['My second criticism model', ' are not well discussed.', ' ']),\n",
       " (1114,\n",
       "  ['the proposed editing procedure',\n",
       "   ' cannot obviously remove',\n",
       "   ' a word from a translation.']),\n",
       " (1115,\n",
       "  ['this',\n",
       "   ' is',\n",
       "   ' a reasonable assumption than can be made for the sake of tractability,']),\n",
       " (1116, ['the baseline models', ' are being used.', ' ']),\n",
       " (1117,\n",
       "  ['the standard objections to absolute positional models (vs. relative positional models)',\n",
       "   ' seem',\n",
       "   ' particularly crucial to bring up in this work,']),\n",
       " (1118, ['more thorough analysis', ' to demonstrate', ' its value.']),\n",
       " (1119,\n",
       "  ['A more thorough analysis',\n",
       "   ' will also likely suggest',\n",
       "   ' some important model variants (for example: is a global model']),\n",
       " (1120,\n",
       "  ['a post-editing model',\n",
       "   ' fixes',\n",
       "   ' outputs with more complex operations more ideal?)']),\n",
       " (1121,\n",
       "  ['I',\n",
       "   ' think',\n",
       "   ' that more could be done to put this work in the context of what has come before and what is currently going on in other parts of ML.']),\n",
       " (1123, ['Arun et', ' to do', ' ']),\n",
       " (1124,\n",
       "  ['The use of an explicit error model',\n",
       "   ' is',\n",
       "   ' rather novel in the context of correction,']),\n",
       " (1126, ['the techniques', ' described', ' they']),\n",
       " (1127, ['I', ' think', \" ''distributional space'' is a bit unclear.\"]),\n",
       " (1128,\n",
       "  ['target sentence',\n",
       "   ' is represented',\n",
       "   \" in terms of distributed word representations via a lookup table'' or something like that.\"]),\n",
       " (1129,\n",
       "  ['the representations',\n",
       "   ' are derived',\n",
       "   ' that from how the words are distributed in the corpus,']),\n",
       " (1130,\n",
       "  ['the model',\n",
       "   ' computes',\n",
       "   ' the distribution over target word types In Section 3, at an absolute position']),\n",
       " (1131, ['It', ' is introduced', ' as the model']),\n",
       " (1132, ['This', ' becomes', ' clearer when reading later in the paper,']),\n",
       " (1133,\n",
       "  ['The use of a fixed sized window for representing the target word in context',\n",
       "   ' seems',\n",
       "   ' ']),\n",
       " (1134,\n",
       "  ['components of S^j',\n",
       "   ' would allow',\n",
       "   ' model 2/3-like responses to be learned- although by leaving them out, the model might behave a bit more like a relative positional model than an absolute positional model, which is probably attractive).']),\n",
       " (1135, ['a fixed window', ' is used', ' to represent the target sentence']),\n",
       " (1136,\n",
       "  ['The relationship between this training objective and pseudo likelihood',\n",
       "   ' might be',\n",
       "   ' worth mentioning.']),\n",
       " (1137,\n",
       "  ['I',\n",
       "   ' believe',\n",
       "   ' this is just a PL objective for a certain global model,']),\n",
       " (1139,\n",
       "  ['I', \" don't completely understand\", ' the rationale for this model']),\n",
       " (1140, ['I', ' am not', ' an expert in machine translation algorithms.']),\n",
       " (1141,\n",
       "  ['A human translator',\n",
       "   ' does not come up away.',\n",
       "   ' with the final translation right']),\n",
       " (1142, ['a rough draft', ' is corrected', ' little by little.']),\n",
       " (1143,\n",
       "  ['The idea behind this paper',\n",
       "   ' is',\n",
       "   ' to implement a similar framework for an automated system.']),\n",
       " (1144, ['This paper', ' is generally well written.', ' ']),\n",
       " (1145,\n",
       "  ['drawings illustrating the architectures',\n",
       "   ' understanding',\n",
       "   ' how the different algorithms relate to one another.']),\n",
       " (1146,\n",
       "  ['you',\n",
       "   ' report',\n",
       "   ' on a preliminary experiment to give an intuition of how difficult the task is.']),\n",
       " (1147,\n",
       "  ['You',\n",
       "   ' should highlight',\n",
       "   ' the links between the task of finding the errors in a guess translation and the task of iterative refinement.']),\n",
       " (1148,\n",
       "  ['you', ' use', ' post-edited text to have a more solid ground-truth?']),\n",
       " (1149,\n",
       "  ['My main concern with this paper',\n",
       "   ' is',\n",
       "   ' that in the experimental section the iterative approach tries to improve upon only one type of machine translation.']),\n",
       " (1150, ['they that approach', ' to improve', ' on?']),\n",
       " (1151,\n",
       "  ['the improvement',\n",
       "   ' comes',\n",
       "   ' from the choice of the initial draft (maybe it was a very bad draft)?']),\n",
       " (1152, ['a lookup table', ' replace*S*', ' each word...']),\n",
       " (1153, ['I', ' might be', ' mistanken']),\n",
       " (1154, ['It', ' is', ' confusing.']),\n",
       " ...]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nc_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 114\n",
      "1272 1272\n"
     ]
    }
   ],
   "source": [
    "test_c_triples_dict = {}\n",
    "test_nc_triples_dict = {}\n",
    "\n",
    "for i in test_c_triples:\n",
    "    test_c_triples_dict[i[0]] = i[1]\n",
    "\n",
    "for i in test_nc_triples:\n",
    "    test_nc_triples_dict[i[0]] = i[1]\n",
    "    \n",
    "print(len(test_c_triples), len(test_c_triples_dict))\n",
    "print(len(test_nc_triples), len(test_nc_triples_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/RoBERTaChunksRecall.txt\", \"r\") as f:\n",
    "    top200ids = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554 Examples are given where appropriate in a clear and coherent manner  • Problem statement well defined mathematically and understandable for a broad audience  • Mentioning of failures and limitations demonstrates a realistic view on the project  • Complexity and time analysis provided  • Paper written so that it's easy for a reader to implement the methods  • Detailed descriptions of all instantiations even parameters and comparison methods  • System specified  • Validation method specified  • Data and repository, as well as cleaning process provided  • Every figure and plot is well explained and interpreted  • Large successful evaluation section provided  • Many different evaluation measures defined to measure different properties of the project  • Different observability modes  • Evaluation against most compatible methods from other sources   • Results are in line with hypothesis  • Thorough appendix clearing any open questions    It would have been good to have a summary/conclusion/future work section   SUMMARY: ACCEPT.\n",
      "1426 Results on tiny datasets\n",
      "1366 a) How does the method scale with k?\n",
      "1239 eqs.5-11), 2) state-of-the-art results on several UDA tasks (Office-Home, ImageCLEF-DA, sim2real on Cityscapes).\n",
      "1296 and how it affects MAE ?\n",
      "597 later.\n",
      "1044 Why not evaluate at least dDVI with diagonal q(w) on    some much larger models and datasets?\n",
      "1302 [4, 5, 3].\"\n",
      "1356 Summary.\n",
      "47 etc.)\n",
      "867 How does the method work on established semantic segmentation datasets with many classes, such as PASCAL?\n",
      "Total absent:  11\n"
     ]
    }
   ],
   "source": [
    "absent_in_both = 0\n",
    "\n",
    "for i in top200ids:\n",
    "    seqid = int(i.strip())\n",
    "    if not seqid in test_c_triples_dict and not seqid in test_nc_triples_dict:\n",
    "        absent_in_both +=1\n",
    "        print(seqid, test_sentences[seqid])\n",
    "print(\"Total absent: \", absent_in_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the whole sent in S, V, and O if OpenIE6 extraction fails "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed text using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./trained_lm/CLMLModelRoBerta/\")\n",
    "model = AutoModel.from_pretrained(\"./trained_lm/CLMLModelRoBerta/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_using_roberta(text):\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReRanking based on weighted SVO similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "90\n",
      "95\n",
      "100\n",
      "105\n",
      "110\n",
      "115\n",
      "120\n",
      "125\n",
      "130\n",
      "135\n",
      "140\n",
      "145\n",
      "150\n",
      "155\n",
      "160\n",
      "165\n",
      "170\n",
      "175\n",
      "180\n",
      "185\n",
      "190\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "wtd_similarity_dict_with_comp = defaultdict(list)\n",
    "wtd_similarity_dict_with_ncomp = defaultdict(list)\n",
    "\n",
    "for _, tid in enumerate(top200ids):\n",
    "    if _%5 == 0:\n",
    "        print(_)\n",
    "    testid = int(tid.strip())\n",
    "    test_triple = []\n",
    "    \n",
    "    if testid in test_c_triples_dict:\n",
    "        test_triple = test_c_triples_dict[testid]\n",
    "    elif testid in test_nc_triples_dict:\n",
    "        test_triple = test_nc_triples_dict[testid]\n",
    "    else:\n",
    "        test_triple = [test_sentences[testid], test_sentences[testid], test_sentences[testid]]\n",
    "    \n",
    "    for j in train_c_triples:\n",
    "        i = j[1]\n",
    "        s1 = None\n",
    "        s2 = None\n",
    "        v1 = None\n",
    "        v2 = None\n",
    "        o1 = None\n",
    "        o2 = None\n",
    "\n",
    "        if str(i[0]) and str(test_triple[0]):\n",
    "            s1 = embed_text_using_roberta(str(i[0])).mean(1).detach().numpy()\n",
    "            s2 = embed_text_using_roberta(str(test_triple[0])).mean(1).detach().numpy()\n",
    "        if s1 is None or s2 is None:\n",
    "            ss_sim = 0.0\n",
    "        else:\n",
    "            ss_sim = 0.3*(1 - spatial.distance.cosine(s1, s2))\n",
    "\n",
    "        if str(i[1]) and str(test_triple[1]):\n",
    "            v1 = embed_text_using_roberta(str(i[1])).mean(1).detach().numpy()\n",
    "            v2 = embed_text_using_roberta(str(test_triple[1])).mean(1).detach().numpy()\n",
    "        if v1 is None or v2 is None:\n",
    "            vv_sim = 0.0\n",
    "        else:\n",
    "            vv_sim = 0.4*(1 - spatial.distance.cosine(v1, v2))\n",
    "\n",
    "        if str(i[2]) and str(test_triple[2]):\n",
    "            o1 = embed_text_using_roberta(str(i[2])).mean(1).detach().numpy()\n",
    "            o2 = embed_text_using_roberta(str(test_triple[2])).mean(1).detach().numpy()\n",
    "        if o1 is None or o2 is None:\n",
    "            oo_sim = 0.0\n",
    "        else:\n",
    "            oo_sim = 0.3*(1 - spatial.distance.cosine(o1, o2))\n",
    "\n",
    "        wtd_sim = ss_sim + vv_sim + oo_sim\n",
    "        wtd_similarity_dict_with_comp[testid].append(wtd_sim)\n",
    "        \n",
    "    for j in train_nc_triples:\n",
    "        i = j[1]\n",
    "        s1 = None\n",
    "        s2 = None\n",
    "        v1 = None\n",
    "        v2 = None\n",
    "        o1 = None\n",
    "        o2 = None\n",
    "        \n",
    "        if str(i[0]) and str(test_triple[0]):\n",
    "            s1 = embed_text_using_roberta(str(i[0])).mean(1).detach().numpy()\n",
    "            s2 = embed_text_using_roberta(str(test_triple[0])).mean(1).detach().numpy()\n",
    "        if s1 is None or s2 is None:\n",
    "            ss_sim = 0.0\n",
    "        else:\n",
    "            ss_sim = 0.3*(1 - spatial.distance.cosine(s1, s2))\n",
    "        \n",
    "        if str(i[1]) and str(test_triple[1]):\n",
    "            v1 = embed_text_using_roberta(str(i[1])).mean(1).detach().numpy()\n",
    "            v2 = embed_text_using_roberta(str(test_triple[1])).mean(1).detach().numpy()\n",
    "        if v1 is None or v2 is None:\n",
    "            vv_sim = 0.0\n",
    "        else:\n",
    "            vv_sim = 0.4*(1 - spatial.distance.cosine(v1, v2))\n",
    "        \n",
    "        if str(i[2]) and str(test_triple[2]):\n",
    "            o1 = embed_text_using_roberta(str(i[2])).mean(1).detach().numpy()\n",
    "            o2 = embed_text_using_roberta(str(test_triple[2])).mean(1).detach().numpy()\n",
    "        if o1 is None or o2 is None:\n",
    "            oo_sim = 0.0\n",
    "        else:\n",
    "            oo_sim = 0.3*(1 - spatial.distance.cosine(o1, o2)) #if o1 and o2 else 0.0\n",
    "        \n",
    "        wtd_sim = ss_sim + vv_sim + oo_sim\n",
    "        wtd_similarity_dict_with_ncomp[testid].append(wtd_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from IPython.display import HTML, display\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wtd_similarity_dict_with_comp.pickle', 'wb') as handle:\n",
    "    pickle.dump(wtd_similarity_dict_with_comp, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wtd_similarity_dict_with_ncomp.pickle', 'wb') as handle:\n",
    "    pickle.dump(wtd_similarity_dict_with_ncomp, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wtd_similarity_dict_with_ncomp), len(wtd_similarity_dict_with_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(sent_sim_dict, k=10, sim_type=\"max\", mean_at=1):\n",
    "    \"\"\"mean_at can take values: 1, 3, 5, 7, 10, 26\"\"\"\n",
    "    \n",
    "    if sim_type == \"max\":\n",
    "        sorted_sims = sorted(sent_sim_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_k_sorted_sims = sorted_sims[0:k]\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        for i in top_k_sorted_sims:\n",
    "            if i[0] in test_c_triples_dict:\n",
    "                tp += 1\n",
    "            elif i[0] in test_nc_triples_dict:\n",
    "                fp += 1\n",
    "            else:\n",
    "                if df_test.loc[i[0]][\"UID\"] == i[0] and df_test.loc[i[0]][\"MComp\"]==1:\n",
    "                    tp+=1\n",
    "                elif df_test.loc[i[0]][\"UID\"] == i[0] and df_test.loc[i[0]][\"MComp\"]==0:\n",
    "                    fp+=1\n",
    "                #print(\"Missing from both sets: \", i)\n",
    "        return tp/(tp+fp)\n",
    "\n",
    "#     elif sim_type == \"mean\":\n",
    "#         sorted_sims = sorted(sent_sim_dict.items(), key=lambda x: x[1][\"mean_\"+str(mean_at)], reverse=True)\n",
    "#         top_k_sorted_sims = sorted_sims[0:k]\n",
    "#         tp = 0\n",
    "#         fp = 0\n",
    "#         for i in top_k_sorted_sims:\n",
    "#             if df.loc[i[0]][\"MComp\"] == 1:\n",
    "#                 tp += 1\n",
    "#             else:\n",
    "#                 fp += 1\n",
    "#         return tp/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "sim_diffs_comp_minus_ncomp = {}\n",
    "\n",
    "for k, v in wtd_similarity_dict_with_comp.items():\n",
    "    diff = max(wtd_similarity_dict_with_comp[k])-max(wtd_similarity_dict_with_ncomp[k])\n",
    "#     if diff > 0:\n",
    "    sim_diffs_comp_minus_ncomp[k] = diff\n",
    "\n",
    "print(len(sim_diffs_comp_minus_ncomp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Precision at:</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">10  </td><td style=\"text-align: right;\">20  </td><td style=\"text-align: right;\">50   </td><td style=\"text-align: right;\">80   </td><td style=\"text-align: right;\">100   </td><td style=\"text-align: right;\">117   </td></tr>\n",
       "<tr><td>Val          </td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 0.8</td><td style=\"text-align: right;\"> 0.8</td><td style=\"text-align: right;\"> 0.66</td><td style=\"text-align: right;\"> 0.64</td><td style=\"text-align: right;\">  0.65</td><td style=\"text-align: right;\">  0.67</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "patk = [1, 5, 10, 20, 50, 80, 100, 117]\n",
    "res_table = [[\"Precision at:\"] + patk, [\"Val\"]]\n",
    "\n",
    "for k in patk:\n",
    "    v1 = precision_at_k(sim_diffs_comp_minus_ncomp, k)\n",
    "    res_table[1].append(round(v1, 2))\n",
    "\n",
    "display(HTML(tabulate.tabulate(res_table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(270, ['the authors', ' add', ' evidence in a larger dataset']),\n",
       " (271,\n",
       "  ['it',\n",
       "   ' to include',\n",
       "   ' a non-deterministic baseline in the experimental comparison?']),\n",
       " (272,\n",
       "  ['failing to compare to results of at least [2, 3]',\n",
       "   ' is',\n",
       "   ' a major drawback of this work.']),\n",
       " (273, ['the largest dataset', ' used', ' ']),\n",
       " (274, ['The tasks 1-5', ' are also', ' completely deterministic,']),\n",
       " (275,\n",
       "  ['additional empirical results',\n",
       "   ' demonstrating',\n",
       "   ' the utility with respect to the state-of-the-art methods (for the same capacity in terms of NNZ)']),\n",
       " (276, ['The baseline', ' used', ' in the paper']),\n",
       " (277,\n",
       "  ['the authors',\n",
       "   ' experiment',\n",
       "   ' on standard benchmarks, a careful experimental analysis Finally,']),\n",
       " (278,\n",
       "  ['the authors',\n",
       "   ' need',\n",
       "   ' more discussion about what this method entails since this is the first peer-review of this work.']),\n",
       " (279,\n",
       "  ['the signal out of a layer',\n",
       "   ' is correlated',\n",
       "   ' with the top singular values,']),\n",
       " (280,\n",
       "  ['the paper',\n",
       "   ' does not have',\n",
       "   ' any baseline model with word order information.']),\n",
       " (281,\n",
       "  ['one construct scenarios where the baseline approach (Amos et al, 2018) fails,',\n",
       "   ' compare',\n",
       "   ' and with the proposed approach?']),\n",
       " (282,\n",
       "  ['no comparison',\n",
       "   ' is made',\n",
       "   ' to the baseline of using matrix sparsification algorithms on the weight matrices']),\n",
       " (283,\n",
       "  ['This paper',\n",
       "   ' provides',\n",
       "   ' a good comparison of the performances for the selected methods.']),\n",
       " (284, ['basic retraining methods', ' described', ' in [1, 2]?']),\n",
       " (285, ['no comparison to the state-of-the-art techniques', ' is given', ' ']),\n",
       " (286,\n",
       "  ['The authors',\n",
       "   ' should provide',\n",
       "   ' experiments in at least one larger dataset like imagenet']),\n",
       " (287, ['I', ' will increase', ' my rating to 8.']),\n",
       " (288,\n",
       "  ['A reasonable experiment/baseline',\n",
       "   ' would be',\n",
       "   ' to train a model-free agent with a small reconstruction loss on top of the learned representation.']),\n",
       " (289,\n",
       "  ['I',\n",
       "   ' think',\n",
       "   \" it's important that the authors experiment with one additional non-neural network benchmark model\"]),\n",
       " (290,\n",
       "  ['I',\n",
       "   ' do not think',\n",
       "   ' this approach is widely compelling as comparisons with informative baselines are missing']),\n",
       " (292,\n",
       "  ['One broad reason for my doubts',\n",
       "   ' is',\n",
       "   \" that the comparisons don't seem to utilise proper hyperparameter tuning for the baseline LSTM.\"]),\n",
       " (293, ['one', ' could probably compare', ' against it in the experiments.']),\n",
       " (294,\n",
       "  [\"why isn't the model from (Watter et al, 2015)\", ' not included', ' ']),\n",
       " (295, ['it', ' is to evaluate', ' it on more datasets.'])]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_c_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{296: ['author', ' missed', ' some important baselines here.']}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentid_triple_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Sent</th>\n",
       "      <th>MComp</th>\n",
       "      <th>Cat</th>\n",
       "      <th>SubCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>296</td>\n",
       "      <td>2019_BJx0sjC5FX</td>\n",
       "      <td>Accept</td>\n",
       "      <td>For example, on page 4, Evaluation, it is uncl...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     UID              PID     Dec  \\\n",
       "296  296  2019_BJx0sjC5FX  Accept   \n",
       "\n",
       "                                                  Sent  MComp  Cat SubCat  \n",
       "296  For example, on page 4, Evaluation, it is uncl...      0  NaN    NaN  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[[296]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
