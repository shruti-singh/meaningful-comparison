{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/shruti/Desktop/iitgn/courses/SEM2/ML/Project/code/PaperAcceptancePrediction/ICLR data/masterdata_unbalanced/\"\n",
    "\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "rev_dict = {}\n",
    "paper_dict = {}\n",
    "dec_dict = {}\n",
    "iclr_arxiv_map = {}\n",
    "\n",
    "for y in years:\n",
    "    rev_dict[y] = pd.read_pickle(data_path + \"off_rev_dict_{}.pkl\".format(y))\n",
    "    paper_dict[y] = pd.read_pickle(data_path + \"papers_{}.pkl\".format(y))\n",
    "    dec_dict[y] = pd.read_pickle(data_path + \"paper_decision_dict_{}.pkl\".format(y))\n",
    "\n",
    "iclr_arxiv_map = pd.read_pickle(\"./data/iclr_arxiv_map.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"InputTestSet-Reviews48_Ann.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Sent</th>\n",
       "      <th>MComp</th>\n",
       "      <th>Cat</th>\n",
       "      <th>SubCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The authors propose to use k-DPP to select a s...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>This paper covers the related work nicely, wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The rest of the paper are also clearly written.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>However, I have some concerns about the propos...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>- It is not clear how to define the kernel, th...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID              PID     Dec  \\\n",
       "0    0  2019_SJf_XhCqKm  Reject   \n",
       "1    1  2019_SJf_XhCqKm  Reject   \n",
       "2    2  2019_SJf_XhCqKm  Reject   \n",
       "3    3  2019_SJf_XhCqKm  Reject   \n",
       "4    4  2019_SJf_XhCqKm  Reject   \n",
       "\n",
       "                                                Sent  MComp  Cat SubCat  \n",
       "0  The authors propose to use k-DPP to select a s...      0  NaN    NaN  \n",
       "1  This paper covers the related work nicely, wit...      0  NaN    NaN  \n",
       "2    The rest of the paper are also clearly written.      0  NaN    NaN  \n",
       "3  However, I have some concerns about the propos...      0  NaN    NaN  \n",
       "4  - It is not clear how to define the kernel, th...      0  NaN    NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dict = {}\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    if not pid in gt_dict:\n",
    "        gt_dict[pid] = {\"dec\": df.loc[i][\"Dec\"], \"mcomp\": set(), \"not_mcomp\": set()}\n",
    "    if df.loc[i][\"MComp\"] == 1:\n",
    "        gt_dict[pid][\"mcomp\"].add(df.loc[i][\"UID\"])\n",
    "    else:\n",
    "        gt_dict[pid][\"not_mcomp\"].add(df.loc[i][\"UID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accept': [48, 644], 'Reject': [69, 744]}\n"
     ]
    }
   ],
   "source": [
    "stats_dict = {\"Accept\": [0, 0], \"Reject\": [0, 0]}\n",
    "\n",
    "for k, v in gt_dict.items():\n",
    "    #print(len(v[\"mcomp\"]), len(v[\"not_mcomp\"]), v[\"dec\"])\n",
    "    stats_dict[v[\"dec\"]][0] += len(v[\"mcomp\"])\n",
    "    stats_dict[v[\"dec\"]][1] += len(v[\"not_mcomp\"])\n",
    "    \n",
    "print(stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestSet length: 32\n",
      " ['2019_SJf_XhCqKm', '2017_Bk0MRI5lg', '2020_SyevYxHtDB', '2018_rJBiunlAW', '2020_rkltE0VKwH', '2018_Hki-ZlbA-', '2019_BJx0sjC5FX', '2020_r1e_FpNFDr', '2020_B1lsXREYvr', '2018_SkZxCk-0Z', '2019_rJzoujRct7', '2018_HkfXMz-Ab', '2017_BJ9fZNqle', '2019_SyxZJn05YX', '2017_B1ckMDqlg', '2017_HJ0NvFzxl', '2017_S1_pAu9xl', '2018_SyYYPdg0-', '2017_BJAA4wKxg', '2019_HyVxPsC9tm', '2019_HylTBhA5tQ', '2019_B1l08oAct7', '2018_H135uzZ0-', '2017_H1oyRlYgg', '2017_r1y1aawlg', '2020_r1eX1yrKwB', '2020_Byg79h4tvB', '2019_H1lFZnR5YX', '2020_BkeWw6VFwr', '2018_HyHmGyZCZ', '2018_HyUNwulC-', '2020_HkgsPhNYPS']\n"
     ]
    }
   ],
   "source": [
    "test_set = list(gt_dict.keys())\n",
    "print(\"TestSet length: %d\\n\"%len(test_set), test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_SJf_XhCqKm     {39, 17, 20, 27, 28, 30}\n",
      "2017_Bk0MRI5lg      {48, 57}\n",
      "2020_SyevYxHtDB     {76, 87}\n",
      "2018_rJBiunlAW      {108, 110, 112, 113, 124, 126}\n",
      "2020_rkltE0VKwH     {160, 155, 184, 159}\n",
      "2018_Hki-ZlbA-      {267, 235, 236, 271}\n",
      "2019_BJx0sjC5FX     {292, 287}\n",
      "2020_r1e_FpNFDr     {312, 322, 315, 308}\n",
      "2020_B1lsXREYvr     {376, 401}\n",
      "2018_SkZxCk-0Z      {449, 443, 445, 486}\n",
      "2019_rJzoujRct7     {518, 519}\n",
      "2018_HkfXMz-Ab      {573, 566}\n",
      "2017_BJ9fZNqle      {627, 623, 615}\n",
      "2019_SyxZJn05YX     {672, 673, 657, 669, 671}\n",
      "2017_B1ckMDqlg      {714, 707}\n",
      "2017_HJ0NvFzxl      {739}\n",
      "2017_S1_pAu9xl      {792, 809, 810, 806}\n",
      "2018_SyYYPdg0-      {834, 867, 868, 869, 870, 872, 873, 844, 830}\n",
      "2017_BJAA4wKxg      {884}\n",
      "2019_HyVxPsC9tm     {931, 933, 905, 909, 912, 913, 919, 926}\n",
      "2019_HylTBhA5tQ     {972, 950}\n",
      "2019_B1l08oAct7     {994, 996, 1064, 1004, 1007, 1044, 1047, 1048, 1055}\n",
      "2018_H135uzZ0-      {1072, 1079}\n",
      "2017_H1oyRlYgg      set()\n",
      "2017_r1y1aawlg      {1125, 1162, 1100, 1102, 1168}\n",
      "2020_r1eX1yrKwB     {1177, 1202, 1212}\n",
      "2020_Byg79h4tvB     {1243, 1268}\n",
      "2019_H1lFZnR5YX     {1281, 1316, 1284, 1318, 1289, 1331, 1333, 1279}\n",
      "2020_BkeWw6VFwr     {1347, 1373}\n",
      "2018_HyHmGyZCZ      {1406, 1421, 1390, 1426}\n",
      "2018_HyUNwulC-      {1451, 1452}\n",
      "2020_HkgsPhNYPS     {1504, 1464, 1497, 1500, 1502}\n"
     ]
    }
   ],
   "source": [
    "for k in test_set:\n",
    "    print('{:20}{}'.format(k, gt_dict[k][\"mcomp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_pool_sentences = [ \n",
    "    \"The method should be compared with other state-of-the-art k-shot learning methods (e.g., Matching Networks by Vinyals et al, 2016).\",\n",
    "    \"It's not clear how this method compares against them.\"\n",
    "    \"Measure: Accuracy difference does not look like a good idea for comparing the baseline method and the proposed one.\",\n",
    "    \"If the authors care to compare their approach to other 1-shot learning methods, then they would have to evaluate their approach with siamese and triplet learning networks.\",\n",
    "    \"Also it is interesting that authors obtained meaningful results on several datasets beating state-of-the-arts based on very simple ideas.\",\n",
    "    \"We should see the performance on other datasets (e.g., some of the other datasets in Wu et al (2018)).\",\n",
    "    \"The authors present a convincing set of results over many translation tasks and compare with very competitive baselines.\",\n",
    "    \"I would like to see an evaluation on (A) the original two datasets of Mikolov et al (without non-nouns), and (B) the larger datasets provided by Drozd et al [3] and Rogers et al [4].\",\n",
    "    \"The comparison with between the representation learned by JMVAE and CVAE might be unfair given that the representation of CVAE is learned conditionally, on the label in the case of MNIST, and should therefore not consider the label in this representation.\",\n",
    "    \"This in itself is not a bad thing, but since there is no comparison of different (simpler) RL agents on the tasks, it is difficult to determine if the tasks selected are challenging.\",\n",
    "    \"It would be great to compare with standard NLP techniques such as Bag of Words followed by SVM.\"\n",
    "]\n",
    "\n",
    "initial_pool_vecs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_for_test = defaultdict(list)\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    sents_for_test[pid].append((df.loc[i][\"UID\"], df.loc[i][\"Sent\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2019_SJf_XhCqKm', '2017_Bk0MRI5lg', '2020_SyevYxHtDB', '2018_rJBiunlAW', '2020_rkltE0VKwH', '2018_Hki-ZlbA-', '2019_BJx0sjC5FX', '2020_r1e_FpNFDr', '2020_B1lsXREYvr', '2018_SkZxCk-0Z', '2019_rJzoujRct7', '2018_HkfXMz-Ab', '2017_BJ9fZNqle', '2019_SyxZJn05YX', '2017_B1ckMDqlg', '2017_HJ0NvFzxl', '2017_S1_pAu9xl', '2018_SyYYPdg0-', '2017_BJAA4wKxg', '2019_HyVxPsC9tm', '2019_HylTBhA5tQ', '2019_B1l08oAct7', '2018_H135uzZ0-', '2017_H1oyRlYgg', '2017_r1y1aawlg', '2020_r1eX1yrKwB', '2020_Byg79h4tvB', '2019_H1lFZnR5YX', '2020_BkeWw6VFwr', '2018_HyHmGyZCZ', '2018_HyUNwulC-', '2020_HkgsPhNYPS'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_for_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Embeddings for Similarity Calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_at_k(predicted_sentids, k):\n",
    "    \n",
    "    local_precision = []\n",
    "    for pid in predicted_sentids:\n",
    "        pred = set(predicted_sentids[pid][0:k])\n",
    "        gt = set(gt_dict[pid][\"mcomp\"])\n",
    "        if len(gt) > 0:\n",
    "            local_precision.append(round(len(pred.intersection(gt))/k, 3))\n",
    "    #print(\"Local precision: \", local_precision)\n",
    "    print(\"Precision@{}: {}\".format(k, np.mean(local_precision)))\n",
    "    return np.mean(local_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall_at_k(predicted_sentids, k):\n",
    "    \n",
    "    local_recall = []\n",
    "    for pid in predicted_sentids:\n",
    "        pred = set(predicted_sentids[pid][0:k])\n",
    "        gt = set(gt_dict[pid][\"mcomp\"])\n",
    "        if len(gt) > 0:\n",
    "            local_recall.append(round(len(pred.intersection(gt))/len(gt), 3))\n",
    "    #print(\"Local recall: \", local_recall)\n",
    "    print(\"Recall@{}: {}\".format(k, np.mean(local_recall)))\n",
    "    return np.mean(local_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. SciBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_cased\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_using_scibert(text):\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in initial_pool_sentences:\n",
    "    vec_emb = embed_text_using_scibert(i).mean(1).detach().numpy()\n",
    "    vec_emb_normalized = vec_emb / norm(vec_emb)\n",
    "    initial_pool_vecs.append(vec_emb_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "sent_vectors = defaultdict(list)\n",
    "skipids = []\n",
    "\n",
    "for pid in sents_for_test:\n",
    "    for sp in sents_for_test[pid]:\n",
    "        try:\n",
    "            vec = embed_text_using_scibert(sp[1]).mean(1).detach().numpy()\n",
    "            sent_vectors[pid].append((sp[0], vec/norm(vec)))\n",
    "        except Exception as ex:\n",
    "            print(sp[1])\n",
    "            skipids.append(sp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_with_initial_pool = defaultdict(list)\n",
    "\n",
    "for pid in sent_vectors:\n",
    "    for sp in sent_vectors[pid]:\n",
    "        if not sp[0] in skipids:\n",
    "            local_sims = []\n",
    "            for ipv in initial_pool_vecs:\n",
    "                local_sims.append(np.inner(sp[1], ipv)[0][0])\n",
    "            sim_with_initial_pool[pid].append((sp[0], max(local_sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min and Max sim sent for each paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 0.8978162) (25, 0.7426945)\n",
      "(56, 0.89206576) (46, 0.6183027)\n",
      "(65, 0.8961332) (77, 0.5202615)\n",
      "(108, 0.92335904) (105, 0.69457334)\n",
      "(155, 0.9079475) (180, 0.7029835)\n",
      "(269, 0.9079767) (252, 0.69661546)\n",
      "(287, 0.8945467) (282, 0.66351205)\n",
      "(312, 0.8921772) (325, 0.6555019)\n",
      "(417, 0.9038178) (428, 0.65535474)\n",
      "(450, 0.906968) (493, 0.6545675)\n",
      "(531, 0.89420986) (522, 0.77269435)\n",
      "(558, 0.8910845) (550, 0.75973654)\n",
      "(623, 0.90252984) (597, 0.6404349)\n",
      "(675, 0.8918824) (690, 0.70532465)\n",
      "(719, 0.91007626) (713, 0.7906028)\n",
      "(732, 0.9052617) (731, 0.7771135)\n",
      "(764, 0.8843342) (803, 0.5859622)\n",
      "(830, 0.91088355) (840, 0.6053677)\n",
      "(884, 0.99999994) (879, 0.81119263)\n",
      "(933, 0.89581996) (935, 0.624375)\n",
      "(945, 0.89922404) (947, 0.6599039)\n",
      "(996, 0.90237373) (1037, 0.648548)\n",
      "(1071, 0.8866335) (1073, 0.76588476)\n",
      "(1086, 0.87391376) (1091, 0.805503)\n",
      "(1139, 0.9032468) (1155, 0.7366502)\n",
      "(1235, 0.8996632) (1216, 0.60312647)\n",
      "(1265, 0.87397873) (1262, 0.73635584)\n",
      "(1275, 0.9222135) (1296, 0.6818665)\n",
      "(1342, 0.8934363) (1353, 0.6480336)\n",
      "(1406, 0.9110269) (1394, 0.6392025)\n",
      "(1443, 0.8891282) (1431, 0.6400988)\n",
      "(1456, 0.89556694) (1455, 0.6420049)\n"
     ]
    }
   ],
   "source": [
    "for k in test_set:\n",
    "    print(max(sim_with_initial_pool[k], key=lambda x: x[1]), min(sim_with_initial_pool[k], key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort sim sentences\n",
    "\n",
    "sorted_sim_with_initial_pool = defaultdict(list)\n",
    "\n",
    "for k in sim_with_initial_pool:\n",
    "    sorted_sims = sorted(sim_with_initial_pool[k], key=lambda x: x[1], reverse=True)\n",
    "    for items in sorted_sims:\n",
    "        sorted_sim_with_initial_pool[k].append(items[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/sent_sin_scibert_32.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sorted_sim_with_initial_pool, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 0.3548387096774194\n",
      "Precision@3: 0.20416129032258065\n",
      "Precision@5: 0.20000000000000004\n",
      "Precision@7: 0.17983870967741933\n",
      "Precision@10: 0.164516129032258\n",
      "Precision@12: 0.16935483870967744\n",
      "Precision@14: 0.1519354838709677\n"
     ]
    }
   ],
   "source": [
    "p_at_k = {}\n",
    "\n",
    "for i in [1, 3, 5, 7, 10, 12, 14]:\n",
    "    p_at_k[i] = calculate_precision_at_k(sorted_sim_with_initial_pool, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1: 0.10529032258064516\n",
      "Recall@3: 0.1792258064516129\n",
      "Recall@5: 0.29732258064516126\n",
      "Recall@7: 0.3443548387096774\n",
      "Recall@10: 0.4500967741935484\n",
      "Recall@12: 0.5956129032258065\n",
      "Recall@14: 0.6110322580645161\n"
     ]
    }
   ],
   "source": [
    "r_at_k = {}\n",
    "\n",
    "for i in [1, 3, 5, 7, 10, 12, 14]:\n",
    "    r_at_k[i] = calculate_recall_at_k(sorted_sim_with_initial_pool, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.16, 3: 0.19, 5: 0.24, 7: 0.24, 10: 0.24, 12: 0.26, 14: 0.24}\n"
     ]
    }
   ],
   "source": [
    "f_score_at_k = {}\n",
    "\n",
    "for i in p_at_k:\n",
    "    f_score_at_k[i] = round((2*p_at_k[i]*r_at_k[i])/(p_at_k[i]+r_at_k[i]), 2)\n",
    "\n",
    "print(f_score_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pid: 2019_SJf_XhCqKm corr pred: 1 out of 6\n",
      "Pid: 2017_Bk0MRI5lg corr pred: 0 out of 2\n",
      "Pid: 2020_SyevYxHtDB corr pred: 1 out of 2\n",
      "Pid: 2018_rJBiunlAW corr pred: 1 out of 6\n",
      "Pid: 2020_rkltE0VKwH corr pred: 1 out of 4\n",
      "Pid: 2018_Hki-ZlbA- corr pred: 1 out of 4\n",
      "Pid: 2019_BJx0sjC5FX corr pred: 1 out of 2\n",
      "Pid: 2020_r1e_FpNFDr corr pred: 2 out of 4\n",
      "Pid: 2020_B1lsXREYvr corr pred: 0 out of 2\n",
      "Pid: 2018_SkZxCk-0Z corr pred: 1 out of 4\n",
      "Pid: 2019_rJzoujRct7 corr pred: 0 out of 2\n",
      "Pid: 2017_BJ9fZNqle corr pred: 3 out of 3\n",
      "Pid: 2019_SyxZJn05YX corr pred: 2 out of 5\n",
      "Pid: 2017_B1ckMDqlg corr pred: 0 out of 2\n"
     ]
    }
   ],
   "source": [
    "for pid in sorted_sim_with_initial_pool:\n",
    "    pred = set(sorted_sim_with_initial_pool[pid][0:5])\n",
    "    gt = set(gt_dict[pid][\"mcomp\"])\n",
    "    if len(gt) > 0:\n",
    "        print(\"Pid: {} corr pred: {} out of {}\".format(pid, len(pred.intersection(gt)), len(gt)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[719, 715, 700, 723, 720]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments are performed on language modeling and machine translation tasks, showing significant gains by increasing the number of experts, compared to both SoA as well as explicitly computationally-matched baseline systems. \n",
      "\n",
      "Paper Weaknesses:\n",
      "--- there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss the use of MoE and other alternatives in terms of computational efficiency and other factors. \n",
      "\n",
      "Experiments applying the proposed approach on RNNs in language modelling task show that it can beat SOTA results with significantly less computation, which is a result of selectively using much more parameters. \n",
      "\n",
      "Overall I think this is a well-described system that achieves good results, using a nifty placement for the MoE that can overcome what otherwise might be a disadvantage for sparse computation. \n",
      "\n",
      "An area that falls a bit short is in presenting plots or statistics on the real computational load and system behavior. \n",
      "\n",
      "========================================================================================\n",
      "Paper Strengths: \n",
      "-- Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting very large datasets in a computationally feasible manner\n",
      "-- The effective batch size for training the MoE drastically increased also\n",
      "-- Interesting experimental results on the effects of increasing the number of MoEs, which is expected. \n",
      "\n",
      "There are also some glitches in the writing, eg: the end of Section 3.1) \n",
      "- The paper is missing some important references in conditional computation (eg: https://arxiv.org/pdf/1308.3432.pdf) which deal with very similar issues in deep learning. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pid = \"2017_B1ckMDqlg\"\n",
    "sent_ids = sorted_sim_with_initial_pool[pid][0:5]\n",
    "for i in sent_ids:\n",
    "    print(df.iloc[i][\"Sent\"], \"\\n\")\n",
    "    \n",
    "print(\"========================================================================================\")\n",
    "for i in gt_dict[pid][\"mcomp\"]:\n",
    "    print(df.iloc[i][\"Sent\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011). \n",
      "\n",
      "The authors propose to use k-DPP to select a set of diverse parameters and use them to search for a good a hyperparameter setting. \n",
      "\n",
      "I think it would have more novelty if some theoretical analyses can be shown on the mixing rate and how good this optimization algorithm is. \n",
      "\n",
      "It is unclear to me if the comparison of wall clock time and accuracy holds for larger number of hyperparameters or against Spearmint with more parallelization. \n",
      "\n",
      "The first experiment by the authors shows that k-DPP-RBF gives better star discrepancy than uniform random search while being comparable to low-discrepancy Sobol sequences in other metrics such as distance from the center or an arbitrary corner (Figure 1). \n",
      "\n",
      "========================================================================================\n",
      "It would be interesting to explore the practicability of the method on more large-scale experiments on image related tasks. \n",
      "\n",
      "I think a clearer emphasis on the novelty, eg: current algorithm with mixing rate analyses or more thorough empirical comparisons will make the paper stronger for resubmission. \n",
      "\n",
      "The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al (2011). \n",
      "\n",
      "Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20) The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al (http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf) and Snoek et al (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study. \n",
      "\n",
      "COMMENTS ON THE CHANGES SINCE THE LAST YEAR\n",
      "I am not convinced by the comparison with Spearmint added by the authors since the previous version. \n",
      "\n",
      "In addition the authors do not compare against more recent work, e.g., \n",
      "@INPROCEEDINGS{falkner-bayesopt17,\n",
      " author = {S. Falkner and A. Klein and F. Hutter},\n",
      " title = {Combining Hyperband and Bayesian Optimization},\n",
      " booktitle = {NIPS 2017 Bayesian Optimization Workshop},\n",
      " year = {2017},\n",
      " month = dec,\n",
      "}\n",
      "@InProceedings{falkner-icml-18,\n",
      " title = {{BOHB}: Robust and Efficient Hyperparameter Optimization at Scale},\n",
      " author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},\n",
      " booktitle = {Proceedings of the 35th International Conference on Machine Learning (ICML 2018)},\n",
      " pages = {1436--1445},\n",
      " year = {2018},\n",
      " month = jul,\n",
      "} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pid = \"2019_SJf_XhCqKm\"\n",
    "sent_ids = sorted_sim_with_initial_pool[pid][0:5]\n",
    "for i in sent_ids:\n",
    "    print(df.iloc[i][\"Sent\"], \"\\n\")\n",
    "    \n",
    "print(\"========================================================================================\")\n",
    "for i in gt_dict[pid][\"mcomp\"]:\n",
    "    print(df.iloc[i][\"Sent\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. USE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_text_using_use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_pool_vecs = []\n",
    "\n",
    "for isent in initial_pool_sentences:\n",
    "    vec = embed_text_using_use([isent])\n",
    "    initial_pool_vecs.append(vec/norm(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, tensorflow.python.framework.ops.EagerTensor)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_pool_vecs), type(initial_pool_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "sent_vectors = defaultdict(list)\n",
    "skipids = []\n",
    "\n",
    "for pid in sents_for_test:\n",
    "    for sp in sents_for_test[pid]:\n",
    "        try:\n",
    "            vec = embed_text_using_use([sp[1]])\n",
    "            sent_vectors[pid].append((sp[0], vec/norm(vec)))\n",
    "        except Exception as ex:\n",
    "            print(sp[1])\n",
    "            skipids.append(sp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_with_initial_pool = defaultdict(list)\n",
    "\n",
    "for pid in sent_vectors:\n",
    "    for sp in sent_vectors[pid]:\n",
    "        local_sims = []\n",
    "        for ipv in initial_pool_vecs:\n",
    "            local_sims.append(np.inner(sp[1], ipv)[0][0])\n",
    "        sim_with_initial_pool[pid].append((sp[0], max(local_sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min and Max sim sent for each paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 0.52893025) (15, 0.039917033)\n",
      "(51, 0.40261608) (47, 0.09980716)\n",
      "(86, 0.46295145) (96, 0.07237847)\n",
      "(108, 0.53479534) (105, 0.087693796)\n",
      "(135, 0.5545818) (196, 0.018506736)\n",
      "(228, 0.4474287) (248, 0.045198977)\n",
      "(281, 0.40989438) (297, 0.12206017)\n",
      "(322, 0.49228573) (359, 0.059276532)\n",
      "(401, 0.49005064) (431, 0.025508143)\n",
      "(479, 0.5199798) (494, 0.025145296)\n",
      "(543, 0.39963934) (522, 0.103784904)\n",
      "(576, 0.41093782) (563, 0.10770255)\n",
      "(626, 0.49329722) (597, 0.04533949)\n",
      "(669, 0.42909694) (694, 0.08125764)\n",
      "(700, 0.4107489) (725, 0.07047744)\n",
      "(739, 0.41832656) (734, 0.10811974)\n",
      "(792, 0.42380494) (796, 0.05690879)\n",
      "(844, 0.5585762) (857, 0.016481042)\n",
      "(884, 1.0) (891, 0.1329079)\n",
      "(925, 0.48231736) (936, 0.031061161)\n",
      "(981, 0.48317257) (982, 0.09248477)\n",
      "(1009, 0.5638101) (1041, 0.060922123)\n",
      "(1079, 0.44311142) (1078, 0.16902666)\n",
      "(1086, 0.3334951) (1090, 0.15959388)\n",
      "(1096, 0.5499792) (1160, 0.12011877)\n",
      "(1190, 0.52012265) (1213, 0.044876657)\n",
      "(1260, 0.47883913) (1240, 0.094746254)\n",
      "(1278, 0.52130896) (1302, 0.03636354)\n",
      "(1373, 0.41586322) (1361, 0.089035764)\n",
      "(1403, 0.52026737) (1394, 0.03778905)\n",
      "(1446, 0.44952574) (1437, 0.11962801)\n",
      "(1463, 0.49757975) (1455, 0.07671106)\n"
     ]
    }
   ],
   "source": [
    "for k in test_set:\n",
    "    print(max(sim_with_initial_pool[k], key=lambda x: x[1]), min(sim_with_initial_pool[k], key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort sim sentences\n",
    "\n",
    "sorted_sim_with_initial_pool = defaultdict(list)\n",
    "\n",
    "for k in sim_with_initial_pool:\n",
    "    sorted_sims = sorted(sim_with_initial_pool[k], key=lambda x: x[1], reverse=True)\n",
    "    for items in sorted_sims:\n",
    "        sorted_sim_with_initial_pool[k].append(items[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/sent_sin_use_32.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sorted_sim_with_initial_pool, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 0.3225806451612903\n",
      "Precision@3: 0.3547096774193549\n",
      "Precision@5: 0.3032258064516129\n",
      "Precision@7: 0.24441935483870958\n",
      "Precision@10: 0.22580645161290322\n",
      "Precision@12: 0.20699999999999996\n",
      "Precision@14: 0.2003548387096774\n"
     ]
    }
   ],
   "source": [
    "p_at_k = {}\n",
    "\n",
    "for i in [1, 3, 5, 7, 10, 12, 14]:\n",
    "    p_at_k[i] = calculate_precision_at_k(sorted_sim_with_initial_pool, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1: 0.1444516129032258\n",
      "Recall@3: 0.35874193548387096\n",
      "Recall@5: 0.4498064516129033\n",
      "Recall@7: 0.49499999999999994\n",
      "Recall@10: 0.6477741935483871\n",
      "Recall@12: 0.6884193548387098\n",
      "Recall@14: 0.7783870967741936\n"
     ]
    }
   ],
   "source": [
    "r_at_k = {}\n",
    "\n",
    "for i in [1, 3, 5, 7, 10, 12, 14]:\n",
    "    r_at_k[i] = calculate_recall_at_k(sorted_sim_with_initial_pool, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.2, 3: 0.36, 5: 0.36, 7: 0.33, 10: 0.33, 12: 0.32, 14: 0.32}\n"
     ]
    }
   ],
   "source": [
    "f_score_at_k = {}\n",
    "\n",
    "for i in p_at_k:\n",
    "    f_score_at_k[i] = round((2*p_at_k[i]*r_at_k[i])/(p_at_k[i]+r_at_k[i]), 2)\n",
    "\n",
    "print(f_score_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
