{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from collections import defaultdict\n",
    "from scipy import spatial\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Feature Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"InputTestSet-Reviews48_Ann_NEW.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Sent</th>\n",
       "      <th>MComp</th>\n",
       "      <th>Cat</th>\n",
       "      <th>SubCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The authors propose to use k-DPP to select a s...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>This paper covers the related work nicely, wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The rest of the paper are also clearly written.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>However, I have some concerns about the propos...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>- It is not clear how to define the kernel, th...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID              PID     Dec  \\\n",
       "0    0  2019_SJf_XhCqKm  Reject   \n",
       "1    1  2019_SJf_XhCqKm  Reject   \n",
       "2    2  2019_SJf_XhCqKm  Reject   \n",
       "3    3  2019_SJf_XhCqKm  Reject   \n",
       "4    4  2019_SJf_XhCqKm  Reject   \n",
       "\n",
       "                                                Sent  MComp  Cat SubCat  \n",
       "0  The authors propose to use k-DPP to select a s...      0  NaN    NaN  \n",
       "1  This paper covers the related work nicely, wit...      0  NaN    NaN  \n",
       "2    The rest of the paper are also clearly written.      0  NaN    NaN  \n",
       "3  However, I have some concerns about the propos...      0  NaN    NaN  \n",
       "4  - It is not clear how to define the kernel, th...      0  NaN    NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1505, 7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_dict = {}\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    if not pid in gt_dict:\n",
    "        gt_dict[pid] = {\"dec\": df.loc[i][\"Dec\"], \"mcomp\": set(), \"not_mcomp\": set()}\n",
    "    if df.loc[i][\"MComp\"] == 1:\n",
    "        gt_dict[pid][\"mcomp\"].add(df.loc[i][\"UID\"])\n",
    "    else:\n",
    "        gt_dict[pid][\"not_mcomp\"].add(df.loc[i][\"UID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accept': [49, 643], 'Reject': [68, 745]}\n"
     ]
    }
   ],
   "source": [
    "stats_dict = {\"Accept\": [0, 0], \"Reject\": [0, 0]}\n",
    "\n",
    "for k, v in gt_dict.items():\n",
    "    #print(len(v[\"mcomp\"]), len(v[\"not_mcomp\"]), v[\"dec\"])\n",
    "    stats_dict[v[\"dec\"]][0] += len(v[\"mcomp\"])\n",
    "    stats_dict[v[\"dec\"]][1] += len(v[\"not_mcomp\"])\n",
    "    \n",
    "print(stats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestSet length: 32\n",
      " ['2019_SJf_XhCqKm', '2017_Bk0MRI5lg', '2020_SyevYxHtDB', '2018_rJBiunlAW', '2020_rkltE0VKwH', '2018_Hki-ZlbA-', '2019_BJx0sjC5FX', '2020_r1e_FpNFDr', '2020_B1lsXREYvr', '2018_SkZxCk-0Z', '2019_rJzoujRct7', '2018_HkfXMz-Ab', '2017_BJ9fZNqle', '2019_SyxZJn05YX', '2017_B1ckMDqlg', '2017_HJ0NvFzxl', '2017_S1_pAu9xl', '2018_SyYYPdg0-', '2017_BJAA4wKxg', '2019_HyVxPsC9tm', '2019_HylTBhA5tQ', '2019_B1l08oAct7', '2018_H135uzZ0-', '2017_H1oyRlYgg', '2017_r1y1aawlg', '2020_r1eX1yrKwB', '2020_Byg79h4tvB', '2019_H1lFZnR5YX', '2020_BkeWw6VFwr', '2018_HyHmGyZCZ', '2018_HyUNwulC-', '2020_HkgsPhNYPS']\n"
     ]
    }
   ],
   "source": [
    "test_set = list(gt_dict.keys())\n",
    "print(\"TestSet length: %d\\n\"%len(test_set), test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_SJf_XhCqKm     {39, 17, 27, 28, 30}\n",
      "2017_Bk0MRI5lg      {57}\n",
      "2020_SyevYxHtDB     {76, 87}\n",
      "2018_rJBiunlAW      {108, 110, 112, 113, 124, 126}\n",
      "2020_rkltE0VKwH     {160, 184, 155, 157, 159}\n",
      "2018_Hki-ZlbA-      {267, 235, 236, 271}\n",
      "2019_BJx0sjC5FX     {292, 287}\n",
      "2020_r1e_FpNFDr     {312, 322, 315, 308}\n",
      "2020_B1lsXREYvr     {376, 401}\n",
      "2018_SkZxCk-0Z      {449, 443, 445, 486}\n",
      "2019_rJzoujRct7     {518, 519}\n",
      "2018_HkfXMz-Ab      {573, 566}\n",
      "2017_BJ9fZNqle      {627, 623, 615}\n",
      "2019_SyxZJn05YX     {672, 673, 657, 669, 671}\n",
      "2017_B1ckMDqlg      {714, 707}\n",
      "2017_HJ0NvFzxl      {739}\n",
      "2017_S1_pAu9xl      {806, 808, 809, 810, 792}\n",
      "2018_SyYYPdg0-      {834, 867, 868, 869, 870, 872, 873, 844, 830}\n",
      "2017_BJAA4wKxg      {884}\n",
      "2019_HyVxPsC9tm     {931, 933, 905, 909, 912, 913, 919, 926}\n",
      "2019_HylTBhA5tQ     {972, 950}\n",
      "2019_B1l08oAct7     {994, 996, 1064, 1004, 1007, 1044, 1047, 1048, 1055}\n",
      "2018_H135uzZ0-      {1072, 1079}\n",
      "2017_H1oyRlYgg      set()\n",
      "2017_r1y1aawlg      {1125, 1162, 1100, 1102, 1168}\n",
      "2020_r1eX1yrKwB     {1177, 1202, 1212}\n",
      "2020_Byg79h4tvB     {1243, 1268}\n",
      "2019_H1lFZnR5YX     {1281, 1316, 1284, 1318, 1289, 1331, 1333, 1279}\n",
      "2020_BkeWw6VFwr     {1347, 1373}\n",
      "2018_HyHmGyZCZ      {1406, 1421, 1390, 1426}\n",
      "2018_HyUNwulC-      {1451, 1452}\n",
      "2020_HkgsPhNYPS     {1504, 1464, 1497, 1500, 1502}\n"
     ]
    }
   ],
   "source": [
    "for k in test_set:\n",
    "    print('{:20}{}'.format(k, gt_dict[k][\"mcomp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_for_test = defaultdict(list)\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    pid = df.loc[i][\"PID\"]\n",
    "    test_sent_raw = str(df.loc[i][\"Sent\"])\n",
    "    \n",
    "    # Replace URLs with [URL]\n",
    "    test_sent_raw = re.sub(r'http[s]?://[a-zA-z\\.\\-/0-9~]*', \"[URL]\", test_sent_raw)\n",
    "    test_sent_raw = re.sub(r'papers.nips.cc/paper/[a-zA-z\\.\\-/0-9~]*', \"[URL]\", test_sent_raw)\n",
    "    test_sent_raw = re.sub(r'arxiv.org/[a-zA-z\\.\\-/0-9~]*', \"[URL]\", test_sent_raw)\n",
    "    \n",
    "    sents_for_test[pid].append((df.loc[i][\"UID\"], test_sent_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>PID</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Sent</th>\n",
       "      <th>MComp</th>\n",
       "      <th>Cat</th>\n",
       "      <th>SubCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>243</td>\n",
       "      <td>2020_ryen_CEFwr</td>\n",
       "      <td>Reject</td>\n",
       "      <td>It extends this approach by introducing an add...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>179</td>\n",
       "      <td>2018_H1LAqMbRW</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Experimentally, the results are rather weak co...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157</td>\n",
       "      <td>2017_HyTqHL5xg</td>\n",
       "      <td>Accept</td>\n",
       "      <td>The experiments are interesting but I'm still ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>146</td>\n",
       "      <td>2017_HyTqHL5xg</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Section 2.2 says they do the latter in the int...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>2017_ByToKu9ll</td>\n",
       "      <td>Reject</td>\n",
       "      <td>4)This paper proposed an improved version of t...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID              PID     Dec  \\\n",
       "0  243  2020_ryen_CEFwr  Reject   \n",
       "1  179   2018_H1LAqMbRW  Reject   \n",
       "2  157   2017_HyTqHL5xg  Accept   \n",
       "3  146   2017_HyTqHL5xg  Accept   \n",
       "4   90   2017_ByToKu9ll  Reject   \n",
       "\n",
       "                                                Sent  MComp   Cat SubCat  \n",
       "0  It extends this approach by introducing an add...      0  None   None  \n",
       "1  Experimentally, the results are rather weak co...      0  None   None  \n",
       "2  The experiments are interesting but I'm still ...      0  None   None  \n",
       "3  Section 2.2 says they do the latter in the int...      0  None   None  \n",
       "4  4)This paper proposed an improved version of t...      0  None   None  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_excel(\"InputTrainSet-Reviews7_Ann.xlsx\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sets = {\"mcomp\": [], \"non_mcomp\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, df_train.shape[0]):\n",
    "    pid = df_train.loc[i][\"PID\"]\n",
    "    train_sent_raw = str(df_train.loc[i][\"Sent\"])\n",
    "    \n",
    "    type_comp = df_train.loc[i][\"MComp\"]\n",
    "    \n",
    "    if type_comp == 1:\n",
    "        train_sets[\"mcomp\"].append(train_sent_raw)\n",
    "    else:\n",
    "        train_sets[\"non_mcomp\"].append(train_sent_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 270)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sets[\"mcomp\"]), len(train_sets[\"non_mcomp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"entities_dict_smaller\", \"r\") as f:\n",
    "    entity_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Material', 'Method', 'Metric', 'Task'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(entity_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('convolutional neural networks', 'Method'),\n",
       " ('convnets', 'Method'),\n",
       " ('recognition', 'Task'),\n",
       " ('visual recognition tasks', 'Task'),\n",
       " ('age estimation', 'Task'),\n",
       " ('head pose estimation', 'Task'),\n",
       " ('multi - label classification', 'Task'),\n",
       " ('semantic segmentation', 'Task'),\n",
       " ('classification', 'Task'),\n",
       " ('deep convnets', 'Method'),\n",
       " ('dldl', 'Method'),\n",
       " ('feature learning', 'Task'),\n",
       " ('deep learning', 'Method'),\n",
       " ('image classification', 'Task'),\n",
       " ('deep learning methods', 'Method'),\n",
       " ('image classification tasks', 'Task'),\n",
       " ('human pose estimation', 'Task'),\n",
       " ('convnet', 'Method'),\n",
       " ('recognition tasks', 'Task'),\n",
       " ('ensemble', 'Method')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_dict.items())[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1784\n"
     ]
    }
   ],
   "source": [
    "entity_key_map = {}\n",
    "for i in entity_dict:\n",
    "    s = re.sub('[^0-9a-zA-Z,:;.?!\\- ]+', '', i)\n",
    "    while s.find(\"  \") > -1:\n",
    "        s = s.replace(\"  \", \" \")\n",
    "    if len(s) > 2:\n",
    "        cl = re.sub('[^0-9a-zA-Z ]+', '', i)\n",
    "        while cl.find(\"  \") > -1:\n",
    "            cl = cl.replace(\"  \", \" \")\n",
    "        entity_key_map[cl.strip()] = i\n",
    "print(len(entity_key_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    }
   ],
   "source": [
    "coun = 0\n",
    "for i in entity_dict:\n",
    "    if len(i) < 5:\n",
    "        coun +=1\n",
    "#         print(i)\n",
    "print(coun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('convolutional neural networks', 'convolutional neural networks'),\n",
       " ('convnets', 'convnets'),\n",
       " ('recognition', 'recognition'),\n",
       " ('visual recognition tasks', 'visual recognition tasks'),\n",
       " ('age estimation', 'age estimation')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity_key_map.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Method': 1191, 'Task': 289, 'Metric': 158, 'Material': 165})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(entity_dict.values())\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(c)\n",
    "reverse_map = defaultdict(list)\n",
    "\n",
    "for k, v in entity_dict.items():\n",
    "    reverse_map[v].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in reverse_map[\"Task\"]:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"MNIST\" in entity_key_map, \"mnist\" in entity_key_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. RoBERTa trained on SciLit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy-transformers            0.6.2\n",
      "tokenizers                    0.7.0\n",
      "transformers                  2.9.0\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3.7 list | grep -E 'transformers|tokenizers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./trained_lm/CLMLModelRoBerta/\")\n",
    "model = AutoModel.from_pretrained(\"./trained_lm/CLMLModelRoBerta/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text_using_roberta(text):\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    return last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_entities(sentence, replace_with_dataset=True):\n",
    "    cleaned_sent = re.sub('[^0-9a-zA-Z,:;.?!\\- ]+', ' ', sentence)\n",
    "    while cleaned_sent.find(\"  \") > -1:\n",
    "        cleaned_sent = cleaned_sent.replace(\"  \", \" \")\n",
    "    \n",
    "    entity_key_map_keys = list(entity_key_map.keys()) # As we will be dunamically adding entries to this dict an dthat will throw an error.\n",
    "    entities_found = []\n",
    "    for i in entity_key_map_keys:\n",
    "        if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "            entities_found.append(i)\n",
    "        elif cleaned_sent.lower().find(\" \" + i + \" \") > -1:\n",
    "            found_idx = cleaned_sent.lower().find(\" \" + i + \" \")\n",
    "            entity_dict[cleaned_sent[found_idx:found_idx+len(\" \" + i + \" \")]] = entity_dict[i]\n",
    "            entity_key_map[cleaned_sent[found_idx:found_idx+len(\" \" + i + \" \")]] = entity_key_map[i]\n",
    "    \n",
    "    entities_found.sort(key=lambda s: len(s))\n",
    "    len_sorted_entities = entities_found.copy()\n",
    "    \n",
    "    subset_entities = []\n",
    "    # Remove subset entities (eg: Among cnn and 3-layer-cnn, prefer the latter)\n",
    "    for fe in len_sorted_entities:\n",
    "        for other_ent in len_sorted_entities:\n",
    "            if fe != other_ent and other_ent.find(fe) > -1:\n",
    "                subset_entities.append(fe)\n",
    "                break\n",
    "    for se in subset_entities:\n",
    "        len_sorted_entities.remove(se)\n",
    "    for maxents in len_sorted_entities:\n",
    "        mask_name = \" \" + entity_dict[entity_key_map[i]].lower() + \" \"\n",
    "        if replace_with_dataset:\n",
    "            if mask_name == \" material \":\n",
    "                mask_name = \" dataset \"\n",
    "        cleaned_sent = cleaned_sent.replace(\" \" + maxents + \" \", mask_name)\n",
    "    words_cleaned = nltk.word_tokenize(cleaned_sent)\n",
    "    dups_removed = [v for i, v in enumerate(words_cleaned) if i == 0 or v != words_cleaned[i-1]]\n",
    "    new_dup_removed_sent = \" \".join(dups_removed)\n",
    "    return new_dup_removed_sent.strip()\n",
    "\n",
    "#     #print(cleaned_sent)\n",
    "#     for i in entity_key_map:\n",
    "#         if cleaned_sent.find(\" \" + i + \" \") > -1:\n",
    "#             #print(\"Substituting ent: {} with mask: {}\".format(i, entity_dict[entity_key_map[i]].lower()))\n",
    "#             cleaned_sent = cleaned_sent.replace(i, entity_dict[entity_key_map[i]].lower())\n",
    "#     return cleaned_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "sp_toks = [\"result\", \"method\", \"task\", \"dataset\", \"metric\", \"baseline\", \"fair\", \"unfair\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chunks_using_spacy_dp(conssentence, replace_with_dataset=True):\n",
    "    \n",
    "#     conssentence = mask_entities(conssentence, replace_with_dataset)\n",
    "#     print(conssentence)\n",
    "    doc = nlp(conssentence)\n",
    "    verb_subtree = []\n",
    "\n",
    "    for s in doc.sents:\n",
    "#         find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"technique\": [], \"task\": [], \"dataset\": [], \"material\": [], \"metric\": []}\n",
    "        find_special_tokens = {\"compar\": [], \"result\": [], \"method\": [], \"baseline\": [], \"task\": [], \n",
    "                               \"dataset\": [],  \"metric\": [], \"unfair\": [], \"fair\": []}\n",
    "\n",
    "        for tok in s:\n",
    "\n",
    "            if tok.text.lower().startswith(\"compar\"):\n",
    "                find_special_tokens[\"compar\"].append(tok)\n",
    "            else:\n",
    "                for k in sp_toks:\n",
    "                    if tok.text.lower().startswith(k):\n",
    "                        find_special_tokens[k].append(tok)\n",
    "                        break\n",
    "\n",
    "        verb_tokens = []\n",
    "        if find_special_tokens[\"compar\"]:\n",
    "            for t in find_special_tokens[\"compar\"]:\n",
    "#                     verb_subtree.append(t.subtree)\n",
    "                if t == s.root:\n",
    "                    simplified_sent = \"\"\n",
    "                    for chh in t.lefts:\n",
    "                        simplified_sent = simplified_sent + \" \" + chh.text\n",
    "                    simplified_sent = simplified_sent + \" \" + t.text\n",
    "                    for chh in t.rights:\n",
    "                        simplified_sent = simplified_sent + \" \" + chh.text\n",
    "#                         print(\"SIMP: \", simplified_sent)\n",
    "                    verb_subtree.append(simplified_sent)\n",
    "                else:\n",
    "                    verb_subtree.append(t.subtree)\n",
    "        else:\n",
    "            for k in sp_toks:\n",
    "                for i in find_special_tokens[k]:\n",
    "                    local_vt = []\n",
    "                    for j in i.ancestors:\n",
    "                        if j.pos_ == \"NOUN\":\n",
    "                            local_vt.append(j)\n",
    "                    if not local_vt:\n",
    "                        for j in i.ancestors:\n",
    "                            if j.pos_ == \"VERB\":\n",
    "                                local_vt.append(j)\n",
    "                    verb_tokens = verb_tokens + local_vt\n",
    "\n",
    "\n",
    "            for i in verb_tokens:\n",
    "                verb_subtree.append(i.subtree)\n",
    "\n",
    "    eecc = []\n",
    "    for i in verb_subtree:\n",
    "        if type(i) == str:\n",
    "            eecc.append(i)\n",
    "        else:\n",
    "            local_chunk = \"\"\n",
    "            for lcaltok in i:\n",
    "                local_chunk = local_chunk + \" \" + lcaltok.text\n",
    "            eecc.append(local_chunk)\n",
    "#     if not eecc:\n",
    "#         print(conssentence)\n",
    "    return list(set(eecc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing vectors of the initial training pool of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool_roberta_vecs = {\"mcomp\": [], \"non_mcomp\": []}\n",
    "single_train_pool_roberta_vecs = {\"mcomp\": [], \"non_mcomp\": []}\n",
    "train_pool_uid_vecs = defaultdict(list)\n",
    "mc_nmc_fake = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_idx = 0\n",
    "\n",
    "for i in train_sets[\"mcomp\"]:\n",
    "    mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(i)\n",
    "    if mcomp_chunks_from_sent:\n",
    "        final_chunks = mcomp_chunks_from_sent\n",
    "    else:\n",
    "        final_chunks = [i]\n",
    "    \n",
    "    mc_nmc_fake[fake_idx] = 1\n",
    "    for single_chunk in final_chunks:\n",
    "        vec = embed_text_using_roberta(single_chunk.strip()).mean(1).detach().numpy()\n",
    "        train_pool_uid_vecs[fake_idx].append(vec / norm(vec))\n",
    "        train_pool_roberta_vecs[\"mcomp\"].append(vec/norm(vec))\n",
    "    \n",
    "    collated_chunk = \" \".join(final_chunks)\n",
    "    vec = embed_text_using_roberta(collated_chunk.strip()).mean(1).detach().numpy()\n",
    "    single_train_pool_roberta_vecs[\"mcomp\"].append(vec/norm(vec))\n",
    "    fake_idx += 1\n",
    "\n",
    "\n",
    "for i in train_sets[\"non_mcomp\"]:\n",
    "    mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(i)\n",
    "    if mcomp_chunks_from_sent:\n",
    "        final_chunks = mcomp_chunks_from_sent\n",
    "    else:\n",
    "        final_chunks = [i]\n",
    "    \n",
    "    mc_nmc_fake[fake_idx] = 0\n",
    "    for single_chunk in final_chunks:\n",
    "        vec = embed_text_using_roberta(single_chunk.strip()).mean(1).detach().numpy()\n",
    "        train_pool_uid_vecs[fake_idx].append(vec / norm(vec))\n",
    "        train_pool_roberta_vecs[\"non_mcomp\"].append(vec/norm(vec))\n",
    "    \n",
    "    collated_chunk = \" \".join(final_chunks)\n",
    "    vec = embed_text_using_roberta(collated_chunk.strip()).mean(1).detach().numpy()\n",
    "    single_train_pool_roberta_vecs[\"non_mcomp\"].append(vec/norm(vec))\n",
    "    fake_idx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_HyVxPsC9tm 938 2\n",
      "2019_HyVxPsC9tm 940 3\n",
      "2018_HyHmGyZCZ 1425 2\n"
     ]
    }
   ],
   "source": [
    "roberta_vectors = defaultdict(dict)\n",
    "\n",
    "skip_uids = []\n",
    "\n",
    "for pid in gt_dict:\n",
    "    roberta_vectors[pid] = {}\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunks = mcomp_chunks_from_sent\n",
    "            else:\n",
    "                final_chunks = [df.loc[mcs][\"Sent\"]]\n",
    "            \n",
    "            roberta_vectors[pid][mcs] = []\n",
    "            for single_chunk in final_chunks:\n",
    "                vec = embed_text_using_roberta(single_chunk.strip()).mean(1).detach().numpy()\n",
    "                roberta_vectors[pid][mcs].append(vec / norm(vec))\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)\n",
    "    \n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        try:\n",
    "            mcomp_chunks_from_sent = extract_chunks_using_spacy_dp(df.loc[mcs][\"Sent\"])\n",
    "            if mcomp_chunks_from_sent:\n",
    "                final_chunks = mcomp_chunks_from_sent\n",
    "            else:\n",
    "                final_chunks = [df.loc[mcs][\"Sent\"]]\n",
    "            \n",
    "            roberta_vectors[pid][mcs] = []\n",
    "            for single_chunk in final_chunks:\n",
    "                vec = embed_text_using_roberta(single_chunk.strip()).mean(1).detach().numpy()\n",
    "                roberta_vectors[pid][mcs].append(vec / norm(vec))\n",
    "        except Exception as ex:\n",
    "            print(pid, mcs, df.loc[mcs][\"Sent\"])\n",
    "            skip_uids.append(mcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 1385\n"
     ]
    }
   ],
   "source": [
    "mcomp_sentences = {}\n",
    "not_mcomp_sentences = {}\n",
    "\n",
    "for pid in gt_dict:\n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            mcomp_sentences[mcs] = pid\n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            not_mcomp_sentences[mcs] = pid\n",
    "print(len(mcomp_sentences), len(not_mcomp_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1505, 769)\n"
     ]
    }
   ],
   "source": [
    "testdf = df.copy()\n",
    "\n",
    "xtest = testdf.drop(columns=[\"PID\", \"Dec\", \"MComp\", \"Sent\", \"Cat\", \"SubCat\"])\n",
    "ytest = testdf.drop(columns=[\"PID\", \"Dec\", \"Sent\", \"Cat\", \"SubCat\"])\n",
    "# print(xtest.head())\n",
    "# print(ytest.head())\n",
    "\n",
    "for i in range(1, 769):\n",
    "    xtest[i] = np.nan\n",
    "\n",
    "for pid in gt_dict:\n",
    "    for mcs in gt_dict[pid][\"mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            xtest.iloc[mcs] = [mcs] + list(np.mean(roberta_vectors[pid][mcs], axis=0)[0])\n",
    "        else:\n",
    "            xtest.iloc[mcs] = [mcs] + list(np.zeros(768))\n",
    "    for mcs in gt_dict[pid][\"not_mcomp\"]:\n",
    "        if not mcs in skip_uids:\n",
    "            xtest.iloc[mcs] = [mcs] + list(np.mean(roberta_vectors[pid][mcs], axis=0)[0])\n",
    "        else:\n",
    "            xtest.iloc[mcs] = [mcs] + list(np.zeros(768))\n",
    "print(xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.054948</td>\n",
       "      <td>0.030747</td>\n",
       "      <td>-0.048182</td>\n",
       "      <td>-0.060914</td>\n",
       "      <td>-0.017881</td>\n",
       "      <td>0.025607</td>\n",
       "      <td>0.028097</td>\n",
       "      <td>-0.045269</td>\n",
       "      <td>-0.072596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018901</td>\n",
       "      <td>-0.025878</td>\n",
       "      <td>-0.027610</td>\n",
       "      <td>0.018274</td>\n",
       "      <td>-0.022146</td>\n",
       "      <td>0.036296</td>\n",
       "      <td>0.016070</td>\n",
       "      <td>0.018421</td>\n",
       "      <td>0.076425</td>\n",
       "      <td>0.005679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.033591</td>\n",
       "      <td>-0.007541</td>\n",
       "      <td>-0.022156</td>\n",
       "      <td>-0.005742</td>\n",
       "      <td>0.007110</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.011970</td>\n",
       "      <td>0.034138</td>\n",
       "      <td>-0.067476</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014143</td>\n",
       "      <td>-0.088997</td>\n",
       "      <td>-0.036821</td>\n",
       "      <td>0.055220</td>\n",
       "      <td>-0.036800</td>\n",
       "      <td>0.007034</td>\n",
       "      <td>0.009393</td>\n",
       "      <td>0.054371</td>\n",
       "      <td>0.041533</td>\n",
       "      <td>0.031771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID         1         2         3         4         5         6         7  \\\n",
       "0    0 -0.054948  0.030747 -0.048182 -0.060914 -0.017881  0.025607  0.028097   \n",
       "1    1  0.033591 -0.007541 -0.022156 -0.005742  0.007110  0.002366  0.011970   \n",
       "\n",
       "          8         9  ...       759       760       761       762       763  \\\n",
       "0 -0.045269 -0.072596  ...  0.018901 -0.025878 -0.027610  0.018274 -0.022146   \n",
       "1  0.034138 -0.067476  ... -0.014143 -0.088997 -0.036821  0.055220 -0.036800   \n",
       "\n",
       "        764       765       766       767       768  \n",
       "0  0.036296  0.016070  0.018421  0.076425  0.005679  \n",
       "1  0.007034  0.009393  0.054371  0.041533  0.031771  \n",
       "\n",
       "[2 rows x 769 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296, 769)\n",
      "(296, 2)\n"
     ]
    }
   ],
   "source": [
    "# xtrain = pd.DataFrame(columns=[\"UID\"]+[str(x) for x in range(1,769)])\n",
    "# ytrain = pd.DataFrame(columns=[\"UID\", \"MComp\"])\n",
    "xtlist = []\n",
    "ytlist = []\n",
    "\n",
    "for i in train_pool_uid_vecs:\n",
    "    xtlist.append([i] + list(np.mean(train_pool_uid_vecs[i], axis=0)[0]))\n",
    "    ytlist.append([i, mc_nmc_fake[i]])\n",
    "\n",
    "xtrain = pd.DataFrame(xtlist)\n",
    "ytrain = pd.DataFrame(ytlist)\n",
    "\n",
    "xtrain = xtrain.rename(columns={0: 'UID'})\n",
    "ytrain = ytrain.rename(columns={0: 'UID', 1: 'MComp'})\n",
    "\n",
    "print(xtrain.shape)\n",
    "print(ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>768</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.032493</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>-0.051368</td>\n",
       "      <td>-0.026692</td>\n",
       "      <td>-0.056419</td>\n",
       "      <td>0.039394</td>\n",
       "      <td>-0.007617</td>\n",
       "      <td>0.018629</td>\n",
       "      <td>-0.056210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008570</td>\n",
       "      <td>-0.049489</td>\n",
       "      <td>0.024508</td>\n",
       "      <td>0.012797</td>\n",
       "      <td>-0.052498</td>\n",
       "      <td>0.011841</td>\n",
       "      <td>0.006839</td>\n",
       "      <td>0.065396</td>\n",
       "      <td>0.068812</td>\n",
       "      <td>-0.004911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.008111</td>\n",
       "      <td>0.037128</td>\n",
       "      <td>-0.026234</td>\n",
       "      <td>-0.042768</td>\n",
       "      <td>-0.041092</td>\n",
       "      <td>0.044333</td>\n",
       "      <td>0.033616</td>\n",
       "      <td>0.020117</td>\n",
       "      <td>-0.067405</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026082</td>\n",
       "      <td>-0.120543</td>\n",
       "      <td>-0.035640</td>\n",
       "      <td>0.029649</td>\n",
       "      <td>-0.085322</td>\n",
       "      <td>0.011763</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.046458</td>\n",
       "      <td>0.045858</td>\n",
       "      <td>0.035250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID         1         2         3         4         5         6         7  \\\n",
       "0    0 -0.032493  0.030220 -0.051368 -0.026692 -0.056419  0.039394 -0.007617   \n",
       "1    1 -0.008111  0.037128 -0.026234 -0.042768 -0.041092  0.044333  0.033616   \n",
       "\n",
       "          8         9  ...       759       760       761       762       763  \\\n",
       "0  0.018629 -0.056210  ... -0.008570 -0.049489  0.024508  0.012797 -0.052498   \n",
       "1  0.020117 -0.067405  ... -0.026082 -0.120543 -0.035640  0.029649 -0.085322   \n",
       "\n",
       "        764       765       766       767       768  \n",
       "0  0.011841  0.006839  0.065396  0.068812 -0.004911  \n",
       "1  0.011763  0.000908  0.046458  0.045858  0.035250  \n",
       "\n",
       "[2 rows x 769 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>MComp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID  MComp\n",
       "0    0      1\n",
       "1    1      1"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ======================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing & results----------------\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# nlp preprocessing\n",
    "import spacy\n",
    "\n",
    "# Models-------------------------\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "import sklearn.gaussian_process.kernels as kls\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "\n",
    "# for visualizing ---------------\n",
    "import pydotplus\n",
    "from sklearn import tree\n",
    "from six import StringIO \n",
    "from IPython.display import Image, display\n",
    "import seaborn as sns\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# General purpose\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dict = {\n",
    "    'DecisionTree': {\"model\": DecisionTreeClassifier(random_state=42), \"params\": {'max_depth': list(range(10, 250, 20))}},\n",
    "    'RandomForest': {\"model\": RandomForestClassifier(random_state=42),\n",
    "                     \"params\": {'n_estimators': list(range(5, 100, 5)), 'max_depth': list(range(10, 250, 20))}},\n",
    "    'LogisticR_L1': {\"model\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "                     \"params\": {'penalty': ['l1'], 'solver': ['liblinear', 'saga']}},\n",
    "    'LogisticR_L2': {\"model\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "                     \"params\": {'penalty': ['l2'], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}},\n",
    "    'LogisticR': {\"model\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "                  \"params\": {'penalty': ['none'], 'solver': ['newton-cg', 'lbfgs', 'sag', 'saga']}},\n",
    "    'RidgeClf': {\"model\": RidgeClassifier(max_iter=1000), \"params\": {}},\n",
    "    'SVC_linear': {\"model\": SVC(random_state=42), \"params\": {'kernel': ['linear'], \n",
    "                                                             'C': [0.5, 1.0, 1.5, 2.0, 2.5]}},\n",
    "    'SVC_poly': {\"model\": SVC(random_state=42),\n",
    "                 \"params\": {'kernel': ['poly'], 'degree': [3, 4, 5], 'gamma': ['scale', 'auto'], \n",
    "                            'C': [0.5, 1.0, 1.5, 2.0, 2.5]}},\n",
    "    'SVC_others': {\"model\": SVC(random_state=42), \"params\": {'kernel': ['rbf', 'sigmoid'], \n",
    "                                                             'gamma': ['scale', 'auto'], \n",
    "                                                             'C': [0.5, 1.0, 1.5, 2.0, 2.5]}},\n",
    "    'GussianNB': {\"model\": GaussianNB(), \"params\": {}},\n",
    "    'KNN': {\"model\": KNeighborsClassifier(), \"params\": {'n_neighbors': list(range(1, 20))}},\n",
    "    'GaussianProcessClf': {\"model\": GaussianProcessClassifier(random_state=42, kernel=kls.RBF()), \"params\": {}},\n",
    "    'Bagging_SVC': {\"model\": BaggingClassifier(random_state=42), \"params\": {'n_estimators': list(range(5, 100, 5)),\n",
    "                                                                            'base_estimator': [SVC(kernel='linear'),\n",
    "                                                                                               SVC(kernel='poly',\n",
    "                                                                                                   degree=3,\n",
    "                                                                                                   gamma='scale')]}},\n",
    "    'BaggingDT': {\"model\": BaggingClassifier(random_state=42), \"params\": {'n_estimators': list(range(5, 100, 5)),\n",
    "                                                                          'base_estimator': [\n",
    "                                                                              DecisionTreeClassifier(random_state=42,\n",
    "                                                                                                     max_depth=10),\n",
    "                                                                              DecisionTreeClassifier(random_state=42,\n",
    "                                                                                                     max_depth=50),\n",
    "                                                                              DecisionTreeClassifier(random_state=42,\n",
    "                                                                                                     max_depth=100)]}},\n",
    "    'AdaBoost': {\"model\": AdaBoostClassifier(random_state=42), \"params\": {'n_estimators': list(range(5, 100, 5)),\n",
    "                                                                          'base_estimator': [DecisionTreeClassifier(\n",
    "                                                                                                 random_state=42,\n",
    "                                                                                                 max_depth=10),\n",
    "                                                                                             DecisionTreeClassifier(\n",
    "                                                                                                 random_state=42,\n",
    "                                                                                                 max_depth=50),\n",
    "                                                                                             DecisionTreeClassifier(\n",
    "                                                                                                 random_state=42,\n",
    "                                                                                                 max_depth=100)]}},\n",
    "    'ExtraTrees': {\"model\": ExtraTreesClassifier(random_state=42), \"params\": {'n_estimators': list(range(5, 105, 5)), \n",
    "                                                                              'max_depth': [10, 50, 100, 250, 400]}},\n",
    "    'MLP_l1': {\"model\": MLPClassifier(random_state=42), \"params\": {'hidden_layer_sizes': [(x,) for x in \n",
    "                                                                                          range(50, 600, 100)], \n",
    "                                                                  'activation': ['logistic', 'tanh', 'relu'],\n",
    "                                                                  'solver': ['adam', 'sgd'], 'early_stopping': \n",
    "                                                                   [True]}},\n",
    "    'MLP_l2': {\"model\": MLPClassifier(random_state=42), \"params\": {'hidden_layer_sizes': [(x, y) for x in \n",
    "                                                                                          range(50, 600, 100) \n",
    "                                                                                          for y in range(50, 360, 100)], \n",
    "                                                                  'activation': ['logistic', 'tanh', 'relu'],\n",
    "                                                                  'solver': ['adam', 'sgd'], 'early_stopping': \n",
    "                                                                                               [True]}},\n",
    "#     'MLP_l3': {\"model\": MLPClassifier(random_state=42), \"params\": {'hidden_layer_sizes': [(x, y, z) for x in \n",
    "#                                                                                           range(50, 600, 100) \n",
    "#                                                                                           for y in range(50, 600, 100)\n",
    "#                                                                                           for z in range(50, 360, 100)], \n",
    "#                                                                   'activation': ['logistic', 'tanh', 'relu'],\n",
    "#                                                                   'solver': ['adam', 'sgd'], 'early_stopping': \n",
    "#                                                                                                [True]}},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree 0.8515819209039549 {'max_depth': 10}\n",
      "RandomForest 0.9155932203389832 {'max_depth': 30, 'n_estimators': 5}\n",
      "LogisticR_L1 0.9122033898305085 {'penalty': 'l1', 'solver': 'liblinear'}\n",
      "LogisticR_L2 0.9122033898305085 {'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "LogisticR 0.9257062146892656 {'penalty': 'none', 'solver': 'newton-cg'}\n",
      "RidgeClf 0.9020903954802261 {}\n",
      "SVC_linear 0.9122033898305085 {'C': 0.5, 'kernel': 'linear'}\n",
      "SVC_poly 0.9122033898305085 {'C': 0.5, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "SVC_others 0.9122033898305085 {'C': 0.5, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "GussianNB 0.9021468926553672 {}\n",
      "KNN 0.9155932203389832 {'n_neighbors': 2}\n",
      "GaussianProcessClf 0.9122033898305085 {}\n",
      "Bagging_SVC 0.9122033898305085 {'base_estimator': SVC(kernel='poly'), 'n_estimators': 15}\n",
      "BaggingDT 0.9156497175141244 {'base_estimator': DecisionTreeClassifier(max_depth=10, random_state=42), 'n_estimators': 30}\n",
      "AdaBoost 0.8546892655367232 {'base_estimator': DecisionTreeClassifier(max_depth=10, random_state=42), 'n_estimators': 5}\n",
      "ExtraTrees 0.9155932203389832 {'max_depth': 10, 'n_estimators': 20}\n",
      "MLP_l1 0.9122033898305085 {'activation': 'logistic', 'early_stopping': True, 'hidden_layer_sizes': (50,), 'solver': 'sgd'}\n",
      "MLP_l2 0.9223728813559322 {'activation': 'tanh', 'early_stopping': True, 'hidden_layer_sizes': (350, 50), 'solver': 'adam'}\n",
      "================================================================================\n",
      "RidgeClassifier(max_iter=1000)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      1388\n",
      "           1       0.89      0.14      0.24       117\n",
      "\n",
      "    accuracy                           0.93      1505\n",
      "   macro avg       0.91      0.57      0.60      1505\n",
      "weighted avg       0.93      0.93      0.91      1505\n",
      "\n",
      "Test acc: 0.9315614617940199\n",
      "Weighted F1 score:  0.9076456642110838\n"
     ]
    }
   ],
   "source": [
    "model_results = pd.DataFrame()\n",
    "model_results['Train_Accuracy'] = None\n",
    "model_results['Test_Accuracy'] = None\n",
    "model_results['best_params'] = None\n",
    "\n",
    "# X_train_final = X_train_normalized.drop(columns=[\"ref_latest\"])\n",
    "# X_test_normalized_remgsdata = X_test_normalized.drop(columns=[\"ref_latest\"])\n",
    "# X_train_normalized_remgsdata = X_train_normalized.copy()\n",
    "# X_test_normalized_remgsdata = X_test_normalized.copy()\n",
    "\n",
    "xtrain_final = xtrain.drop(columns=[\"UID\"])\n",
    "ytrain_final = ytrain.drop(columns=[\"UID\"])\n",
    "\n",
    "xtest_final = xtest.drop(columns=[\"UID\"])\n",
    "ytest_final = ytest.drop(columns=[\"UID\"])\n",
    "\n",
    "\n",
    "best_clf_ours = None\n",
    "best_clf_val = 0\n",
    "\n",
    "for clf_name, clf in clf_dict.items():\n",
    "    classifier = GridSearchCV(clf['model'], clf['params'], n_jobs=5)\n",
    "    classifier.fit(xtrain_final, ytrain_final)\n",
    "    best_model = classifier.best_estimator_\n",
    "    print(clf_name, classifier.best_score_, classifier.best_params_)\n",
    "    \n",
    "    y_predicted = best_model.predict(xtest_final)\n",
    "    test_acc = accuracy_score(ytest_final, y_predicted)\n",
    "    \n",
    "    if test_acc > best_clf_val:\n",
    "        best_clf_val = test_acc\n",
    "        best_clf_ours = best_model\n",
    "    \n",
    "    model_results.loc[clf_name, ['Train_Accuracy', 'Test_Accuracy', 'best_params']] = [classifier.best_score_, test_acc, classifier.best_params_]\n",
    "    clsr = classification_report(ytest_final, y_predicted)\n",
    "\n",
    "print(\"================================================================================\")\n",
    "print(best_clf_ours)\n",
    "best_y_hat = best_clf_ours.predict(xtest_final)\n",
    "clsr = classification_report(ytest_final, best_y_hat)\n",
    "print(clsr)\n",
    "test_acc = accuracy_score(ytest_final, best_y_hat)\n",
    "print(\"Test acc:\", test_acc )\n",
    "print(\"Weighted F1 score: \", f1_score(ytest_final, best_y_hat, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9315614617940199"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9104834491519092, 0.5676556072809675, 0.6006054750402576, None)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(ytest_final, best_y_hat, average='macro')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree 0.8515819209039549 {'max_depth': 10}\n",
      "RandomForest 0.9155932203389832 {'max_depth': 30, 'n_estimators': 5}\n",
      "LogisticR_L1 0.9122033898305085 {'penalty': 'l1', 'solver': 'liblinear'}\n",
      "LogisticR_L2 0.9122033898305085 {'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "LogisticR 0.9257062146892656 {'penalty': 'none', 'solver': 'newton-cg'}\n",
      "RidgeClf 0.9020903954802261 {}\n",
      "SVC_linear 0.9122033898305085 {'C': 0.5, 'kernel': 'linear'}\n",
      "SVC_poly 0.9122033898305085 {'C': 0.5, 'degree': 3, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "SVC_others 0.9122033898305085 {'C': 0.5, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "GussianNB 0.9021468926553672 {}\n",
      "KNN 0.9155932203389832 {'n_neighbors': 2}\n",
      "GaussianProcessClf 0.9122033898305085 {}\n",
      "Bagging_SVC 0.9122033898305085 {'base_estimator': SVC(kernel='poly'), 'n_estimators': 15}\n",
      "BaggingDT 0.9156497175141244 {'base_estimator': DecisionTreeClassifier(max_depth=10, random_state=42), 'n_estimators': 30}\n",
      "AdaBoost 0.8546892655367232 {'base_estimator': DecisionTreeClassifier(max_depth=10, random_state=42), 'n_estimators': 5}\n",
      "ExtraTrees 0.9155932203389832 {'max_depth': 10, 'n_estimators': 20}\n",
      "MLP_l1 0.9122033898305085 {'activation': 'logistic', 'early_stopping': True, 'hidden_layer_sizes': (50,), 'solver': 'sgd'}\n",
      "MLP_l2 0.9223728813559322 {'activation': 'tanh', 'early_stopping': True, 'hidden_layer_sizes': (350, 50), 'solver': 'adam'}\n",
      "================================================================================\n",
      "GaussianNB()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95      1388\n",
      "           1       0.43      0.58      0.49       117\n",
      "\n",
      "    accuracy                           0.91      1505\n",
      "   macro avg       0.70      0.76      0.72      1505\n",
      "weighted avg       0.92      0.91      0.91      1505\n",
      "\n",
      "Test acc: 0.9069767441860465\n",
      "Weighted F1 score:  0.9133400840111751\n"
     ]
    }
   ],
   "source": [
    "model_results = pd.DataFrame()\n",
    "model_results['Train_Accuracy'] = None\n",
    "model_results['Test_Accuracy'] = None\n",
    "model_results['best_params'] = None\n",
    "\n",
    "# X_train_final = X_train_normalized.drop(columns=[\"ref_latest\"])\n",
    "# X_test_normalized_remgsdata = X_test_normalized.drop(columns=[\"ref_latest\"])\n",
    "# X_train_normalized_remgsdata = X_train_normalized.copy()\n",
    "# X_test_normalized_remgsdata = X_test_normalized.copy()\n",
    "\n",
    "xtrain_final = xtrain.drop(columns=[\"UID\"])\n",
    "ytrain_final = ytrain.drop(columns=[\"UID\"])\n",
    "\n",
    "xtest_final = xtest.drop(columns=[\"UID\"])\n",
    "ytest_final = ytest.drop(columns=[\"UID\"])\n",
    "\n",
    "\n",
    "best_clf_ours = None\n",
    "best_clf_val = 0\n",
    "\n",
    "for clf_name, clf in clf_dict.items():\n",
    "    classifier = GridSearchCV(clf['model'], clf['params'], n_jobs=5)\n",
    "    classifier.fit(xtrain_final, ytrain_final)\n",
    "    best_model = classifier.best_estimator_\n",
    "    print(clf_name, classifier.best_score_, classifier.best_params_)\n",
    "    \n",
    "    y_predicted = best_model.predict(xtest_final)\n",
    "    test_acc_macro = precision_recall_fscore_support(ytest_final, y_predicted, average='macro')[2]#accuracy_score(ytest_final, y_predicted)\n",
    "    \n",
    "    if test_acc_macro > best_clf_val:\n",
    "        best_clf_val = test_acc_macro\n",
    "        best_clf_ours = best_model\n",
    "    \n",
    "    model_results.loc[clf_name, ['Train_Accuracy', 'Test_Accuracy', 'best_params']] = [classifier.best_score_, test_acc_macro, classifier.best_params_]\n",
    "    clsr = classification_report(ytest_final, y_predicted)\n",
    "\n",
    "print(\"================================================================================\")\n",
    "print(best_clf_ours)\n",
    "best_y_hat = best_clf_ours.predict(xtest_final)\n",
    "clsr = classification_report(ytest_final, best_y_hat)\n",
    "print(clsr)\n",
    "test_acc = accuracy_score(ytest_final, best_y_hat)\n",
    "print(\"Test acc:\", test_acc )\n",
    "print(\"Weighted F1 score: \", f1_score(ytest_final, best_y_hat, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
