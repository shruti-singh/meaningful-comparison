{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/shruti/Desktop/iitgn/courses/SEM2/ML/Project/code/PaperAcceptancePrediction/ICLR data/masterdata_unbalanced/\"\n",
    "\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "rev_dict = {}\n",
    "paper_dict = {}\n",
    "dec_dict = {}\n",
    "iclr_arxiv_map = {}\n",
    "\n",
    "for y in years:\n",
    "    rev_dict[y] = pd.read_pickle(data_path + \"off_rev_dict_{}.pkl\".format(y))\n",
    "    paper_dict[y] = pd.read_pickle(data_path + \"papers_{}.pkl\".format(y))\n",
    "    dec_dict[y] = pd.read_pickle(data_path + \"paper_decision_dict_{}.pkl\".format(y))\n",
    "\n",
    "iclr_arxiv_map = pd.read_pickle(\"./data/iclr_arxiv_map.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were thinking about this problem for EACL:\n",
    "    \n",
    "Given a review text, can we identify the text span that talks about the comparisons.\n",
    "After identification, can we identify different aspects associated with it:\n",
    "1. Is it positive or negative?\n",
    "2. What module does it talks about (dataset, model, metric, etc.)\n",
    "3. Does it suggests some papers to cite?\n",
    "4. We can also suggest the overlap between different reviews in terms of comparison comments. \n",
    "This can help us understanding how much reviews are coherent\n",
    "\n",
    "Motivation: The system would be more useful for metareviewers. Authors can also be benefited, but they \n",
    "can always read the review and get this info.  We can always say that with the increase in exponential \n",
    "growth in conference submissions, the evaluation task for meta-reviewers is becoming more and more hard. \n",
    "The current system would help the metareviews task in quickly glancing the multiple reviews for a given paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a test set of reviews for manual annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019_HylsgnCcFQ\n",
      "2020_H1x-3xSKDr\n",
      "2019_rJedbn0ctQ\n"
     ]
    }
   ],
   "source": [
    "test_set = [\"2020_r1e_FpNFDr\", \"2020_SyevYxHtDB\", \"2019_SyxZJn05YX\", \"2019_BJx0sjC5FX\", \"2018_SkZxCk-0Z\", \n",
    "            \"2018_HkfXMz-Ab\", \"2017_B1ckMDqlg\", \"2017_HJ0NvFzxl\", \"2020_B1lsXREYvr\", \"2020_rkltE0VKwH\", \n",
    "            \"2018_Hki-ZlbA-\", \"2018_rJBiunlAW\", \"2019_rJzoujRct7\", \"2019_SJf_XhCqKm\", \"2017_Bk0MRI5lg\",\n",
    "            \"2017_BJ9fZNqle\"]\n",
    "\n",
    "\n",
    "# list(iclr_arxiv_map.keys())[0:20]\n",
    "counter = 0\n",
    "\n",
    "for k, v in iclr_arxiv_map.items():\n",
    "    if not k in test_set:\n",
    "        if v[\"label\"] == \"Reject\":\n",
    "            print(k)\n",
    "            counter += 1\n",
    "        if counter > 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_augmentation_2ndphase = [\n",
    "    # Rejected\n",
    "    \"2020_Byg79h4tvB\", \"2020_r1eX1yrKwB\", \"2019_HyVxPsC9tm\", \"2019_H1lFZnR5YX\", \"2018_SyYYPdg0-\", \n",
    "    \"2018_HyHmGyZCZ\", \"2017_BJAA4wKxg\", \"2017_r1y1aawlg\",\n",
    "    # Accepted\n",
    "    \"2020_BkeWw6VFwr\", \"2020_HkgsPhNYPS\", \"2019_HylTBhA5tQ\", \"2019_B1l08oAct7\", \"2018_H135uzZ0-\", \n",
    "    \"2018_HyUNwulC-\", \"2017_S1_pAu9xl\", \"2017_H1oyRlYgg\"\n",
    "]\n",
    "\n",
    "train_set = [\n",
    "    # Rejected\n",
    "    \"2020_SJegkkrYPS\", \"2020_ryen_CEFwr\", \"2019_ByxAOoR5K7\", \"2019_r1eO_oCqtQ\", \"2018_Bk6qQGWRb\", \n",
    "    \"2018_H1LAqMbRW\", \"2017_ByToKu9ll\", \"2017_H1Gq5Q9el\",\n",
    "    # Accepted\n",
    "    \"2020_ryxC6kSYPr\", \"2020_rJld3hEYvS\", \"2019_r1eEG20qKQ\", \"2019_HJfwJ2A5KX\", \"2018_BJ8c3f-0b\", \n",
    "    \"2018_SJx9GQb0-\",  \"2017_S1Bb3D5gg\", \"2017_HyTqHL5xg\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_list = defaultdict(dict)\n",
    "\n",
    "for k in test_set_augmentation_2ndphase:\n",
    "    if k in iclr_arxiv_map:\n",
    "        y = int(k.split(\"_\")[0])\n",
    "        k_wo_y = k.split(\"_\", 1)[1]\n",
    "        \n",
    "        sents_list[k][\"pid\"] = k\n",
    "        sents_list[k][\"sents\"] = []\n",
    "        sents_list[k][\"dec\"] = iclr_arxiv_map[k][\"label\"]\n",
    "        \n",
    "        for rev_num in rev_dict[y][k_wo_y]:\n",
    "            rev_text = rev_num[\"content\"][\"review\"]\n",
    "\n",
    "            rev_text = rev_text.strip()\n",
    "            \n",
    "            \n",
    "            rev_text = re.sub(\"(?<=[ \\(])e[\\.]?g[\\.]?:? \", \"eg: \", rev_text)\n",
    "            rev_text = re.sub(\" et[\\.]? al[\\.]\", \" et al\", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])i[\\.]?e[\\.] \", \"ie \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])Fig[\\.]? \", \"Figure \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])fig[\\.]? \", \"figure \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])Eq[\\.]? \", \"Equation \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])Alg\\. \", \"Algorithm \", rev_text)\n",
    "            rev_text = re.sub(\"Eq[\\.]? \", \"Equation \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])eq\\. \", \"equation \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])Sec[\\.]? \", \"Section \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])sec\\. \", \"section \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=: [0-9])(\\.) \", \")\", rev_text)\n",
    "            rev_text = re.sub(\"(?<=; [0-9])(\\.) \", \")\", rev_text)\n",
    "            rev_text = re.sub(\"(?<=\\n[0-9])(\\.) \", \")\", rev_text)\n",
    "            ####rev_text = re.sub(\"(?<=[0-9])(\\.) \", \") \", rev_text)\n",
    "            rev_text = re.sub(\"\\n\\\\n\", \"\\n\", rev_text)\n",
    "\n",
    "            sent_text = nltk.sent_tokenize(rev_text)\n",
    "            for s in sent_text:\n",
    "                sents_list[k][\"sents\"].append(s)\n",
    "    else:\n",
    "        print(\"Not found: %s, use another key\"%k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['2017_S1_pAu9xl', '2018_SyYYPdg0-', '2017_BJAA4wKxg', '2019_HyVxPsC9tm', '2019_HylTBhA5tQ', '2019_B1l08oAct7', '2018_H135uzZ0-', '2017_H1oyRlYgg', '2017_r1y1aawlg', '2020_r1eX1yrKwB', '2020_Byg79h4tvB', '2019_H1lFZnR5YX', '2020_BkeWw6VFwr', '2018_HyHmGyZCZ', '2018_HyUNwulC-', '2020_HkgsPhNYPS']),\n",
       " 16)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_list.keys(), len(list(sents_list.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>dec</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017_S1_pAu9xl</td>\n",
       "      <td>Accept</td>\n",
       "      <td>The paper shows a different approach to a tern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017_S1_pAu9xl</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Strengths:\\n1)The paper shows performance impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017_S1_pAu9xl</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Weaknesses:\\n1)The paper is very incremental.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017_S1_pAu9xl</td>\n",
       "      <td>Accept</td>\n",
       "      <td>2)The paper is addressed to a very narrow audi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017_S1_pAu9xl</td>\n",
       "      <td>Accept</td>\n",
       "      <td>The paper very clearly assumes that the reader...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              pid     dec                                               sent\n",
       "0  2017_S1_pAu9xl  Accept  The paper shows a different approach to a tern...\n",
       "1  2017_S1_pAu9xl  Accept  Strengths:\\n1)The paper shows performance impr...\n",
       "2  2017_S1_pAu9xl  Accept      Weaknesses:\\n1)The paper is very incremental.\n",
       "3  2017_S1_pAu9xl  Accept  2)The paper is addressed to a very narrow audi...\n",
       "4  2017_S1_pAu9xl  Accept  The paper very clearly assumes that the reader..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['pid', 'dec', 'sent'])\n",
    "\n",
    "i = 0\n",
    "for k in sents_list:\n",
    "    dec = sents_list[k][\"dec\"]\n",
    "    pid = sents_list[k][\"pid\"]\n",
    "    for s in sents_list[k][\"sents\"]:\n",
    "        df.loc[i] = [pid, dec, s]\n",
    "        i += 1\n",
    "\n",
    "df.to_excel(\"testset_16_phase2Ann.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_list = defaultdict(dict)\n",
    "\n",
    "for k in train_set:\n",
    "    if k in iclr_arxiv_map:\n",
    "        y = int(k.split(\"_\")[0])\n",
    "        k_wo_y = k.split(\"_\", 1)[1]\n",
    "        \n",
    "        sents_list[k][\"pid\"] = k\n",
    "        sents_list[k][\"sents\"] = []\n",
    "        sents_list[k][\"dec\"] = iclr_arxiv_map[k][\"label\"]\n",
    "        \n",
    "        for rev_num in rev_dict[y][k_wo_y]:\n",
    "            rev_text = rev_num[\"content\"][\"review\"]\n",
    "\n",
    "            rev_text = rev_text.strip()\n",
    "            \n",
    "            \n",
    "            rev_text = re.sub(\"(?<=[ \\(])e[\\.]?g[\\.]?:? \", \"eg: \", rev_text)\n",
    "            rev_text = re.sub(\" et[\\.]? al[\\.]\", \" et al\", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])i[\\.]?e[\\.] \", \"ie \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])Fig[\\.]? \", \"Figure \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])fig[\\.]? \", \"figure \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])Eq[\\.]? \", \"Equation \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])Alg\\. \", \"Algorithm \", rev_text)\n",
    "            rev_text = re.sub(\"Eq[\\.]? \", \"Equation \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])eq\\. \", \"equation \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])Sec[\\.]? \", \"Section \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])sec\\. \", \"section \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=: [0-9])(\\.) \", \")\", rev_text)\n",
    "            rev_text = re.sub(\"(?<=; [0-9])(\\.) \", \")\", rev_text)\n",
    "            rev_text = re.sub(\"(?<=\\n[0-9])(\\.) \", \")\", rev_text)\n",
    "            ####rev_text = re.sub(\"(?<=[0-9])(\\.) \", \") \", rev_text)\n",
    "            rev_text = re.sub(\"\\n\\\\n\", \"\\n\", rev_text)\n",
    "\n",
    "            sent_text = nltk.sent_tokenize(rev_text)\n",
    "            for s in sent_text:\n",
    "                sents_list[k][\"sents\"].append(s)\n",
    "    else:\n",
    "        print(\"Not found: %s, use another key\"%k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['2019_r1eO_oCqtQ', '2019_HJfwJ2A5KX', '2017_ByToKu9ll', '2017_HyTqHL5xg', '2018_H1LAqMbRW', '2020_ryen_CEFwr', '2020_ryxC6kSYPr', '2017_H1Gq5Q9el', '2020_SJegkkrYPS', '2017_S1Bb3D5gg', '2020_rJld3hEYvS', '2018_SJx9GQb0-', '2018_Bk6qQGWRb', '2019_r1eEG20qKQ', '2018_BJ8c3f-0b', '2019_ByxAOoR5K7']),\n",
       " 16)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_list.keys(), len(list(sents_list.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>dec</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019_r1eO_oCqtQ</td>\n",
       "      <td>Reject</td>\n",
       "      <td>In the vein of recent work on learning “tickin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019_r1eO_oCqtQ</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The performance of the modified g-LSTM is comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019_r1eO_oCqtQ</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The authors argue that g-LSTM results in bette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019_r1eO_oCqtQ</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Additionally, it is proposed that one can redu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019_r1eO_oCqtQ</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Finally, a technique for gradually transitioni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pid     dec                                               sent\n",
       "0  2019_r1eO_oCqtQ  Reject  In the vein of recent work on learning “tickin...\n",
       "1  2019_r1eO_oCqtQ  Reject  The performance of the modified g-LSTM is comp...\n",
       "2  2019_r1eO_oCqtQ  Reject  The authors argue that g-LSTM results in bette...\n",
       "3  2019_r1eO_oCqtQ  Reject  Additionally, it is proposed that one can redu...\n",
       "4  2019_r1eO_oCqtQ  Reject  Finally, a technique for gradually transitioni..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['pid', 'dec', 'sent'])\n",
    "\n",
    "i = 0\n",
    "for k in sents_list:\n",
    "    dec = sents_list[k][\"dec\"]\n",
    "    pid = sents_list[k][\"pid\"]\n",
    "    for s in sents_list[k][\"sents\"]:\n",
    "        df.loc[i] = [pid, dec, s]\n",
    "        i += 1\n",
    "\n",
    "df.to_excel(\"trainset_16_phase2Ann.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\"2020_r1e_FpNFDr\", \"2020_SyevYxHtDB\", \"2019_SyxZJn05YX\", \"2019_BJx0sjC5FX\", \"2018_SkZxCk-0Z\", \n",
    "            \"2018_HkfXMz-Ab\", \"2017_B1ckMDqlg\", \"2017_HJ0NvFzxl\", \"2020_B1lsXREYvr\", \"2020_rkltE0VKwH\", \n",
    "            \"2018_Hki-ZlbA-\", \"2018_rJBiunlAW\", \"2019_rJzoujRct7\", \"2019_SJf_XhCqKm\", \"2017_Bk0MRI5lg\",\n",
    "            \"2017_BJ9fZNqle\"]\n",
    "\n",
    "sents_list = defaultdict(dict)\n",
    "\n",
    "for k in test_set:\n",
    "    if k in iclr_arxiv_map:\n",
    "        y = int(k.split(\"_\")[0])\n",
    "        k_wo_y = k.split(\"_\", 1)[1]\n",
    "        \n",
    "        sents_list[k][\"pid\"] = k\n",
    "        sents_list[k][\"sents\"] = []\n",
    "        sents_list[k][\"dec\"] = iclr_arxiv_map[k][\"label\"]\n",
    "        \n",
    "        for rev_num in rev_dict[y][k_wo_y]:\n",
    "            rev_text = rev_num[\"content\"][\"review\"]\n",
    "\n",
    "            rev_text = rev_text.strip()\n",
    "            \n",
    "            \n",
    "            rev_text = re.sub(\"(?<=[ \\(])e[\\.]?g[\\.]?:? \", \"eg: \", rev_text)\n",
    "            rev_text = re.sub(\" et[\\.]? al[\\.]\", \" et al\", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])i[\\.]?e[\\.] \", \"ie \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])Fig[\\.]? \", \"Figure \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])fig[\\.]? \", \"figure \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])Eq[\\.]? \", \"Equation \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])Alg\\. \", \"Algorithm \", rev_text)\n",
    "            rev_text = re.sub(\"Eq[\\.]? \", \"Equation \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])eq\\. \", \"equation \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])Sec[\\.]? \", \"Section \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])sec\\. \", \"section \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=: [0-9])(\\.) \", \")\", rev_text)\n",
    "            rev_text = re.sub(\"(?<=; [0-9])(\\.) \", \")\", rev_text)\n",
    "            ####rev_text = re.sub(\"(?<=[0-9])(\\.) \", \") \", rev_text)\n",
    "            rev_text = re.sub(\"\\n\\\\n\", \"\\n\", rev_text)\n",
    "\n",
    "            sent_text = nltk.sent_tokenize(rev_text)\n",
    "            for s in sent_text:\n",
    "                sents_list[k][\"sents\"].append(s)\n",
    "    else:\n",
    "        print(\"Not found: %s, use another key\"%k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['2019_SJf_XhCqKm', '2017_Bk0MRI5lg', '2020_SyevYxHtDB', '2018_rJBiunlAW', '2020_rkltE0VKwH', '2018_Hki-ZlbA-', '2019_BJx0sjC5FX', '2020_r1e_FpNFDr', '2020_B1lsXREYvr', '2018_SkZxCk-0Z', '2019_rJzoujRct7', '2018_HkfXMz-Ab', '2017_BJ9fZNqle', '2019_SyxZJn05YX', '2017_B1ckMDqlg', '2017_HJ0NvFzxl']),\n",
       " 16)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_list.keys(), len(list(sents_list.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['pid', 'dec', 'sent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the test set of 16 reviews for manual annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for k in sents_list:\n",
    "    dec = sents_list[k][\"dec\"]\n",
    "    pid = sents_list[k][\"pid\"]\n",
    "    for s in sents_list[k][\"sents\"]:\n",
    "        df.loc[i] = [pid, dec, s]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>dec</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The authors propose to use k-DPP to select a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>This paper covers the related work nicely, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The rest of the paper are also clearly written.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>However, I have some concerns about the propos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>- It is not clear how to define the kernel, th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pid     dec                                               sent\n",
       "0  2019_SJf_XhCqKm  Reject  The authors propose to use k-DPP to select a s...\n",
       "1  2019_SJf_XhCqKm  Reject  This paper covers the related work nicely, wit...\n",
       "2  2019_SJf_XhCqKm  Reject    The rest of the paper are also clearly written.\n",
       "3  2019_SJf_XhCqKm  Reject  However, I have some concerns about the propos...\n",
       "4  2019_SJf_XhCqKm  Reject  - It is not clear how to define the kernel, th..."
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"testset_16.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the initial set of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/initialsetmcomp.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117,          pid                                               sent  mcomp\n",
       " 0  S1HcOI5le  The idea of the paper is interesting there are...      1\n",
       " 1  S1HcOI5le  It's not clear how this method compares agains...      1\n",
       " 2  S1HcOI5le  Measure: Accuracy difference does not look lik...      1\n",
       " 3  S1HcOI5le  Instead the authors could position this work a...      1\n",
       " 4  S1HcOI5le  If the authors care to compare their approach ...      1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size, df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use above to find sentences from 2020 reviews for 10 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_dict = defaultdict(list)\n",
    "stop = 1\n",
    "\n",
    "for y in [2020]:\n",
    "    for k in rev_dict[y]:\n",
    "        \n",
    "        if stop > 100:\n",
    "            break\n",
    "        stop += 1 \n",
    "            \n",
    "        year_key = str(y) + \"_\" + k\n",
    "        if year_key in iclr_arxiv_map:\n",
    "            for rev_num in rev_dict[y][k]:\n",
    "                rev_text = rev_num[\"content\"][\"review\"]\n",
    "                \n",
    "                rev_text = re.sub(\" e[\\.]?g[\\.]?:? \", \" eg: \", rev_text)\n",
    "                rev_text = re.sub(\" et[\\.]? al[\\.]\", \" et al\", rev_text)\n",
    "                rev_text = re.sub(\" i[\\.]?e[\\.] \", \" ie \", rev_text)\n",
    "                rev_text = re.sub(\"\\\\n\", \" \", rev_text)\n",
    "                \n",
    "                sent_text = nltk.sent_tokenize(rev_text)\n",
    "                \n",
    "                for s in sent_text:\n",
    "                    sents_dict[year_key].append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/2020_test_sents.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sents_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/2020_test_sents.pkl\", \"rb\") as f:\n",
    "    sents_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSim with USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_mcomp_set = list(df[\"sent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_embs = embed(initial_mcomp_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "print(initial_embs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_embeddings = defaultdict(list)\n",
    "\n",
    "for k, v in sents_dict.items():\n",
    "    rev_embeddings[k] = embed(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2020_Syx-bCEFPS', '2020_Bkf4XgrKvS', '2020_Sygn20VtwH', '2020_ryl0cAVtPH', '2020_SJe3KCNKPr', '2020_Bylx-TNKvH', '2020_r1lEd64YwH', '2020_rJx1Na4Fwr', '2020_SklOUpEYvB', '2020_H1ebhnEYDH', '2020_rJx7wlSYvB', '2020_rJxAo2VYwr', '2020_HJe_yR4Fwr', '2020_ryl3ygHYDB', '2020_SJxWS64FwH', '2020_HygDF1rYDB', '2020_H1ekF2EYDH', '2020_r1lnigSFDr', '2020_ryx4TlHKDS', '2020_S1xJikHtDH', '2020_Bkl8YR4YDB', '2020_B1eP504YDr', '2020_SJeLIgBKPS', '2020_Hyg9anEFPS', '2020_HJeTo2VFwH', '2020_BklhsgSFvB', '2020_BylaUTNtPS', '2020_rylfl6VFDH', '2020_rkg8xTEtvB', '2020_BkgUB1SYPS', '2020_BkgZxpVFvH', '2020_SJeC2TNYwB', '2020_Hke0oa4KwS', '2020_HJlWWJSFDH', '2020_rJg3zxBYwH', '2020_B1elqkrKPH', '2020_rylvAA4YDB', '2020_SJgob6NKvH', '2020_Hye4WaVYwr', '2020_SJgs8TVtvr', '2020_r1l1myStwr', '2020_rklEj2EFvB', '2020_HkgsPhNYPS', '2020_B1gXYR4YDH', '2020_BJlkgaNKvr', '2020_B1l0wp4tvr', '2020_HJedXaEtvS', '2020_HJxrVA4FDS', '2020_rJx8I1rFwr', '2020_r1ltgp4FwS', '2020_SJlVY04FwH', '2020_r1lczkHKPr', '2020_HyljzgHtwS', '2020_BygpAp4Ywr', '2020_BkxFi2VYvS', '2020_SJeQi1HKDH', '2020_ryljMpNtwr', '2020_HJloElBYvB', '2020_Bkl5kxrKDr', '2020_Hyx5qhEYvH', '2020_SJe5P6EYvS', '2020_BJg73xHtvr', '2020_Ske9VANKDH', '2020_BJge3TNKwH', '2020_B1eBoJStwr', '2020_SygRikHtvS', '2020_ryeG924twB', '2020_SJlHwkBYDH', '2020_B1l3M64KwB', '2020_HygkpxStvr', '2020_SJeqs6EFvB', '2020_SyeRIgBYDB', '2020_BygfrANKvB', '2020_rkxZCJrtwS', '2020_SJgdpxHFvH', '2020_B1xIj3VYvr', '2020_HyxJhCEFDS', '2020_SJxNzgSKvH', '2020_Byx55pVKDB', '2020_r1nSxrKPH', '2020_BJgMFxrYPB', '2020_HJlvCR4KDS', '2020_r1l-VeSKwS', '2020_ByxCrerKvS', '2020_HyeaSkrYPH', '2020_Hkxbz1HKvr', '2020_SJx-j64FDr', '2020_SJeuueSYDH'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But the most challenging part of the research in this direction is that, although the theory suggests we can get nice confidence information (in population), when we use the deep neural networks, the quality of confidence estimation can be very bad (overconfidence in empirical estimation) compared with the high accuracy we can achieve.\n",
      " The paper provides a comparison of several uncertainty measures that have been proposed for neural networks.\n",
      "The authors propose the use of expected Bayes factors as aggregate measures of how much information is conveyed by a measure of uncertainty, and compares several proposed approaches based on this measure on CIFAR-10/100 and ImageNet-1k.\n",
      "The use of Bayes factors for measuring the quality of the uncertainty estimates, and the comparison of various existing methods on that measure seems useful.\n",
      "Overall, even if there is no clear evidence that one method to evaluate uncertainty is consistently better than the others, I feel that the idea of using Bayes factors to compare uncertainty measures is possibly impactful.\n",
      "Perhaps compare the Bayes ratio with RMS Calibration Error, AURRA, or AUROC for error detection.\n"
     ]
    }
   ],
   "source": [
    "for s in sents_dict[\"2020_Hke0oa4KwS\"]:\n",
    "    if s.find(\"compar\") > -1:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Summary: This paper proposes an uncertainty measure called an implied loss.',\n",
       "  'The authors suggest that it is a simple way to quantify the uncertainty of the model.',\n",
       "  'It is suggested that \"Low implied loss (uncertainty) means a high probability of correct classification on the test set.\".',\n",
       "  'They suggest that the analysis of an implied loss justifies the maximum confidence value of softmax-cross entropy.',\n",
       "  'They also extend to evaluate Top-k uncertainty (the uncertainty whether our prediction is in the Top-5 maximum values of our confidence score or not).',\n",
       "  '======================================================== Clarity: I found that this paper content does not seem to be difficult mathematically, however, it is difficult to follow the paper and here I list several parts that can potentially be improved:  1.',\n",
       "  'INTRO: the sentence \"Our implied loss interpretation justifies both methods, since we demonstrate that \"both these quantities\" are uncertainty measures.\".',\n",
       "  'What is both quantities here, the maximum softmax probability and something?',\n",
       "  '2.',\n",
       "  'INTRO: first contribution, accurate estimates of the probability that the classification of the model on a test set is correct.',\n",
       "  'What do you mean by this sentence?',\n",
       "  \"I couldn't see the accuracy of the estimation in Table 2.\",\n",
       "  '3. second contribution in INTRO: what is the meaning of \"a consistent manner\" here.',\n",
       "  'If it means \"successfully\", is there a case that your method fail?',\n",
       "  'It would be nice to be precise when describing the contribution.',\n",
       "  'Figure 2 is more like a small example but can be considered difficult to convince to the readers about the capability of the proposed method.',\n",
       "  '4.',\n",
       "  'Figure 1 can be much improved.',\n",
       "  'I found the caption is hard to understand.',\n",
       "  'Loss (y-axis) may be written as Kullback-Leibler loss to be more precise in this context (if I understood correctly).',\n",
       "  'minor comment: Figure 1(a) [colon missing], since Figure 1(b) has \":\" right after.',\n",
       "  'The authors made an effort to explain the color in the caption of Figure 1(b) but the explanation of (yellow) is missing.',\n",
       "  'The explanation of how to train a network to get Figure 1(a) seems to be missing.',\n",
       "  'It seems -log(f_(1)) should be -log(f^{sort}_(1)).',\n",
       "  \"Finally, I'm not sure what is P(Correct|x).\",\n",
       "  'Is this histogram suggested that when the U_1 is large and the P(Correct|x) is small, we get a correct prediction, or maybe this means the ratio of correct prediction under the value of loss (histogram)?',\n",
       "  'I think it would help the reader to make it more concise.',\n",
       "  'In the description in page 4 -\\\\log(p_{max}) seems never define before, was it a typo?',\n",
       "  '5.1 First page, last sentence: what do you mean by the current setting.',\n",
       "  'Before this point there is no explanation about the problem setting, only we are interested in quantifying an uncertainty measure of the networks.',\n",
       "  \"5.2 First page, last sentence:  I'm not familiar with Bayes factors, is the last sentence your contribution or it's the finding of the existing work, if it's the latter one, it would be nice to cite them.\",\n",
       "  'However, I found this sentence a bit vague: Bayes factor more informative (not sure what is the definition of informative here) better than Brier score under the situation where prob of correct classification is high (is this means high accuracy on the test set?).',\n",
       "  '6.',\n",
       "  'PRIOR-WORK: Although the authors suggested many existing works, it would be highly useful if the authors discuss the relationship between existing works and their proposed work, e.g., where to put your work in the literature.',\n",
       "  \"And since they proposed an uncertainty function, it would be nice to see a few definitions of uncertainty existing works described (doesn't need to be mathematical formulation, I think just an intuitive explanation is sufficient).\",\n",
       "  '7.',\n",
       "  'I am confused with the definition of an implied loss.',\n",
       "  'It is first defined in page 3 with a fixed k (as 1) as a loss where the prediction is correct but in Eq.',\n",
       "  '(4), it looks like a set with one element where y is the maximum prediction score, not a correct label.',\n",
       "  'Then there is U_k(x), if k!=1, is this an implied loss?',\n",
       "  'Although in (5) it is a real number, not a set anymore, I read this paper and took it as a \"yes U_k(x) is also an implied loss\".',\n",
       "  'Also it would be better to define Kullback-Leibler here to be concise and kind to the readers.',\n",
       "  \"Then in def 4.1, it's mentioned that the uncertainty measure U_k(x) is an implied loss if the event has high expected loss, does that mean if the event has low expected loss, it is not an implied loss?\",\n",
       "  'My opinion is that the authors may use a definition environment and define precisely what is an implied loss.',\n",
       "  'For example, given \"\\\\ell: X x Y \\\\to R, a correct label y \\\\in Y, and an integer k <= K (number of classes), an implied loss is defined as\" to avoid confusion.',\n",
       "  '8.',\n",
       "  'Def 4.1: U(x) without subscript is undefined (perhaps U_k(x)?).',\n",
       "  'What is an element of the set S^\\\\eps_k?',\n",
       "  'If it is a set then what is the meaning of the event S has a high expected loss?',\n",
       "  'Does the definition of implied loss after Eq.',\n",
       "  '(6) and the definition of implied loss in Eq.',\n",
       "  '(5) identical when it is Kullback-Leibler loss?',\n",
       "  '9.',\n",
       "  'Theorem 4.2: S^\\\\eps seems to be undefined without k. Moreover, how to interpret the bound in (7), it would be nicer to explain the bound after stating the theorem.',\n",
       "  'It is only an inequality that says the left-hand side is smaller or equal to the right-hand side.',\n",
       "  'And does (7) hold for any y?',\n",
       "  'And how tight is the bound?',\n",
       "  '10.',\n",
       "  'Remark 4.3: what is e_k?',\n",
       "  'what is k here if k in U is set to one?',\n",
       "  'And the name of the remark, how to interpret (8) as \"Neural networks are always overconfident?\"',\n",
       "  'Is this about neural networks or this apply to any function?',\n",
       "  '11.',\n",
       "  'Sec 5: Figure 6 is not in the main body but the appendix...',\n",
       "  'If this a mistake (and the paper is supposed to be 9 pages without ref.)',\n",
       "  'or it is supposed to be in the appendix?',\n",
       "  \"If It's in the appendix, it would be better to mention that this figure is in the appendix.\",\n",
       "  '12.',\n",
       "  \"Sec5: since Bayes factor is highly used in this paper to motivate the use of the measure, I don't think it is a good idea to put the explanation of Bayes factor in the appendix, i.e., it is impossible to understand this paper without knowing Bayes factor.\",\n",
       "  '13: Fig 6: I think it is better and kinder to use U_1, U_5 in the legend of the plot instead of f. What is the model entropy?',\n",
       "  '14.',\n",
       "  'Table 1: why there is \"-\" in CIFAR-10, it is better to clarify it in the paper (or maybe I missed it).',\n",
       "  \"I am not sure how to interpret the result, if the higher the better, does that mean the Loss is great?, I'm confused with the experimental results.\",\n",
       "  '15.',\n",
       "  'Before the beginning of 6.2: Tables 7 and 6 are in the appendix and we should state clearly it is in the appendix.',\n",
       "  '======================================================== Comments: This paper lacks of clarity and difficult to understand.',\n",
       "  'Although it is claimed to be better than existing measure, I am not convinced about that despite many experiments were conducted unfortunately.',\n",
       "  'For the criticism of using the maximum confidence of the softmax score from the softmax-cross entropy loss may not quantify the uncertainty, it is known theoretically that the score of softmax-cross entropy corresponds to p(y|x) if our prediction function achieve the global minimizer this loss function and our function class to be considered is all measurable functions (Zhang, JMLR2004: Statistical Analysis of Some Multi-Category Large Margin Classification Method).',\n",
       "  'For other losses, see Williamson+ JMLR2016: Composite Multiclass Losses.',\n",
       "  'However, it may not be accurate empirically when we use a deep network as it is reported in Guo+, 2017.',\n",
       "  'Thus, one direction is to do post-processing or finding a way to modify a network.',\n",
       "  'For U_1(k), I feel that it should suffer from the same problem as using maximum confidence score of softmax.',\n",
       "  'Extending to top-k may have a good point when discussing about uncertainty and I believe it is good to explore that direction.',\n",
       "  'For experiments, I would like to know how many trials did the authors run the experiment?',\n",
       "  'and it would be helpful to see the standard deviation of the reported value.',\n",
       "  'I believe this paper can still be improved a lot.',\n",
       "  'For these reasons, I vote to reject this paper this time.',\n",
       "  '======================================================== Minor comments: there exists the writing convention of \"Top 5\", \"top 5\", \"top5\".',\n",
       "  \"It's better to pick one way to describe it if there is no reason to make it different.\",\n",
       "  '======================================================== After the rebuttal: I have read the rebuttal.',\n",
       "  \"I appreciate the authors' effort to modify the paper.\",\n",
       "  'Also, please let me state that I totally agree that the problem the authors try to solve is indeed important and relevant for using machine learning in the real-world.',\n",
       "  'I feel that the structure of the modified version is better than the first version.',\n",
       "  'It is a nice to include the explanation of the Bayes factor in the main body.',\n",
       "  'Appendix C is also very useful for everyone to understand the Bayes factor.',\n",
       "  'As the author requested, I have read through the whole revised paper (including the appendix) carefully .',\n",
       "  'I am aware of the positive sides of this paper.',\n",
       "  'For example, it is interesting that we can find wrongly labeled data.',\n",
       "  'Utilizing the uncertainty information for several applications.',\n",
       "  'However, I found that the paper still requires a lot of modifications.',\n",
       "  'The authors have modified many of my concerns, but still several of them were not addressed.',\n",
       "  'I also emphasized the comments for parts that are unrelated to clarity (please see below).',\n",
       "  'For these reasons, I decide not to change my score.',\n",
       "  'Below are my comments after reading the rebuttal (which some of them may be overlapped with the issues that were not addressed in the revised version).',\n",
       "  '============================ General comments:  1.',\n",
       "  'Although the work is about tackling the empirical confidence estimation problem, Theorem 3.4 and Eq.',\n",
       "  '(5) are providing insights about the population, not finite data points for empirical estimation.',\n",
       "  'If we focus on the population case, it is known that the minimizer of the softmax cross-entropy loss must be a conditional probability $p(y|x)$.',\n",
       "  'Thus, it is natural that as we minimize such a loss, the probability of correct classification must be high, since we can pick the best choice for classification, i.e., argmax of $p(y|x)$.',\n",
       "  'But the most challenging part of the research in this direction is that, although the theory suggests we can get nice confidence information (in population), when we use the deep neural networks, the quality of confidence estimation can be very bad (overconfidence in empirical estimation) compared with the high accuracy we can achieve.',\n",
       "  'As a result, a finer theory that can quantify the quality of confidence estimate for the finite sample case is highly needed, but I think Theorem 3.4 fails to capture this.',\n",
       "  'I am aware that this theorem only concerns the KL loss.',\n",
       "  'I believe that even only for KL-loss, the result can be significant if we discuss a finer theory for empirical estimation.',\n",
       "  '2.',\n",
       "  'Regarding the Remark 3.5 (Neural networks are always overconfident), in my understanding, the result has no relationship with neural networks at all since it is true regardless of the hypothesis class of interest (e.g., linear models, kernel models, deep networks).',\n",
       "  'This is because it is simply the definition of a loss function (and the result in the paper focus on KL loss, but I believe we can derive for many other losses).',\n",
       "  'We know that the quality of confidence estimation of simple models can be better although the accuracy is worse.',\n",
       "  'Thus, if the objective is to visualize the problem of neural networks, Remark 3.5 does not seem to help and adding neural networks in the remark title can be misleading.',\n",
       "  'Thus, the implication of Remark 3.5 is insufficient to state that Neural networks are always overconfident.',\n",
       "  '3.',\n",
       "  'What is the advantage of an implied loss?',\n",
       "  'It seems the paper has two separate stories, the first one is implied loss (Sec.',\n",
       "  '3) and then move on to Bayes factor (Sec.4).',\n",
       "  'Then, there is an adversarial detection problem using the gradient norm in the last experiment.',\n",
       "  'From Sec.',\n",
       "  '4, the discussion about implied loss is very limited and if I understand correctly, the $-\\\\log p_\\\\max$ and $-log\\\\sum p_{1:5}$ (the latter seems to require a superscript $sort$) are the implied loss, which does not seem to have the clear advantages over other methods.',\n",
       "  'My impression is that the contributions of this paper are unclear and I do not know what is the main point of this paper.',\n",
       "  'While the abstract dedicates most space to highlight the implied loss (nothing about Bayes factor), the conclusion dedicates most space to highlight the Bayes factor.',\n",
       "  'Improving the connection between two parts may help to signify the contributions.',\n",
       "  'Despite all that, I do like the idea of introducing the Bayes factor in this paper.',\n",
       "  '============================ Clarity:  1.',\n",
       "  'Most importantly, I think the clear and solid definition of implied loss is missing.',\n",
       "  'According to Definition 3.1, \"The uncertainty measure $U_k(x)$ is an implied loss if the event $S^\\\\epsilon_k$ has high expected loss\".',\n",
       "  'I believe the implied loss is one of the most important contributions of this paper.',\n",
       "  'I am not sure what does it means by \"if the event $S^\\\\epsilon_k$ has high expected loss\" How can we define \"high expected loss\"?.',\n",
       "  'I tried to read the paper many times to understand what exactly is the implied loss, and what is the scope of implied loss (what is and what is not an implied loss).',\n",
       "  '2.',\n",
       "  'Following my first issue on clarity, what is the definition of uncertainty measure in this paper?',\n",
       "  'According to the paper, it is defined roughly as $U_k$, whose statistics allow us to better estimate $p_k$.',\n",
       "  'And in the abstract, it is emphasized that if uncertainty measure is an implied loss, then low uncertainty means a high probability of correct classification.',\n",
       "  'Is there any uncertainty measure that is not implied loss?',\n",
       "  'Also in the introduction, it seems that both softmax and the model entropy are uncertainty measures, are they implied losses?',\n",
       "  'Is MC-dropout an uncertainty measure or even an implied loss in this context?',\n",
       "  '3.',\n",
       "  'Eq.',\n",
       "  '(11) left side: I think it is useful to the expectation with respect to which variable, I believe it is $B$.',\n",
       "  'As a result, is it a typo to have $Y_i$ instead of $B_i$ on the left-hand side of (11)?',\n",
       "  '4.',\n",
       "  'Example 3.3: If we set $k=1$ according to the definition of the implied loss at the last sentence of this example.',\n",
       "  'Will it contradict $U_1(x)$ defined just right before that sentence?',\n",
       "  'Because $y_w$ will become the $2$-nd ranked label, which I feel it is different from $U_1(x)$ defined in Example 3.3  5.',\n",
       "  'I think it is better to clearly state that the figures/tables are in the appendix when referring to them from the main body.',\n",
       "  'I saw the authors refer to Table 5 in Sec.',\n",
       "  '4.1, Figure 5 in Sec.',\n",
       "  '4.2 and Tables 6 and 7 in Sec.',\n",
       "  '4.3.',\n",
       "  'All of them are in the appendix.',\n",
       "  'And it seems some of them are highly needed.',\n",
       "  'For example, the authors said that \"by fine graining the bins we can capture relatively small ... on the order of 20\" (before Sec.',\n",
       "  '4.3), then suggested the reader to see Figure 5.',\n",
       "  'I feel Figure 5 must be included in the main body because it is hard to understand that without seeing the figure.',\n",
       "  '6.',\n",
       "  'I think all figures that have $f_{(1)}$ (Figures 1a, 1b, and 5) must have a superscript $sort$ for all of them.',\n",
       "  'Otherwise, it is wrong.',\n",
       "  '7.',\n",
       "  'Kullback-Leibler loss is used extensively here without definition.',\n",
       "  'It is important to clarify the clear definition of it.',\n",
       "  '$U_1(x)$ also used extensively for the KL loss case and non-KL loss cases.',\n",
       "  'This can make the paper hard to read.',\n",
       "  'I suggest using $U^{\\\\mathrm{KL}}_1(x)$ when referring to the uncertainty measure with respect to the KL-loss.',\n",
       "  '8.',\n",
       "  'I saw $-\\\\log(p_\\\\max), -\\\\log(f_{(1)}), -\\\\log(f_1^{sort}), U_1$.',\n",
       "  'Are these all refer to the same thing?',\n",
       "  \"And I found that sometimes the argument $(x)$ is ignored in the paper sometimes it doesn't in a quite random way.\",\n",
       "  'If they are the same, it would be nice to unify them.',\n",
       "  '9.',\n",
       "  'The caption of figure 2(b) is uninformative.',\n",
       "  'The authors may consider improving it.',\n",
       "  '10.',\n",
       "  'It would be very helpful to add the implication or interpretation of the theoretical results to help the readers understand the intuition of the proven results.',\n",
       "  'For example, how does Eq.',\n",
       "  '(3) implies that when small uncertainty implies high chance of correct classification.',\n",
       "  '============================ Minor typos: INTRO: \"uncertainly measures\" -> \"uncertainty measures\" 5.2: \"on-distribution\" -> \"in-distribution\" Conclusion: logpmax -> write clearly with $$ should be better  Minor comments in the appendix: 1.',\n",
       "  'How does Appendix A related to implied loss or Bayes factor in this paper?',\n",
       "  'Did I miss something?',\n",
       "  '2.',\n",
       "  'Figure 5: \\t2.1 y-axis: is it Bayes ratio or Bayes factor?',\n",
       "  'It seems the Bayes ratio and Bayes factor to be a different thing.',\n",
       "  \"Even if it is the same in some literature, I think it's better to use the Bayes factor here for the consistency of this paper.\",\n",
       "  '2.2 Caption: \"entropy\" -> \"model entropy\".',\n",
       "  '2.3 Caption: Is it mistakes or do you want to insist on using $U_1$ and $U_5$ in the caption but using different notions in the figure?',\n",
       "  '(in figure, they are $-\\\\log(f_{(1)})$ and $-\\\\log(\\\\sum f_{(1:5)})$).',\n",
       "  '3.',\n",
       "  'Appendix C: Eq.',\n",
       "  '16 and Eq.',\n",
       "  '19: I believe there is a typo.',\n",
       "  'I think it should be $BF(X|Y_1), BF(X|Y_2), BF(X|Y_3)$, respectively.',\n",
       "  '4.',\n",
       "  'Appendix C: Eq.',\n",
       "  '18-right: does it need to sum to 1 not 0.3+0.5+0.3 = 1.1?',\n",
       "  ' The paper provides a comparison of several uncertainty measures that have been proposed for neural networks.',\n",
       "  'The goal of the work is to alleviate the lack of clear interpretability of softmax outputs as measures of uncertainty in neural networks for multi-class classification (in particular, here image classification).',\n",
       "  'The authors propose the use of expected Bayes factors as aggregate measures of how much information is conveyed by a measure of uncertainty, and compares several proposed approaches based on this measure on CIFAR-10/100 and ImageNet-1k.',\n",
       "  'The authors also discuss the application of uncertainty measures to detect mislabeled or ambiguous images, detecting out-of-distribution samples and adversarial examples.',\n",
       "  'Passing by, the authors propose a measure based on the norm of the gradient to detect adversarial examples that seems to work well.',\n",
       "  'There are interesting ideas in the paper.',\n",
       "  'The use of Bayes factors for measuring the quality of the uncertainty estimates, and the comparison of various existing methods on that measure seems useful.',\n",
       "  'The experiments on various tasks also have their own merits.The extension to top-k uncertainty is also interesting.',\n",
       "  'On the other hand, I found the paper difficult to follow because the contributions are scattered over the paper and the appendices without clear link.',\n",
       "  'A lot of space is devoted to the definition of the implied risk, which does not bring much to the overall interpretation of the results.',\n",
       "  'The criterion for detecting adversarial examples based on the norm of the gradient appears at the end of the paper somewhat independently from what was before, and the definition and computation of the Expected Bayes factor is entirely deferred to Appendices, which makes the main paper not really self-contained.',\n",
       "  'The paper also lacks a conclusion.',\n",
       "  'The various uncertainty measures seem to perform differently depending on the datasets, and it is unclear what the authors recommend in the end.',\n",
       "  'Overall, it seems to me that with a bit of restructuration, the paper would be an interesting contribution -- for instance in terms of the assessment of dropout variance vs direct model entropy.',\n",
       "  'It seems to me though that for now the paper lacks coherence and clarity in the message.',\n",
       "  '====== after author rebuttal  The rebuttal answer most of my concerns, and I raised my score to weak accept.',\n",
       "  'Overall, even if there is no clear evidence that one method to evaluate uncertainty is consistently better than the others, I feel that the idea of using Bayes factors to compare uncertainty measures is possibly impactful.',\n",
       "  'This paper shows that the maximum softmax probability is useful for uncertainty estimation on in-class data and not just for detecting out-of-distribution data.',\n",
       "  'They argue this with the Bayes Ratio, which here is an uncertainty estimation quality measure that seems worth exploring more for assessing the quality of uncertainty estimation techniques.',\n",
       "  \"The community is still in need of a good uncertainty estimation measure (Brier score is too tangled with accuracy, has low numerical resolution, and hardly penalizes consistent overconfidence; AUROC for error detection doesn't budge; the Soft-F1 score is without theoretical motivation; area under the response rate recall curve is too close to 1 when accuracy is high), and this could be it.\",\n",
       "  'They evaluate on ImageNet-1K, which most uncertainty estimation papers fail to consider.',\n",
       "  'By considering CIFAR-10, CIFAR-100, and ImageNet, we get fuller picture of the ranking of the utility of many uncertainty estimators, which is somewhat important for the community.',\n",
       "  'This paper is currently borderline, in that what it proposes is simple, but perhaps too simple.',\n",
       "  'Its empirical contributions are fairly minimal, though its proposal of the Bayes Ratio for uncertainty estimation quality assessment could quite impactful.',\n",
       "  'Smaller Notes: Figure 1 should be on page 3 not 2.',\n",
       "  'Some content in Appendix B should be incorporated in Section 5.1  > In the odds have increased If  B.3 should have a worked example in our setting as well Eqn (19) should say > 65 not <65  Notation of Y is confusing when it means a binning of U.',\n",
       "  'Perhaps use \"B\"?',\n",
       "  'Is the histogram adaptively computed?',\n",
       "  'Is that what “chosen to have equal weight” means?',\n",
       "  'Perhaps compare the Bayes ratio with RMS Calibration Error, AURRA, or AUROC for error detection.',\n",
       "  'Update: The other reviewers are concerned about lack of clarity which is separate from why I like the paper.'],\n",
       " 231)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_dict[\"2020_Hke0oa4KwS\"], len(rev_embeddings[\"2020_Hke0oa4KwS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = defaultdict(dict)\n",
    "\n",
    "for k in rev_embeddings:\n",
    "    for _, i in enumerate(rev_embeddings[k]):\n",
    "        sims[k][_] = []\n",
    "        for sent_emb in initial_embs:\n",
    "            sims[k][_].append(np.inner(i, sent_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sims_per_sent = defaultdict(list)\n",
    "for k in sims:\n",
    "    local_sent_sims = []\n",
    "    for sid in sims[k]:\n",
    "        local_sent_sims.append((sid, max(sims[k][sid])))\n",
    "    local_sent_sims_sorted = sorted(local_sent_sims, key=lambda x: x[1], reverse=True)\n",
    "    max_sims_per_sent[k] = local_sent_sims_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6950005 0.44239438\n",
      "[0.6950005, 0.6543123, 0.6250528, 0.61709094, 0.6132989, 0.60757875, 0.5873779, 0.57469136, 0.5672308, 0.5643765, 0.56402373, 0.5636523, 0.5614678, 0.56079316, 0.55891705, 0.55869263, 0.55829895, 0.55511045, 0.5549477, 0.5543808, 0.5518889, 0.55046445, 0.5497707, 0.54956484, 0.5495119, 0.5463648, 0.5454315, 0.5449186, 0.5416362, 0.53818166, 0.5375632, 0.5361639, 0.5358979, 0.5353979, 0.5346217, 0.5345441, 0.5338491, 0.53194886, 0.5292664, 0.5270127, 0.5243573, 0.5241016, 0.5239669, 0.5222249, 0.52117246, 0.5201142, 0.51940846, 0.5186783, 0.51826525, 0.51785743, 0.5137969, 0.5111599, 0.50997114, 0.5092687, 0.508452, 0.5078472, 0.5073638, 0.5056021, 0.505427, 0.5048554, 0.5040755, 0.5015641, 0.5010345, 0.50088227, 0.50031805, 0.49547097, 0.49505472, 0.49421135, 0.49331182, 0.49257076, 0.4917462, 0.48961833, 0.48952138, 0.48946032, 0.48726648, 0.48720586, 0.48191735, 0.47699964, 0.47338888, 0.47296995, 0.47137085, 0.46578664, 0.4632785, 0.4620925, 0.4559152, 0.45266336, 0.44744566, 0.44239438]\n"
     ]
    }
   ],
   "source": [
    "all_maxs = []\n",
    "for k in max_sims_per_sent:\n",
    "    all_maxs.append(max_sims_per_sent[k][0][1])\n",
    "print(max(all_maxs), min(all_maxs))\n",
    "print(sorted(all_maxs, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(31, 0.47699964),\n",
       " (43, 0.47099784),\n",
       " (13, 0.44567394),\n",
       " (48, 0.43486825),\n",
       " (51, 0.42300326),\n",
       " (0, 0.41074166),\n",
       " (46, 0.41057122),\n",
       " (11, 0.40413868),\n",
       " (32, 0.40090787),\n",
       " (28, 0.3968842)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sims_per_sent[\"2020_Bkf4XgrKvS\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(columns=[\"pid\", \"sent\", \"sim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in max_sims_per_sent:\n",
    "    for sent_num in max_sims_per_sent[i][0:5]:\n",
    "        eval_df = eval_df.append({\"pid\": i, \"sent\": sents_dict[i][sent_num[0]], \"sim\": sent_num[1]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>sent</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020_Syx-bCEFPS</td>\n",
       "      <td>2, we observe that the test accuracy for fine-...</td>\n",
       "      <td>0.507847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020_Syx-bCEFPS</td>\n",
       "      <td>=========== Summary: This paper introduces two...</td>\n",
       "      <td>0.453779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020_Syx-bCEFPS</td>\n",
       "      <td>In this paper, the authorsestablish a large be...</td>\n",
       "      <td>0.453430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020_Syx-bCEFPS</td>\n",
       "      <td>This paper contributes a new dataset for testi...</td>\n",
       "      <td>0.442424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020_Syx-bCEFPS</td>\n",
       "      <td>&gt; Inception-ResNet-V2 (Szegedy et al, 2017) is...</td>\n",
       "      <td>0.425780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pid                                               sent  \\\n",
       "0  2020_Syx-bCEFPS  2, we observe that the test accuracy for fine-...   \n",
       "1  2020_Syx-bCEFPS  =========== Summary: This paper introduces two...   \n",
       "2  2020_Syx-bCEFPS  In this paper, the authorsestablish a large be...   \n",
       "3  2020_Syx-bCEFPS  This paper contributes a new dataset for testi...   \n",
       "4  2020_Syx-bCEFPS  > Inception-ResNet-V2 (Szegedy et al, 2017) is...   \n",
       "\n",
       "        sim  \n",
       "0  0.507847  \n",
       "1  0.453779  \n",
       "2  0.453430  \n",
       "3  0.442424  \n",
       "4  0.425780  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_excel(\"evaluate_use_sims.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/singh_shruti/workspace/meaningful_comparison/meaningful-comparison/mcomp_text_extraction'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_top10 = pd.DataFrame(columns=[\"pid\", \"sent\", \"sim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in max_sims_per_sent:\n",
    "    for sent_num in max_sims_per_sent[i][0:10]:\n",
    "        eval_df_top10 = eval_df_top10.append({\"pid\": i, \"sent\": sents_dict[i][sent_num[0]], \"sim\": sent_num[1]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_top10.to_excel(\"evaluate_use_sims_top10_sents.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(40, 0.5078472),\n",
       " (0, 0.45377856),\n",
       " (25, 0.45342988),\n",
       " (30, 0.44242394),\n",
       " (41, 0.42578),\n",
       " (26, 0.4224671),\n",
       " (28, 0.41788375),\n",
       " (1, 0.41554937),\n",
       " (14, 0.4098686),\n",
       " (23, 0.409846),\n",
       " (21, 0.40348357),\n",
       " (18, 0.39842135),\n",
       " (31, 0.39458615),\n",
       " (11, 0.38681322),\n",
       " (8, 0.3792917),\n",
       " (7, 0.3791846),\n",
       " (16, 0.37650996),\n",
       " (35, 0.3764558),\n",
       " (39, 0.3650995),\n",
       " (20, 0.3583181)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sims_per_sent[\"2020_Syx-bCEFPS\"][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract top 10 sentences for each review using keyword based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"baseline\", \"compar\", \"et al\", \"sota\", \"state of the art\", \"state-of-the-art\", \"underperform\", \"outperform\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_sims = defaultdict(dict)\n",
    "\n",
    "for k in rev_embeddings:\n",
    "    for _, i in enumerate(rev_embeddings[k]):\n",
    "        for kw in keywords:\n",
    "            if sents_dict[k][_].lower().find(kw) > -1:\n",
    "                kw_sims[k][_] = sims[k][_]\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 56)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rev_embeddings[\"2020_Bkf4XgrKvS\"]), len(sents_dict[\"2020_Bkf4XgrKvS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sims_per_sent_with_kws = defaultdict(list)\n",
    "\n",
    "for k in kw_sims:\n",
    "    local_sent_sims = []\n",
    "    for sid in kw_sims[k]:\n",
    "        local_sent_sims.append((sid, max(kw_sims[k][sid])))\n",
    "    local_sent_sims_sorted = sorted(local_sent_sims, key=lambda x: x[1], reverse=True)\n",
    "    max_sims_per_sent_with_kws[k] = local_sent_sims_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6950005 0.37948126\n",
      "[0.6950005, 0.6543123, 0.61709094, 0.6132989, 0.60757875, 0.5873779, 0.57469136, 0.5672308, 0.56402373, 0.56079316, 0.55891705, 0.55869263, 0.55046445, 0.5497707, 0.5495119, 0.5463648, 0.5454315, 0.53931415, 0.53818166, 0.53693724, 0.5358979, 0.5345441, 0.5338491, 0.5277494, 0.5241016, 0.52117246, 0.5194628, 0.51940846, 0.5186783, 0.51785743, 0.5137969, 0.5111274, 0.51009715, 0.50997114, 0.50939703, 0.508452, 0.5073638, 0.5062192, 0.5048554, 0.5015641, 0.50144523, 0.4999418, 0.49964732, 0.49880302, 0.49560162, 0.49547097, 0.49421135, 0.49203223, 0.49075627, 0.49026197, 0.48952138, 0.48946032, 0.48882446, 0.48563325, 0.48484242, 0.48116815, 0.47912854, 0.47839898, 0.47706, 0.47338888, 0.47076106, 0.47059554, 0.4632785, 0.46246776, 0.4620925, 0.45951104, 0.4495302, 0.44873565, 0.44744566, 0.44567394, 0.44481036, 0.44366318, 0.4419094, 0.4401729, 0.43419603, 0.43233782, 0.42578, 0.4249261, 0.41760656, 0.40755904, 0.407412, 0.40453553, 0.39963016, 0.39556628, 0.3909938, 0.37948126]\n"
     ]
    }
   ],
   "source": [
    "all_maxs_with_kws = []\n",
    "for k in max_sims_per_sent_with_kws:\n",
    "    all_maxs_with_kws.append(max_sims_per_sent_with_kws[k][0][1])\n",
    "print(max(all_maxs_with_kws), min(all_maxs_with_kws))\n",
    "print(sorted(all_maxs_with_kws, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_top10_with_kw = pd.DataFrame(columns=[\"pid\", \"sent\", \"sim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in max_sims_per_sent_with_kws:\n",
    "    if len(max_sims_per_sent_with_kws[i]) > 9:\n",
    "        for sent_num in max_sims_per_sent_with_kws[i][0:10]:\n",
    "            eval_df_top10_with_kw = eval_df_top10_with_kw.append({\"pid\": i, \"sent\": sents_dict[i][sent_num[0]], \"sim\": sent_num[1]}, ignore_index=True)\n",
    "    else:\n",
    "        available_sents = []\n",
    "        for sent_num in max_sims_per_sent_with_kws[i][0:10]:\n",
    "            available_sents.append(sent_num[0])\n",
    "            eval_df_top10_with_kw = eval_df_top10_with_kw.append({\"pid\": i, \"sent\": sents_dict[i][sent_num[0]], \"sim\": sent_num[1]}, ignore_index=True)\n",
    "        \n",
    "        total_count = len(available_sents)\n",
    "        for sent_num in max_sims_per_sent[i]:\n",
    "            if total_count < 9:\n",
    "                if not sent_num[0] in available_sents:\n",
    "                    eval_df_top10_with_kw = eval_df_top10_with_kw.append({\"pid\": i, \"sent\": sents_dict[i][sent_num[0]], \"sim\": sent_num[1]}, ignore_index=True)\n",
    "                    total_count += 1\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_top10_with_kw.to_excel(\"evaluate_use_sims_top10_sents_with_kws_wo_duplicates.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract top 100 similar sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2020_test_sents_ALL.pkl\", \"rb\") as f:\n",
    "    all_comp_potential_sents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16879"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(all_comp_potential_sents[k]) for k in all_comp_potential_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16879"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_mappings = {}\n",
    "\n",
    "for k in all_comp_potential_sents:\n",
    "    for _, i in enumerate(all_comp_potential_sents[k]):\n",
    "        sent_mappings[k + str(_)] = (i, embed([i]))\n",
    "len(sent_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2020_BkgM7xHYwH0',\n",
       " ('It would be helpful to include some experiments indicating that this was the case (for the baseline) and that this method does indeed help with this problem.',\n",
       "  <tf.Tensor: id=3639130, shape=(1, 512), dtype=float32, numpy=\n",
       "  array([[ 3.82600687e-02, -9.52417310e-03,  4.00788290e-03,\n",
       "          -9.26006287e-02, -7.38939792e-02, -1.13683753e-02,\n",
       "          -7.67421490e-03, -1.00507801e-02,  4.56398949e-02,\n",
       "           6.65699244e-02, -3.76119651e-02,  1.87283847e-02,\n",
       "          -6.73146977e-04,  5.91846816e-02, -4.64790314e-02,\n",
       "           6.89068362e-02, -4.41112854e-02,  6.64667413e-02,\n",
       "          -3.89787890e-02, -6.48101717e-02, -3.48372688e-03,\n",
       "           6.52439073e-02, -3.50743085e-02,  4.53783758e-02,\n",
       "          -5.39826229e-02,  2.79634837e-02,  2.07222048e-02,\n",
       "          -8.30886513e-02,  3.16294953e-02,  2.58914847e-02,\n",
       "           1.41347591e-02, -1.75061692e-02,  1.65479798e-02,\n",
       "           5.91456145e-02, -2.45515034e-02,  7.63032734e-02,\n",
       "          -6.00931942e-02, -1.97790153e-02, -1.67240761e-02,\n",
       "          -1.76012591e-02,  5.18639758e-02,  1.38996625e-02,\n",
       "           2.87261121e-02,  8.12982246e-02,  3.65719981e-02,\n",
       "          -1.62570309e-02,  2.21437886e-02, -1.47556122e-02,\n",
       "           3.35550234e-02,  2.19819602e-02,  1.92371160e-02,\n",
       "          -2.61572599e-02, -2.40987148e-02, -5.30024571e-03,\n",
       "           7.65733840e-03,  6.06066957e-02, -5.34570254e-02,\n",
       "          -1.53213553e-02, -7.46244192e-03,  2.91576553e-02,\n",
       "          -2.62975972e-02, -6.44748937e-03,  1.61099099e-02,\n",
       "          -5.34195378e-02,  3.59588601e-02, -8.62171948e-02,\n",
       "          -3.68700325e-02, -1.74170136e-02, -3.54178622e-03,\n",
       "           1.58865824e-02,  6.42237663e-02, -4.54510190e-02,\n",
       "          -1.90568727e-03, -4.14880626e-02, -4.42169756e-02,\n",
       "           1.84813868e-02,  1.70071777e-02, -4.79978807e-02,\n",
       "          -4.02958542e-02,  8.10734704e-02, -5.53326942e-02,\n",
       "           1.40055483e-02,  4.10186918e-03,  3.12263463e-02,\n",
       "           2.65524667e-02,  1.88845377e-02,  4.52050641e-02,\n",
       "          -3.26217972e-02, -3.87230739e-02,  3.51349004e-02,\n",
       "           2.90203318e-02, -2.62119039e-03,  2.66947355e-02,\n",
       "           3.97431031e-02, -2.60856878e-02,  4.00369391e-02,\n",
       "           7.46083856e-02,  3.41803185e-03, -9.95637197e-03,\n",
       "          -1.07129008e-01, -2.98971292e-02,  9.78978127e-02,\n",
       "          -7.12918267e-02, -5.70173897e-02,  6.60902113e-02,\n",
       "           3.98744158e-02, -4.46162149e-02, -6.07660562e-02,\n",
       "           2.60678679e-02, -4.17779796e-02,  6.16139919e-02,\n",
       "           8.46423209e-02,  8.98872092e-02,  3.47300246e-03,\n",
       "          -1.36057641e-02, -4.51681428e-02, -1.46012800e-02,\n",
       "          -2.04658639e-02,  2.57602073e-02,  1.64468139e-02,\n",
       "           3.34918089e-02, -1.65479239e-02,  5.30222431e-02,\n",
       "          -5.77744050e-03,  2.21690834e-02, -8.95723030e-02,\n",
       "           9.20611620e-03,  3.78089547e-02, -1.16681186e-02,\n",
       "          -4.11820486e-02, -4.67012338e-02,  2.65861638e-02,\n",
       "           7.35156089e-02, -5.79463840e-02,  4.97808233e-02,\n",
       "          -6.94394633e-02, -7.74158686e-02, -5.45455003e-03,\n",
       "          -1.08360738e-01,  2.53459699e-02, -8.56158230e-03,\n",
       "           2.42715701e-03, -6.45916387e-02,  7.01548392e-03,\n",
       "           6.94184825e-02, -1.55331157e-02,  7.03474209e-02,\n",
       "           4.60418686e-02,  5.86185716e-02,  1.15865059e-02,\n",
       "           1.79247782e-02,  4.14845608e-02, -2.56007221e-02,\n",
       "          -2.49818768e-02, -7.54087046e-03, -2.99503673e-02,\n",
       "           8.69929120e-02,  2.38577439e-03,  4.89597134e-02,\n",
       "           7.28265196e-03, -3.76055134e-03,  1.34822521e-02,\n",
       "           1.66433286e-02,  5.74916508e-03,  2.61035748e-02,\n",
       "           3.12706009e-02, -6.86214957e-03, -3.79652493e-02,\n",
       "          -8.45162570e-02, -2.05150247e-02,  3.12152784e-02,\n",
       "           1.65003743e-02, -4.13380824e-02, -9.65999812e-03,\n",
       "           4.26694676e-02,  5.01508340e-02, -3.01065110e-02,\n",
       "           4.37654927e-02,  6.78745881e-02,  5.56541346e-02,\n",
       "           1.49849737e-02, -2.29732376e-02, -4.21488881e-02,\n",
       "           7.02147707e-02,  6.76613860e-03,  1.74598824e-02,\n",
       "           6.59853816e-02,  8.94863233e-02, -5.87020442e-02,\n",
       "           1.85822267e-02,  7.09215850e-02,  7.88058620e-03,\n",
       "           6.03999645e-02,  8.46474692e-02,  1.82471078e-04,\n",
       "           7.02511445e-02, -7.04966486e-02,  4.01388817e-02,\n",
       "          -1.09037375e-02, -4.12128344e-02,  1.47286002e-02,\n",
       "           2.04292256e-02,  3.36621255e-02,  5.13062812e-02,\n",
       "           2.02886052e-02, -5.28564677e-02, -2.26451643e-02,\n",
       "           1.65507570e-02, -2.62962803e-02,  5.18942401e-02,\n",
       "          -5.98163791e-02, -2.98280697e-02, -3.18153687e-02,\n",
       "          -1.43580860e-03,  2.18465254e-02,  5.07093631e-02,\n",
       "          -5.79633471e-03, -3.00825164e-02,  7.07997456e-02,\n",
       "           2.46520601e-02, -4.99457121e-02,  3.72733809e-02,\n",
       "           1.14039909e-02, -8.78640544e-03,  2.80012432e-02,\n",
       "           4.57392372e-02, -9.50973630e-02,  6.06169999e-02,\n",
       "          -2.54036356e-02, -2.32373420e-02,  1.92650612e-02,\n",
       "          -5.97025417e-02,  2.67579649e-02, -5.50004281e-03,\n",
       "           2.58219447e-02, -5.06202132e-02,  6.96459040e-02,\n",
       "          -2.61656791e-02,  1.87802780e-02,  3.52816842e-02,\n",
       "           8.72586444e-02,  1.25890654e-02,  4.39297780e-02,\n",
       "          -2.37266552e-02,  6.52233586e-02, -6.67795017e-02,\n",
       "          -1.00990981e-02, -4.06601876e-02,  6.61237398e-03,\n",
       "          -5.39752375e-03, -5.66320308e-02, -3.75703424e-02,\n",
       "          -7.47246370e-02,  3.40586230e-02,  2.35261787e-02,\n",
       "           4.44020703e-02, -3.15571800e-02,  1.03292219e-01,\n",
       "          -3.67488377e-02, -8.93640984e-03, -1.95492599e-02,\n",
       "           6.77729100e-02,  6.85478887e-03, -8.41069594e-02,\n",
       "          -5.11905970e-03, -6.34852871e-02, -3.04146223e-02,\n",
       "           2.27451697e-02,  1.13500506e-02,  1.31321838e-02,\n",
       "           9.19154584e-02,  1.01121530e-01,  1.50892287e-02,\n",
       "          -4.66395766e-02,  4.70629930e-02, -1.18637420e-02,\n",
       "           5.05217686e-02, -2.22843084e-02,  4.09918241e-02,\n",
       "          -3.37406509e-02, -6.34392798e-02, -3.97119261e-02,\n",
       "           4.11696769e-02,  4.03916687e-02, -6.98272735e-02,\n",
       "          -5.46489097e-02, -4.15206477e-02,  6.97989464e-02,\n",
       "          -8.32767785e-03, -3.57064325e-03, -1.47753600e-02,\n",
       "           5.21362871e-02, -1.19464686e-02, -1.85236223e-02,\n",
       "           9.87195522e-02, -9.88950208e-03, -4.62232297e-03,\n",
       "          -2.46402901e-02, -3.06201093e-02, -6.18003942e-02,\n",
       "           4.66215499e-02,  2.15884652e-02, -5.52926287e-02,\n",
       "          -5.86360991e-02, -7.58867487e-02, -6.51921108e-02,\n",
       "           7.39087313e-02,  3.68851833e-02,  3.71261463e-02,\n",
       "           7.71288993e-03,  7.32523669e-03,  7.80570805e-02,\n",
       "           2.12492105e-02,  4.20507938e-02, -5.85427135e-02,\n",
       "           6.03169017e-02,  7.33058080e-02,  8.68430454e-03,\n",
       "           8.87477119e-03,  4.28607687e-02,  1.20959280e-03,\n",
       "          -3.28628942e-02,  5.26577011e-02,  5.46919741e-02,\n",
       "          -6.94440603e-02, -3.67391594e-02,  2.21996419e-02,\n",
       "          -4.41644079e-04, -7.61725903e-02,  3.98454629e-02,\n",
       "           7.86774307e-02,  1.49520263e-02, -1.16837835e-02,\n",
       "          -4.99203056e-02,  2.57125869e-02,  3.34084854e-02,\n",
       "           4.92390394e-02, -1.69738568e-02,  1.10091167e-02,\n",
       "           1.52257234e-02, -3.01062297e-02,  2.29349118e-02,\n",
       "           8.83213431e-03, -1.40308235e-02, -1.06141746e-01,\n",
       "          -2.52872147e-03,  3.07382625e-02, -9.57595650e-03,\n",
       "          -8.37265328e-02, -3.39100906e-03, -4.36516739e-02,\n",
       "          -1.54046481e-02, -1.79825854e-02,  2.73311622e-02,\n",
       "           7.81869441e-02, -9.79441684e-03, -9.77722034e-02,\n",
       "           4.73388694e-02,  5.51729351e-02, -7.16502592e-02,\n",
       "          -1.02906279e-01, -3.95632274e-02, -9.24449041e-02,\n",
       "          -7.37655461e-02,  1.37471678e-02,  4.48665768e-03,\n",
       "          -7.92136192e-02,  5.65759465e-02,  6.60975873e-02,\n",
       "          -7.38355666e-02,  1.41188465e-02,  4.93023321e-02,\n",
       "           2.82258373e-02, -4.58762534e-02, -7.19257351e-03,\n",
       "          -6.20345883e-02, -2.48001702e-02,  3.19238268e-02,\n",
       "          -1.98639631e-02, -6.46461267e-03, -2.74634268e-02,\n",
       "           1.91648584e-02,  3.35150287e-02,  8.35996717e-02,\n",
       "          -7.70213604e-02,  3.06077916e-02,  1.07321411e-01,\n",
       "          -1.17582576e-02,  1.25311734e-02,  3.64952572e-02,\n",
       "          -1.65059939e-02, -1.52784970e-03, -2.80269869e-02,\n",
       "          -4.52245548e-02,  1.17213139e-02,  3.00340191e-03,\n",
       "          -2.97606783e-03, -5.76391071e-02,  3.39318514e-02,\n",
       "          -8.17003697e-02,  6.95786476e-02,  7.49756917e-02,\n",
       "           4.13236022e-02, -3.33148614e-02, -2.51241401e-02,\n",
       "          -4.14834470e-02, -1.70658864e-02,  4.20895070e-02,\n",
       "          -3.61037254e-02,  1.44840386e-02, -4.83854674e-02,\n",
       "           2.12451573e-02,  5.59421219e-02,  8.21890589e-03,\n",
       "           4.42699604e-02, -5.91440946e-02, -2.69909371e-02,\n",
       "           6.77298605e-02,  2.10197493e-02, -8.47256836e-03,\n",
       "           1.18299779e-02, -5.59431203e-02,  3.43927331e-02,\n",
       "           3.40494989e-05,  1.88268367e-02,  7.27081351e-05,\n",
       "           1.96505319e-02,  6.14080541e-02,  2.80629639e-02,\n",
       "           4.22913954e-02,  1.52595434e-02, -8.51367135e-03,\n",
       "          -4.60961908e-02, -4.30237083e-03,  1.23654958e-02,\n",
       "           5.71088381e-02,  5.39210960e-05,  2.28236765e-02,\n",
       "           3.90760601e-02,  2.47292016e-02,  4.52747643e-02,\n",
       "          -1.00772910e-01,  4.58264686e-02, -5.07596228e-03,\n",
       "          -8.38075113e-03,  4.55306247e-02, -9.89091466e-04,\n",
       "          -2.21470892e-02,  2.37961486e-02,  7.27847889e-02,\n",
       "          -2.32405663e-02,  6.15333114e-03,  6.41849823e-03,\n",
       "           6.55476004e-02, -1.44941043e-02, -6.83407560e-02,\n",
       "          -1.57853104e-02, -6.36174232e-02,  3.49936411e-02,\n",
       "           2.89330631e-02,  1.78643856e-02,  1.02495989e-02,\n",
       "          -1.64158903e-02,  1.65447351e-02,  1.69344973e-02,\n",
       "          -2.76057664e-02, -3.39079946e-02,  1.40109230e-02,\n",
       "          -9.12450440e-03, -5.36465161e-02, -1.02395182e-02,\n",
       "          -1.62990522e-02, -4.65465188e-02,  3.17986682e-02,\n",
       "           2.56590303e-02,  2.47529093e-02, -1.18592065e-02,\n",
       "           1.35925543e-02,  1.89864505e-02,  3.42628150e-03,\n",
       "           3.19304466e-02, -2.58878227e-02, -4.05898876e-02,\n",
       "          -7.88449124e-02, -1.95260381e-03, -6.56675324e-02,\n",
       "          -9.18015465e-03,  3.06328218e-02, -4.85038497e-02,\n",
       "           3.26941535e-02,  1.82698411e-03, -3.64442058e-02,\n",
       "           5.18589281e-03, -4.91538942e-02,  4.34224010e-02,\n",
       "          -3.02254483e-02,  3.97052430e-02, -2.39734608e-03,\n",
       "           6.76141679e-02, -3.38837914e-02, -1.31060816e-02,\n",
       "           2.16253903e-02,  2.72331364e-03, -6.09049052e-02,\n",
       "          -1.05623389e-02, -4.69675846e-02, -5.14307953e-02,\n",
       "          -5.68965822e-03, -6.07035831e-02,  6.02637529e-02,\n",
       "           7.51002207e-02, -3.34032737e-02]], dtype=float32)>))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sent_mappings.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sims_dict = defaultdict(list)\n",
    "# max_sims_per_sentence = defaultdict(int)\n",
    "count = 0\n",
    "\n",
    "for ite in sent_mappings:\n",
    "    if not ite in max_sims_per_sentence:\n",
    "        count += 1\n",
    "        for sent_emb in initial_embs:\n",
    "            sims_dict[ite].append(np.inner(sent_mappings[ite][1], sent_emb)[0])\n",
    "        max_sims_per_sentence[ite] = max(sims_dict[ite])\n",
    "\n",
    "        if count > 5000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_max_sims = sorted(max_sims_per_sentence.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2020_BylRkAEKDH9', 0.7344125),\n",
       " ('2020_BJg4NgBKvH13', 0.7061932),\n",
       " ('2020_Hkg0olStDr1', 0.70500076),\n",
       " ('2020_Hkg0olStDr0', 0.7050007),\n",
       " ('2020_rylfl6VFDH4', 0.6950005)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_max_sims[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000_df = pd.DataFrame(columns=['pid', 'sent', 'sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted_max_sims[0:1000]:\n",
    "    top_1000_df = top_1000_df.append({\"pid\": i[0], \"sent\": sent_mappings[i[0]][0], \"sim\": i[1]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000_df.to_excel(\"top_1000_similar_mcomp_sentences.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"With this knowledge, we have run a large number of experiments (1,840) to faithfully compare the existing methods.\"\n",
    "vec = embed([s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29652703\n",
      "0.30603814\n",
      "0.3059607\n",
      "0.16510145\n",
      "0.25347525\n",
      "0.079709984\n",
      "0.25372633\n",
      "0.2122944\n",
      "0.2943849\n",
      "0.34907377\n",
      "0.3447559\n",
      "0.20344344\n",
      "0.29056942\n",
      "0.26877886\n",
      "0.075643644\n",
      "0.13323496\n",
      "0.15270266\n",
      "0.18165548\n",
      "0.09214949\n",
      "0.18762672\n",
      "-0.04017473\n",
      "0.19602987\n",
      "0.25877717\n",
      "0.26004434\n",
      "0.26084474\n",
      "-0.050904255\n",
      "0.39915735\n",
      "0.31180298\n",
      "0.0858697\n",
      "0.19444376\n",
      "0.27616215\n",
      "0.11376745\n",
      "0.07652992\n",
      "0.25433397\n",
      "0.28549474\n",
      "0.35478094\n",
      "0.3464392\n",
      "-0.0019737743\n",
      "0.11035041\n"
     ]
    }
   ],
   "source": [
    "for sent_emb in initial_embs:\n",
    "    print(np.inner(vec, sent_emb)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered 2020 test set (local copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"baseline\", \"compar\", \"et al\", \"SOTA\", \"state of the art\", \"state-of-the-art\", \"underperform\", \"outperform\"]\n",
    "\n",
    "kw_poor = [\"novel\", \"evaluat\", \"benchmark\", \"contribution\", \"contrast\", \"method\", \"result\", \"significan\", \n",
    "           \"approach\", \"performance\", \"technique\", \"report\", \"experiment\", \"propose\", \"model\", \"discuss\", \n",
    "           \"problem\", \"task\", \"metric\", \"score\", \"publication\", \"analyze\", \"analyse\", \"analysis\", \n",
    "           \"replicate\", \"submission\"]\n",
    "\n",
    "# &, et al, [1,2][7-9,0-1][0-9][0-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2017, 2018, 2019, 2020]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing year:  2017\n",
      "Changing year:  2018\n"
     ]
    }
   ],
   "source": [
    "sents_list = []\n",
    "stop = False\n",
    "\n",
    "for y in years[:-1]:\n",
    "    if stop:\n",
    "        break\n",
    "    for k in rev_dict[y]:\n",
    "        \n",
    "        if len(sents_list) > 800*(y-2016) and y != 2019:\n",
    "            change_year = True\n",
    "            print(\"Changing year: \", y)\n",
    "            break\n",
    "        if len(sents_list) > 5000:\n",
    "            stop = True\n",
    "            break\n",
    "            \n",
    "        year_key = str(y) + \"_\" + k\n",
    "        if year_key in iclr_arxiv_map:\n",
    "            for rev_num in rev_dict[y][k]:\n",
    "                rev_text = rev_num[\"content\"][\"review\"]\n",
    "                \n",
    "                rev_text = re.sub(\" e[\\.]?g[\\.]?:? \", \" eg: \", rev_text)\n",
    "                rev_text = re.sub(\" et[\\.]? al[\\.]\", \" et al\", rev_text)\n",
    "                rev_text = re.sub(\" i[\\.]?e[\\.] \", \" ie \", rev_text)\n",
    "                rev_text = re.sub(\"\\\\n\", \" \", rev_text)\n",
    "                \n",
    "                sent_text = nltk.sent_tokenize(rev_text)\n",
    "                \n",
    "                for s in sent_text:\n",
    "                    for kw in keywords:\n",
    "                        if s.find(kw) > -1:\n",
    "                            sents_list.append((k, s))\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sents_list, columns=['pid', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BydrOIcle</td>\n",
       "      <td>So, this comparison might just be showing that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>Several points are appealing about this approa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pid                                           sentence\n",
       "0  BydrOIcle  So, this comparison might just be showing that...\n",
       "1  SyOvg6jxx  Several points are appealing about this approa..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BydrOIcle</td>\n",
       "      <td>So, this comparison might just be showing that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>Several points are appealing about this approa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>In addition, there are results for comparison ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>The results indicate that the approach clearly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>It seems like the technique could be easily us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>The paper addresses an important problem (expl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>It is a nice alternative approach to the one o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>Specifically, I am not as concerned about beat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>The figure S9 from Mnih et al points to instan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>Without \"feature engineering\", the authors ach...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pid                                           sentence\n",
       "0  BydrOIcle  So, this comparison might just be showing that...\n",
       "1  SyOvg6jxx  Several points are appealing about this approa...\n",
       "2  SyOvg6jxx  In addition, there are results for comparison ...\n",
       "3  SyOvg6jxx  The results indicate that the approach clearly...\n",
       "4  SyOvg6jxx  It seems like the technique could be easily us...\n",
       "5  SyOvg6jxx  The paper addresses an important problem (expl...\n",
       "6  SyOvg6jxx  It is a nice alternative approach to the one o...\n",
       "7  SyOvg6jxx  Specifically, I am not as concerned about beat...\n",
       "8  SyOvg6jxx  The figure S9 from Mnih et al points to instan...\n",
       "9  SyOvg6jxx  Without \"feature engineering\", the authors ach..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"ann_comparison_only.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a test set of 20 reviews for manual annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_B1lsXREYvr\n",
      "2020_rkltE0VKwH\n",
      "2018_Hki-ZlbA-\n",
      "2018_rJBiunlAW\n",
      "2018_B1nxTzbRZ\n",
      "2019_rJzoujRct7\n",
      "2019_SJf_XhCqKm\n",
      "2018_BJ_QxP1AZ\n",
      "2019_HklbTjRcKX\n",
      "2020_Sygn20VtwH\n",
      "2018_BkeC_J-R-\n"
     ]
    }
   ],
   "source": [
    "# list(iclr_arxiv_map.keys())[0:20]\n",
    "counter = 0\n",
    "\n",
    "for k, v in iclr_arxiv_map.items():\n",
    "    if v[\"label\"] == \"Reject\":\n",
    "        print(k)\n",
    "        counter += 1\n",
    "    if counter > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\"2020_r1e_FpNFDr\", \"2020_SyevYxHtDB\", \"2019_SyxZJn05YX\", \"2019_BJx0sjC5FX\", \"2018_SkZxCk-0Z\", \n",
    "            \"2018_HkfXMz-Ab\", \"2017_B1ckMDqlg\", \"2017_HJ0NvFzxl\", \"2020_B1lsXREYvr\", \"2020_rkltE0VKwH\", \n",
    "            \"2018_Hki-ZlbA-\", \"2018_rJBiunlAW\", \"2019_rJzoujRct7\", \"2019_SJf_XhCqKm\", \"2017_Bk0MRI5lg\",\n",
    "            \"2017_BJ9fZNqle\"]\n",
    "\n",
    "sents_list = defaultdict(dict)\n",
    "\n",
    "for k in test_set:\n",
    "    if k in iclr_arxiv_map:\n",
    "        y = int(k.split(\"_\")[0])\n",
    "        k_wo_y = k.split(\"_\", 1)[1]\n",
    "        \n",
    "        sents_list[k][\"pid\"] = k\n",
    "        sents_list[k][\"sents\"] = []\n",
    "        sents_list[k][\"dec\"] = iclr_arxiv_map[k][\"label\"]\n",
    "        \n",
    "        for rev_num in rev_dict[y][k_wo_y]:\n",
    "            rev_text = rev_num[\"content\"][\"review\"]\n",
    "\n",
    "            rev_text = rev_text.strip()\n",
    "            \n",
    "            \n",
    "            rev_text = re.sub(\"(?<=[ \\(])e[\\.]?g[\\.]?:? \", \"eg: \", rev_text)\n",
    "            rev_text = re.sub(\" et[\\.]? al[\\.]\", \" et al\", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])i[\\.]?e[\\.] \", \"ie \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])Fig[\\.]? \", \"Figure \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])fig[\\.]? \", \"figure \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])Eq[\\.]? \", \"Equation \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])Alg\\. \", \"Algorithm \", rev_text)\n",
    "            rev_text = re.sub(\"Eq[\\.]? \", \"Equation \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])eq\\. \", \"equation \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])Sec[\\.]? \", \"Section \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=[ \\(])sec\\. \", \"section \", rev_text)\n",
    "            rev_text = re.sub(\"(?<=: [0-9])(\\.) \", \")\", rev_text)\n",
    "            rev_text = re.sub(\"(?<=; [0-9])(\\.) \", \")\", rev_text)\n",
    "            ####rev_text = re.sub(\"(?<=[0-9])(\\.) \", \") \", rev_text)\n",
    "            rev_text = re.sub(\"\\n\\\\n\", \"\\n\", rev_text)\n",
    "\n",
    "            sent_text = nltk.sent_tokenize(rev_text)\n",
    "            for s in sent_text:\n",
    "                sents_list[k][\"sents\"].append(s)\n",
    "    else:\n",
    "        print(\"Not found: %s, use another key\"%k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['2019_SJf_XhCqKm', '2017_Bk0MRI5lg', '2020_SyevYxHtDB', '2018_rJBiunlAW', '2020_rkltE0VKwH', '2018_Hki-ZlbA-', '2019_BJx0sjC5FX', '2020_r1e_FpNFDr', '2020_B1lsXREYvr', '2018_SkZxCk-0Z', '2019_rJzoujRct7', '2018_HkfXMz-Ab', '2017_BJ9fZNqle', '2019_SyxZJn05YX', '2017_B1ckMDqlg', '2017_HJ0NvFzxl']),\n",
       " 16)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_list.keys(), len(list(sents_list.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['pid', 'dec', 'sent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the test set of 20 reviews for manual annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for k in sents_list:\n",
    "    dec = sents_list[k][\"dec\"]\n",
    "    pid = sents_list[k][\"pid\"]\n",
    "    for s in sents_list[k][\"sents\"]:\n",
    "        df.loc[i] = [pid, dec, s]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>dec</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The authors propose to use k-DPP to select a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>This paper covers the related work nicely, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>The rest of the paper are also clearly written.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>However, I have some concerns about the propos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019_SJf_XhCqKm</td>\n",
       "      <td>Reject</td>\n",
       "      <td>- It is not clear how to define the kernel, th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pid     dec                                               sent\n",
       "0  2019_SJf_XhCqKm  Reject  The authors propose to use k-DPP to select a s...\n",
       "1  2019_SJf_XhCqKm  Reject  This paper covers the related work nicely, wit...\n",
       "2  2019_SJf_XhCqKm  Reject    The rest of the paper are also clearly written.\n",
       "3  2019_SJf_XhCqKm  Reject  However, I have some concerns about the propos...\n",
       "4  2019_SJf_XhCqKm  Reject  - It is not clear how to define the kernel, th..."
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"testset_16.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
