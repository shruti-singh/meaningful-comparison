{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/shruti/Desktop/iitgn/courses/SEM2/ML/Project/code/PaperAcceptancePrediction/ICLR data/masterdata_unbalanced/\"\n",
    "\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "rev_dict = {}\n",
    "paper_dict = {}\n",
    "dec_dict = {}\n",
    "iclr_arxiv_map = {}\n",
    "\n",
    "for y in years:\n",
    "    rev_dict[y] = pd.read_pickle(data_path + \"off_rev_dict_{}.pkl\".format(y))\n",
    "    paper_dict[y] = pd.read_pickle(data_path + \"papers_{}.pkl\".format(y))\n",
    "    dec_dict[y] = pd.read_pickle(data_path + \"paper_decision_dict_{}.pkl\".format(y))\n",
    "\n",
    "iclr_arxiv_map = pd.read_pickle(\"./data/iclr_arxiv_map.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were thinking about this problem for EACL:\n",
    "    \n",
    "Given a review text, can we identify the text span that talks about the comparisons.\n",
    "After identification, can we identify different aspects associated with it:\n",
    "1. Is it positive or negative?\n",
    "2. What module does it talks about (dataset, model, metric, etc.)\n",
    "3. Does it suggests some papers to cite?\n",
    "4. We can also suggest the overlap between different reviews in terms of comparison comments. \n",
    "This can help us understanding how much reviews are coherent\n",
    "\n",
    "Motivation: The system would be more useful for metareviewers. Authors can also be benefited, but they \n",
    "can always read the review and get this info.  We can always say that with the increase in exponential \n",
    "growth in conference submissions, the evaluation task for meta-reviewers is becoming more and more hard. \n",
    "The current system would help the metareviews task in quickly glancing the multiple reviews for a given paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the initial set of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/initialsetmcomp.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57,          pid                                               sent  mcomp\n",
       " 0  S1HcOI5le  The idea of the paper is interesting there are...      1\n",
       " 1  S1HcOI5le  It's not clear how this method compares agains...      1\n",
       " 2  S1HcOI5le  Measure: Accuracy difference does not look lik...      1\n",
       " 3  S1HcOI5le  Instead the authors could position this work a...      1\n",
       " 4  S1HcOI5le  If the authors care to compare their approach ...      1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size, df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use above to find sentences from 2020 reviews for 10 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_dict = defaultdict(list)\n",
    "stop = 1\n",
    "\n",
    "for y in [2020]:\n",
    "    for k in rev_dict[y]:\n",
    "        \n",
    "        if stop > 10:\n",
    "            break\n",
    "        stop += 1 \n",
    "            \n",
    "        year_key = str(y) + \"_\" + k\n",
    "        if year_key in iclr_arxiv_map:\n",
    "            for rev_num in rev_dict[y][k]:\n",
    "                rev_text = rev_num[\"content\"][\"review\"]\n",
    "                \n",
    "                rev_text = re.sub(\" e[\\.]?g[\\.]?:? \", \" eg: \", rev_text)\n",
    "                rev_text = re.sub(\" et[\\.]? al[\\.]\", \" et al\", rev_text)\n",
    "                rev_text = re.sub(\" i[\\.]?e[\\.] \", \" ie \", rev_text)\n",
    "                rev_text = re.sub(\"\\\\n\", \" \", rev_text)\n",
    "                \n",
    "                sent_text = nltk.sent_tokenize(rev_text)\n",
    "                \n",
    "                for s in sent_text:\n",
    "                    sents_dict[year_key].append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This paper introduces the compression risk in domain-invariant representations.',\n",
       " 'Learning domain-invariant representations leads to larger compression risks and potentially worse adaptability.',\n",
       " 'To this end, the authors presents gamma(H) to measure the compression risk.',\n",
       " 'Learning weighted representations to control source error, domain discrepancy, and compression simultaneously leads to a better tradeoff between invariance and compression, which is verified by experimental results.',\n",
       " 'The paper presents an in-depth analysis of compression and invariance, which provides some insight.',\n",
       " 'However, I have several concerns: * In Section 4, the authors propose a regularization to ensure h belongs to H_0.',\n",
       " 'How is the regularization chosen?',\n",
       " 'How does it perform on other datasets?',\n",
       " 'Experimental results only on digit datasets are not convincing.',\n",
       " '* In Section 5, the authors introduce weighted representations to alleviate the curse of invariance.',\n",
       " 'However, they do not provide experiments to validate their improvement.',\n",
       " '* The organization of this manuscript is poor and difficult to follow.',\n",
       " 'Starting from Section 3, the authors use several definitions to introduce their main theorem.',\n",
       " 'However, these definitions are somewhat misleading.',\n",
       " 'I cannot get the point until the end of Section 3.',\n",
       " 'Besides, the notations are confusing, so I have to go back to the previous sections in case of misunderstanding.',\n",
       " 'Summary ------- This paper presents a revisit of existing theoretical frameworks in unsupervised domain adaptation in the context of learning invariant representation.',\n",
       " 'They propose a novel bound that involves trainable terms taking into account some compression information and a novel interpretation of adaptability.',\n",
       " 'The authors mention also contribution showing that weighting representations can be a way to improve the analysis.',\n",
       " 'Evaluation ----- The ideas are novel and the result brings novel and interesting light on the difficult problem of unsupervised domain adaptation.',\n",
       " 'However, the practical interest in terms of applicability of the proposed framework is not fully demonstrated, the properties of the proposed analysis have to be studied more in details and some parts better justified.',\n",
       " 'The experimental evaluation brings some interesting behavior but is somewhat limited.',\n",
       " 'The weighting aspect of the contribution is not supported by any experiment.',\n",
       " 'Other comments ------------  -I am a but puzzled by the use of the term \"compression\".',\n",
       " 'This is maybe subjective, but in the context of learning representation, I would have interpreted it as a way to sparsify the representation, and thus compression could then be measured with respect to a given norm (L2?)',\n",
       " 'or another criterion (Kolmogoroff, ...).',\n",
       " 'In the paper, the notion of compression is related to a reduction of the hypothesis space after application of a transformation \\\\phi, so I am wondering if using \"hypothesis space reduction\" would not be more appropriate.',\n",
       " 'In this case, however, there are maybe links with structural risk minimization that could be investigated here.',\n",
       " 'A side remark: there is no particular restriction on the space of transformations, we wonder if it would be useful to indicate if all the possible transformations are included as subspaces of a given latent space.',\n",
       " 'Since, to be very general, one can imagine the existence of an unbounded number of transformations that correspond to an increase of the input dimension.',\n",
       " 'For transformations leading to different representations of different dimensions, the way the deduced hypothesis can be compared should also be indicated (for defining properly the inclusion H(\\\\phi_1)\\\\subset H(\\\\phi_2).',\n",
       " 'On the other hand, the authors seem to need the use of norms over transformations as illustrated in the definition of H_0^\\\\eta in the experimental section.',\n",
       " 'So I suggest that the analysis could be revisited by directly incorporating (representation) norms in the theoretical framework and in particular for defining more properly H_0.',\n",
       " '-One weakness of the theoretical framework is for me the lack of definition of H_0 in Section 3.',\n",
       " 'We just know that it is included between two classes of hypothesis of interest, but there is no clear characterisation of H_0 which makes the analysis fuzzy: we have a bound that involves an object without any clear definition and it is for me difficult to really interpret the bound.',\n",
       " 'Trying to define H_0 with some restrictions related to the norm of the transformations, as evoked before, could be a way to address this point (and actually the way the experiments are done tend to confirm this point).',\n",
       " '-Another weak point is the lack of qualitative analyse of the bound in Inequality 3 (the same applies for Inequality 5).',\n",
       " 'I would have appreciated if the authors could provide an analysis similar to the one of (Mansour et al, COLT 2009) - it is cited in the paper - when they compared their result to the one of (Ben-David et al, 2007).',\n",
       " 'For example, what happens when source is equal to the target, when is the bound significantly loose, significantly tight, different from other existing results, ...',\n",
       " 'In particular, if we compare the bound with the one of Ben-David et al (we can also consider the one of Mansour et al), there is two additional term, one is weighted by a factor 2, another one involved a supremum and one can think that this bound is rather loose and does not provide any insightful information and said differently it could not give a strong framework for practical considerations.',\n",
       " 'I may understand that when the bound is tight we could deduce that the compression term is low, but finding cases leading to a tight interesting bound does not seem obvious.',\n",
       " '-The experimental evaluation presents some expected behavior in the context of the bound, but I miss a real study trying to make use of the proposed framework to do adaptation in practice with comparisons to other strategies.',\n",
       " 'Additionally, having additional studies with other models and tasks will probably reinforce the analysis.',\n",
       " '-At the beginning of Section 3.2, the authors mention that they restrict their analysis to the square loss, however I think the analysis is true for larger class of losses with more general properties.',\n",
       " 'In the experimental evaluation, the cross entropy is used, so I think that the experimental evaluation should also be consistent with the theoretical analysis by considering the square loss.',\n",
       " '-Paragraph below Definition 5 is unclear: the notion of L2 norm has not been introduced in this context, so the message of the authors is a bit unclear.',\n",
       " '-I do not find the notation \\\\gamma(\\\\phi,H) appropriate, I woud rather suggest to use \\\\gamma(H\\\\cdot \\\\phi)  -The biblioggrgaphy can be improved by adding the right conferences/journals where the papers have been published in addition to the ArXiv reference.',\n",
       " 'This submission provides a new theoretical framework for domain adaptation.',\n",
       " 'In order to tackle the adaptability term in the classical domain adaptation theory, this submission proposes a new upper bound that enlarge the hypothesis space in the adaptability term.',\n",
       " 'A weighted version of this theory is also given.',\n",
       " 'Authors further support their conclusion by empirical results.',\n",
       " 'Pros: 1.',\n",
       " 'This submission studies an important problem in domain adaptation.',\n",
       " '2.',\n",
       " 'This submission proposes new theoretical insight about compression and adaptability.',\n",
       " '3.',\n",
       " 'The conclusions of this paper can be partially proved by the empirical results.',\n",
       " 'Cons: 1.',\n",
       " 'As the author says in their future work, the source constraint is too strong that need to control the feature unchanged across all source domain.',\n",
       " 'For this condition is not build on samples but on the support of source domain.',\n",
       " 'It seems that authors use $L_0$ to constrain \\\\phi’ to have same value with \\\\phi on source dataset, which may be only a small part with zero measure of source support set.',\n",
       " '2.',\n",
       " 'There is no generalization error analysis for these upper bounds.',\n",
       " 'This submission provides weighted version of the main theory in the section 5.',\n",
       " 'It seems that weighted version of upper bound could be further minimized by find a good weight.',\n",
       " 'But add weight will add variance in the complexity term [A].',\n",
       " '3.',\n",
       " 'This submission adds \\\\beta term to change the adaptability term of $\\\\tilde{\\\\mathcal{H}}$ into the adaptability term of $\\\\mathcal{H}_0$.',\n",
       " 'The reason why \\\\beta can be estimated from finite sample is not clarified, which is the premise of being trainable and should be mainly discussed in this paper.',\n",
       " 'We can see that to estimate \\\\beta is a domain adaptation problem under the fact that the labeled functions are same.',\n",
       " '\\\\beta is a term that can’t be computed from small finite samples if there is no more assumption: It is not easy to approximate $\\\\tilde{f}_s$ uniformly, otherwise the estimation of \\\\beta will suffer from distribution shift.',\n",
       " 'This submission claimed that the term can be trainable by giving Proposition 2, a proof of the consistency.',\n",
       " 'However, this proposition is built on the assumption that there exists a series of \\\\phi minimizing the distribution distance to zero.',\n",
       " 'But this is impossible for finite sample estimation, when there will always be generalization error of estimating the distribution distance.',\n",
       " 'Furthermore, it is usually impossible to make the two embedded distributions completely the same in empirical.',\n",
       " 'In addition, if there is a series of \\\\phi, how to control other terms in the upper bound?',\n",
       " 'Every \\\\phi will induce new $\\\\tilde{\\\\mathcal{H}}$ and $\\\\mathcal{H}_0$ which will change all other terms.',\n",
       " 'In summary, the main theory in this submission changes the unknown adaptability to a new term that is very hard to estimate.',\n",
       " 'And there is no sufficient empirical or theoretical evidence in this paper that could support the fact that \\\\beta is small.',\n",
       " 'The contribution is therefore limited.',\n",
       " '4.',\n",
       " 'The theory also fails to give upper bounds on compression term and adaptability term, or some explicit upper bounds for certain hypothesis spaces as examples.',\n",
       " 'Readers could not have a clear image of how large will these terms be.',\n",
       " 'Furthermore, if the support sets of source and target domain coincide a lot, the adaptability of $\\\\mathcal{H}^s$ will not be too smaller than the previous one.',\n",
       " '5.',\n",
       " 'The organization of the submission makes it hard to read: a)\\tThe symbol of this submission is chaotic.',\n",
       " 'For example, $\\\\tilde{\\\\mathcal{H}}$, $\\\\tilde{\\\\mathcal{H}}^s$ ,$\\\\tilde{\\\\mathcal{H}}_h$ are defined based on $\\\\phi$, $\\\\pi(h)$ is defined based on $\\\\tilde{\\\\mathcal{H}}$, but all these facts are not revealed in their symbols.',\n",
       " 'b)\\tFor clarity, all loss functions defined in Section 4 should be stated in a independent line.',\n",
       " 'The Section 5 should be moved to the front of Experiment part.',\n",
       " 'c)\\tI recommend the authors to restate all new defined symbols as a list on the top of appendix.',\n",
       " 'It really troubles me during checking the proof.',\n",
       " 'I think this submission discusses about an important problem and provides new insight, but it is not a thorough theoretical work because of above reasons.',\n",
       " 'So, I vote for rejecting this paper.',\n",
       " '[A] Cortes, Corinna, Yishay Mansour, and Mehryar Mohri.',\n",
       " '\"Learning bounds for importance weighting.\"',\n",
       " 'Advances in neural information processing systems.',\n",
       " '2010.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_dict[\"2020_B1xGxgSYvH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"baseline\", \"compar\", \"et al\", \"SOTA\", \"state of the art\", \"state-of-the-art\", \"underperform\", \"outperform\"]\n",
    "\n",
    "kw_poor = [\"novel\", \"evaluat\", \"benchmark\", \"contribution\", \"contrast\", \"method\", \"result\", \"significan\", \n",
    "           \"approach\", \"performance\", \"technique\", \"report\", \"experiment\", \"propose\", \"model\", \"discuss\", \n",
    "           \"problem\", \"task\", \"metric\", \"score\", \"publication\", \"analyze\", \"analyse\", \"analysis\", \n",
    "           \"replicate\", \"submission\"]\n",
    "\n",
    "# &, et al, [1,2][7-9,0-1][0-9][0-9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2017, 2018, 2019, 2020]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing year:  2017\n",
      "Changing year:  2018\n"
     ]
    }
   ],
   "source": [
    "sents_list = []\n",
    "stop = False\n",
    "\n",
    "for y in years[:-1]:\n",
    "    if stop:\n",
    "        break\n",
    "    for k in rev_dict[y]:\n",
    "        \n",
    "        if len(sents_list) > 800*(y-2016) and y != 2019:\n",
    "            change_year = True\n",
    "            print(\"Changing year: \", y)\n",
    "            break\n",
    "        if len(sents_list) > 5000:\n",
    "            stop = True\n",
    "            break\n",
    "            \n",
    "        year_key = str(y) + \"_\" + k\n",
    "        if year_key in iclr_arxiv_map:\n",
    "            for rev_num in rev_dict[y][k]:\n",
    "                rev_text = rev_num[\"content\"][\"review\"]\n",
    "                \n",
    "                rev_text = re.sub(\" e[\\.]?g[\\.]?:? \", \" eg: \", rev_text)\n",
    "                rev_text = re.sub(\" et[\\.]? al[\\.]\", \" et al\", rev_text)\n",
    "                rev_text = re.sub(\" i[\\.]?e[\\.] \", \" ie \", rev_text)\n",
    "                rev_text = re.sub(\"\\\\n\", \" \", rev_text)\n",
    "                \n",
    "                sent_text = nltk.sent_tokenize(rev_text)\n",
    "                \n",
    "                for s in sent_text:\n",
    "                    for kw in keywords:\n",
    "                        if s.find(kw) > -1:\n",
    "                            sents_list.append((k, s))\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sents_list, columns=['pid', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BydrOIcle</td>\n",
       "      <td>So, this comparison might just be showing that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>Several points are appealing about this approa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pid                                           sentence\n",
       "0  BydrOIcle  So, this comparison might just be showing that...\n",
       "1  SyOvg6jxx  Several points are appealing about this approa..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BydrOIcle</td>\n",
       "      <td>So, this comparison might just be showing that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>Several points are appealing about this approa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>In addition, there are results for comparison ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>The results indicate that the approach clearly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>It seems like the technique could be easily us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>The paper addresses an important problem (expl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>It is a nice alternative approach to the one o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>Specifically, I am not as concerned about beat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>The figure S9 from Mnih et al points to instan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>Without \"feature engineering\", the authors ach...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pid                                           sentence\n",
       "0  BydrOIcle  So, this comparison might just be showing that...\n",
       "1  SyOvg6jxx  Several points are appealing about this approa...\n",
       "2  SyOvg6jxx  In addition, there are results for comparison ...\n",
       "3  SyOvg6jxx  The results indicate that the approach clearly...\n",
       "4  SyOvg6jxx  It seems like the technique could be easily us...\n",
       "5  SyOvg6jxx  The paper addresses an important problem (expl...\n",
       "6  SyOvg6jxx  It is a nice alternative approach to the one o...\n",
       "7  SyOvg6jxx  Specifically, I am not as concerned about beat...\n",
       "8  SyOvg6jxx  The figure S9 from Mnih et al points to instan...\n",
       "9  SyOvg6jxx  Without \"feature engineering\", the authors ach..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"ann_comparison_only.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%nautilus` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
