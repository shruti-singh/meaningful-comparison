{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/shruti/Desktop/iitgn/courses/SEM2/ML/Project/code/PaperAcceptancePrediction/ICLR data/masterdata_unbalanced/\"\n",
    "\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "rev_dict = {}\n",
    "paper_dict = {}\n",
    "dec_dict = {}\n",
    "iclr_arxiv_map = {}\n",
    "\n",
    "for y in years:\n",
    "    rev_dict[y] = pd.read_pickle(data_path + \"off_rev_dict_{}.pkl\".format(y))\n",
    "    paper_dict[y] = pd.read_pickle(data_path + \"papers_{}.pkl\".format(y))\n",
    "    dec_dict[y] = pd.read_pickle(data_path + \"paper_decision_dict_{}.pkl\".format(y))\n",
    "\n",
    "iclr_arxiv_map = pd.read_pickle(\"./data/iclr_arxiv_map.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were thinking about this problem for EACL:\n",
    "    \n",
    "Given a review text, can we identify the text span that talks about the comparisons.\n",
    "After identification, can we identify different aspects associated with it:\n",
    "1. Is it positive or negative?\n",
    "2. What module does it talks about (dataset, model, metric, etc.)\n",
    "3. Does it suggests some papers to cite?\n",
    "4. We can also suggest the overlap between different reviews in terms of comparison comments. \n",
    "This can help us understanding how much reviews are coherent\n",
    "\n",
    "Motivation: The system would be more useful for metareviewers. Authors can also be benefited, but they \n",
    "can always read the review and get this info.  We can always say that with the increase in exponential \n",
    "growth in conference submissions, the evaluation task for meta-reviewers is becoming more and more hard. \n",
    "The current system would help the metareviews task in quickly glancing the multiple reviews for a given paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the initial set of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/initialsetmcomp.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117,          pid                                               sent  mcomp\n",
       " 0  S1HcOI5le  The idea of the paper is interesting there are...      1\n",
       " 1  S1HcOI5le  It's not clear how this method compares agains...      1\n",
       " 2  S1HcOI5le  Measure: Accuracy difference does not look lik...      1\n",
       " 3  S1HcOI5le  Instead the authors could position this work a...      1\n",
       " 4  S1HcOI5le  If the authors care to compare their approach ...      1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size, df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use above to find sentences from 2020 reviews for 10 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_dict = defaultdict(list)\n",
    "stop = 1\n",
    "\n",
    "for y in [2020]:\n",
    "    for k in rev_dict[y]:\n",
    "        \n",
    "        if stop > 100:\n",
    "            break\n",
    "        stop += 1 \n",
    "            \n",
    "        year_key = str(y) + \"_\" + k\n",
    "        if year_key in iclr_arxiv_map:\n",
    "            for rev_num in rev_dict[y][k]:\n",
    "                rev_text = rev_num[\"content\"][\"review\"]\n",
    "                \n",
    "                rev_text = re.sub(\" e[\\.]?g[\\.]?:? \", \" eg: \", rev_text)\n",
    "                rev_text = re.sub(\" et[\\.]? al[\\.]\", \" et al\", rev_text)\n",
    "                rev_text = re.sub(\" i[\\.]?e[\\.] \", \" ie \", rev_text)\n",
    "                rev_text = re.sub(\"\\\\n\", \" \", rev_text)\n",
    "                \n",
    "                sent_text = nltk.sent_tokenize(rev_text)\n",
    "                \n",
    "                for s in sent_text:\n",
    "                    sents_dict[year_key].append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/2020_test_sents.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sents_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.Module(\"/home/shruti/Desktop/research/meaningful_comparison/use_models/universal-sentence-encoder-lite_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"I am a sentence for which I would like to get its embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Signature expects multiple inputs. Use a dict.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ec819c923b30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmessage_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/research/venv/lib/python3.5/site-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, _sentinel, signature, as_dict)\u001b[0m\n\u001b[1;32m    248\u001b[0m     dict_inputs = _convert_dict_inputs(\n\u001b[1;32m    249\u001b[0m         inputs, self._spec.get_input_info_dict(signature=signature,\n\u001b[0;32m--> 250\u001b[0;31m                                                tags=self._tags))\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     dict_outputs = self._impl.create_apply_graph(\n",
      "\u001b[0;32m~/Desktop/research/venv/lib/python3.5/site-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m_convert_dict_inputs\u001b[0;34m(inputs, tensor_info_map)\u001b[0m\n\u001b[1;32m    448\u001b[0m       \u001b[0mto\u001b[0m \u001b[0mfeed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msignature\u001b[0m \u001b[0minstantiation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m   \"\"\"\n\u001b[0;32m--> 450\u001b[0;31m   \u001b[0mdict_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_dict_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_info_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m   return tensor_info.convert_dict_to_compatible_tensor(dict_inputs,\n\u001b[1;32m    452\u001b[0m                                                        tensor_info_map)\n",
      "\u001b[0;32m~/Desktop/research/venv/lib/python3.5/site-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m_prepare_dict_inputs\u001b[0;34m(inputs, tensor_info_map)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Signature expects no inputs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Signature expects multiple inputs. Use a dict.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m   \u001b[0mdict_inputs_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Signature expects multiple inputs. Use a dict."
     ]
    }
   ],
   "source": [
    "# Reduce logging output.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings = session.run(embed(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in sents_dict.items():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"baseline\", \"compar\", \"et al\", \"SOTA\", \"state of the art\", \"state-of-the-art\", \"underperform\", \"outperform\"]\n",
    "\n",
    "kw_poor = [\"novel\", \"evaluat\", \"benchmark\", \"contribution\", \"contrast\", \"method\", \"result\", \"significan\", \n",
    "           \"approach\", \"performance\", \"technique\", \"report\", \"experiment\", \"propose\", \"model\", \"discuss\", \n",
    "           \"problem\", \"task\", \"metric\", \"score\", \"publication\", \"analyze\", \"analyse\", \"analysis\", \n",
    "           \"replicate\", \"submission\"]\n",
    "\n",
    "# &, et al, [1,2][7-9,0-1][0-9][0-9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2017, 2018, 2019, 2020]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing year:  2017\n",
      "Changing year:  2018\n"
     ]
    }
   ],
   "source": [
    "sents_list = []\n",
    "stop = False\n",
    "\n",
    "for y in years[:-1]:\n",
    "    if stop:\n",
    "        break\n",
    "    for k in rev_dict[y]:\n",
    "        \n",
    "        if len(sents_list) > 800*(y-2016) and y != 2019:\n",
    "            change_year = True\n",
    "            print(\"Changing year: \", y)\n",
    "            break\n",
    "        if len(sents_list) > 5000:\n",
    "            stop = True\n",
    "            break\n",
    "            \n",
    "        year_key = str(y) + \"_\" + k\n",
    "        if year_key in iclr_arxiv_map:\n",
    "            for rev_num in rev_dict[y][k]:\n",
    "                rev_text = rev_num[\"content\"][\"review\"]\n",
    "                \n",
    "                rev_text = re.sub(\" e[\\.]?g[\\.]?:? \", \" eg: \", rev_text)\n",
    "                rev_text = re.sub(\" et[\\.]? al[\\.]\", \" et al\", rev_text)\n",
    "                rev_text = re.sub(\" i[\\.]?e[\\.] \", \" ie \", rev_text)\n",
    "                rev_text = re.sub(\"\\\\n\", \" \", rev_text)\n",
    "                \n",
    "                sent_text = nltk.sent_tokenize(rev_text)\n",
    "                \n",
    "                for s in sent_text:\n",
    "                    for kw in keywords:\n",
    "                        if s.find(kw) > -1:\n",
    "                            sents_list.append((k, s))\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sents_list, columns=['pid', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BydrOIcle</td>\n",
       "      <td>So, this comparison might just be showing that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>Several points are appealing about this approa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pid                                           sentence\n",
       "0  BydrOIcle  So, this comparison might just be showing that...\n",
       "1  SyOvg6jxx  Several points are appealing about this approa..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BydrOIcle</td>\n",
       "      <td>So, this comparison might just be showing that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>Several points are appealing about this approa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>In addition, there are results for comparison ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>The results indicate that the approach clearly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>It seems like the technique could be easily us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>The paper addresses an important problem (expl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>It is a nice alternative approach to the one o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>Specifically, I am not as concerned about beat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>The figure S9 from Mnih et al points to instan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SyOvg6jxx</td>\n",
       "      <td>Without \"feature engineering\", the authors ach...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pid                                           sentence\n",
       "0  BydrOIcle  So, this comparison might just be showing that...\n",
       "1  SyOvg6jxx  Several points are appealing about this approa...\n",
       "2  SyOvg6jxx  In addition, there are results for comparison ...\n",
       "3  SyOvg6jxx  The results indicate that the approach clearly...\n",
       "4  SyOvg6jxx  It seems like the technique could be easily us...\n",
       "5  SyOvg6jxx  The paper addresses an important problem (expl...\n",
       "6  SyOvg6jxx  It is a nice alternative approach to the one o...\n",
       "7  SyOvg6jxx  Specifically, I am not as concerned about beat...\n",
       "8  SyOvg6jxx  The figure S9 from Mnih et al points to instan...\n",
       "9  SyOvg6jxx  Without \"feature engineering\", the authors ach..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"ann_comparison_only.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%nautilus` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
